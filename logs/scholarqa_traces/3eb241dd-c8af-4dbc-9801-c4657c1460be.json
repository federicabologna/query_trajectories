{
    "query": "Data selection as important problem training good language models which been research the ordering datapoints for LMs easy to hard curriculum sequence of data points such LM a sample efficient manner",
    "user_id": "lib_user",
    "task_id": "3eb241dd-c8af-4dbc-9801-c4657c1460be",
    "timestamp": "2025-06-24T00:31:22.056853",
    "n_retrieval": 256,
    "n_retrieved": 265,
    "n_candidates": 38,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.516936,
    "decomposed_query": {
        "rewritten_query": "Data selection and ordering datapoints from easy to hard curriculum for training language models in a sample efficient manner.",
        "keyword_query": "data selection ordering datapoints easy hard curriculum language models sample efficient",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009738,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Efficient Pretraining Data Selection for Language Models via Multi-Actor Collaboration",
            "venue": "",
            "year": 2024,
            "reference_count": 54,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.08102, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2318978696",
                    "name": "Tianyi Bai"
                },
                {
                    "authorId": "2302788310",
                    "name": "Ling Yang"
                },
                {
                    "authorId": "2325175258",
                    "name": "Zhen Hao Wong"
                },
                {
                    "authorId": "2367277175",
                    "name": "Fupeng Sun"
                },
                {
                    "authorId": "2233445161",
                    "name": "Jiahui Peng"
                },
                {
                    "authorId": "2366068443",
                    "name": "Xinlin Zhuang"
                },
                {
                    "authorId": "2325489246",
                    "name": "Chi Zhang"
                },
                {
                    "authorId": "2325195689",
                    "name": "Lijun Wu"
                },
                {
                    "authorId": "2289911484",
                    "name": "Jiantao Qiu"
                },
                {
                    "authorId": "2302813081",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "2303407552",
                    "name": "Binhang Yuan"
                },
                {
                    "authorId": "2291040348",
                    "name": "Conghui He"
                }
            ],
            "abstract": "Efficient data selection is crucial to accelerate the pretraining of language model (LMs). While various methods have been proposed to enhance data efficiency, limited research has addressed the inherent conflicts between these approaches to achieve optimal data selection for LM pretraining. To tackle this problem, we propose a multi-actor collaborative data selection mechanism: each data selection method independently prioritizes data based on its criterion and updates its prioritization rules using the current state of the model, functioning as an independent actor for data selection; and a console is designed to adjust the impacts of different actors at various stages and dynamically integrate information from all actors throughout the LM pretraining process. We conduct extensive empirical studies to evaluate our multi-actor framework. The experimental results demonstrate that our approach significantly improves data efficiency, accelerates convergence in LM pretraining, and achieves an average relative performance gain up to $10.5\\%$ across multiple language model benchmarks compared to the state-of-the-art methods.",
            "corpus_id": 273233719,
            "sentences": [
                {
                    "corpus_id": "273233719",
                    "title": "Efficient Pretraining Data Selection for Language Models via Multi-Actor Collaboration",
                    "text": "Efficient data selection is crucial for the pretraining of large language models (LLMs), as the quality of training data significantly impacts the statistical efficiency of the training procedure and the model performance (Brown, 2020;Du et al., 2022;Chowdhery et al., 2023). Recently, we have witnessed numerous approaches, such as filtering high-quality data (Xie et al., 2023b;Wettig et al., 2024), mixing data from multiple domains (Xie et al., 2023a;Liu et al., 2024), and selecting data that optimally boosts downstream task performance dynamically (Engstrom et al., 2024;Yu et al., 2024), which aim to improve data efficiency by prioritizing more informative training samples. However, these methods often operate independently or in isolated settings, limiting their potential when integrated into a collaborative framework. In this work, we want to explore how to effectively, flexibly, and robustly combine these advanced data selection techniques through the dynamic pretraining process, addressing the challenges of optimizing data efficiency for LLM pretraining at scale. Nowadays, various heuristic methods have been proposed to provide measurements for the data samples used during LLM pre-training, aiming to optimize data efficiency by selecting or weighting the most informative training examples. However, we observe that integrating multiple data selection and mixing strategies presents significant challenges due to their inherent conflicts. For example, high-quality data identified by scoring functions may not align with data that strongly impact model performance as measured by influence functions (Engstrom et al., 2024); similar conflicts also exists between other methods -further details are enumerated in \u00a72. These observations actually motivate us to launch a systematic discussion about how to effectively integrate these methods during the dynamic pretraining process that provides superior data efficiency for LLM pretraining. \n\nOn the other hand, effectively integrating these data selection methods into a single framework is much harder to implement than to ask for. In fact, one may have to explore an exponential space to find the optimal combination for different data sampling schemas. Such a heavy burden will be Figure 1: Statistics of the SlimPajama dataset.",
                    "score": 0.42151116072472744,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 275
                        },
                        {
                            "start": 276,
                            "end": 683
                        },
                        {
                            "start": 684,
                            "end": 832
                        },
                        {
                            "start": 833,
                            "end": 1084
                        },
                        {
                            "start": 1085,
                            "end": 1315
                        },
                        {
                            "start": 1316,
                            "end": 1463
                        },
                        {
                            "start": 1464,
                            "end": 1740
                        },
                        {
                            "start": 1741,
                            "end": 1962
                        },
                        {
                            "start": 1965,
                            "end": 2105
                        },
                        {
                            "start": 2106,
                            "end": 2228
                        },
                        {
                            "start": 2229,
                            "end": 2304
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 235,
                            "end": 251,
                            "matchedPaperCorpusId": "245124124"
                        },
                        {
                            "start": 251,
                            "end": 274,
                            "matchedPaperCorpusId": "247951931"
                        },
                        {
                            "start": 361,
                            "end": 380,
                            "matchedPaperCorpusId": "256627727"
                        },
                        {
                            "start": 380,
                            "end": 400,
                            "matchedPaperCorpusId": "267681974"
                        },
                        {
                            "start": 436,
                            "end": 455,
                            "matchedPaperCorpusId": "258741043"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96337890625
                }
            ],
            "relevance_judgement": 0.96337890625,
            "relevance_judgment_input_expanded": "# Title: Efficient Pretraining Data Selection for Language Models via Multi-Actor Collaboration\n# Venue: \n# Authors: Tianyi Bai, Ling Yang, Zhen Hao Wong, Fupeng Sun, Jiahui Peng, Xinlin Zhuang, Chi Zhang, Lijun Wu, Jiantao Qiu, Wentao Zhang, Binhang Yuan, Conghui He\n## Abstract\nEfficient data selection is crucial to accelerate the pretraining of language model (LMs). While various methods have been proposed to enhance data efficiency, limited research has addressed the inherent conflicts between these approaches to achieve optimal data selection for LM pretraining. To tackle this problem, we propose a multi-actor collaborative data selection mechanism: each data selection method independently prioritizes data based on its criterion and updates its prioritization rules using the current state of the model, functioning as an independent actor for data selection; and a console is designed to adjust the impacts of different actors at various stages and dynamically integrate information from all actors throughout the LM pretraining process. We conduct extensive empirical studies to evaluate our multi-actor framework. The experimental results demonstrate that our approach significantly improves data efficiency, accelerates convergence in LM pretraining, and achieves an average relative performance gain up to $10.5\\%$ across multiple language model benchmarks compared to the state-of-the-art methods.\n## INTRODUCTION\nEfficient data selection is crucial for the pretraining of large language models (LLMs), as the quality of training data significantly impacts the statistical efficiency of the training procedure and the model performance (Brown, 2020;Du et al., 2022;Chowdhery et al., 2023). Recently, we have witnessed numerous approaches, such as filtering high-quality data (Xie et al., 2023b;Wettig et al., 2024), mixing data from multiple domains (Xie et al., 2023a;Liu et al., 2024), and selecting data that optimally boosts downstream task performance dynamically (Engstrom et al., 2024;Yu et al., 2024), which aim to improve data efficiency by prioritizing more informative training samples. However, these methods often operate independently or in isolated settings, limiting their potential when integrated into a collaborative framework. In this work, we want to explore how to effectively, flexibly, and robustly combine these advanced data selection techniques through the dynamic pretraining process, addressing the challenges of optimizing data efficiency for LLM pretraining at scale. Nowadays, various heuristic methods have been proposed to provide measurements for the data samples used during LLM pre-training, aiming to optimize data efficiency by selecting or weighting the most informative training examples. However, we observe that integrating multiple data selection and mixing strategies presents significant challenges due to their inherent conflicts. For example, high-quality data identified by scoring functions may not align with data that strongly impact model performance as measured by influence functions (Engstrom et al., 2024); similar conflicts also exists between other methods -further details are enumerated in \u00a72. These observations actually motivate us to launch a systematic discussion about how to effectively integrate these methods during the dynamic pretraining process that provides superior data efficiency for LLM pretraining. \n\nOn the other hand, effectively integrating these data selection methods into a single framework is much harder to implement than to ask for. In fact, one may have to explore an exponential space to find the optimal combination for different data sampling schemas. Such a heavy burden will be Figure 1: Statistics of the SlimPajama dataset.",
            "reference_string": "[273233719 | Bai et al. | 2024 | Citations: 6]"
        },
        {
            "title": "Efficient Alignment of Large Language Models via Data Sampling",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 25,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.10545, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "21302492",
                    "name": "Amrit Khera"
                },
                {
                    "authorId": "2213553962",
                    "name": "Rajat Ghosh"
                },
                {
                    "authorId": "2267726934",
                    "name": "Debojyoti Dutta"
                }
            ],
            "abstract": "LLM alignment ensures that large language models behave safely and effectively by aligning their outputs with human values, goals, and intentions. Aligning LLMs employ huge amounts of data, computation, and time. Moreover, curating data with human feedback is expensive and takes time. Recent research depicts the benefit of data engineering in the fine-tuning and pre-training paradigms to bring down such costs. However, alignment differs from the afore-mentioned paradigms and it is unclear if data efficient alignment is feasible. In this work, we first aim to understand how the performance of LLM alignment scales with data. We find out that LLM alignment performance follows an exponential plateau pattern which tapers off post a rapid initial increase. Based on this, we identify data subsampling as a viable method to reduce resources required for alignment. Further, we propose an information theory-based methodology for efficient alignment by identifying a small high quality subset thereby reducing the computation and time required by alignment. We evaluate the proposed methodology over multiple datasets and compare the results. We find that the model aligned using our proposed methodology outperforms other sampling methods and performs comparable to the model aligned with the full dataset while using less than 10% data, leading to greater than 90% savings in costs, resources, and faster LLM alignment.",
            "corpus_id": 274130626,
            "sentences": [
                {
                    "corpus_id": "274130626",
                    "title": "Efficient Alignment of Large Language Models via Data Sampling",
                    "text": "Data selection is a well known problem in literature with many algorithms such as filtering, coresets, importance sampling and more working towards the same goal [22]. Here, we present data selection in the domain of LLMs and natural language and how they can enable data efficient training. \n\nLIMA, proposed by [30] employs a small high quality dataset for fine-tuning. They show that by carefully curating only a 1000 data points, they are able to achieve remarkable performance which is generalizable to unseen data as well, thereby suggesting limited high quality tuning data is sufficient. \n\nAnother prominent study by [16] demonstrates that sample quality can reduce the data requirement without compromising the downstream performance. They investigate data engineering strategies in the fine-tuning paradigm from multiple facets to identify the characterises of good instruction tuning data. DEITA, the model family tuned by their proposed strategy to automatically select a complex and high quality dataset achieves comparable performance to open source models while using only a tenth of the data. \n\nThe study by [22], aims to explore data selection strategies for efficient pre-training of LLMs. They investigate methods targeting the coverage and quality of the sampled dataset and propose Density and Ask-LLM sampling focusing on coverage and quality respectively. They carry out extensive evaluation against other samplers and find their methods to be the best in their categories. The models trained on their sampled data achieves comparable or improved performance to the models trained with the full dataset while converging 70% faster. \n\nHence, while there has been many studies focusing on data efficient fine-tuning and pre-tuning, to the best of our knowledge such strategies have not been studied in the alignment paradigm. In this study, we explore how LLM alignment scales with data and identify data engineering and sampling strategies as viable methods to enable efficient alignment. Further, we propose a novel strategy based on information theory to sample a diverse and high quality subsample outperforming other sampling strategies for efficient alignment.",
                    "score": 0.40178926447992963,
                    "section_title": "A.2 Related Works",
                    "char_start_offset": 14183,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 167
                        },
                        {
                            "start": 168,
                            "end": 291
                        },
                        {
                            "start": 294,
                            "end": 370
                        },
                        {
                            "start": 371,
                            "end": 594
                        },
                        {
                            "start": 597,
                            "end": 742
                        },
                        {
                            "start": 743,
                            "end": 899
                        },
                        {
                            "start": 900,
                            "end": 1107
                        },
                        {
                            "start": 1110,
                            "end": 1206
                        },
                        {
                            "start": 1207,
                            "end": 1377
                        },
                        {
                            "start": 1378,
                            "end": 1495
                        },
                        {
                            "start": 1496,
                            "end": 1653
                        },
                        {
                            "start": 1656,
                            "end": 1845
                        },
                        {
                            "start": 1846,
                            "end": 2009
                        },
                        {
                            "start": 2010,
                            "end": 2186
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96337890625
                }
            ],
            "relevance_judgement": 0.96337890625,
            "relevance_judgment_input_expanded": "# Title: Efficient Alignment of Large Language Models via Data Sampling\n# Venue: arXiv.org\n# Authors: Amrit Khera, Rajat Ghosh, Debojyoti Dutta\n## Abstract\nLLM alignment ensures that large language models behave safely and effectively by aligning their outputs with human values, goals, and intentions. Aligning LLMs employ huge amounts of data, computation, and time. Moreover, curating data with human feedback is expensive and takes time. Recent research depicts the benefit of data engineering in the fine-tuning and pre-training paradigms to bring down such costs. However, alignment differs from the afore-mentioned paradigms and it is unclear if data efficient alignment is feasible. In this work, we first aim to understand how the performance of LLM alignment scales with data. We find out that LLM alignment performance follows an exponential plateau pattern which tapers off post a rapid initial increase. Based on this, we identify data subsampling as a viable method to reduce resources required for alignment. Further, we propose an information theory-based methodology for efficient alignment by identifying a small high quality subset thereby reducing the computation and time required by alignment. We evaluate the proposed methodology over multiple datasets and compare the results. We find that the model aligned using our proposed methodology outperforms other sampling methods and performs comparable to the model aligned with the full dataset while using less than 10% data, leading to greater than 90% savings in costs, resources, and faster LLM alignment.\n## A.2 Related Works\nData selection is a well known problem in literature with many algorithms such as filtering, coresets, importance sampling and more working towards the same goal [22]. Here, we present data selection in the domain of LLMs and natural language and how they can enable data efficient training. \n\nLIMA, proposed by [30] employs a small high quality dataset for fine-tuning. They show that by carefully curating only a 1000 data points, they are able to achieve remarkable performance which is generalizable to unseen data as well, thereby suggesting limited high quality tuning data is sufficient. \n\nAnother prominent study by [16] demonstrates that sample quality can reduce the data requirement without compromising the downstream performance. They investigate data engineering strategies in the fine-tuning paradigm from multiple facets to identify the characterises of good instruction tuning data. DEITA, the model family tuned by their proposed strategy to automatically select a complex and high quality dataset achieves comparable performance to open source models while using only a tenth of the data. \n\nThe study by [22], aims to explore data selection strategies for efficient pre-training of LLMs. They investigate methods targeting the coverage and quality of the sampled dataset and propose Density and Ask-LLM sampling focusing on coverage and quality respectively. They carry out extensive evaluation against other samplers and find their methods to be the best in their categories. The models trained on their sampled data achieves comparable or improved performance to the models trained with the full dataset while converging 70% faster. \n\nHence, while there has been many studies focusing on data efficient fine-tuning and pre-tuning, to the best of our knowledge such strategies have not been studied in the alignment paradigm. In this study, we explore how LLM alignment scales with data and identify data engineering and sampling strategies as viable methods to enable efficient alignment. Further, we propose a novel strategy based on information theory to sample a diverse and high quality subsample outperforming other sampling strategies for efficient alignment.",
            "reference_string": "[274130626 | Khera et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 44,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.14115, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2243875495",
                    "name": "Ziche Liu"
                },
                {
                    "authorId": "2307471917",
                    "name": "Rui Ke"
                },
                {
                    "authorId": "2303951713",
                    "name": "Feng Jiang"
                },
                {
                    "authorId": "2218230700",
                    "name": "Haizhou Li"
                }
            ],
            "abstract": "Data selection for fine-tuning large language models (LLMs) aims to choose a high-quality subset from existing datasets, allowing the trained model to outperform baselines trained on the full dataset. However, the expanding body of research lacks a clear, unified framework, and the variability in experimental settings complicates systematic comparisons. While existing surveys comprehensively overview the stages and methods of data selection, they often overlook an in-depth exploration of the fine-tuning phase. In this paper, we conduct a focused review of recent data selection techniques for fine-tuning LLMs, analyzing a dozen key studies. We introduce a novel three-stage scheme - comprising feature extraction, criteria design, and selector evaluation - to systematically categorize and evaluate these methods. Additionally, we propose a unified comparison approach that incorporates ratio-based efficiency and ranking-based feasibility metrics to address inconsistencies across experiments. Our findings reveal that methods emphasizing more targeted quality measurement achieve higher efficiency but at the cost of feasibility. Finally, we discuss trends and highlight four key challenges in fine-tuning data selection, offering potential directions for future research.",
            "corpus_id": 270620248,
            "sentences": [
                {
                    "corpus_id": "270620248",
                    "title": "Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models",
                    "text": "Supervised fine-tuning (SFT) leverages small amounts of instruction-pair data to unlock large language models' instruction-following capabilities and improve generalization across various tasks (Radford et al., 2019;Wei et al., 2022;Singh et al., 2023;Zhang et al., 2023;Albalak et al., 2024). Recent research highlights that data quality is more critical than data quantity for effective fine-tuning (Nakkiran et al., 2020;Shumailov et al., 2024;Zhou et al., 2024;Jindal et al., 2024). As a result, several data curation techniques have been proposed, such as data selection (Chen et al., 2024;Li et al., 2024b), data evolution (Wang et al., 2023;Xu et al., 2023), and data reflection (Mukherjee et al., 2023;Yin et al., 2023). Data selection, in particular, involves choosing a high-quality subset from a candidate dataset based on specific selection criteria, enhancing the model's performance while improving training efficiency by reducing the number of samples. Unlike data augmentation or polishing, it focuses on selecting inherently higherquality samples, as shown in Figure 1. However, despite the rapid development of data selection methods, there is currently no unified framework for systematically guiding and comparing these methods, as experimental settings vary widely across studies. Although some surveys (Bommasani et al., 2021;Albalak et al., 2024;Wang et al., 2024) have reviewed data curation techniques in the contexts of pretraining, fine-tuning, and reinforcement learning, they generally provide high-level overviews and lack in-depth discussions  of the fine-tuning stage. This gap makes it difficult for researchers to conduct focused, sustained studies on this crucial phase. To address this, our survey provides a fine-grained review of data selection methods for instruction fine-tuning LLMs, rethinking existing approaches, proposing a unified comparison method, and outlining key trends and challenges in the field.",
                    "score": 0.4396506628741437,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 293
                        },
                        {
                            "start": 294,
                            "end": 486
                        },
                        {
                            "start": 487,
                            "end": 728
                        },
                        {
                            "start": 729,
                            "end": 967
                        },
                        {
                            "start": 968,
                            "end": 1086
                        },
                        {
                            "start": 1087,
                            "end": 1301
                        },
                        {
                            "start": 1302,
                            "end": 1600
                        },
                        {
                            "start": 1601,
                            "end": 1705
                        },
                        {
                            "start": 1706,
                            "end": 1949
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 216,
                            "end": 233,
                            "matchedPaperCorpusId": "237416585"
                        },
                        {
                            "start": 233,
                            "end": 252,
                            "matchedPaperCorpusId": "266163375"
                        },
                        {
                            "start": 271,
                            "end": 292,
                            "matchedPaperCorpusId": "268032975"
                        },
                        {
                            "start": 401,
                            "end": 424,
                            "matchedPaperCorpusId": "207808916"
                        },
                        {
                            "start": 424,
                            "end": 447,
                            "matchedPaperCorpusId": "271448069"
                        },
                        {
                            "start": 447,
                            "end": 465,
                            "matchedPaperCorpusId": "258822910"
                        },
                        {
                            "start": 576,
                            "end": 595,
                            "matchedPaperCorpusId": "259937133"
                        },
                        {
                            "start": 595,
                            "end": 612,
                            "matchedPaperCorpusId": "261076515"
                        },
                        {
                            "start": 629,
                            "end": 648,
                            "matchedPaperCorpusId": "254877310"
                        },
                        {
                            "start": 686,
                            "end": 710,
                            "matchedPaperCorpusId": "259075316"
                        },
                        {
                            "start": 710,
                            "end": 727,
                            "matchedPaperCorpusId": "258841263"
                        },
                        {
                            "start": 1324,
                            "end": 1348,
                            "matchedPaperCorpusId": "237091588"
                        },
                        {
                            "start": 1348,
                            "end": 1369,
                            "matchedPaperCorpusId": "268032975"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9599609375
                }
            ],
            "relevance_judgement": 0.9599609375,
            "relevance_judgment_input_expanded": "# Title: Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Ziche Liu, Rui Ke, Feng Jiang, Haizhou Li\n## Abstract\nData selection for fine-tuning large language models (LLMs) aims to choose a high-quality subset from existing datasets, allowing the trained model to outperform baselines trained on the full dataset. However, the expanding body of research lacks a clear, unified framework, and the variability in experimental settings complicates systematic comparisons. While existing surveys comprehensively overview the stages and methods of data selection, they often overlook an in-depth exploration of the fine-tuning phase. In this paper, we conduct a focused review of recent data selection techniques for fine-tuning LLMs, analyzing a dozen key studies. We introduce a novel three-stage scheme - comprising feature extraction, criteria design, and selector evaluation - to systematically categorize and evaluate these methods. Additionally, we propose a unified comparison approach that incorporates ratio-based efficiency and ranking-based feasibility metrics to address inconsistencies across experiments. Our findings reveal that methods emphasizing more targeted quality measurement achieve higher efficiency but at the cost of feasibility. Finally, we discuss trends and highlight four key challenges in fine-tuning data selection, offering potential directions for future research.\n## Introduction\nSupervised fine-tuning (SFT) leverages small amounts of instruction-pair data to unlock large language models' instruction-following capabilities and improve generalization across various tasks (Radford et al., 2019;Wei et al., 2022;Singh et al., 2023;Zhang et al., 2023;Albalak et al., 2024). Recent research highlights that data quality is more critical than data quantity for effective fine-tuning (Nakkiran et al., 2020;Shumailov et al., 2024;Zhou et al., 2024;Jindal et al., 2024). As a result, several data curation techniques have been proposed, such as data selection (Chen et al., 2024;Li et al., 2024b), data evolution (Wang et al., 2023;Xu et al., 2023), and data reflection (Mukherjee et al., 2023;Yin et al., 2023). Data selection, in particular, involves choosing a high-quality subset from a candidate dataset based on specific selection criteria, enhancing the model's performance while improving training efficiency by reducing the number of samples. Unlike data augmentation or polishing, it focuses on selecting inherently higherquality samples, as shown in Figure 1. However, despite the rapid development of data selection methods, there is currently no unified framework for systematically guiding and comparing these methods, as experimental settings vary widely across studies. Although some surveys (Bommasani et al., 2021;Albalak et al., 2024;Wang et al., 2024) have reviewed data curation techniques in the contexts of pretraining, fine-tuning, and reinforcement learning, they generally provide high-level overviews and lack in-depth discussions  of the fine-tuning stage. This gap makes it difficult for researchers to conduct focused, sustained studies on this crucial phase. To address this, our survey provides a fine-grained review of data selection methods for instruction fine-tuning LLMs, rethinking existing approaches, proposing a unified comparison method, and outlining key trends and challenges in the field.",
            "reference_string": "[270620248 | Liu et al. | 2024 | Citations: 2]"
        },
        {
            "title": "DsDm: Model-Aware Dataset Selection with Datamodels",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 96,
            "citation_count": 61,
            "influential_citation_count": 9,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.12926, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "39468283",
                    "name": "Logan Engstrom"
                },
                {
                    "authorId": "2280334462",
                    "name": "Axel Feldmann"
                },
                {
                    "authorId": "143826246",
                    "name": "A. Ma\u0327dry"
                }
            ],
            "abstract": "When selecting data for training large-scale models, standard practice is to filter for examples that match human notions of data quality. Such filtering yields qualitatively clean datapoints that intuitively should improve model behavior. However, in practice the opposite can often happen: we find that selecting according to similarity with\"high quality\"data sources may not increase (and can even hurt) performance compared to randomly selecting data. To develop better methods for selecting data, we start by framing dataset selection as an optimization problem that we can directly solve for: given target tasks, a learning algorithm, and candidate data, select the subset that maximizes model performance. This framework thus avoids handpicked notions of data quality, and instead models explicitly how the learning process uses train datapoints to predict on the target tasks. Our resulting method greatly improves language model (LM) performance on both pre-specified tasks and previously unseen tasks. Specifically, choosing target tasks representative of standard LM problems and evaluating on diverse held-out benchmarks, our selected datasets provide a 2x compute multiplier over baseline methods.",
            "corpus_id": 267094971,
            "sentences": [
                {
                    "corpus_id": "267094971",
                    "title": "DsDm: Model-Aware Dataset Selection with Datamodels",
                    "text": "When selecting data for training large-scale models, standard practice is to filter for examples that match human notions of data quality. Such filtering yields qualitatively clean datapoints that intuitively should improve model behavior. However, in practice the opposite can often happen: we find that selecting according to similarity with\"high quality\"data sources may not increase (and can even hurt) performance compared to randomly selecting data. To develop better methods for selecting data, we start by framing dataset selection as an optimization problem that we can directly solve for: given target tasks, a learning algorithm, and candidate data, select the subset that maximizes model performance. This framework thus avoids handpicked notions of data quality, and instead models explicitly how the learning process uses train datapoints to predict on the target tasks. Our resulting method greatly improves language model (LM) performance on both pre-specified tasks and previously unseen tasks. Specifically, choosing target tasks representative of standard LM problems and evaluating on diverse held-out benchmarks, our selected datasets provide a 2x compute multiplier over baseline methods.",
                    "score": 0.4602536121049412,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9541015625
                },
                {
                    "corpus_id": "267094971",
                    "title": "DsDm: Model-Aware Dataset Selection with Datamodels",
                    "text": "While we can calculate the performance of a specific training set by training a model on that set (and then evaluating), it is (generally) unclear how to calculate the best possible training subset without examining every possible subset one by one, a computationally infeasible procedure. \n\nWe instead approximate the optimal subset by (approximately) modeling how the learning algorithm actually uses training data to predict. Specifically, in Section 2, we model target task performance as a function of training subset using datamodels (which efficiently approximate the mapping between training subset and model performance (Ilyas et al., 2022)), and select the subset that maximizes our estimate. Then, in Section 3, we demonstrate that our resulting method, dataset selection with datamodels (DsDm), consistently improves language model performance on diverse target tasks (e.g., SQuAD (Rajpurkar et al., 2016) and LAMBADA (Paperno et al., 2016)), even when existing selection methods do not. \n\nDsDm-selected data can improve performance on pre-specified tasks. However, in practice we train large-scale models to generalize to yet unseen tasks. Our framework suggests a principled approach to selecting data in this scenario too: choose target tasks similar to those we expect at deployment time, then select the optimal dataset subset for these target tasks. Following this strategy, in Section 4, we choose target tasks that cover a range of natural language problem categories (SQuAD, Jeopardy (MosaicML, 2023), and LAMBADA), and select data from C4, a canonical web crawl (Raffel et al., 2020). Our selections deliver a 2\u00d7 compute multiplier on a diverse set of test benchmarks: DsDm-selected datasets yield LMs that perform as well as those trained with 2\u00d7 the compute budget on randomly selected data (we train up to 1.8B parameter models). In contrast, no baseline method outperforms randomly selecting data-even at the same compute budget.",
                    "score": 0.3946961894953656,
                    "section_title": "Introduction",
                    "char_start_offset": 2008,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 289
                        },
                        {
                            "start": 292,
                            "end": 428
                        },
                        {
                            "start": 429,
                            "end": 702
                        },
                        {
                            "start": 703,
                            "end": 999
                        },
                        {
                            "start": 1002,
                            "end": 1068
                        },
                        {
                            "start": 1069,
                            "end": 1152
                        },
                        {
                            "start": 1153,
                            "end": 1367
                        },
                        {
                            "start": 1368,
                            "end": 1606
                        },
                        {
                            "start": 1607,
                            "end": 1854
                        },
                        {
                            "start": 1855,
                            "end": 1955
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1584,
                            "end": 1605,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91650390625
                }
            ],
            "relevance_judgement": 0.9541015625,
            "relevance_judgment_input_expanded": "# Title: DsDm: Model-Aware Dataset Selection with Datamodels\n# Venue: International Conference on Machine Learning\n# Authors: Logan Engstrom, Axel Feldmann, A. Ma\u0327dry\n## Abstract\nWhen selecting data for training large-scale models, standard practice is to filter for examples that match human notions of data quality. Such filtering yields qualitatively clean datapoints that intuitively should improve model behavior. However, in practice the opposite can often happen: we find that selecting according to similarity with\"high quality\"data sources may not increase (and can even hurt) performance compared to randomly selecting data. To develop better methods for selecting data, we start by framing dataset selection as an optimization problem that we can directly solve for: given target tasks, a learning algorithm, and candidate data, select the subset that maximizes model performance. This framework thus avoids handpicked notions of data quality, and instead models explicitly how the learning process uses train datapoints to predict on the target tasks. Our resulting method greatly improves language model (LM) performance on both pre-specified tasks and previously unseen tasks. Specifically, choosing target tasks representative of standard LM problems and evaluating on diverse held-out benchmarks, our selected datasets provide a 2x compute multiplier over baseline methods.\n## Introduction\nWhile we can calculate the performance of a specific training set by training a model on that set (and then evaluating), it is (generally) unclear how to calculate the best possible training subset without examining every possible subset one by one, a computationally infeasible procedure. \n\nWe instead approximate the optimal subset by (approximately) modeling how the learning algorithm actually uses training data to predict. Specifically, in Section 2, we model target task performance as a function of training subset using datamodels (which efficiently approximate the mapping between training subset and model performance (Ilyas et al., 2022)), and select the subset that maximizes our estimate. Then, in Section 3, we demonstrate that our resulting method, dataset selection with datamodels (DsDm), consistently improves language model performance on diverse target tasks (e.g., SQuAD (Rajpurkar et al., 2016) and LAMBADA (Paperno et al., 2016)), even when existing selection methods do not. \n\nDsDm-selected data can improve performance on pre-specified tasks. However, in practice we train large-scale models to generalize to yet unseen tasks. Our framework suggests a principled approach to selecting data in this scenario too: choose target tasks similar to those we expect at deployment time, then select the optimal dataset subset for these target tasks. Following this strategy, in Section 4, we choose target tasks that cover a range of natural language problem categories (SQuAD, Jeopardy (MosaicML, 2023), and LAMBADA), and select data from C4, a canonical web crawl (Raffel et al., 2020). Our selections deliver a 2\u00d7 compute multiplier on a diverse set of test benchmarks: DsDm-selected datasets yield LMs that perform as well as those trained with 2\u00d7 the compute budget on randomly selected data (we train up to 1.8B parameter models). In contrast, no baseline method outperforms randomly selecting data-even at the same compute budget.",
            "reference_string": "[267094971 | Engstrom et al. | 2024 | Citations: 61]"
        },
        {
            "title": "QuRating: Selecting High-Quality Data for Training Language Models",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 79,
            "influential_citation_count": 17,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.09739, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2127066887",
                    "name": "Alexander Wettig"
                },
                {
                    "authorId": "2284268826",
                    "name": "Aatmik Gupta"
                },
                {
                    "authorId": "2323513320",
                    "name": "Saumya Malik"
                },
                {
                    "authorId": "50536468",
                    "name": "Danqi Chen"
                }
            ],
            "abstract": "Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that can capture human intuitions about data quality. In this paper, we investigate four qualities - writing style, required expertise, facts&trivia, and educational value - and find that LLMs are able to discern these qualities, especially when making pairwise judgments of texts. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity. When we sample using quality ratings as logits over documents, our models obtain lower perplexity and stronger in-context learning performance than baselines. Our best model is based on educational value and performs similarly to a model trained with uniform sampling for 50% more steps. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications.",
            "corpus_id": 267681974,
            "sentences": [
                {
                    "corpus_id": "267681974",
                    "title": "QuRating: Selecting High-Quality Data for Training Language Models",
                    "text": "Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that can capture human intuitions about data quality. In this paper, we investigate four qualities - writing style, required expertise, facts&trivia, and educational value - and find that LLMs are able to discern these qualities, especially when making pairwise judgments of texts. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity. When we sample using quality ratings as logits over documents, our models obtain lower perplexity and stronger in-context learning performance than baselines. Our best model is based on educational value and performs similarly to a model trained with uniform sampling for 50% more steps. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications.",
                    "score": 0.40965251188195867,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94970703125
                }
            ],
            "relevance_judgement": 0.94970703125,
            "relevance_judgment_input_expanded": "# Title: QuRating: Selecting High-Quality Data for Training Language Models\n# Venue: International Conference on Machine Learning\n# Authors: Alexander Wettig, Aatmik Gupta, Saumya Malik, Danqi Chen\n## Abstract\nSelecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that can capture human intuitions about data quality. In this paper, we investigate four qualities - writing style, required expertise, facts&trivia, and educational value - and find that LLMs are able to discern these qualities, especially when making pairwise judgments of texts. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity. When we sample using quality ratings as logits over documents, our models obtain lower perplexity and stronger in-context learning performance than baselines. Our best model is based on educational value and performs similarly to a model trained with uniform sampling for 50% more steps. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications.\n",
            "reference_string": "[267681974 | Wettig et al. | 2024 | Citations: 79]"
        },
        {
            "title": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2024,
            "reference_count": 387,
            "citation_count": 13,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.02085, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2267903811",
                    "name": "Yulei Qin"
                },
                {
                    "authorId": "2184288111",
                    "name": "Yuncheng Yang"
                },
                {
                    "authorId": "2314919460",
                    "name": "Pengcheng Guo"
                },
                {
                    "authorId": "2315086401",
                    "name": "Gang Li"
                },
                {
                    "authorId": "2314834176",
                    "name": "Hang Shao"
                },
                {
                    "authorId": "2315154473",
                    "name": "Yuchen Shi"
                },
                {
                    "authorId": "2267376632",
                    "name": "Zihan Xu"
                },
                {
                    "authorId": "2258793023",
                    "name": "Yun Gu"
                },
                {
                    "authorId": "2257346950",
                    "name": "Ke Li"
                },
                {
                    "authorId": "2314886050",
                    "name": "Xing Sun"
                }
            ],
            "abstract": "Instruction tuning plays a critical role in aligning large language models (LLMs) with human preference. Despite the vast amount of open instruction datasets, naively training a LLM on all existing instructions may not be optimal and practical. To pinpoint the most beneficial datapoints, data assessment and selection methods have been proposed in the fields of natural language processing (NLP) and deep learning. However, under the context of instruction tuning, there still exists a gap in knowledge on what kind of data evaluation metrics can be employed and how they can be integrated into the selection mechanism. To bridge this gap, we present a comprehensive review on existing literature of data assessment and selection especially for instruction tuning of LLMs. We systematically categorize all applicable methods into quality-based, diversity-based, and importance-based ones where a unified, fine-grained taxonomy is structured. For each category, representative methods are elaborated to describe the landscape of relevant research. In addition, comparison between the latest methods is conducted on their officially reported results to provide in-depth discussions on their limitations. Finally, we summarize the open challenges and propose the promosing avenues for future studies. All related contents are available at https://github.com/yuleiqin/fantastic-data-engineering.",
            "corpus_id": 271710435,
            "sentences": [
                {
                    "corpus_id": "271710435",
                    "title": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models",
                    "text": "In this work, we aim to unify a wide array of data assessment and selection methods under the context of instruction tuning of LLMs. As revealed from the probabilistic view (John and Draper, 1975;Murphy, 2012;Albalak et al., 2024), the statistical patterns inherent in datasets determines the modeling performance. The overall evaluation of instruction datapoints not only deciphers the distribution in various aspects (e.g., composition, task, and domain) and also help cherry-pick the most beneficial subsets for higher performance with less training cost. Through this survey, we demonstrate that: 1) existing resourceful data assessment methods can be categorized into three main perspectives: quality, diversity, and importance (see Fig. 1). 2) a systematic view of selection methods can be unified even they more or less exhibit coupling with the assessment techniques (see Fig. 2). It is noted that quality, diversity, and importance might be used interchangeably without strict discrimination in previous studies. But here we provide a rationalized organization taxonomy for structured elaboration. Despite the goal of being comprehensive, the present survey only provides details of certain typical, representative methods to avoid being tediously long. We hope the in-depth explanations and discussions on the selected methods provide insights into developing robust data assessment and selection pipelines for further studies. \n\n1.1 Related Surveys (Liu et al., 2024d) studies the mainstream datasets for building LLMs, including the pre-training corpora, instruction tuning datasets, preference datasets, evaluation benchmarks, and traditional NLP datasets. (Albalak et al., 2024) presents a systematic overview of constructing the data pipeline for language models. Any selection method, either via distribution matching or diversification, can be composed of: 1) utility function; 2) selection mechanism. During different stages of the pipeline (e.g., language filtering, data quality, domain knowledge, deduplication, toxic and explicit content removal, and data mixing), the selection method should be adjusted according to different selection objectives. (Wang et al., 2024a) focuses on the data preparation for instruction tuning.",
                    "score": 0.4361205550587199,
                    "section_title": "Introduction",
                    "char_start_offset": 1432,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 132
                        },
                        {
                            "start": 133,
                            "end": 314
                        },
                        {
                            "start": 315,
                            "end": 558
                        },
                        {
                            "start": 559,
                            "end": 746
                        },
                        {
                            "start": 747,
                            "end": 888
                        },
                        {
                            "start": 889,
                            "end": 1021
                        },
                        {
                            "start": 1022,
                            "end": 1106
                        },
                        {
                            "start": 1107,
                            "end": 1262
                        },
                        {
                            "start": 1263,
                            "end": 1437
                        },
                        {
                            "start": 1440,
                            "end": 1669
                        },
                        {
                            "start": 1670,
                            "end": 1778
                        },
                        {
                            "start": 1779,
                            "end": 1918
                        },
                        {
                            "start": 1919,
                            "end": 2171
                        },
                        {
                            "start": 2172,
                            "end": 2248
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 173,
                            "end": 196,
                            "matchedPaperCorpusId": "120997070"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.943359375
                },
                {
                    "corpus_id": "271710435",
                    "title": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models",
                    "text": "We aim at finding the most informative subset S b \u2282 S from the entire set S under the given budget |S b | \u2264 b. Mathematically, the selection of S b requires the quantitative evaluation q(\u2022) on each datapoint x i and an elaborated sampling mechanism \u03c0: \n\nwhere \u03c0(\u2022, b) denotes the sampling process with a maximum b datapoints. With respect to the detailed implementation of \u03c0, either an iterative, greedy algorithm or a batch-wise heuristic rule can be adopted for compatibility with q(\u2022). The expected benefits of such selection include: 1) the reduction of noise by ignoring those mislabeled, mismatched instruction-response pairs, 2) the re-balance of data distributions by down-sampling those easy, common, and similar examples while up-sampling hard, rare ones, and 3) the expedition of training in return for efficient iterations of LLMs.",
                    "score": 0.42007879352921085,
                    "section_title": "Data Assessment and Selection",
                    "char_start_offset": 9509,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 110
                        },
                        {
                            "start": 111,
                            "end": 251
                        },
                        {
                            "start": 254,
                            "end": 325
                        },
                        {
                            "start": 326,
                            "end": 488
                        },
                        {
                            "start": 489,
                            "end": 843
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8564453125
                }
            ],
            "relevance_judgement": 0.943359375,
            "relevance_judgment_input_expanded": "# Title: Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models\n# Venue: Trans. Mach. Learn. Res.\n# Authors: Yulei Qin, Yuncheng Yang, Pengcheng Guo, Gang Li, Hang Shao, Yuchen Shi, Zihan Xu, Yun Gu, Ke Li, Xing Sun\n## Abstract\nInstruction tuning plays a critical role in aligning large language models (LLMs) with human preference. Despite the vast amount of open instruction datasets, naively training a LLM on all existing instructions may not be optimal and practical. To pinpoint the most beneficial datapoints, data assessment and selection methods have been proposed in the fields of natural language processing (NLP) and deep learning. However, under the context of instruction tuning, there still exists a gap in knowledge on what kind of data evaluation metrics can be employed and how they can be integrated into the selection mechanism. To bridge this gap, we present a comprehensive review on existing literature of data assessment and selection especially for instruction tuning of LLMs. We systematically categorize all applicable methods into quality-based, diversity-based, and importance-based ones where a unified, fine-grained taxonomy is structured. For each category, representative methods are elaborated to describe the landscape of relevant research. In addition, comparison between the latest methods is conducted on their officially reported results to provide in-depth discussions on their limitations. Finally, we summarize the open challenges and propose the promosing avenues for future studies. All related contents are available at https://github.com/yuleiqin/fantastic-data-engineering.\n## Introduction\nIn this work, we aim to unify a wide array of data assessment and selection methods under the context of instruction tuning of LLMs. As revealed from the probabilistic view (John and Draper, 1975;Murphy, 2012;Albalak et al., 2024), the statistical patterns inherent in datasets determines the modeling performance. The overall evaluation of instruction datapoints not only deciphers the distribution in various aspects (e.g., composition, task, and domain) and also help cherry-pick the most beneficial subsets for higher performance with less training cost. Through this survey, we demonstrate that: 1) existing resourceful data assessment methods can be categorized into three main perspectives: quality, diversity, and importance (see Fig. 1). 2) a systematic view of selection methods can be unified even they more or less exhibit coupling with the assessment techniques (see Fig. 2). It is noted that quality, diversity, and importance might be used interchangeably without strict discrimination in previous studies. But here we provide a rationalized organization taxonomy for structured elaboration. Despite the goal of being comprehensive, the present survey only provides details of certain typical, representative methods to avoid being tediously long. We hope the in-depth explanations and discussions on the selected methods provide insights into developing robust data assessment and selection pipelines for further studies. \n\n1.1 Related Surveys (Liu et al., 2024d) studies the mainstream datasets for building LLMs, including the pre-training corpora, instruction tuning datasets, preference datasets, evaluation benchmarks, and traditional NLP datasets. (Albalak et al., 2024) presents a systematic overview of constructing the data pipeline for language models. Any selection method, either via distribution matching or diversification, can be composed of: 1) utility function; 2) selection mechanism. During different stages of the pipeline (e.g., language filtering, data quality, domain knowledge, deduplication, toxic and explicit content removal, and data mixing), the selection method should be adjusted according to different selection objectives. (Wang et al., 2024a) focuses on the data preparation for instruction tuning.\n\n## Data Assessment and Selection\nWe aim at finding the most informative subset S b \u2282 S from the entire set S under the given budget |S b | \u2264 b. Mathematically, the selection of S b requires the quantitative evaluation q(\u2022) on each datapoint x i and an elaborated sampling mechanism \u03c0: \n\nwhere \u03c0(\u2022, b) denotes the sampling process with a maximum b datapoints. With respect to the detailed implementation of \u03c0, either an iterative, greedy algorithm or a batch-wise heuristic rule can be adopted for compatibility with q(\u2022). The expected benefits of such selection include: 1) the reduction of noise by ignoring those mislabeled, mismatched instruction-response pairs, 2) the re-balance of data distributions by down-sampling those easy, common, and similar examples while up-sampling hard, rare ones, and 3) the expedition of training in return for efficient iterations of LLMs.",
            "reference_string": "[271710435 | Qin et al. | 2024 | Citations: 13]"
        },
        {
            "title": "Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 55,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.11378, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2321492993",
                    "name": "Simon Yu"
                },
                {
                    "authorId": "2146033755",
                    "name": "Liangyu Chen"
                },
                {
                    "authorId": "2321458071",
                    "name": "Sara Ahmadian"
                },
                {
                    "authorId": "2818759",
                    "name": "Marzieh Fadaee"
                }
            ],
            "abstract": "Finetuning large language models on instruction data is crucial for enhancing pre-trained knowledge and improving instruction-following capabilities. As instruction datasets proliferate, selecting optimal data for effective training becomes increasingly important. This work addresses the question: How can we determine the optimal subset of data for effective training? While existing research often emphasizes local criteria like instance quality for subset selection, we argue that a global approach focused on data diversity is more critical. Our method employs k-means clustering to ensure the selected subset effectively represents the full dataset. We propose an iterative refinement method inspired by active learning techniques to resample instances from clusters, reassessing each cluster's importance and sampling weight in every training iteration. This approach reduces the effect of outliers and automatically filters out clusters containing low-quality data. Through extensive evaluation across natural language reasoning, general world knowledge, code and math reasoning tasks, and by fine-tuning models from various families, we observe consistent improvements, achieving a 7% increase over random selection and a 3.8% improvement over state-of-the-art sampling methods. Our work highlights the significance of diversity-first sampling when finetuning LLMs to enhance performance across a broad array of evaluation tasks. Our code is available at https://github.com/for-ai/iterative-data-selection.",
            "corpus_id": 272694496,
            "sentences": [
                {
                    "corpus_id": "272694496",
                    "title": "Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement",
                    "text": "In this paper, we present a novel approach to selecting a subset of data and optimizing the fine-tuning of language models. Our method involved a scalable sampling technique that maximizes diversity and efficiency in subset selection. Through our proposed k-means-quality (kMQ) algorithm and iterative selection process, we demonstrated significant performance improvements over strong baselines while maintaining a limited training budget. Our contributions include an efficient instruction selection algorithm, the release of our encoded instruction dataset, and a systematic analysis of our method's effectiveness across a range of tasks. Our method outperforms existing baselines, achieving up to 7% improvement in a wide range of evaluation tasks. \n\nBy addressing the challenge of optimal instruct data selection, our work paves the way for more efficient and effective finetuning of language models, making them more accessible and affordable for deployment, especially in resource-constrained settings. We believe that our findings will contribute significantly to the ongoing research in language model optimization and their real-world applications. \n\n7 Limitations and Future Work \n\nWhile our proposed method has shown promising results, there are a few limitations to consider. \n\nOur evaluation focused on a specific set of tasks, and future work can aim to validate our method's effectiveness across a broader range of language models and tasks, including data selection in the pre-training stage and alignment (Yu et al., 2024;Muldrew et al., 2024). Furthermore, our iterative selection process relies on early training signals, and we only presented this as a pilot study to encourage further research. Future work could explore alternative model feedback mechanisms to refine the selected instruction data subsets, especially in mitigating the potential for reward hacking in the iterative refinement process (Pan et al., 2024). \n\nFinally, while we considered diversity and difficulty crucial factors, other characteristics of instruction data could be explored to enhance the finetuning process further. Addressing these limitations and extending this research will contribute to more robust and adaptable language models, capable of excelling in a wide range of real-world applications. \n\nBroader Impact If the data selection process fails to capture important aspects of the full dataset, it could lead to biased or inconsistent outputs from the finetuned models.",
                    "score": 0.4396506628741437,
                    "section_title": "Conclusion",
                    "char_start_offset": 26229,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 123
                        },
                        {
                            "start": 124,
                            "end": 234
                        },
                        {
                            "start": 235,
                            "end": 440
                        },
                        {
                            "start": 441,
                            "end": 641
                        },
                        {
                            "start": 642,
                            "end": 752
                        },
                        {
                            "start": 755,
                            "end": 1009
                        },
                        {
                            "start": 1010,
                            "end": 1158
                        },
                        {
                            "start": 1161,
                            "end": 1190
                        },
                        {
                            "start": 1193,
                            "end": 1288
                        },
                        {
                            "start": 1291,
                            "end": 1562
                        },
                        {
                            "start": 1563,
                            "end": 1716
                        },
                        {
                            "start": 1717,
                            "end": 1943
                        },
                        {
                            "start": 1946,
                            "end": 2119
                        },
                        {
                            "start": 2120,
                            "end": 2303
                        },
                        {
                            "start": 2306,
                            "end": 2481
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9375
                }
            ],
            "relevance_judgement": 0.9375,
            "relevance_judgment_input_expanded": "# Title: Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement\n# Venue: arXiv.org\n# Authors: Simon Yu, Liangyu Chen, Sara Ahmadian, Marzieh Fadaee\n## Abstract\nFinetuning large language models on instruction data is crucial for enhancing pre-trained knowledge and improving instruction-following capabilities. As instruction datasets proliferate, selecting optimal data for effective training becomes increasingly important. This work addresses the question: How can we determine the optimal subset of data for effective training? While existing research often emphasizes local criteria like instance quality for subset selection, we argue that a global approach focused on data diversity is more critical. Our method employs k-means clustering to ensure the selected subset effectively represents the full dataset. We propose an iterative refinement method inspired by active learning techniques to resample instances from clusters, reassessing each cluster's importance and sampling weight in every training iteration. This approach reduces the effect of outliers and automatically filters out clusters containing low-quality data. Through extensive evaluation across natural language reasoning, general world knowledge, code and math reasoning tasks, and by fine-tuning models from various families, we observe consistent improvements, achieving a 7% increase over random selection and a 3.8% improvement over state-of-the-art sampling methods. Our work highlights the significance of diversity-first sampling when finetuning LLMs to enhance performance across a broad array of evaluation tasks. Our code is available at https://github.com/for-ai/iterative-data-selection.\n## Conclusion\nIn this paper, we present a novel approach to selecting a subset of data and optimizing the fine-tuning of language models. Our method involved a scalable sampling technique that maximizes diversity and efficiency in subset selection. Through our proposed k-means-quality (kMQ) algorithm and iterative selection process, we demonstrated significant performance improvements over strong baselines while maintaining a limited training budget. Our contributions include an efficient instruction selection algorithm, the release of our encoded instruction dataset, and a systematic analysis of our method's effectiveness across a range of tasks. Our method outperforms existing baselines, achieving up to 7% improvement in a wide range of evaluation tasks. \n\nBy addressing the challenge of optimal instruct data selection, our work paves the way for more efficient and effective finetuning of language models, making them more accessible and affordable for deployment, especially in resource-constrained settings. We believe that our findings will contribute significantly to the ongoing research in language model optimization and their real-world applications. \n\n7 Limitations and Future Work \n\nWhile our proposed method has shown promising results, there are a few limitations to consider. \n\nOur evaluation focused on a specific set of tasks, and future work can aim to validate our method's effectiveness across a broader range of language models and tasks, including data selection in the pre-training stage and alignment (Yu et al., 2024;Muldrew et al., 2024). Furthermore, our iterative selection process relies on early training signals, and we only presented this as a pilot study to encourage further research. Future work could explore alternative model feedback mechanisms to refine the selected instruction data subsets, especially in mitigating the potential for reward hacking in the iterative refinement process (Pan et al., 2024). \n\nFinally, while we considered diversity and difficulty crucial factors, other characteristics of instruction data could be explored to enhance the finetuning process further. Addressing these limitations and extending this research will contribute to more robust and adaptable language models, capable of excelling in a wide range of real-world applications. \n\nBroader Impact If the data selection process fails to capture important aspects of the full dataset, it could lead to biased or inconsistent outputs from the finetuned models.",
            "reference_string": "[272694496 | Yu et al. | 2024 | Citations: 7]"
        },
        {
            "title": "Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 73,
            "citation_count": 22,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.11820, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257132704",
                    "name": "Yiding Jiang"
                },
                {
                    "authorId": "2325951293",
                    "name": "Allan Zhou"
                },
                {
                    "authorId": "2261439316",
                    "name": "Zhili Feng"
                },
                {
                    "authorId": "49288855",
                    "name": "Sadhika Malladi"
                },
                {
                    "authorId": "2242257227",
                    "name": "J. Kolter"
                }
            ],
            "abstract": "The composition of pretraining data is a key determinant of foundation models' performance, but there is no standard guideline for allocating a limited computational budget across different data sources. Most current approaches either rely on extensive experiments with smaller models or dynamic data adjustments that also require proxy models, both of which significantly increase the workflow complexity and computational overhead. In this paper, we introduce Adaptive Data Optimization (ADO), an algorithm that optimizes data distributions in an online fashion, concurrent with model training. Unlike existing techniques, ADO does not require external knowledge, proxy models, or modifications to the model update. Instead, ADO uses per-domain scaling laws to estimate the learning potential of each domain during training and adjusts the data mixture accordingly, making it more scalable and easier to integrate. Experiments demonstrate that ADO can achieve comparable or better performance than prior methods while maintaining computational efficiency across different computation scales, offering a practical solution for dynamically adjusting data distribution without sacrificing flexibility or increasing costs. Beyond its practical benefits, ADO also provides a new perspective on data collection strategies via scaling laws.",
            "corpus_id": 273350576,
            "sentences": [
                {
                    "corpus_id": "273350576",
                    "title": "Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws",
                    "text": "Data curation and selection. For current large language models, compute often poses a greater constraint than data availability, making data selection crucial (Albalak et al., 2024). A widely used approach is data filtering (Soboleva et al., 2023;Penedo et al., 2023;2024), where undesirable data points are removed based on heuristics like perplexity, repetition (Lee et al., 2021), or semantic similarity (Abbas et al., 2023). This filtering process is foundational for constructing most largescale datasets today. After filtering, data is often categorized into subsets or domains (e.g., code, books), and deciding how much data to use from each domain becomes an important step. \n\nTwo primary strategies for data selection are prevalent: one focuses on deciding whether individual data points should be included based on various criteria (Mindermann et al., 2022;Engstrom et al., 2024), and the other uses all available data but samples from different domains with varying probabilities (Xie et al., 2024;Fan et al., 2023;Albalak et al., 2023). While data selection aims to enhance training efficiency, these methods may introduce considerable computational overhead (Xie et al., 2024;Chen et al., 2024;Fan et al., 2023). Moreover, Kaddour et al. (2023) show that under the same computational budget, these methods often fail to surpass standard training, and Wang et al. (2024) proves that the data selection's efficacy depends on the user's utility function. \n\nAnother line of work focuses on selecting pre-training data that aligns more closely with downstream tasks (Kang et al., 2024). Data selection has also been explored for computer vision. For example, Evans et al. (2023) use a small reference model to select data for CLIP, while others propose pruning batches based on diversity criteria (Qin et al., 2024;Hong et al., 2024) to improve training efficiency. \n\nNeural scaling laws.",
                    "score": 0.4024982055363106,
                    "section_title": "RELATED WORKS",
                    "char_start_offset": 28377,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 28
                        },
                        {
                            "start": 29,
                            "end": 182
                        },
                        {
                            "start": 183,
                            "end": 428
                        },
                        {
                            "start": 429,
                            "end": 516
                        },
                        {
                            "start": 517,
                            "end": 682
                        },
                        {
                            "start": 685,
                            "end": 1048
                        },
                        {
                            "start": 1049,
                            "end": 1225
                        },
                        {
                            "start": 1226,
                            "end": 1464
                        },
                        {
                            "start": 1467,
                            "end": 1594
                        },
                        {
                            "start": 1595,
                            "end": 1653
                        },
                        {
                            "start": 1654,
                            "end": 1873
                        },
                        {
                            "start": 1876,
                            "end": 1896
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 842,
                            "end": 867,
                            "matchedPaperCorpusId": "235743024"
                        },
                        {
                            "start": 1026,
                            "end": 1047,
                            "matchedPaperCorpusId": "265658930"
                        },
                        {
                            "start": 1189,
                            "end": 1207,
                            "matchedPaperCorpusId": "260203057"
                        },
                        {
                            "start": 1574,
                            "end": 1593,
                            "matchedPaperCorpusId": "269605945"
                        },
                        {
                            "start": 1805,
                            "end": 1823,
                            "matchedPaperCorpusId": "257427625"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9326171875
                },
                {
                    "corpus_id": "273350576",
                    "title": "Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws",
                    "text": "A significant limitation of almost all existing methods (with the exception of Albalak et al. (2023)) is their requirement for additional computational steps beyond standard training. This extra complexity entails substantial modifications to the training pipeline, making it challenging to integrate these methods when other components are varied (e.g., the architecture, tokenizer, or optimizer). The consequence of this usage barrier is that these methods have not been tested widely, making them less popular for those training a new model. To enable broader adoption of data mixture optimization, we argue that it must be implemented in an online fashion, concurrent with model training, without disrupting the standard training process, which means that the data mixture must adapt itself based on the feedback from the model. This is closely related to curriculum learning (Bengio et al., 2009), a training strategy where models are progressively exposed to domains in a curated order. \n\nAt a fundamental level, this work develops and empirically investigates questions about the existence of good pretraining curricula and the feasibility of cheaply identifying them. We first demonstrate in controlled experiments that good curricula can generally be found with more computation and that it is hard to accurately predict larger models' performance with only a few small models. This leads to our main contribution, Adaptive Data Optimization (ADO): an algorithm for adaptively adjusting the data mixture online during training (Figure 1). In experiments on language models up to 1.3B parameters trained on the Pile (Gao et al., 2020), we find that ADO improves performance across a variety of common benchmarks and improves validation loss on SlimPajama (Soboleva et al., 2023) and FineWeb (Penedo et al., 2023), both of which are considered to be higher-quality datasets. Most importantly, ADO achieves these without requiring significant additional computation (less than 0.4% wallclock time for 1.3B), proxy models, or extensive modification to the training pipelines. The drawbacks of small proxy models. At first glance, transferring data selection strategies from smaller to larger models seems plausible, with positive results in specific cases, such as in Mindermann et al. ( 2022) and Xia et al. (2024b).",
                    "score": 0.4167215766755078,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 1964,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 183
                        },
                        {
                            "start": 184,
                            "end": 398
                        },
                        {
                            "start": 399,
                            "end": 544
                        },
                        {
                            "start": 545,
                            "end": 832
                        },
                        {
                            "start": 833,
                            "end": 992
                        },
                        {
                            "start": 995,
                            "end": 1175
                        },
                        {
                            "start": 1176,
                            "end": 1386
                        },
                        {
                            "start": 1387,
                            "end": 1547
                        },
                        {
                            "start": 1548,
                            "end": 1881
                        },
                        {
                            "start": 1882,
                            "end": 2080
                        },
                        {
                            "start": 2081,
                            "end": 2117
                        },
                        {
                            "start": 2118,
                            "end": 2322
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 79,
                            "end": 100,
                            "matchedPaperCorpusId": "265658930"
                        },
                        {
                            "start": 880,
                            "end": 901,
                            "matchedPaperCorpusId": "873046"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72216796875
                }
            ],
            "relevance_judgement": 0.9326171875,
            "relevance_judgment_input_expanded": "# Title: Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws\n# Venue: International Conference on Learning Representations\n# Authors: Yiding Jiang, Allan Zhou, Zhili Feng, Sadhika Malladi, J. Kolter\n## Abstract\nThe composition of pretraining data is a key determinant of foundation models' performance, but there is no standard guideline for allocating a limited computational budget across different data sources. Most current approaches either rely on extensive experiments with smaller models or dynamic data adjustments that also require proxy models, both of which significantly increase the workflow complexity and computational overhead. In this paper, we introduce Adaptive Data Optimization (ADO), an algorithm that optimizes data distributions in an online fashion, concurrent with model training. Unlike existing techniques, ADO does not require external knowledge, proxy models, or modifications to the model update. Instead, ADO uses per-domain scaling laws to estimate the learning potential of each domain during training and adjusts the data mixture accordingly, making it more scalable and easier to integrate. Experiments demonstrate that ADO can achieve comparable or better performance than prior methods while maintaining computational efficiency across different computation scales, offering a practical solution for dynamically adjusting data distribution without sacrificing flexibility or increasing costs. Beyond its practical benefits, ADO also provides a new perspective on data collection strategies via scaling laws.\n## INTRODUCTION\nA significant limitation of almost all existing methods (with the exception of Albalak et al. (2023)) is their requirement for additional computational steps beyond standard training. This extra complexity entails substantial modifications to the training pipeline, making it challenging to integrate these methods when other components are varied (e.g., the architecture, tokenizer, or optimizer). The consequence of this usage barrier is that these methods have not been tested widely, making them less popular for those training a new model. To enable broader adoption of data mixture optimization, we argue that it must be implemented in an online fashion, concurrent with model training, without disrupting the standard training process, which means that the data mixture must adapt itself based on the feedback from the model. This is closely related to curriculum learning (Bengio et al., 2009), a training strategy where models are progressively exposed to domains in a curated order. \n\nAt a fundamental level, this work develops and empirically investigates questions about the existence of good pretraining curricula and the feasibility of cheaply identifying them. We first demonstrate in controlled experiments that good curricula can generally be found with more computation and that it is hard to accurately predict larger models' performance with only a few small models. This leads to our main contribution, Adaptive Data Optimization (ADO): an algorithm for adaptively adjusting the data mixture online during training (Figure 1). In experiments on language models up to 1.3B parameters trained on the Pile (Gao et al., 2020), we find that ADO improves performance across a variety of common benchmarks and improves validation loss on SlimPajama (Soboleva et al., 2023) and FineWeb (Penedo et al., 2023), both of which are considered to be higher-quality datasets. Most importantly, ADO achieves these without requiring significant additional computation (less than 0.4% wallclock time for 1.3B), proxy models, or extensive modification to the training pipelines. The drawbacks of small proxy models. At first glance, transferring data selection strategies from smaller to larger models seems plausible, with positive results in specific cases, such as in Mindermann et al. ( 2022) and Xia et al. (2024b).\n\n## RELATED WORKS\nData curation and selection. For current large language models, compute often poses a greater constraint than data availability, making data selection crucial (Albalak et al., 2024). A widely used approach is data filtering (Soboleva et al., 2023;Penedo et al., 2023;2024), where undesirable data points are removed based on heuristics like perplexity, repetition (Lee et al., 2021), or semantic similarity (Abbas et al., 2023). This filtering process is foundational for constructing most largescale datasets today. After filtering, data is often categorized into subsets or domains (e.g., code, books), and deciding how much data to use from each domain becomes an important step. \n\nTwo primary strategies for data selection are prevalent: one focuses on deciding whether individual data points should be included based on various criteria (Mindermann et al., 2022;Engstrom et al., 2024), and the other uses all available data but samples from different domains with varying probabilities (Xie et al., 2024;Fan et al., 2023;Albalak et al., 2023). While data selection aims to enhance training efficiency, these methods may introduce considerable computational overhead (Xie et al., 2024;Chen et al., 2024;Fan et al., 2023). Moreover, Kaddour et al. (2023) show that under the same computational budget, these methods often fail to surpass standard training, and Wang et al. (2024) proves that the data selection's efficacy depends on the user's utility function. \n\nAnother line of work focuses on selecting pre-training data that aligns more closely with downstream tasks (Kang et al., 2024). Data selection has also been explored for computer vision. For example, Evans et al. (2023) use a small reference model to select data for CLIP, while others propose pruning batches based on diversity criteria (Qin et al., 2024;Hong et al., 2024) to improve training efficiency. \n\nNeural scaling laws.",
            "reference_string": "[273350576 | Jiang et al. | 2024 | Citations: 22]"
        },
        {
            "title": "Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models",
            "venue": "Neural Information Processing Systems",
            "year": 2023,
            "reference_count": 75,
            "citation_count": 65,
            "influential_citation_count": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2307.14430",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.14430, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48622329",
                    "name": "Mayee F. Chen"
                },
                {
                    "authorId": "2069037355",
                    "name": "Nicholas Roberts"
                },
                {
                    "authorId": "144383716",
                    "name": "K. Bhatia"
                },
                {
                    "authorId": "39597242",
                    "name": "Jue Wang"
                },
                {
                    "authorId": "1776014",
                    "name": "Ce Zhang"
                },
                {
                    "authorId": "2186982588",
                    "name": "Frederic Sala"
                },
                {
                    "authorId": "1803218",
                    "name": "Christopher R\u00e9"
                }
            ],
            "abstract": "The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training. Using this intuition, our framework formalizes the notion of a skill and of an ordered set of skills in terms of the associated data. First, using both synthetic and real data, we demonstrate that these ordered skill sets exist, and that their existence enables more advanced skills to be learned with less data when we train on their prerequisite skills. Second, using our proposed framework, we introduce an online data sampling algorithm, Skill-It, over mixtures of skills for both continual pre-training and fine-tuning regimes, where the objective is to efficiently learn multiple skills in the former and an individual skill in the latter. On the LEGO synthetic in the continual pre-training setting, Skill-It obtains 36.5 points higher accuracy than random sampling. On the Natural Instructions dataset in the fine-tuning setting, Skill-It reduces the validation loss on the target skill by 13.6% versus training on data associated with the target skill itself. We apply our skills framework on the recent RedPajama dataset to continually pre-train a 3B-parameter LM, achieving higher accuracy on the LM Evaluation Harness with 1B tokens than the baseline approach of sampling uniformly over data sources with 3B tokens.",
            "corpus_id": 260203057,
            "sentences": [
                {
                    "corpus_id": "260203057",
                    "title": "Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models",
                    "text": "For Figure 1: Inspired by how humans acquire knowledge, we hypothesize that LMs best learn skills in a particular order and that this can help improve our understanding and training of LMs. We show that these ordered skill sets exist in real data, which enables skills to be learned with less data given that we train on their prerequisite skills. We then propose SKILL-IT, an online data selection algorithm that learns skills quickly by exploiting their ordering. instance, we partitioned the Alpaca dataset [56] by instruction type-a technique used to capture dataset diversity [62]-but we found that sampling based on instruction types and random sampling resulted in similar model performance, suggesting that not just any existing notion of data groups can characterize skills. \n\nSecond, these definitions of skills must be used to construct sampling distributions to actually improve model training. \n\nTo develop criteria for a data selection algorithm that learns skills efficiently, we identify challenges that naive selection approaches face. The standard approach of random uniform sampling over data fails to learn skills optimally due to not accounting for skill imbalance and ordering. Skills can be distributed unevenly in the data, with more complex skills being rare-for instance, Spanish and question generation (QG) are 5% and 4% of the Natural Instructions dataset [63], respectively, but Spanish QG is only 0.2%. Random sampling also provides no mechanism for taking into account a particular training order and dependency structure on skills. More sophisticated techniques like curriculum learning account for sample-level ordering, but not skills or their dependencies. Our goal framework must account for these issues of imbalance and ordering. \n\nSkill-based framework We define a skill as a unit of behavior that a model can learn using an associated slice of data (Definition 2.1). An ordered skill set is a collection of skills with a directed skills graph that is neither complete nor empty, where an edge from a prerequisite skill to a skill exists if the amount of training it takes to learn the skill can be reduced if the prerequisite skill is also learned (Definition 2.2, Figure 1 left, center). We show that ordered skill sets exist in synthetic and real datasets using this operational definition.",
                    "score": 0.5219447886238668,
                    "section_title": "Introduction",
                    "char_start_offset": 2180,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 189
                        },
                        {
                            "start": 190,
                            "end": 347
                        },
                        {
                            "start": 348,
                            "end": 465
                        },
                        {
                            "start": 466,
                            "end": 783
                        },
                        {
                            "start": 786,
                            "end": 906
                        },
                        {
                            "start": 909,
                            "end": 1052
                        },
                        {
                            "start": 1053,
                            "end": 1199
                        },
                        {
                            "start": 1200,
                            "end": 1433
                        },
                        {
                            "start": 1434,
                            "end": 1564
                        },
                        {
                            "start": 1565,
                            "end": 1692
                        },
                        {
                            "start": 1693,
                            "end": 1768
                        },
                        {
                            "start": 1771,
                            "end": 1907
                        },
                        {
                            "start": 1908,
                            "end": 2229
                        },
                        {
                            "start": 2230,
                            "end": 2333
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.916015625
                },
                {
                    "corpus_id": "260203057",
                    "title": "Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models",
                    "text": "The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training. Using this intuition, our framework formalizes the notion of a skill and of an ordered set of skills in terms of the associated data. First, using both synthetic and real data, we demonstrate that these ordered skill sets exist, and that their existence enables more advanced skills to be learned with less data when we train on their prerequisite skills. Second, using our proposed framework, we introduce an online data sampling algorithm, Skill-It, over mixtures of skills for both continual pre-training and fine-tuning regimes, where the objective is to efficiently learn multiple skills in the former and an individual skill in the latter. On the LEGO synthetic in the continual pre-training setting, Skill-It obtains 36.5 points higher accuracy than random sampling. On the Natural Instructions dataset in the fine-tuning setting, Skill-It reduces the validation loss on the target skill by 13.6% versus training on data associated with the target skill itself. We apply our skills framework on the recent RedPajama dataset to continually pre-train a 3B-parameter LM, achieving higher accuracy on the LM Evaluation Harness with 1B tokens than the baseline approach of sampling uniformly over data sources with 3B tokens.",
                    "score": 0.5548131000763677,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8984375
                },
                {
                    "corpus_id": "260203057",
                    "title": "Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models",
                    "text": "We show that ordered skill sets exist in synthetic and real datasets using this operational definition. Interestingly, the existence of these ordered skill sets unveils that one can learn a skill quickly not by training solely on that skill, but on a mixture of that skill and prerequisite skills. For instance, in Figure 3 we observe that Spanish QG can be learned more efficiently when the model also learns English QG and Spanish-we can achieve 4% lower validation loss than training on only Spanish QG over a fixed budget of overall training steps. \n\nNext, given an ordered skill set to train on, we use our framework to propose methods for how to select data so that the LM learn skills faster: skill-stratified sampling and an online generalization, SKILL-IT. We address the issue of unevenly distributed skills in datasets by proposing skill-stratified sampling, a simple approach that allows us to explicitly optimize for learning skills by uniformly sampling relevant skills (such as a target skill and its prerequisite skills in fine-tuning). Skill-stratified sampling uses the construction of the ordered skill set but is static, which does not incorporate the ordering as training proceeds and results in oversampling skills that may be already learned early on in training. We address this issue by proposing an online data selection algorithm, SKILL-IT, for selecting mixtures of training skills that allocates more weight towards learning skills that are not yet learned or towards prerequisite influential skills (Figure 1 right). SKILL-IT is derived from an online optimization problem over the training skills for minimizing loss on a set of evaluation skills given a fixed budget of data and the skills graph. SKILL-IT is inspired by online mirror descent and can be adapted for continual pre-training, fine-tuning, or out-of-domain evaluation depending on the relationship between the evaluation skill set and the training skill set. \n\nWe evaluate SKILL-IT on synthetic and real datasets at two model scales, 125M and 1.3B parameters. For the continual pre-training setting, we show on the LEGO synthetic [72] that we obtain a 35.8 point improvement in accuracy over randomly selecting training data and curriculum learning [3].",
                    "score": 0.4341755079359133,
                    "section_title": "Introduction",
                    "char_start_offset": 4410,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 103
                        },
                        {
                            "start": 104,
                            "end": 297
                        },
                        {
                            "start": 298,
                            "end": 552
                        },
                        {
                            "start": 555,
                            "end": 765
                        },
                        {
                            "start": 766,
                            "end": 1052
                        },
                        {
                            "start": 1053,
                            "end": 1286
                        },
                        {
                            "start": 1287,
                            "end": 1546
                        },
                        {
                            "start": 1547,
                            "end": 1728
                        },
                        {
                            "start": 1729,
                            "end": 1953
                        },
                        {
                            "start": 1956,
                            "end": 2054
                        },
                        {
                            "start": 2055,
                            "end": 2248
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.875
                },
                {
                    "corpus_id": "260203057",
                    "title": "Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models",
                    "text": "Data attribution Another perspective on understanding training data is data attribution, which seeks to identify what data is responsible for particular model behaviors. Influence functions [28] and shapley values [16] are two ways to quantify the role of individual samples. Datamodels [23] fit a model to predict behavior given a subset of training data, providing a framework for understanding individual samples as well as dataset counterfactuals. Simfluence [20] fits a Markov process to a set of training trajectories for finer-grained understanding of how data impacts training. We focus on understanding how groups of data associated with skills elicit broader model capabilities, and utilize this understanding to select data for more efficient training. \n\nCurriculum learning Curriculum learning [3] proposes to show the model data in order from easy samples to hard ones. Various criteria have been used to determine hardness, and anticurriculum as well as various pacing functions and mixing rates have been explored [54]. Curriculum learning can also be performed at the group level [60]. More sophisticated approaches include parametrizing each sample with a dynamic importance [52], and also accounting for irrelevant and noisy data [38]. Our approach similarly utilizes a curriculum, but it is defined over a skills graph and does not necessarily align with training on easiest to hardest skills. \n\nHow LMs learn Many different explanations for how LMs learn from data have been proposed. One hypothesis is that there exist discrete, universal building blocks of LM knowledge called quanta, and power law scaling emerges from a learning over a particular distribution of quanta in the right order [37]. Another is that chain of thought reasoning emerges due to local clusters of latent variables that influence each other, which can be validated by studying the LM's ability to do conditional inference given intermediate variables [48]. Others have provided theoretical analysis of how transformers learn topics by studying co-occurrences of words in the training data [34]. Empirically, how models learn is still a mystery-for instance, models trained on code are found to perform fairly well at commensense reasoning [36]. Our work initiates a study on how LMs learn various skills and how to exploit this for better data selection.",
                    "score": 0.3966155884751822,
                    "section_title": "Related work",
                    "char_start_offset": 34443,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 169
                        },
                        {
                            "start": 170,
                            "end": 275
                        },
                        {
                            "start": 276,
                            "end": 451
                        },
                        {
                            "start": 452,
                            "end": 585
                        },
                        {
                            "start": 586,
                            "end": 763
                        },
                        {
                            "start": 766,
                            "end": 882
                        },
                        {
                            "start": 883,
                            "end": 1034
                        },
                        {
                            "start": 1035,
                            "end": 1101
                        },
                        {
                            "start": 1102,
                            "end": 1253
                        },
                        {
                            "start": 1254,
                            "end": 1412
                        },
                        {
                            "start": 1415,
                            "end": 1504
                        },
                        {
                            "start": 1505,
                            "end": 1718
                        },
                        {
                            "start": 1719,
                            "end": 1953
                        },
                        {
                            "start": 1954,
                            "end": 2091
                        },
                        {
                            "start": 2092,
                            "end": 2241
                        },
                        {
                            "start": 2242,
                            "end": 2351
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 214,
                            "end": 218,
                            "matchedPaperCorpusId": "102350503"
                        },
                        {
                            "start": 806,
                            "end": 809,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 1029,
                            "end": 1033,
                            "matchedPaperCorpusId": "231709290"
                        },
                        {
                            "start": 1192,
                            "end": 1196,
                            "matchedPaperCorpusId": "209438234"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.84619140625
                }
            ],
            "relevance_judgement": 0.916015625,
            "relevance_judgment_input_expanded": "# Title: Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models\n# Venue: Neural Information Processing Systems\n# Authors: Mayee F. Chen, Nicholas Roberts, K. Bhatia, Jue Wang, Ce Zhang, Frederic Sala, Christopher R\u00e9\n## Abstract\nThe quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training. Using this intuition, our framework formalizes the notion of a skill and of an ordered set of skills in terms of the associated data. First, using both synthetic and real data, we demonstrate that these ordered skill sets exist, and that their existence enables more advanced skills to be learned with less data when we train on their prerequisite skills. Second, using our proposed framework, we introduce an online data sampling algorithm, Skill-It, over mixtures of skills for both continual pre-training and fine-tuning regimes, where the objective is to efficiently learn multiple skills in the former and an individual skill in the latter. On the LEGO synthetic in the continual pre-training setting, Skill-It obtains 36.5 points higher accuracy than random sampling. On the Natural Instructions dataset in the fine-tuning setting, Skill-It reduces the validation loss on the target skill by 13.6% versus training on data associated with the target skill itself. We apply our skills framework on the recent RedPajama dataset to continually pre-train a 3B-parameter LM, achieving higher accuracy on the LM Evaluation Harness with 1B tokens than the baseline approach of sampling uniformly over data sources with 3B tokens.\n## Introduction\nFor Figure 1: Inspired by how humans acquire knowledge, we hypothesize that LMs best learn skills in a particular order and that this can help improve our understanding and training of LMs. We show that these ordered skill sets exist in real data, which enables skills to be learned with less data given that we train on their prerequisite skills. We then propose SKILL-IT, an online data selection algorithm that learns skills quickly by exploiting their ordering. instance, we partitioned the Alpaca dataset [56] by instruction type-a technique used to capture dataset diversity [62]-but we found that sampling based on instruction types and random sampling resulted in similar model performance, suggesting that not just any existing notion of data groups can characterize skills. \n\nSecond, these definitions of skills must be used to construct sampling distributions to actually improve model training. \n\nTo develop criteria for a data selection algorithm that learns skills efficiently, we identify challenges that naive selection approaches face. The standard approach of random uniform sampling over data fails to learn skills optimally due to not accounting for skill imbalance and ordering. Skills can be distributed unevenly in the data, with more complex skills being rare-for instance, Spanish and question generation (QG) are 5% and 4% of the Natural Instructions dataset [63], respectively, but Spanish QG is only 0.2%. Random sampling also provides no mechanism for taking into account a particular training order and dependency structure on skills. More sophisticated techniques like curriculum learning account for sample-level ordering, but not skills or their dependencies. Our goal framework must account for these issues of imbalance and ordering. \n\nSkill-based framework We define a skill as a unit of behavior that a model can learn using an associated slice of data (Definition 2.1). An ordered skill set is a collection of skills with a directed skills graph that is neither complete nor empty, where an edge from a prerequisite skill to a skill exists if the amount of training it takes to learn the skill can be reduced if the prerequisite skill is also learned (Definition 2.2, Figure 1 left, center). We show that ordered skill sets exist in synthetic and real datasets using this operational definition.\n...\nWe show that ordered skill sets exist in synthetic and real datasets using this operational definition. Interestingly, the existence of these ordered skill sets unveils that one can learn a skill quickly not by training solely on that skill, but on a mixture of that skill and prerequisite skills. For instance, in Figure 3 we observe that Spanish QG can be learned more efficiently when the model also learns English QG and Spanish-we can achieve 4% lower validation loss than training on only Spanish QG over a fixed budget of overall training steps. \n\nNext, given an ordered skill set to train on, we use our framework to propose methods for how to select data so that the LM learn skills faster: skill-stratified sampling and an online generalization, SKILL-IT. We address the issue of unevenly distributed skills in datasets by proposing skill-stratified sampling, a simple approach that allows us to explicitly optimize for learning skills by uniformly sampling relevant skills (such as a target skill and its prerequisite skills in fine-tuning). Skill-stratified sampling uses the construction of the ordered skill set but is static, which does not incorporate the ordering as training proceeds and results in oversampling skills that may be already learned early on in training. We address this issue by proposing an online data selection algorithm, SKILL-IT, for selecting mixtures of training skills that allocates more weight towards learning skills that are not yet learned or towards prerequisite influential skills (Figure 1 right). SKILL-IT is derived from an online optimization problem over the training skills for minimizing loss on a set of evaluation skills given a fixed budget of data and the skills graph. SKILL-IT is inspired by online mirror descent and can be adapted for continual pre-training, fine-tuning, or out-of-domain evaluation depending on the relationship between the evaluation skill set and the training skill set. \n\nWe evaluate SKILL-IT on synthetic and real datasets at two model scales, 125M and 1.3B parameters. For the continual pre-training setting, we show on the LEGO synthetic [72] that we obtain a 35.8 point improvement in accuracy over randomly selecting training data and curriculum learning [3].\n\n## Related work\nData attribution Another perspective on understanding training data is data attribution, which seeks to identify what data is responsible for particular model behaviors. Influence functions [28] and shapley values [16] are two ways to quantify the role of individual samples. Datamodels [23] fit a model to predict behavior given a subset of training data, providing a framework for understanding individual samples as well as dataset counterfactuals. Simfluence [20] fits a Markov process to a set of training trajectories for finer-grained understanding of how data impacts training. We focus on understanding how groups of data associated with skills elicit broader model capabilities, and utilize this understanding to select data for more efficient training. \n\nCurriculum learning Curriculum learning [3] proposes to show the model data in order from easy samples to hard ones. Various criteria have been used to determine hardness, and anticurriculum as well as various pacing functions and mixing rates have been explored [54]. Curriculum learning can also be performed at the group level [60]. More sophisticated approaches include parametrizing each sample with a dynamic importance [52], and also accounting for irrelevant and noisy data [38]. Our approach similarly utilizes a curriculum, but it is defined over a skills graph and does not necessarily align with training on easiest to hardest skills. \n\nHow LMs learn Many different explanations for how LMs learn from data have been proposed. One hypothesis is that there exist discrete, universal building blocks of LM knowledge called quanta, and power law scaling emerges from a learning over a particular distribution of quanta in the right order [37]. Another is that chain of thought reasoning emerges due to local clusters of latent variables that influence each other, which can be validated by studying the LM's ability to do conditional inference given intermediate variables [48]. Others have provided theoretical analysis of how transformers learn topics by studying co-occurrences of words in the training data [34]. Empirically, how models learn is still a mystery-for instance, models trained on code are found to perform fairly well at commensense reasoning [36]. Our work initiates a study on how LMs learn various skills and how to exploit this for better data selection.",
            "reference_string": "[260203057 | Chen et al. | 2023 | Citations: 65]"
        },
        {
            "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 21,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.10430, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "7565696",
                    "name": "Dheeraj Mekala"
                },
                {
                    "authorId": "2284673632",
                    "name": "Alex Nguyen"
                },
                {
                    "authorId": "2284595153",
                    "name": "Jingbo Shang"
                }
            ],
            "abstract": "Instruction-tuning language models has become a crucial step in aligning them for general use. Typically, this process involves extensive training on large datasets, incurring high training costs. In this paper, we introduce a novel training data selection based on the learning percentage of the samples. We assert that current language models possess the capability to autonomously select high-quality training data, leading to comparable or improved performance compared to training on the entire dataset. Our experiments span different-sized models, revealing that this characteristic holds for models ranging from 1B (small) to 13B (large) in size. Moreover, we demonstrate an interesting finding that the data hardness transfers across model sizes, and a smaller 350M model can effectively curate high-quality training data with hard samples for a larger 13B model, resulting in an equally or superior instruction-tuned model compared to training on the complete dataset. Utilizing open-sourced OPT and Llama-2 models up to 13B in size, two publicly available instruction-tuning training datasets and evaluated by both automatic metrics&humans, our paper introduces a novel approach to training data selection, showcasing a more efficient alternative.",
            "corpus_id": 267740312,
            "sentences": [
                {
                    "corpus_id": "267740312",
                    "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
                    "text": "In this paper, we delve into the measurement of sample difficulty from the model's perspective. Drawing inspiration from the learning order metric in (Mekala et al., 2022a), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered. We then select the most difficult sample subsets based on this ranking and instruction-tune a language model. Our experiments involve two instruction-tuning datasets, Alpaca-Data (Taori et al., 2023), and Dolly (Conover et al., 2023), with performance measured using automated metrics such as Al-pacaEval (Li et al., 2023b) and human evaluation. \n\nOur main findings indicate that language models can autonomously select training data, achieving performance equal to or better than training on the entire dataset. Furthermore, this characteristic scales across different model sizes, ranging from smaller ones (1B) to larger ones (13B) \n\n1 in parameters. As the size of the language model increases, we observe a consistent reduction in the minimum amount of data needed to surpass the performance of a model trained on the entire dataset. Interestingly, we observe that the data hardness also transfers across models, meaning samples considered difficult by smaller models are similarly challenging for larger models. Moreover, we note that this transferability improves with the size of the smaller model, eventually achieving comparable quality, beyond a size threshold, to that attained by selfselection conducted by larger models. Our study employs open-sourced models such as OPT (Zhang et al., 2022) and Llama-2 (Touvron et al., 2023) to support these findings. The remainder of the paper is structured as follows: initially, we describe the experimental setup encompassing the language models, the datasets employed, and the evaluation metrics utilized (section 2). Subsequently, we present our learning percentage-based difficulty metric and analyze it in detail (section 3). Following this, we optimize the proposed metric and introduce an equally effective, approximate, and faster metric (section 4). Ultimately, we analyze the challenging data identified through this metric (section 5).",
                    "score": 0.43100096255817705,
                    "section_title": "arXiv:2402.10430v1 [cs.CL] 16 Feb 2024",
                    "char_start_offset": 1531,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 95
                        },
                        {
                            "start": 96,
                            "end": 330
                        },
                        {
                            "start": 331,
                            "end": 429
                        },
                        {
                            "start": 430,
                            "end": 539
                        },
                        {
                            "start": 540,
                            "end": 775
                        },
                        {
                            "start": 778,
                            "end": 942
                        },
                        {
                            "start": 943,
                            "end": 1064
                        },
                        {
                            "start": 1067,
                            "end": 1083
                        },
                        {
                            "start": 1084,
                            "end": 1268
                        },
                        {
                            "start": 1269,
                            "end": 1447
                        },
                        {
                            "start": 1448,
                            "end": 1664
                        },
                        {
                            "start": 1665,
                            "end": 1797
                        },
                        {
                            "start": 1798,
                            "end": 2002
                        },
                        {
                            "start": 2003,
                            "end": 2113
                        },
                        {
                            "start": 2114,
                            "end": 2241
                        },
                        {
                            "start": 2242,
                            "end": 2329
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 150,
                            "end": 172,
                            "matchedPaperCorpusId": "249060677"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9140625
                },
                {
                    "corpus_id": "267740312",
                    "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
                    "text": "Instruction-tuning language models has become a crucial step in aligning them for general use. Typically, this process involves extensive training on large datasets, incurring high training costs. In this paper, we introduce a novel training data selection based on the learning percentage of the samples. We assert that current language models possess the capability to autonomously select high-quality training data, leading to comparable or improved performance compared to training on the entire dataset. Our experiments span different-sized models, revealing that this characteristic holds for models ranging from 1B (small) to 13B (large) in size. Moreover, we demonstrate an interesting finding that the data hardness transfers across model sizes, and a smaller 350M model can effectively curate high-quality training data with hard samples for a larger 13B model, resulting in an equally or superior instruction-tuned model compared to training on the complete dataset. Utilizing open-sourced OPT and Llama-2 models up to 13B in size, two publicly available instruction-tuning training datasets and evaluated by both automatic metrics&humans, our paper introduces a novel approach to training data selection, showcasing a more efficient alternative.",
                    "score": 0.4838030793742177,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90869140625
                }
            ],
            "relevance_judgement": 0.9140625,
            "relevance_judgment_input_expanded": "# Title: Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Dheeraj Mekala, Alex Nguyen, Jingbo Shang\n## Abstract\nInstruction-tuning language models has become a crucial step in aligning them for general use. Typically, this process involves extensive training on large datasets, incurring high training costs. In this paper, we introduce a novel training data selection based on the learning percentage of the samples. We assert that current language models possess the capability to autonomously select high-quality training data, leading to comparable or improved performance compared to training on the entire dataset. Our experiments span different-sized models, revealing that this characteristic holds for models ranging from 1B (small) to 13B (large) in size. Moreover, we demonstrate an interesting finding that the data hardness transfers across model sizes, and a smaller 350M model can effectively curate high-quality training data with hard samples for a larger 13B model, resulting in an equally or superior instruction-tuned model compared to training on the complete dataset. Utilizing open-sourced OPT and Llama-2 models up to 13B in size, two publicly available instruction-tuning training datasets and evaluated by both automatic metrics&humans, our paper introduces a novel approach to training data selection, showcasing a more efficient alternative.\n## arXiv:2402.10430v1 [cs.CL] 16 Feb 2024\nIn this paper, we delve into the measurement of sample difficulty from the model's perspective. Drawing inspiration from the learning order metric in (Mekala et al., 2022a), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered. We then select the most difficult sample subsets based on this ranking and instruction-tune a language model. Our experiments involve two instruction-tuning datasets, Alpaca-Data (Taori et al., 2023), and Dolly (Conover et al., 2023), with performance measured using automated metrics such as Al-pacaEval (Li et al., 2023b) and human evaluation. \n\nOur main findings indicate that language models can autonomously select training data, achieving performance equal to or better than training on the entire dataset. Furthermore, this characteristic scales across different model sizes, ranging from smaller ones (1B) to larger ones (13B) \n\n1 in parameters. As the size of the language model increases, we observe a consistent reduction in the minimum amount of data needed to surpass the performance of a model trained on the entire dataset. Interestingly, we observe that the data hardness also transfers across models, meaning samples considered difficult by smaller models are similarly challenging for larger models. Moreover, we note that this transferability improves with the size of the smaller model, eventually achieving comparable quality, beyond a size threshold, to that attained by selfselection conducted by larger models. Our study employs open-sourced models such as OPT (Zhang et al., 2022) and Llama-2 (Touvron et al., 2023) to support these findings. The remainder of the paper is structured as follows: initially, we describe the experimental setup encompassing the language models, the datasets employed, and the evaluation metrics utilized (section 2). Subsequently, we present our learning percentage-based difficulty metric and analyze it in detail (section 3). Following this, we optimize the proposed metric and introduce an equally effective, approximate, and faster metric (section 4). Ultimately, we analyze the challenging data identified through this metric (section 5).",
            "reference_string": "[267740312 | Mekala et al. | 2024 | Citations: 21]"
        },
        {
            "title": "Curriculum Learning for Domain Adaptation in Neural Machine Translation",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2019,
            "reference_count": 48,
            "citation_count": 124,
            "influential_citation_count": 17,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1905.05816",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.05816, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "49469742",
                    "name": "Xuan Zhang"
                },
                {
                    "authorId": "38581144",
                    "name": "Pamela Shapiro"
                },
                {
                    "authorId": "48387892",
                    "name": "Manish Kumar"
                },
                {
                    "authorId": "145324163",
                    "name": "Paul McNamee"
                },
                {
                    "authorId": "2954727",
                    "name": "Marine Carpuat"
                },
                {
                    "authorId": "1800354",
                    "name": "Kevin Duh"
                }
            ],
            "abstract": "We introduce a curriculum learning approach to adapt generic neural machine translation models to a specific domain. Samples are grouped by their similarities to the domain of interest and each group is fed to the training algorithm with a particular schedule. This approach is simple to implement on top of any neural framework or architecture, and consistently outperforms both unadapted and adapted baselines in experiments with two distinct domains and two language pairs.",
            "corpus_id": 155089817,
            "sentences": [
                {
                    "corpus_id": "155089817",
                    "title": "Curriculum Learning for Domain Adaptation in Neural Machine Translation",
                    "text": "We adopt similarity metrics from prior work on data selection to score examples for curriculum learning. Let I be an in-domain corpus, and N be a unlabeled-domain data set. Data selection models rank sentences in N according to a domain similarity measure with respect to I, and choose top n samples from N by a cut-off threshold for further training purpose. We examine two data selection methods, Moore-Lewis method (Moore and Lewis, 2010) and cynical data selection (Axelrod, 2017). \n\nMoore-Lewis Method Each sentence s in N is assigned a cross-entropy difference score, \n\nwhere H I (s) is the per-word cross-entropy of s according to a language model trained on I, and H N (s) is the per-word cross-entropy of s according to a language model trained on a random sample of N with roughly the same size as I. A lower cross-entropy difference indicates that s is more like the in-domain data and less like the unlabeled-domain data. \n\nCynical Data Selection Iteratively select sentence s from N to construct a training corpus that would approximately model I. At each iteration, each sentence is scored by the expected cross-entropy change from adding it to the already selected subset of N . The selected sentence is the one which most decreases H n , the cross-entropy between previously selected n-sentence corpus and I.",
                    "score": 0.5410374710905349,
                    "section_title": "Domain Similarity Scoring",
                    "char_start_offset": 3566,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 104
                        },
                        {
                            "start": 105,
                            "end": 172
                        },
                        {
                            "start": 173,
                            "end": 359
                        },
                        {
                            "start": 360,
                            "end": 485
                        },
                        {
                            "start": 488,
                            "end": 573
                        },
                        {
                            "start": 576,
                            "end": 810
                        },
                        {
                            "start": 811,
                            "end": 933
                        },
                        {
                            "start": 936,
                            "end": 1060
                        },
                        {
                            "start": 1061,
                            "end": 1193
                        },
                        {
                            "start": 1194,
                            "end": 1324
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.912109375
                }
            ],
            "relevance_judgement": 0.912109375,
            "relevance_judgment_input_expanded": "# Title: Curriculum Learning for Domain Adaptation in Neural Machine Translation\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Xuan Zhang, Pamela Shapiro, Manish Kumar, Paul McNamee, Marine Carpuat, Kevin Duh\n## Abstract\nWe introduce a curriculum learning approach to adapt generic neural machine translation models to a specific domain. Samples are grouped by their similarities to the domain of interest and each group is fed to the training algorithm with a particular schedule. This approach is simple to implement on top of any neural framework or architecture, and consistently outperforms both unadapted and adapted baselines in experiments with two distinct domains and two language pairs.\n## Domain Similarity Scoring\nWe adopt similarity metrics from prior work on data selection to score examples for curriculum learning. Let I be an in-domain corpus, and N be a unlabeled-domain data set. Data selection models rank sentences in N according to a domain similarity measure with respect to I, and choose top n samples from N by a cut-off threshold for further training purpose. We examine two data selection methods, Moore-Lewis method (Moore and Lewis, 2010) and cynical data selection (Axelrod, 2017). \n\nMoore-Lewis Method Each sentence s in N is assigned a cross-entropy difference score, \n\nwhere H I (s) is the per-word cross-entropy of s according to a language model trained on I, and H N (s) is the per-word cross-entropy of s according to a language model trained on a random sample of N with roughly the same size as I. A lower cross-entropy difference indicates that s is more like the in-domain data and less like the unlabeled-domain data. \n\nCynical Data Selection Iteratively select sentence s from N to construct a training corpus that would approximately model I. At each iteration, each sentence is scored by the expected cross-entropy change from adding it to the already selected subset of N . The selected sentence is the one which most decreases H n , the cross-entropy between previously selected n-sentence corpus and I.",
            "reference_string": "[155089817 | Zhang et al. | 2019 | Citations: 124]"
        },
        {
            "title": "Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 41,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.15950, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2316558829",
                    "name": "Lior Belenki"
                },
                {
                    "authorId": "2274120058",
                    "name": "Alekh Agarwal"
                },
                {
                    "authorId": "2346974785",
                    "name": "Tianze Shi"
                },
                {
                    "authorId": "2288931206",
                    "name": "Kristina Toutanova"
                }
            ],
            "abstract": "We propose a method to optimize language model pre-training data mixtures through efficient approximation of the cross-entropy loss corresponding to each candidate mixture via a Mixture of Data Experts (MDE). We use this approximation as a source of additional features in a regression model, trained from observations of model loss for a small number of mixtures. Experiments with Transformer decoder-only language models in the range of 70M to 1B parameters on the SlimPajama dataset show that our method achieves significantly better performance than approaches that train regression models using only the mixture rates as input features. Combining this improved optimization method with an objective that takes into account cross-entropy on end task data leads to superior performance on few-shot downstream evaluations. We also provide theoretical insights on why aggregation of data expert predictions can provide good approximations to model losses for data mixtures.",
            "corpus_id": 276575595,
            "sentences": [
                {
                    "corpus_id": "276575595",
                    "title": "Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert Models",
                    "text": "There is an extensive body of work on data selection and mixture optimization for pretraining language models. Albalak et al. (2024) offer a comprehensive recent survey. Approaches for data selection and cleaning consider different granularities of data, such as token-level, sample-level (individual documents or sentences can be selected or weighted), and group-level (where we consider samples in large groups assumed to have common characteristics, often derived from meta-data such as the web domain (like Wikipedia) or source collection name (such as C4). \n\nClosest to our focus is work selecting or sampling data at the level of large sample groups, often termed domains. We will limit our overview to methods optimizing such group-level data mixture sampling rates. Data mixture sampling rates can be static over the course of model training, or dynamic, forming a curriculum over sampling rates which could for example facilitate faster progress through learning easier skills first. Dynamic mix-tures for pre-training have been considered in e.g. (Albalak et al., 2023;Piergiovanni et al., 2023); we focus on static mixtures in this work. \n\nOnline optimization of domain mixture rates through proxies DOGE (Fan et al., 2024) presents an efficient method to optimize data mixture rates through a first-order bi-level optimization approach. DOGE showed successful optimization of the average hedlout domain loss through proxies of size 124M parameters and smaller, with compute cost 2x the cost of training a single proxy model. Our approach is simpler to implement as it does not require changes in the training algorithm for language models, and also offers the possibility to derive optimal weights for a set of different criteria while reusing the same proxy models. DOREMI (Xie et al., 2023) also proposes an online method which optimizes a loss derived from training data, and has similar computational requirements to those of DOGE. For comparison, we train full-scale models with mixture rates optimized through DOGE and DOREMI and report results in Section 4.",
                    "score": 0.43587983836893207,
                    "section_title": "Related work",
                    "char_start_offset": 5936,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 110
                        },
                        {
                            "start": 111,
                            "end": 169
                        },
                        {
                            "start": 170,
                            "end": 561
                        },
                        {
                            "start": 564,
                            "end": 678
                        },
                        {
                            "start": 679,
                            "end": 773
                        },
                        {
                            "start": 774,
                            "end": 992
                        },
                        {
                            "start": 993,
                            "end": 1056
                        },
                        {
                            "start": 1057,
                            "end": 1148
                        },
                        {
                            "start": 1151,
                            "end": 1348
                        },
                        {
                            "start": 1349,
                            "end": 1536
                        },
                        {
                            "start": 1537,
                            "end": 1778
                        },
                        {
                            "start": 1779,
                            "end": 1947
                        },
                        {
                            "start": 1948,
                            "end": 2076
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1216,
                            "end": 1234,
                            "matchedPaperCorpusId": "264439382"
                        },
                        {
                            "start": 1786,
                            "end": 1804,
                            "matchedPaperCorpusId": "258741043"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90380859375
                }
            ],
            "relevance_judgement": 0.90380859375,
            "relevance_judgment_input_expanded": "# Title: Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert Models\n# Venue: arXiv.org\n# Authors: Lior Belenki, Alekh Agarwal, Tianze Shi, Kristina Toutanova\n## Abstract\nWe propose a method to optimize language model pre-training data mixtures through efficient approximation of the cross-entropy loss corresponding to each candidate mixture via a Mixture of Data Experts (MDE). We use this approximation as a source of additional features in a regression model, trained from observations of model loss for a small number of mixtures. Experiments with Transformer decoder-only language models in the range of 70M to 1B parameters on the SlimPajama dataset show that our method achieves significantly better performance than approaches that train regression models using only the mixture rates as input features. Combining this improved optimization method with an objective that takes into account cross-entropy on end task data leads to superior performance on few-shot downstream evaluations. We also provide theoretical insights on why aggregation of data expert predictions can provide good approximations to model losses for data mixtures.\n## Related work\nThere is an extensive body of work on data selection and mixture optimization for pretraining language models. Albalak et al. (2024) offer a comprehensive recent survey. Approaches for data selection and cleaning consider different granularities of data, such as token-level, sample-level (individual documents or sentences can be selected or weighted), and group-level (where we consider samples in large groups assumed to have common characteristics, often derived from meta-data such as the web domain (like Wikipedia) or source collection name (such as C4). \n\nClosest to our focus is work selecting or sampling data at the level of large sample groups, often termed domains. We will limit our overview to methods optimizing such group-level data mixture sampling rates. Data mixture sampling rates can be static over the course of model training, or dynamic, forming a curriculum over sampling rates which could for example facilitate faster progress through learning easier skills first. Dynamic mix-tures for pre-training have been considered in e.g. (Albalak et al., 2023;Piergiovanni et al., 2023); we focus on static mixtures in this work. \n\nOnline optimization of domain mixture rates through proxies DOGE (Fan et al., 2024) presents an efficient method to optimize data mixture rates through a first-order bi-level optimization approach. DOGE showed successful optimization of the average hedlout domain loss through proxies of size 124M parameters and smaller, with compute cost 2x the cost of training a single proxy model. Our approach is simpler to implement as it does not require changes in the training algorithm for language models, and also offers the possibility to derive optimal weights for a set of different criteria while reusing the same proxy models. DOREMI (Xie et al., 2023) also proposes an online method which optimizes a loss derived from training data, and has similar computational requirements to those of DOGE. For comparison, we train full-scale models with mixture rates optimized through DOGE and DOREMI and report results in Section 4.",
            "reference_string": "[276575595 | Belenki et al. | 2025 | Citations: 0]"
        },
        {
            "title": "The Trade-offs of Domain Adaptation for Neural Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2021,
            "reference_count": 52,
            "citation_count": 22,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.acl-long.264.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2109.10274, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3310951",
                    "name": "Dan Iter"
                },
                {
                    "authorId": "2529182",
                    "name": "David Grangier"
                }
            ],
            "abstract": "This work connects language model adaptation with concepts of machine learning theory. We consider a training setup with a large out-of-domain set and a small in-domain set. We derive how the benefit of training a model on either set depends on the size of the sets and the distance between their underlying distributions. We analyze how out-of-domain pre-training before in-domain fine-tuning achieves better generalization than either solution independently. Finally, we present how adaptation techniques based on data selection, such as importance sampling, intelligent data selection and influence functions, can be presented in a common framework which highlights their similarity and also their subtle differences.",
            "corpus_id": 237581256,
            "sentences": [
                {
                    "corpus_id": "237581256",
                    "title": "The Trade-offs of Domain Adaptation for Neural Language Models",
                    "text": "Intelligent selection (Moore and Lewis, 2010; Axelrod et al., 2011) and contrastive data selection, its extension to neural networks (van der Wees et al., 2017;Wang et al., 2018), have been introduced in the language modeling literature. We show that these methods are closely related to importance sampling, even if their original papers does not mention this link.\n\nIntelligent selection selects training samples from an out-of-domain dataset according to the log-odd between an in-domain language model and an out-of-domain language model. Typically, a binary decision is taken per sentence by comparing the average log-odd to a threshold \u03c4 ,\n\nCompared to importance sampling, the weights are binarized, i.e.\n\nb IntSel D,T (y) = I log w imp D,T (y) > \u03c4 .\n\nThe binarization decision was certainly driven by convenience as most n-gram language models training packages did not support weighted likelihood optimization when intelligent selection was introduced. Binarization also has the advantage of down-weighting extreme positive weight values from large log P (y|\u03b8 T ) due to over-fitting on the small set T . More recently, intelligent selection has been extended to neural models (van der Wees et al., 2017;Wang et al., 2018). Contrastive data selection (Wang et al., 2018) suggests to fine tune the in-domain model log P (y|\u03b8 T ) from logP (y|\u03b8 D ) and also observes that selection scores can efficiently be estimated from a model with a much smaller capacity than the final trained model.\n\nDynamic selection (van der Wees et al., 2017) proposes to increase the selection threshold \u03c4 t as training progresses, gradually transitioning from generic to in-domain training. This gradual adaptation of neural network is related to curriculum learning (Bengio et al., 2009) which studies the ordering of examples and tasks during model training.\n\nIntelligent selection methods have been applied both for unconditional models (language modeling) and conditional models (machine translation). In the conditional case, intelligent selection computes\n\nThis ratio of conditional probabilities is different from the ratio of join probabilities stemming from importance sampling, i.e.\n\nThe two ratio matches when P (x|T ) = P (x",
                    "score": 0.5417362610666266,
                    "section_title": "Intelligent Selection",
                    "char_start_offset": 19062,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 46,
                            "end": 67,
                            "matchedPaperCorpusId": "10766958"
                        },
                        {
                            "start": 133,
                            "end": 160,
                            "matchedPaperCorpusId": "7921428"
                        },
                        {
                            "start": 160,
                            "end": 178,
                            "matchedPaperCorpusId": "52156056"
                        },
                        {
                            "start": 1186,
                            "end": 1213,
                            "matchedPaperCorpusId": "7921428"
                        },
                        {
                            "start": 1213,
                            "end": 1231,
                            "matchedPaperCorpusId": "52156056"
                        },
                        {
                            "start": 1260,
                            "end": 1279,
                            "matchedPaperCorpusId": "52156056"
                        },
                        {
                            "start": 1516,
                            "end": 1543,
                            "matchedPaperCorpusId": "7921428"
                        },
                        {
                            "start": 1753,
                            "end": 1774,
                            "matchedPaperCorpusId": "873046"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9013671875
                }
            ],
            "relevance_judgement": 0.9013671875,
            "relevance_judgment_input_expanded": "# Title: The Trade-offs of Domain Adaptation for Neural Language Models\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Dan Iter, David Grangier\n## Abstract\nThis work connects language model adaptation with concepts of machine learning theory. We consider a training setup with a large out-of-domain set and a small in-domain set. We derive how the benefit of training a model on either set depends on the size of the sets and the distance between their underlying distributions. We analyze how out-of-domain pre-training before in-domain fine-tuning achieves better generalization than either solution independently. Finally, we present how adaptation techniques based on data selection, such as importance sampling, intelligent data selection and influence functions, can be presented in a common framework which highlights their similarity and also their subtle differences.\n## Intelligent Selection\nIntelligent selection (Moore and Lewis, 2010; Axelrod et al., 2011) and contrastive data selection, its extension to neural networks (van der Wees et al., 2017;Wang et al., 2018), have been introduced in the language modeling literature. We show that these methods are closely related to importance sampling, even if their original papers does not mention this link.\n\nIntelligent selection selects training samples from an out-of-domain dataset according to the log-odd between an in-domain language model and an out-of-domain language model. Typically, a binary decision is taken per sentence by comparing the average log-odd to a threshold \u03c4 ,\n\nCompared to importance sampling, the weights are binarized, i.e.\n\nb IntSel D,T (y) = I log w imp D,T (y) > \u03c4 .\n\nThe binarization decision was certainly driven by convenience as most n-gram language models training packages did not support weighted likelihood optimization when intelligent selection was introduced. Binarization also has the advantage of down-weighting extreme positive weight values from large log P (y|\u03b8 T ) due to over-fitting on the small set T . More recently, intelligent selection has been extended to neural models (van der Wees et al., 2017;Wang et al., 2018). Contrastive data selection (Wang et al., 2018) suggests to fine tune the in-domain model log P (y|\u03b8 T ) from logP (y|\u03b8 D ) and also observes that selection scores can efficiently be estimated from a model with a much smaller capacity than the final trained model.\n\nDynamic selection (van der Wees et al., 2017) proposes to increase the selection threshold \u03c4 t as training progresses, gradually transitioning from generic to in-domain training. This gradual adaptation of neural network is related to curriculum learning (Bengio et al., 2009) which studies the ordering of examples and tasks during model training.\n\nIntelligent selection methods have been applied both for unconditional models (language modeling) and conditional models (machine translation). In the conditional case, intelligent selection computes\n\nThis ratio of conditional probabilities is different from the ratio of join probabilities stemming from importance sampling, i.e.\n\nThe two ratio matches when P (x|T ) = P (x",
            "reference_string": "[237581256 | Iter et al. | 2021 | Citations: 22]"
        },
        {
            "title": "MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 66,
            "citation_count": 37,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.06046, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2275526493",
                    "name": "Zichun Yu"
                },
                {
                    "authorId": "2305649905",
                    "name": "Spandan Das"
                },
                {
                    "authorId": "2275170116",
                    "name": "Chenyan Xiong"
                }
            ],
            "abstract": "Pretraining data selection has the potential to improve language model pretraining efficiency by utilizing higher-quality data from massive web data corpora. Current data selection methods, which rely on either hand-crafted rules or larger reference models, are conducted statically and do not capture the evolving data preferences during pretraining. In this paper, we introduce model-aware data selection with data influence models (MATES), where a data influence model continuously adapts to the evolving data preferences of the pretraining model and then selects the data most effective for the current pretraining progress. Specifically, we collect oracle data influence by locally probing the pretraining model and fine-tune a small data influence model to approximate it accurately. The data influence model then predicts data influence over the whole pretraining corpus and selects the most influential data for the next pretraining stage. Experiments of pretraining 410M and 1B models on the C4 dataset demonstrate that MATES significantly outperforms random data selection on extensive downstream tasks. It doubles the gains achieved by the state-of-the-art data selection approach that leverages larger reference models and reduces the total FLOPs required to reach certain performances by half. Further analyses validate the effectiveness of the locally probed oracle data influence and the approximation with data influence models. Our code is open-sourced at https://github.com/cxcscmu/MATES.",
            "corpus_id": 270371045,
            "sentences": [
                {
                    "corpus_id": "270371045",
                    "title": "MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models",
                    "text": "Nevertheless, the computational cost remains prohibitive for model-aware data selection, which requires tracking the evolving data preferences of pretraining models on the fly. \n\nOn the other hand, many researchers have proposed curriculum learning strategies that dynamically adjust the data distribution in the pretraining [11; 24; 47; 51; 56; 62]. ELECTRA-style models [11] incorporated curriculum learning in their pretraining process by synchronously training the model with an auxiliary generator, which provided more and more difficult training signals for the discriminator. This implicit curriculum significantly improved the pretraining efficiency on denoising language models [4; 11; 19; 39; 65]. Other methods have explicitly designed the curriculum for pretraining data selection, such as decreasing gradient norms [56], least certainty [24; 51], and increasing expertise [62], demonstrating the benefits of adapting the pretraining data signal according to the model's ever-changing preferences.",
                    "score": 0.4829560418233687,
                    "section_title": "Introduction",
                    "char_start_offset": 5964,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 176
                        },
                        {
                            "start": 179,
                            "end": 350
                        },
                        {
                            "start": 351,
                            "end": 582
                        },
                        {
                            "start": 583,
                            "end": 707
                        },
                        {
                            "start": 708,
                            "end": 1009
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 372,
                            "end": 376,
                            "matchedPaperCorpusId": "208229926"
                        },
                        {
                            "start": 828,
                            "end": 832,
                            "matchedPaperCorpusId": "264935129"
                        },
                        {
                            "start": 885,
                            "end": 889,
                            "matchedPaperCorpusId": "267681974"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8916015625
                }
            ],
            "relevance_judgement": 0.8916015625,
            "relevance_judgment_input_expanded": "# Title: MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models\n# Venue: Neural Information Processing Systems\n# Authors: Zichun Yu, Spandan Das, Chenyan Xiong\n## Abstract\nPretraining data selection has the potential to improve language model pretraining efficiency by utilizing higher-quality data from massive web data corpora. Current data selection methods, which rely on either hand-crafted rules or larger reference models, are conducted statically and do not capture the evolving data preferences during pretraining. In this paper, we introduce model-aware data selection with data influence models (MATES), where a data influence model continuously adapts to the evolving data preferences of the pretraining model and then selects the data most effective for the current pretraining progress. Specifically, we collect oracle data influence by locally probing the pretraining model and fine-tune a small data influence model to approximate it accurately. The data influence model then predicts data influence over the whole pretraining corpus and selects the most influential data for the next pretraining stage. Experiments of pretraining 410M and 1B models on the C4 dataset demonstrate that MATES significantly outperforms random data selection on extensive downstream tasks. It doubles the gains achieved by the state-of-the-art data selection approach that leverages larger reference models and reduces the total FLOPs required to reach certain performances by half. Further analyses validate the effectiveness of the locally probed oracle data influence and the approximation with data influence models. Our code is open-sourced at https://github.com/cxcscmu/MATES.\n## Introduction\nNevertheless, the computational cost remains prohibitive for model-aware data selection, which requires tracking the evolving data preferences of pretraining models on the fly. \n\nOn the other hand, many researchers have proposed curriculum learning strategies that dynamically adjust the data distribution in the pretraining [11; 24; 47; 51; 56; 62]. ELECTRA-style models [11] incorporated curriculum learning in their pretraining process by synchronously training the model with an auxiliary generator, which provided more and more difficult training signals for the discriminator. This implicit curriculum significantly improved the pretraining efficiency on denoising language models [4; 11; 19; 39; 65]. Other methods have explicitly designed the curriculum for pretraining data selection, such as decreasing gradient norms [56], least certainty [24; 51], and increasing expertise [62], demonstrating the benefits of adapting the pretraining data signal according to the model's ever-changing preferences.",
            "reference_string": "[270371045 | Yu et al. | 2024 | Citations: 37]"
        },
        {
            "title": "DoGE: Domain Reweighting with Generalization Estimation",
            "venue": "International Conference on Machine Learning",
            "year": 2023,
            "reference_count": 41,
            "citation_count": 43,
            "influential_citation_count": 7,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.15393, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261456109",
                    "name": "Simin Fan"
                },
                {
                    "authorId": "2435537",
                    "name": "Matteo Pagliardini"
                },
                {
                    "authorId": "2256984280",
                    "name": "Martin Jaggi"
                }
            ],
            "abstract": "The coverage and composition of the pretraining data significantly impacts the generalization ability of Large Language Models (LLMs). Despite its importance, recent LLMs still rely on heuristics and trial and error to increase or reduce the influence of data-domains. We propose DOmain reweighting with Generalization Estimation (DoGE), which optimizes the probability of sampling from each domain (domain weights) in a principled way. Our approach is a two-stage process consisting of (i) training a proxy model to obtain domain weights using a bi-level optimization algorithm; (ii) training a larger base model by sampling training domains according to the learned domain weights. In our experiments, we extensively show how DoGE improves the generalization of the base model to any target data mixture. On the SlimPajama dataset, our base model gets better perplexity and few-shot reasoning accuracies across $6$ tasks compared to baseline methods. Moreover, aiming to generalize to out-of-domain target tasks, which is unseen in the pretraining corpus (OOD domain), DoGE can effectively identify inter-domain dependencies, and consistently achieves better test perplexity on the target domain.",
            "corpus_id": 264439382,
            "sentences": [
                {
                    "corpus_id": "264439382",
                    "title": "DoGE: Domain Reweighting with Generalization Estimation",
                    "text": "Data Selection for Language Modeling. Many works show how a rigorously selected training corpus can effectively   However, due to scalability issues, most traditional data selection methods fail to be applicable for pretraining. \n\nClassifier-based data filtering techniques are commonly used to construct a pretraining corpus (Gao et al., 2020;Penedo et al., 2023). Everaert & Potts (2023) propose GIO to select a subset that minimizes the KL-divergence to the target distribution, yet incurs high computation complexity. 2023) propose to build an online resampling curriculum by exploiting the dependency relationship among skills represented by a directed skill graph. While the computation cost for constructing the skill graph limits its applicability to general language model pretraining.",
                    "score": 0.41045944639441756,
                    "section_title": "Related Work",
                    "char_start_offset": 21997,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 37
                        },
                        {
                            "start": 38,
                            "end": 228
                        },
                        {
                            "start": 231,
                            "end": 365
                        },
                        {
                            "start": 366,
                            "end": 521
                        },
                        {
                            "start": 522,
                            "end": 670
                        },
                        {
                            "start": 671,
                            "end": 794
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88818359375
                }
            ],
            "relevance_judgement": 0.88818359375,
            "relevance_judgment_input_expanded": "# Title: DoGE: Domain Reweighting with Generalization Estimation\n# Venue: International Conference on Machine Learning\n# Authors: Simin Fan, Matteo Pagliardini, Martin Jaggi\n## Abstract\nThe coverage and composition of the pretraining data significantly impacts the generalization ability of Large Language Models (LLMs). Despite its importance, recent LLMs still rely on heuristics and trial and error to increase or reduce the influence of data-domains. We propose DOmain reweighting with Generalization Estimation (DoGE), which optimizes the probability of sampling from each domain (domain weights) in a principled way. Our approach is a two-stage process consisting of (i) training a proxy model to obtain domain weights using a bi-level optimization algorithm; (ii) training a larger base model by sampling training domains according to the learned domain weights. In our experiments, we extensively show how DoGE improves the generalization of the base model to any target data mixture. On the SlimPajama dataset, our base model gets better perplexity and few-shot reasoning accuracies across $6$ tasks compared to baseline methods. Moreover, aiming to generalize to out-of-domain target tasks, which is unseen in the pretraining corpus (OOD domain), DoGE can effectively identify inter-domain dependencies, and consistently achieves better test perplexity on the target domain.\n## Related Work\nData Selection for Language Modeling. Many works show how a rigorously selected training corpus can effectively   However, due to scalability issues, most traditional data selection methods fail to be applicable for pretraining. \n\nClassifier-based data filtering techniques are commonly used to construct a pretraining corpus (Gao et al., 2020;Penedo et al., 2023). Everaert & Potts (2023) propose GIO to select a subset that minimizes the KL-divergence to the target distribution, yet incurs high computation complexity. 2023) propose to build an online resampling curriculum by exploiting the dependency relationship among skills represented by a directed skill graph. While the computation cost for constructing the skill graph limits its applicability to general language model pretraining.",
            "reference_string": "[264439382 | Fan et al. | 2023 | Citations: 43]"
        },
        {
            "title": "Irreducible Curriculum for Language Model Pretraining",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 19,
            "citation_count": 11,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.15389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261456109",
                    "name": "Simin Fan"
                },
                {
                    "authorId": "2256984280",
                    "name": "Martin Jaggi"
                }
            ],
            "abstract": "Automatic data selection and curriculum design for training large language models is challenging, with only a few existing methods showing improvements over standard training. Furthermore, current schemes focus on domain-level selection, overlooking the more fine-grained contributions of each individual training point. It is difficult to apply traditional datapoint selection methods on large language models: most online batch selection methods perform two-times forward or backward passes, which introduces considerable extra costs with large-scale models. To mitigate these obstacles, we propose irreducible curriculum as a curriculum learning algorithm for language model pretraining, which prioritizes samples with higher learnability. Specifically, to avoid prohibitive extra computation overhead, we simulate the sample loss along the main model's training trajectory using a small-scale proxy model. Our experiments on the RedPajama-1B dataset demonstrate a consistent improvement on validation perplexity across all 7 domains compared to random uniform baseline and the anti-curriculum strategy. Our method also reduces the sharpness of the network and illustrates a better 5-shot accuracy on MMLU benchmarks.",
            "corpus_id": 264439064,
            "sentences": [
                {
                    "corpus_id": "264439064",
                    "title": "Irreducible Curriculum for Language Model Pretraining",
                    "text": "Automatic data selection and curriculum design for training large language models is challenging, with only a few existing methods showing improvements over standard training. Furthermore, current schemes focus on domain-level selection, overlooking the more fine-grained contributions of each individual training point. It is difficult to apply traditional datapoint selection methods on large language models: most online batch selection methods perform two-times forward or backward passes, which introduces considerable extra costs with large-scale models. To mitigate these obstacles, we propose irreducible curriculum as a curriculum learning algorithm for language model pretraining, which prioritizes samples with higher learnability. Specifically, to avoid prohibitive extra computation overhead, we simulate the sample loss along the main model's training trajectory using a small-scale proxy model. Our experiments on the RedPajama-1B dataset demonstrate a consistent improvement on validation perplexity across all 7 domains compared to random uniform baseline and the anti-curriculum strategy. Our method also reduces the sharpness of the network and illustrates a better 5-shot accuracy on MMLU benchmarks.",
                    "score": 0.6515444245883804,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88525390625
                },
                {
                    "corpus_id": "264439064",
                    "title": "Irreducible Curriculum for Language Model Pretraining",
                    "text": "The evolution of language models demonstrates impressive generation and reasoning abilities from a continuous scaling-up of model size and training corpus [Brown et al., 2020, Chowdhery et al., 2022, Kaplan et al., 2020, Hoffmann et al., 2022], along with very significant growth in computation cost. Besides the quantity [Kaplan et al., 2020] and quality [Lee et al., 2022, Longpre et al., 2023] of training data, several recent works show that the ordering [Chen et al., 2023] and composition [Xie et al., 2023] of data during the training can highly impact performance and efficiency. Xie et al. [2023] proposes DOREMI, which determines the optimal domain mixture to construct the pretraining corpus using auxiliary models; Chen et al. [2023] introduces an online selection scheme, which seeks to dynamically update the mixture of data from each skill-set at each training step. While DOREMI and SKILL-IT demonstrates great downstream performance, their curriculum are built upon the group-level where all the instances within one group (domain/skill) shares the same sampling probability, without looking inside each domains on the sample-specific attributions. \n\nNevertheless, few research have succeeded applying traditional sample-level selection schemes for large langauge model pretraining. Online batch selection methods [Loshchilov and Hutter, 2015, Katharopoulos and Fleuret, 2018, Jiang et al., 2019, Schaul et al., 2015] select hard samples with high loss or high gradient norm, which require a second forward/backward pass. That introduces large extra computation costs when the model size is large, which hurts the scalability. On the other side, Campos [2021] shows that the linguistic-based curriculum learning failed to improve on causal language model pretraining. Schaul et al. [2015] introduces RHO-LOSS, which is an online batch selection scheme based on the gap between current training loss and an irreducible loss term Preprint. Under review.",
                    "score": 0.5237703684655631,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 300
                        },
                        {
                            "start": 301,
                            "end": 587
                        },
                        {
                            "start": 588,
                            "end": 881
                        },
                        {
                            "start": 882,
                            "end": 1165
                        },
                        {
                            "start": 1168,
                            "end": 1299
                        },
                        {
                            "start": 1300,
                            "end": 1538
                        },
                        {
                            "start": 1539,
                            "end": 1643
                        },
                        {
                            "start": 1644,
                            "end": 1784
                        },
                        {
                            "start": 1785,
                            "end": 1954
                        },
                        {
                            "start": 1955,
                            "end": 1968
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 155,
                            "end": 174,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1359,
                            "end": 1392,
                            "matchedPaperCorpusId": "3663876"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8525390625
                }
            ],
            "relevance_judgement": 0.88525390625,
            "relevance_judgment_input_expanded": "# Title: Irreducible Curriculum for Language Model Pretraining\n# Venue: arXiv.org\n# Authors: Simin Fan, Martin Jaggi\n## Abstract\nAutomatic data selection and curriculum design for training large language models is challenging, with only a few existing methods showing improvements over standard training. Furthermore, current schemes focus on domain-level selection, overlooking the more fine-grained contributions of each individual training point. It is difficult to apply traditional datapoint selection methods on large language models: most online batch selection methods perform two-times forward or backward passes, which introduces considerable extra costs with large-scale models. To mitigate these obstacles, we propose irreducible curriculum as a curriculum learning algorithm for language model pretraining, which prioritizes samples with higher learnability. Specifically, to avoid prohibitive extra computation overhead, we simulate the sample loss along the main model's training trajectory using a small-scale proxy model. Our experiments on the RedPajama-1B dataset demonstrate a consistent improvement on validation perplexity across all 7 domains compared to random uniform baseline and the anti-curriculum strategy. Our method also reduces the sharpness of the network and illustrates a better 5-shot accuracy on MMLU benchmarks.\n## Introduction\nThe evolution of language models demonstrates impressive generation and reasoning abilities from a continuous scaling-up of model size and training corpus [Brown et al., 2020, Chowdhery et al., 2022, Kaplan et al., 2020, Hoffmann et al., 2022], along with very significant growth in computation cost. Besides the quantity [Kaplan et al., 2020] and quality [Lee et al., 2022, Longpre et al., 2023] of training data, several recent works show that the ordering [Chen et al., 2023] and composition [Xie et al., 2023] of data during the training can highly impact performance and efficiency. Xie et al. [2023] proposes DOREMI, which determines the optimal domain mixture to construct the pretraining corpus using auxiliary models; Chen et al. [2023] introduces an online selection scheme, which seeks to dynamically update the mixture of data from each skill-set at each training step. While DOREMI and SKILL-IT demonstrates great downstream performance, their curriculum are built upon the group-level where all the instances within one group (domain/skill) shares the same sampling probability, without looking inside each domains on the sample-specific attributions. \n\nNevertheless, few research have succeeded applying traditional sample-level selection schemes for large langauge model pretraining. Online batch selection methods [Loshchilov and Hutter, 2015, Katharopoulos and Fleuret, 2018, Jiang et al., 2019, Schaul et al., 2015] select hard samples with high loss or high gradient norm, which require a second forward/backward pass. That introduces large extra computation costs when the model size is large, which hurts the scalability. On the other side, Campos [2021] shows that the linguistic-based curriculum learning failed to improve on causal language model pretraining. Schaul et al. [2015] introduces RHO-LOSS, which is an online batch selection scheme based on the gap between current training loss and an irreducible loss term Preprint. Under review.",
            "reference_string": "[264439064 | Fan et al. | 2023 | Citations: 11]"
        },
        {
            "title": "Learning to select data for transfer learning with Bayesian Optimization",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2017,
            "reference_count": 52,
            "citation_count": 186,
            "influential_citation_count": 16,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D17-1038.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1707.05246, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2884561",
                    "name": "Sebastian Ruder"
                },
                {
                    "authorId": "2022124",
                    "name": "Barbara Plank"
                }
            ],
            "abstract": "Domain similarity measures can be used to gauge adaptability and select suitable data for transfer learning, but existing approaches define ad hoc measures that are deemed suitable for respective tasks. Inspired by work on curriculum learning, we propose to learn data selection measures using Bayesian Optimization and evaluate them across models, domains and tasks. Our learned measures outperform existing domain similarity measures significantly on three tasks: sentiment analysis, part-of-speech tagging, and parsing. We show the importance of complementing similarity with diversity, and that learned measures are\u2013to some degree\u2013transferable across models, domains, and even tasks.",
            "corpus_id": 7403346,
            "sentences": [
                {
                    "corpus_id": "7403346",
                    "title": "Learning to select data for transfer learning with Bayesian Optimization",
                    "text": "Most prior work on data selection for transfer learning focuses on phrase-based machine translation. Typically language models are leveraged via perplexity or cross-entropy scoring to select target data (Moore and Lewis, 2010;Axelrod et al., 2011;Duh et al., 2013;Mirkin and Besacier, 2014). A recent study investigates data selection for neural machine translation (van der Wees et al., 2017). Perplexity was also used to select training data for dependency parsing (S\u00f8gaard, 2011), but has been found to be less suitable for tasks such as sentiment analysis (Ruder et al., 2017). In general, there are fewer studies on data selection for other tasks, e.g., constituent parsing (McClosky et al., 2010), dependency parsing (Plank and van Noord, 2011;S\u00f8gaard, 2011) and sentiment analysis (Remus, 2012). Work on predicting task accuracy is related, but can be seen as complementary (Ravi et al., 2008;Van Asch and Daelemans, 2010). \n\nMany domain similarity metrics have been proposed. Blitzer et al. (2007) show that proxy A distance can be used to measure the adaptability between two domains in order to determine examples for annotation. Van Asch and Daelemans (2010) find that R\u00e9nyi divergence outperforms other metrics in predicting POS tagging accuracy, while Plank and van Noord (2011) observe that topic distribution-based representations with Jensen-Shannon divergence perform best for data selection for parsing. Remus (2012) apply Jensen-Shannon divergence to select training examples for sentiment analysis. Finally, Wu and Huang (2016) propose a similarity metric based on a sentiment graph. We test previously explored similarity metrics and complement them with diversity. \n\nVery recently interest emerged in curriculum learning (Bengio et al., 2009). It is inspired by human active learning by providing easier examples at initial learning stages (e.g., by curriculum strategies such as growing vocabulary size). Curriculum learning employs a range of data metrics, but aims at altering the order in which the entire training data is selected, rather than selecting data.",
                    "score": 0.4266271516981342,
                    "section_title": "Related work",
                    "char_start_offset": 24617,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 100
                        },
                        {
                            "start": 101,
                            "end": 291
                        },
                        {
                            "start": 292,
                            "end": 394
                        },
                        {
                            "start": 395,
                            "end": 581
                        },
                        {
                            "start": 582,
                            "end": 802
                        },
                        {
                            "start": 803,
                            "end": 930
                        },
                        {
                            "start": 933,
                            "end": 983
                        },
                        {
                            "start": 984,
                            "end": 1139
                        },
                        {
                            "start": 1140,
                            "end": 1421
                        },
                        {
                            "start": 1422,
                            "end": 1518
                        },
                        {
                            "start": 1519,
                            "end": 1603
                        },
                        {
                            "start": 1604,
                            "end": 1686
                        },
                        {
                            "start": 1689,
                            "end": 1765
                        },
                        {
                            "start": 1766,
                            "end": 1927
                        },
                        {
                            "start": 1928,
                            "end": 2086
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 203,
                            "end": 226,
                            "matchedPaperCorpusId": "8170227"
                        },
                        {
                            "start": 226,
                            "end": 247,
                            "matchedPaperCorpusId": "10766958"
                        },
                        {
                            "start": 247,
                            "end": 264,
                            "matchedPaperCorpusId": "2030497"
                        },
                        {
                            "start": 264,
                            "end": 290,
                            "matchedPaperCorpusId": "15488580"
                        },
                        {
                            "start": 375,
                            "end": 393,
                            "matchedPaperCorpusId": "7921428"
                        },
                        {
                            "start": 679,
                            "end": 702,
                            "matchedPaperCorpusId": "10585087"
                        },
                        {
                            "start": 723,
                            "end": 750,
                            "matchedPaperCorpusId": "7362580"
                        },
                        {
                            "start": 881,
                            "end": 900,
                            "matchedPaperCorpusId": "9429298"
                        },
                        {
                            "start": 900,
                            "end": 929,
                            "matchedPaperCorpusId": "7986308"
                        },
                        {
                            "start": 1140,
                            "end": 1169,
                            "matchedPaperCorpusId": "7986308"
                        },
                        {
                            "start": 1265,
                            "end": 1291,
                            "matchedPaperCorpusId": "7362580"
                        },
                        {
                            "start": 1528,
                            "end": 1547,
                            "matchedPaperCorpusId": "17859685"
                        },
                        {
                            "start": 1743,
                            "end": 1764,
                            "matchedPaperCorpusId": "873046"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88427734375
                }
            ],
            "relevance_judgement": 0.88427734375,
            "relevance_judgment_input_expanded": "# Title: Learning to select data for transfer learning with Bayesian Optimization\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Sebastian Ruder, Barbara Plank\n## Abstract\nDomain similarity measures can be used to gauge adaptability and select suitable data for transfer learning, but existing approaches define ad hoc measures that are deemed suitable for respective tasks. Inspired by work on curriculum learning, we propose to learn data selection measures using Bayesian Optimization and evaluate them across models, domains and tasks. Our learned measures outperform existing domain similarity measures significantly on three tasks: sentiment analysis, part-of-speech tagging, and parsing. We show the importance of complementing similarity with diversity, and that learned measures are\u2013to some degree\u2013transferable across models, domains, and even tasks.\n## Related work\nMost prior work on data selection for transfer learning focuses on phrase-based machine translation. Typically language models are leveraged via perplexity or cross-entropy scoring to select target data (Moore and Lewis, 2010;Axelrod et al., 2011;Duh et al., 2013;Mirkin and Besacier, 2014). A recent study investigates data selection for neural machine translation (van der Wees et al., 2017). Perplexity was also used to select training data for dependency parsing (S\u00f8gaard, 2011), but has been found to be less suitable for tasks such as sentiment analysis (Ruder et al., 2017). In general, there are fewer studies on data selection for other tasks, e.g., constituent parsing (McClosky et al., 2010), dependency parsing (Plank and van Noord, 2011;S\u00f8gaard, 2011) and sentiment analysis (Remus, 2012). Work on predicting task accuracy is related, but can be seen as complementary (Ravi et al., 2008;Van Asch and Daelemans, 2010). \n\nMany domain similarity metrics have been proposed. Blitzer et al. (2007) show that proxy A distance can be used to measure the adaptability between two domains in order to determine examples for annotation. Van Asch and Daelemans (2010) find that R\u00e9nyi divergence outperforms other metrics in predicting POS tagging accuracy, while Plank and van Noord (2011) observe that topic distribution-based representations with Jensen-Shannon divergence perform best for data selection for parsing. Remus (2012) apply Jensen-Shannon divergence to select training examples for sentiment analysis. Finally, Wu and Huang (2016) propose a similarity metric based on a sentiment graph. We test previously explored similarity metrics and complement them with diversity. \n\nVery recently interest emerged in curriculum learning (Bengio et al., 2009). It is inspired by human active learning by providing easier examples at initial learning stages (e.g., by curriculum strategies such as growing vocabulary size). Curriculum learning employs a range of data metrics, but aims at altering the order in which the entire training data is selected, rather than selecting data.",
            "reference_string": "[7403346 | Ruder et al. | 2017 | Citations: 186]"
        },
        {
            "title": "Learning What Data to Learn",
            "venue": "arXiv.org",
            "year": 2017,
            "reference_count": 37,
            "citation_count": 79,
            "influential_citation_count": 8,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1702.08635, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144566102",
                    "name": "Yang Fan"
                },
                {
                    "authorId": "143853336",
                    "name": "Fei Tian"
                },
                {
                    "authorId": "143826491",
                    "name": "Tao Qin"
                },
                {
                    "authorId": "152441498",
                    "name": "Jiang Bian"
                },
                {
                    "authorId": "2110264337",
                    "name": "Tie-Yan Liu"
                }
            ],
            "abstract": "Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \\emph{\\textbf{N}eural \\textbf{D}ata \\textbf{F}ilter} (\\textbf{NDF}), to explore automatic and adaptive data selection in the training process. In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks. Taking neural network training with stochastic gradient descent (SGD) as an example, comprehensive experiments with respect to various neural network modeling (e.g., multi-layer perceptron networks, convolutional neural networks and recurrent neural networks) and several applications (e.g., image classification and text understanding) demonstrate that NDF powered SGD can achieve comparable accuracy with standard SGD process by using less data and fewer iterations.",
            "corpus_id": 388785,
            "sentences": [
                {
                    "corpus_id": "388785",
                    "title": "Learning What Data to Learn",
                    "text": "Training data plays a critical role in machine learning. The data selection strategy along the training process could significantly impact the performance of the learned model. For example, an appropriate strategy of removing redundant data can lead to a better model by using less computational efforts. For another example, previous studies on curriculum learning (Bengio et al., 2009) and self-paced learning (Kumar et al., 2010) reveal some principles of how tailoring data based on its 'hardness' can favor the model training; that is, easy data instances are important at the early age of model training, while at later age, harder training examples tend to be more effective to improve the model, since easy ones bring minor changes. \n\nThese facts reveal that how to feed training samples into a machine learning system is nontrivial and feeding data in a totally random order is not always a good choice. To explore a better data selection strategy for training, previous works including curriculum learning (CL) and self-paced learning (SPL) adopt simple heuristic rules, such as shuffling the sequence length to train language model (Bengio et al., 2009), or abandoning training instances whose loss values are larger than a human-defined threshold (Kumar et al., 2010;Jiang et al., 2014a). Such human-defined rules are a little restricted to certain tasks and cannot be generalized to broader learning scenarios, since different learning tasks may yield different optimal data selection rules, and even one learning task may need data with various properties to optimize at different training stages. Therefore it remains an open problem how to automatically and dynamically allocate appropriate training data at different stages of machine learning? \n\nTo find a solution to the above problem, we design twofold intuitive principles: on one hand, the data selection strategy should be general enougg, such that it can be naturally applied to different learning scenarios without further particularly human-designed efforts; on the other hand, the strategy should be forward-looking, in that its choice at every step along the training leads to better long-term reward, rather than temporarily fitting to current stage. \n\nFollowing these principles, we propose a new data selection framework, based on deep reinforcement learning (DRL).",
                    "score": 0.46563423949429217,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 56
                        },
                        {
                            "start": 57,
                            "end": 176
                        },
                        {
                            "start": 177,
                            "end": 304
                        },
                        {
                            "start": 305,
                            "end": 740
                        },
                        {
                            "start": 743,
                            "end": 912
                        },
                        {
                            "start": 913,
                            "end": 1300
                        },
                        {
                            "start": 1301,
                            "end": 1611
                        },
                        {
                            "start": 1612,
                            "end": 1761
                        },
                        {
                            "start": 1764,
                            "end": 2229
                        },
                        {
                            "start": 2232,
                            "end": 2346
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 366,
                            "end": 387,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 412,
                            "end": 432,
                            "matchedPaperCorpusId": "1977996"
                        },
                        {
                            "start": 1143,
                            "end": 1164,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 1259,
                            "end": 1279,
                            "matchedPaperCorpusId": "1977996"
                        },
                        {
                            "start": 1279,
                            "end": 1299,
                            "matchedPaperCorpusId": "207216972"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88134765625
                }
            ],
            "relevance_judgement": 0.88134765625,
            "relevance_judgment_input_expanded": "# Title: Learning What Data to Learn\n# Venue: arXiv.org\n# Authors: Yang Fan, Fei Tian, Tao Qin, Jiang Bian, Tie-Yan Liu\n## Abstract\nMachine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \\emph{\\textbf{N}eural \\textbf{D}ata \\textbf{F}ilter} (\\textbf{NDF}), to explore automatic and adaptive data selection in the training process. In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks. Taking neural network training with stochastic gradient descent (SGD) as an example, comprehensive experiments with respect to various neural network modeling (e.g., multi-layer perceptron networks, convolutional neural networks and recurrent neural networks) and several applications (e.g., image classification and text understanding) demonstrate that NDF powered SGD can achieve comparable accuracy with standard SGD process by using less data and fewer iterations.\n## Introduction\nTraining data plays a critical role in machine learning. The data selection strategy along the training process could significantly impact the performance of the learned model. For example, an appropriate strategy of removing redundant data can lead to a better model by using less computational efforts. For another example, previous studies on curriculum learning (Bengio et al., 2009) and self-paced learning (Kumar et al., 2010) reveal some principles of how tailoring data based on its 'hardness' can favor the model training; that is, easy data instances are important at the early age of model training, while at later age, harder training examples tend to be more effective to improve the model, since easy ones bring minor changes. \n\nThese facts reveal that how to feed training samples into a machine learning system is nontrivial and feeding data in a totally random order is not always a good choice. To explore a better data selection strategy for training, previous works including curriculum learning (CL) and self-paced learning (SPL) adopt simple heuristic rules, such as shuffling the sequence length to train language model (Bengio et al., 2009), or abandoning training instances whose loss values are larger than a human-defined threshold (Kumar et al., 2010;Jiang et al., 2014a). Such human-defined rules are a little restricted to certain tasks and cannot be generalized to broader learning scenarios, since different learning tasks may yield different optimal data selection rules, and even one learning task may need data with various properties to optimize at different training stages. Therefore it remains an open problem how to automatically and dynamically allocate appropriate training data at different stages of machine learning? \n\nTo find a solution to the above problem, we design twofold intuitive principles: on one hand, the data selection strategy should be general enougg, such that it can be naturally applied to different learning scenarios without further particularly human-designed efforts; on the other hand, the strategy should be forward-looking, in that its choice at every step along the training leads to better long-term reward, rather than temporarily fitting to current stage. \n\nFollowing these principles, we propose a new data selection framework, based on deep reinforcement learning (DRL).",
            "reference_string": "[388785 | Fan et al. | 2017 | Citations: 79]"
        },
        {
            "title": "Scalable Data Ablation Approximations for Language Models through Modular Training and Merging",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 45,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.15661, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2166313248",
                    "name": "Clara Na"
                },
                {
                    "authorId": "2124977543",
                    "name": "Ian Magnusson"
                },
                {
                    "authorId": "47286118",
                    "name": "A. Jha"
                },
                {
                    "authorId": "20662387",
                    "name": "Tom Sherborne"
                },
                {
                    "authorId": "2268272",
                    "name": "Emma Strubell"
                },
                {
                    "authorId": "2285322061",
                    "name": "Jesse Dodge"
                },
                {
                    "authorId": "2697425",
                    "name": "Pradeep Dasigi"
                }
            ],
            "abstract": "Training data compositions for Large Language Models (LLMs) can significantly affect their downstream performance. However, a thorough data ablation study exploring large sets of candidate data mixtures is typically prohibitively expensive since the full effect is seen only after training the models; this can lead practitioners to settle for sub-optimal data mixtures. We propose an efficient method for approximating data ablations which trains individual models on subsets of a training corpus and reuses them across evaluations of combinations of subsets.In continued pre-training experiments, we find that, given an arbitrary evaluation set, the perplexity score of a single model trained on a candidate set of data is strongly correlated with perplexity scores of parameter averages of models trained on distinct partitions of that data. From this finding, we posit that researchers and practitioners can conduct inexpensive simulations of data ablations by maintaining a pool of models that were each trained on partitions of a large training corpus, and assessing candidate data mixtures by evaluating parameter averages of combinations of these models. This approach allows for substantial improvements in amortized training efficiency \u2013 scaling only linearly with respect to new data \u2013 by enabling reuse of previous training computation, opening new avenues for improving model performance through rigorous, incremental data assessment and mixing.",
            "corpus_id": 273502726,
            "sentences": [
                {
                    "corpus_id": "273502726",
                    "title": "Scalable Data Ablation Approximations for Language Models through Modular Training and Merging",
                    "text": "Our method's efficiency advantage is greater and more straightforward to calculate for the experiments in \u00a75.1, since we sample partitions directly from \"base components\" P \u2032 \u2208 C.  (Dodge et al., 2021;Gao et al., 2020;Computer, 2023;Penedo et al., 2023;Soldaini et al., 2024). The individual components of these mixtures and their relative sizes are typically chosen based on some intrinsic measure of data quality as it is often prohibitively expensive to perform thorough data ablations to create mixtures optimizing for downstream performance. \n\nEfficient data selection Given that data ablations on large language models is expensive, one class of approaches relies on approximating them on smaller models. Relevant work studies scaling laws for model parameters vs training tokens (Hoffmann et al., 2022;Biderman et al., 2023), empirical effects of including or excluding different sources of data (Longpre et al., 2023), and the effects of training over multiple epochs vs new training tokens (Muennighoff et al., 2023). Previous work has also explored improving domain-specific fit via continued pre-training (Gururangan et al., 2020), predicting domain fit using lexical features (Reid et al., 2022), or improving general test-time adaptation via dynamic data selection, either by distributionally robust optimization with a small proxy model (Oren et al., 2019;Xie et al., 2023) or online using a multi-armed bandit approach (Albalak et al., 2023). Additional previous works aim to adapt to known downstream tasks via data selection, including at the individual example level (Wang et al., 2020) or even by explicitly fine-tuning models on many tasks (Aghajanyan et al., 2021). Notable recent and concurrent work proposes scalable influence functions, traditionally used at only very small scales, as a method for selecting better training data (Choe et al., 2024;Yu et al., 2024). Another concurrent preliminary work (Thrush et al., 2024) proposes a training-free method of selecting data to improve performance on downstream tasks.",
                    "score": 0.45392564772692223,
                    "section_title": "Efficiency analysis",
                    "char_start_offset": 26315,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 276
                        },
                        {
                            "start": 277,
                            "end": 546
                        },
                        {
                            "start": 549,
                            "end": 710
                        },
                        {
                            "start": 711,
                            "end": 1026
                        },
                        {
                            "start": 1027,
                            "end": 1457
                        },
                        {
                            "start": 1458,
                            "end": 1686
                        },
                        {
                            "start": 1687,
                            "end": 1890
                        },
                        {
                            "start": 1891,
                            "end": 2042
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 181,
                            "end": 201,
                            "matchedPaperCorpusId": "237568724"
                        },
                        {
                            "start": 233,
                            "end": 253,
                            "matchedPaperCorpusId": "268096300"
                        },
                        {
                            "start": 809,
                            "end": 831,
                            "matchedPaperCorpusId": "257921893"
                        },
                        {
                            "start": 999,
                            "end": 1025,
                            "matchedPaperCorpusId": "258888192"
                        },
                        {
                            "start": 1116,
                            "end": 1141,
                            "matchedPaperCorpusId": "216080466"
                        },
                        {
                            "start": 1188,
                            "end": 1207,
                            "matchedPaperCorpusId": "252907924"
                        },
                        {
                            "start": 1351,
                            "end": 1370,
                            "matchedPaperCorpusId": "202541009"
                        },
                        {
                            "start": 1434,
                            "end": 1456,
                            "matchedPaperCorpusId": "265658930"
                        },
                        {
                            "start": 1660,
                            "end": 1685,
                            "matchedPaperCorpusId": "231718729"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87158203125
                }
            ],
            "relevance_judgement": 0.87158203125,
            "relevance_judgment_input_expanded": "# Title: Scalable Data Ablation Approximations for Language Models through Modular Training and Merging\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Clara Na, Ian Magnusson, A. Jha, Tom Sherborne, Emma Strubell, Jesse Dodge, Pradeep Dasigi\n## Abstract\nTraining data compositions for Large Language Models (LLMs) can significantly affect their downstream performance. However, a thorough data ablation study exploring large sets of candidate data mixtures is typically prohibitively expensive since the full effect is seen only after training the models; this can lead practitioners to settle for sub-optimal data mixtures. We propose an efficient method for approximating data ablations which trains individual models on subsets of a training corpus and reuses them across evaluations of combinations of subsets.In continued pre-training experiments, we find that, given an arbitrary evaluation set, the perplexity score of a single model trained on a candidate set of data is strongly correlated with perplexity scores of parameter averages of models trained on distinct partitions of that data. From this finding, we posit that researchers and practitioners can conduct inexpensive simulations of data ablations by maintaining a pool of models that were each trained on partitions of a large training corpus, and assessing candidate data mixtures by evaluating parameter averages of combinations of these models. This approach allows for substantial improvements in amortized training efficiency \u2013 scaling only linearly with respect to new data \u2013 by enabling reuse of previous training computation, opening new avenues for improving model performance through rigorous, incremental data assessment and mixing.\n## Efficiency analysis\nOur method's efficiency advantage is greater and more straightforward to calculate for the experiments in \u00a75.1, since we sample partitions directly from \"base components\" P \u2032 \u2208 C.  (Dodge et al., 2021;Gao et al., 2020;Computer, 2023;Penedo et al., 2023;Soldaini et al., 2024). The individual components of these mixtures and their relative sizes are typically chosen based on some intrinsic measure of data quality as it is often prohibitively expensive to perform thorough data ablations to create mixtures optimizing for downstream performance. \n\nEfficient data selection Given that data ablations on large language models is expensive, one class of approaches relies on approximating them on smaller models. Relevant work studies scaling laws for model parameters vs training tokens (Hoffmann et al., 2022;Biderman et al., 2023), empirical effects of including or excluding different sources of data (Longpre et al., 2023), and the effects of training over multiple epochs vs new training tokens (Muennighoff et al., 2023). Previous work has also explored improving domain-specific fit via continued pre-training (Gururangan et al., 2020), predicting domain fit using lexical features (Reid et al., 2022), or improving general test-time adaptation via dynamic data selection, either by distributionally robust optimization with a small proxy model (Oren et al., 2019;Xie et al., 2023) or online using a multi-armed bandit approach (Albalak et al., 2023). Additional previous works aim to adapt to known downstream tasks via data selection, including at the individual example level (Wang et al., 2020) or even by explicitly fine-tuning models on many tasks (Aghajanyan et al., 2021). Notable recent and concurrent work proposes scalable influence functions, traditionally used at only very small scales, as a method for selecting better training data (Choe et al., 2024;Yu et al., 2024). Another concurrent preliminary work (Thrush et al., 2024) proposes a training-free method of selecting data to improve performance on downstream tasks.",
            "reference_string": "[273502726 | Na et al. | 2024 | Citations: 5]"
        },
        {
            "title": "SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 72,
            "citation_count": 23,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.07384, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2291083514",
                    "name": "Yu Yang"
                },
                {
                    "authorId": "2290975932",
                    "name": "Siddhartha Mishra"
                },
                {
                    "authorId": "2290912016",
                    "name": "Jeffrey N Chiang"
                },
                {
                    "authorId": "2389094",
                    "name": "Baharan Mirzasoleiman"
                }
            ],
            "abstract": "Despite the effectiveness of data selection for large language models (LLMs) during pretraining and instruction fine-tuning phases, improving data efficiency in supervised fine-tuning (SFT) for specialized domains poses significant challenges due to the complexity of fine-tuning data. To bridge this gap, we introduce an effective and scalable data selection method for SFT, SmallToLarge (S2L), which leverages training trajectories from small models to guide the data selection for larger models. We demonstrate through extensive experiments that S2L significantly improves data efficiency in SFT for mathematical problem-solving, reducing the training data to just 11% of the original MathInstruct dataset (Yue et al., 2023) to match full dataset performance while outperforming state-of-the-art data selection algorithms by an average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably, selecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the most challenging MATH (Hendrycks et al., 2021) benchmark, improving Phi-2 (Li et al., 2023b) by 16.6%. In clinical text summarization on the MIMIC-III dataset (Johnson et al., 2016), S2L again outperforms training on the full dataset using only 50% of the data. Notably, S2L can perform data selection using a reference model 40x smaller than the target model, proportionally reducing the cost of data selection.",
            "corpus_id": 268363364,
            "sentences": [
                {
                    "corpus_id": "268363364",
                    "title": "SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models",
                    "text": "This paper introduces a data selection method for large language models (LLMs), aiming to enhance the data efficiency in the supervised fine-tuning (SFT) of these models.\n\nPositive Impacts: Our method, by reducing the data requirements for training LLMs, can make fine-tuning LLMs more effective and accessible.This could lead to broader participation in AI research and application development across various fields, including healthcare and education.\n\nNegative Impacts: Our method does not inherently involve or encourage applications with direct negative societal impacts.The focus is on a generic improvement in the field of machine learning, particularly in the training of LLMs.\n\nTable 4. Complete results used for selecting the best reference model for each one-shot data selection baseline.The choice of early-finetuning (step 1000) and end-of-fine-tuning checkpoint follows (Marion et al., 2023).The best results selected for range of mathematical disciplines from early algebra to calculus and linear algebra, covering various types of questions such as multi-step reasoning, arithmetic word problems, and problems from mathematics competitions.This variety ensures a comprehensive assessment across both in-domain and out-domain tasks.\n\nPipeline.We utilize the pipeline provided by (Yue et al., 2023), designed to first determine whether the model can be prompted to generate a code snippet.This code snippet, if successfully generated, should be executable and produce the correct answer when run.This code-based evaluation is also used for Phi models (Li et al., 2023b).In cases where the model does not directly produce a viable code solution, we employ a \"think step-by-step\" prompting strategy (Wei et al., 2022b).This method prompts the model to break down its reasoning process, a technique that has been widely proven",
                    "score": 0.4064586197702927,
                    "section_title": "Impact Statements",
                    "char_start_offset": 23937,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 170
                        },
                        {
                            "start": 172,
                            "end": 311
                        },
                        {
                            "start": 311,
                            "end": 453
                        },
                        {
                            "start": 455,
                            "end": 576
                        },
                        {
                            "start": 576,
                            "end": 685
                        },
                        {
                            "start": 687,
                            "end": 799
                        },
                        {
                            "start": 799,
                            "end": 906
                        },
                        {
                            "start": 906,
                            "end": 1156
                        },
                        {
                            "start": 1156,
                            "end": 1247
                        },
                        {
                            "start": 1249,
                            "end": 1258
                        },
                        {
                            "start": 1258,
                            "end": 1403
                        },
                        {
                            "start": 1403,
                            "end": 1510
                        },
                        {
                            "start": 1510,
                            "end": 1584
                        },
                        {
                            "start": 1584,
                            "end": 1731
                        },
                        {
                            "start": 1731,
                            "end": 1837
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1711,
                            "end": 1730,
                            "matchedPaperCorpusId": "246411621"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86669921875
                }
            ],
            "relevance_judgement": 0.86669921875,
            "relevance_judgment_input_expanded": "# Title: SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models\n# Venue: Neural Information Processing Systems\n# Authors: Yu Yang, Siddhartha Mishra, Jeffrey N Chiang, Baharan Mirzasoleiman\n## Abstract\nDespite the effectiveness of data selection for large language models (LLMs) during pretraining and instruction fine-tuning phases, improving data efficiency in supervised fine-tuning (SFT) for specialized domains poses significant challenges due to the complexity of fine-tuning data. To bridge this gap, we introduce an effective and scalable data selection method for SFT, SmallToLarge (S2L), which leverages training trajectories from small models to guide the data selection for larger models. We demonstrate through extensive experiments that S2L significantly improves data efficiency in SFT for mathematical problem-solving, reducing the training data to just 11% of the original MathInstruct dataset (Yue et al., 2023) to match full dataset performance while outperforming state-of-the-art data selection algorithms by an average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably, selecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the most challenging MATH (Hendrycks et al., 2021) benchmark, improving Phi-2 (Li et al., 2023b) by 16.6%. In clinical text summarization on the MIMIC-III dataset (Johnson et al., 2016), S2L again outperforms training on the full dataset using only 50% of the data. Notably, S2L can perform data selection using a reference model 40x smaller than the target model, proportionally reducing the cost of data selection.\n## Impact Statements\nThis paper introduces a data selection method for large language models (LLMs), aiming to enhance the data efficiency in the supervised fine-tuning (SFT) of these models.\n\nPositive Impacts: Our method, by reducing the data requirements for training LLMs, can make fine-tuning LLMs more effective and accessible.This could lead to broader participation in AI research and application development across various fields, including healthcare and education.\n\nNegative Impacts: Our method does not inherently involve or encourage applications with direct negative societal impacts.The focus is on a generic improvement in the field of machine learning, particularly in the training of LLMs.\n\nTable 4. Complete results used for selecting the best reference model for each one-shot data selection baseline.The choice of early-finetuning (step 1000) and end-of-fine-tuning checkpoint follows (Marion et al., 2023).The best results selected for range of mathematical disciplines from early algebra to calculus and linear algebra, covering various types of questions such as multi-step reasoning, arithmetic word problems, and problems from mathematics competitions.This variety ensures a comprehensive assessment across both in-domain and out-domain tasks.\n\nPipeline.We utilize the pipeline provided by (Yue et al., 2023), designed to first determine whether the model can be prompted to generate a code snippet.This code snippet, if successfully generated, should be executable and produce the correct answer when run.This code-based evaluation is also used for Phi models (Li et al., 2023b).In cases where the model does not directly produce a viable code solution, we employ a \"think step-by-step\" prompting strategy (Wei et al., 2022b).This method prompts the model to break down its reasoning process, a technique that has been widely proven",
            "reference_string": "[268363364 | Yang et al. | 2024 | Citations: 23]"
        },
        {
            "title": "Entropy Law: The Story Behind Data Compression and LLM Performance",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 37,
            "citation_count": 27,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.06645, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2260649859",
                    "name": "Mingjia Yin"
                },
                {
                    "authorId": "2276003321",
                    "name": "Chuhan Wu"
                },
                {
                    "authorId": "2283529247",
                    "name": "Yufei Wang"
                },
                {
                    "authorId": "2256768674",
                    "name": "Hao Wang"
                },
                {
                    "authorId": "2260810851",
                    "name": "Wei Guo"
                },
                {
                    "authorId": "2282544603",
                    "name": "Yasheng Wang"
                },
                {
                    "authorId": "2293683997",
                    "name": "Yong Liu"
                },
                {
                    "authorId": "2284295184",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "1862782",
                    "name": "Defu Lian"
                },
                {
                    "authorId": "2113754294",
                    "name": "Enhong Chen"
                }
            ],
            "abstract": "Data is the cornerstone of large language models (LLMs), but not all data is useful for model learning. Carefully selected data can better elicit the capabilities of LLMs with much less computational overhead. Most methods concentrate on evaluating the quality of individual samples in data selection, while the combinatorial effects among samples are neglected. Even if each sample is of perfect quality, their combinations may be suboptimal in teaching LLMs due to their intrinsic homogeneity or contradiction. In this paper, we aim to uncover the underlying relationships between LLM performance and data selection. Inspired by the information compression nature of LLMs, we uncover an ``entropy law'' that connects LLM performance with data compression ratio and first-epoch training loss, which reflect the information redundancy of a dataset and the mastery of inherent knowledge encoded in this dataset, respectively. Through both theoretical deduction and empirical evaluation, we find that model performance is negatively correlated to the compression ratio of training data, which usually yields a lower training loss. Based on the findings of the entropy law, we propose a quite efficient and universal data selection method named \\textbf{ZIP} for training LLMs, which aim to prioritize data subsets exhibiting a low compression ratio. Based on a multi-stage algorithm that selects diverse data in a greedy manner, we can obtain a good data subset with satisfactory diversity. Extensive experiments have been conducted to validate the entropy law and the superiority of ZIP across different LLM backbones and alignment stages. We also present an interesting application of entropy law that can detect potential performance risks at the beginning of model training.",
            "corpus_id": 271064746,
            "sentences": [
                {
                    "corpus_id": "271064746",
                    "title": "Entropy Law: The Story Behind Data Compression and LLM Performance",
                    "text": "Data is the cornerstone of large language models (LLMs), but not all data is useful for model learning. Carefully selected data can better elicit the capabilities of LLMs with much less computational overhead. Most methods concentrate on evaluating the quality of individual samples in data selection, while the combinatorial effects among samples are neglected. Even if each sample is of perfect quality, their combinations may be suboptimal in teaching LLMs due to their intrinsic homogeneity or contradiction. In this paper, we aim to uncover the underlying relationships between LLM performance and data selection. Inspired by the information compression nature of LLMs, we uncover an ``entropy law'' that connects LLM performance with data compression ratio and first-epoch training loss, which reflect the information redundancy of a dataset and the mastery of inherent knowledge encoded in this dataset, respectively. Through both theoretical deduction and empirical evaluation, we find that model performance is negatively correlated to the compression ratio of training data, which usually yields a lower training loss. Based on the findings of the entropy law, we propose a quite efficient and universal data selection method named \\textbf{ZIP} for training LLMs, which aim to prioritize data subsets exhibiting a low compression ratio. Based on a multi-stage algorithm that selects diverse data in a greedy manner, we can obtain a good data subset with satisfactory diversity. Extensive experiments have been conducted to validate the entropy law and the superiority of ZIP across different LLM backbones and alignment stages. We also present an interesting application of entropy law that can detect potential performance risks at the beginning of model training.",
                    "score": 0.4304753103877838,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85693359375
                }
            ],
            "relevance_judgement": 0.85693359375,
            "relevance_judgment_input_expanded": "# Title: Entropy Law: The Story Behind Data Compression and LLM Performance\n# Venue: arXiv.org\n# Authors: Mingjia Yin, Chuhan Wu, Yufei Wang, Hao Wang, Wei Guo, Yasheng Wang, Yong Liu, Ruiming Tang, Defu Lian, Enhong Chen\n## Abstract\nData is the cornerstone of large language models (LLMs), but not all data is useful for model learning. Carefully selected data can better elicit the capabilities of LLMs with much less computational overhead. Most methods concentrate on evaluating the quality of individual samples in data selection, while the combinatorial effects among samples are neglected. Even if each sample is of perfect quality, their combinations may be suboptimal in teaching LLMs due to their intrinsic homogeneity or contradiction. In this paper, we aim to uncover the underlying relationships between LLM performance and data selection. Inspired by the information compression nature of LLMs, we uncover an ``entropy law'' that connects LLM performance with data compression ratio and first-epoch training loss, which reflect the information redundancy of a dataset and the mastery of inherent knowledge encoded in this dataset, respectively. Through both theoretical deduction and empirical evaluation, we find that model performance is negatively correlated to the compression ratio of training data, which usually yields a lower training loss. Based on the findings of the entropy law, we propose a quite efficient and universal data selection method named \\textbf{ZIP} for training LLMs, which aim to prioritize data subsets exhibiting a low compression ratio. Based on a multi-stage algorithm that selects diverse data in a greedy manner, we can obtain a good data subset with satisfactory diversity. Extensive experiments have been conducted to validate the entropy law and the superiority of ZIP across different LLM backbones and alignment stages. We also present an interesting application of entropy law that can detect potential performance risks at the beginning of model training.\n",
            "reference_string": "[271064746 | Yin et al. | 2024 | Citations: 27]"
        },
        {
            "title": "Cold-Start Data Selection for Better Few-shot Language Model Fine-tuning: A Prompt-based Uncertainty Propagation Approach",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 75,
            "citation_count": 23,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.acl-long.141.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-long.141, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1633124736",
                    "name": "Yue Yu"
                },
                {
                    "authorId": "46752897",
                    "name": "Rongzhi Zhang"
                },
                {
                    "authorId": "2115801998",
                    "name": "Ran Xu"
                },
                {
                    "authorId": "47540245",
                    "name": "Jieyu Zhang"
                },
                {
                    "authorId": "3363642",
                    "name": "Jiaming Shen"
                },
                {
                    "authorId": "145657504",
                    "name": "Chao Zhang"
                }
            ],
            "abstract": "We present PATRON, a prompt-based data selection method for pre-trained language model fine-tuning under cold-start scenarios, i.e., no initial labeled data are available. In PATRON, we design (1) a prompt-based uncertainty propagation approach to estimate the importance of data points and (2) a partition-then-rewrite (PTR) strategy to promote sample diversity when querying for annotations. Experiments on six text classification datasets show that PATRON outperforms the strongest cold-start data selection baselines by up to 6.9%. Besides, with 128 labels only, PATRON achieves 91.0% and 92.1% of the fully supervised performance based on vanilla fine-tuning and prompt-based learning respectively. Our implementation of PATRON will be published upon acceptance.",
            "corpus_id": 252280753,
            "sentences": [
                {
                    "corpus_id": "252280753",
                    "title": "Cold-Start Data Selection for Better Few-shot Language Model Fine-tuning: A Prompt-based Uncertainty Propagation Approach",
                    "text": "We study cold-start data selection for text classification with c classes formulated as follows: Given a pool of unlabeled samples D u = {x j } U j=1 and an empty training set D l = \u2205, we aim to fine-tune a pre-trained language model M denoted as f (\u2022; \u03b8) under limited labeling budget |B| interactively: In each round, we use an acquisition function F(\u2022) to query b samples denoted as Q from D u . Next, the acquired samples are labeled and moved from D u to D l . Then we fine-tune the pre-trained language model f (\u2022; \u03b8) with D l to maximize the performance on downstream classification tasks. The above steps can either be one-round (Chang et al., 2021;Hacohen et al., 2022) (b = |B| in this case) or repeated for multiple rounds (Yuan et al., 2020) (b = |B|/|Rounds|) until reaching the budget |B|.",
                    "score": 0.4557219806117095,
                    "section_title": "Problem Formulation",
                    "char_start_offset": 8461,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 398
                        },
                        {
                            "start": 399,
                            "end": 465
                        },
                        {
                            "start": 466,
                            "end": 596
                        },
                        {
                            "start": 597,
                            "end": 803
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 637,
                            "end": 657,
                            "matchedPaperCorpusId": "235755110"
                        },
                        {
                            "start": 657,
                            "end": 677,
                            "matchedPaperCorpusId": "246634642"
                        },
                        {
                            "start": 734,
                            "end": 753,
                            "matchedPaperCorpusId": "224724415"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8447265625
                }
            ],
            "relevance_judgement": 0.8447265625,
            "relevance_judgment_input_expanded": "# Title: Cold-Start Data Selection for Better Few-shot Language Model Fine-tuning: A Prompt-based Uncertainty Propagation Approach\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Yue Yu, Rongzhi Zhang, Ran Xu, Jieyu Zhang, Jiaming Shen, Chao Zhang\n## Abstract\nWe present PATRON, a prompt-based data selection method for pre-trained language model fine-tuning under cold-start scenarios, i.e., no initial labeled data are available. In PATRON, we design (1) a prompt-based uncertainty propagation approach to estimate the importance of data points and (2) a partition-then-rewrite (PTR) strategy to promote sample diversity when querying for annotations. Experiments on six text classification datasets show that PATRON outperforms the strongest cold-start data selection baselines by up to 6.9%. Besides, with 128 labels only, PATRON achieves 91.0% and 92.1% of the fully supervised performance based on vanilla fine-tuning and prompt-based learning respectively. Our implementation of PATRON will be published upon acceptance.\n## Problem Formulation\nWe study cold-start data selection for text classification with c classes formulated as follows: Given a pool of unlabeled samples D u = {x j } U j=1 and an empty training set D l = \u2205, we aim to fine-tune a pre-trained language model M denoted as f (\u2022; \u03b8) under limited labeling budget |B| interactively: In each round, we use an acquisition function F(\u2022) to query b samples denoted as Q from D u . Next, the acquired samples are labeled and moved from D u to D l . Then we fine-tune the pre-trained language model f (\u2022; \u03b8) with D l to maximize the performance on downstream classification tasks. The above steps can either be one-round (Chang et al., 2021;Hacohen et al., 2022) (b = |B| in this case) or repeated for multiple rounds (Yuan et al., 2020) (b = |B|/|Rounds|) until reaching the budget |B|.",
            "reference_string": "[252280753 | Yu et al. | 2023 | Citations: 23]"
        },
        {
            "title": "Orion-14B: Open-source Multilingual Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 58,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.12246, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280380985",
                    "name": "Du Chen"
                },
                {
                    "authorId": "2280373513",
                    "name": "Yi Huang"
                },
                {
                    "authorId": "2280377400",
                    "name": "Xiaopu Li"
                },
                {
                    "authorId": "2280380419",
                    "name": "Yongqiang Li"
                },
                {
                    "authorId": "2280377865",
                    "name": "Yongqiang Liu"
                },
                {
                    "authorId": "1557313084",
                    "name": "Haihui Pan"
                },
                {
                    "authorId": "2280411885",
                    "name": "Leichao Xu"
                },
                {
                    "authorId": "2109546143",
                    "name": "Dacheng Zhang"
                },
                {
                    "authorId": "2280368716",
                    "name": "Zhipeng Zhang"
                },
                {
                    "authorId": "2281415463",
                    "name": "Kun Han"
                }
            ],
            "abstract": "In this study, we introduce Orion-14B, a collection of multilingual large language models with 14 billion parameters. We utilize a data scheduling approach to train a foundational model on a diverse corpus of 2.5 trillion tokens, sourced from texts in English, Chinese, Japanese, Korean, and other languages. Additionally, we fine-tuned a series of models tailored for conversational applications and other specific use cases. Our evaluation results demonstrate that Orion-14B achieves state-of-the-art performance across a broad spectrum of tasks. We make the Orion-14B model family and its associated code publicly accessible https://github.com/OrionStarAI/Orion, aiming to inspire future research and practical applications in the field.",
            "corpus_id": 267095066,
            "sentences": [
                {
                    "corpus_id": "267095066",
                    "title": "Orion-14B: Open-source Multilingual Large Language Models",
                    "text": "Training large language models requires hundreds of billions to trillions of tokens. On the other side, while numerous theoretical and empirical studies have examined the interplay between model size and training data volume, there is no universally accepted methodology for scheduling training data. Considering that humans acquire knowledge in a deliberate order (Evanson et al., 2023), it is plausible that language models might also benefit from a structured learning order when processing training data. Curriculum learning (Bengio et al., 2009) has been suggested as a method to organize the learning process by progressively increasing the complexity of the training data. However, most prior studies have concentrated on sample-level data and smaller datasets. Chen et al. ( 2023) employed a skills-based framework for training data selection and continuous pretraining with a 3B-parameter language model. This approach achieved greater accuracy compared to the baseline method of uniform data source sampling, suggesting the potential efficacy of strategic data scheduling. \n\nIn training the Orion-14B model, we intentionally develop a data scheduling strategy that organizes training data to incrementally increase its complexity. We divide the training data into several stages based on the data sources and their complexity. These stages are differentiated by the mix ratios of data sources. Initial stages primarily include data with common knowledge, such as web pages and news articles. In the subsequent stages, we gradually augment the proportion of data containing more complex knowledge, including textbooks, academic papers, and source code. Additionally, the linguistic diversity of the training data is expanded progressively from English and Chinese to Japanese and Korean. The brief structure of our training data schedule is depicted in Table 3. \n\nTo assess the effectiveness of the data scheduling approach, we monitor the loss on a validation set throughout the training process. This validation set consists of 5,000 documents, each unseen in the   To our knowledge, the training of most prior LLMs utilized fully shuffling the training data, which was then fed into the model in a random sequence. Orion-14B is the first LLM trained with a specific data scheduling strategy. The evaluation results indicate that this model demonstrates impressive performance in language understanding tasks at its early stages and rapidly enhances its capabilities in reasoning and academic tasks in later stages, aligning with our data scheduling policy.",
                    "score": 0.4447512572062485,
                    "section_title": "Data Scheduling",
                    "char_start_offset": 13523,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 84
                        },
                        {
                            "start": 85,
                            "end": 300
                        },
                        {
                            "start": 301,
                            "end": 508
                        },
                        {
                            "start": 509,
                            "end": 679
                        },
                        {
                            "start": 680,
                            "end": 768
                        },
                        {
                            "start": 769,
                            "end": 913
                        },
                        {
                            "start": 914,
                            "end": 1082
                        },
                        {
                            "start": 1085,
                            "end": 1240
                        },
                        {
                            "start": 1241,
                            "end": 1336
                        },
                        {
                            "start": 1337,
                            "end": 1403
                        },
                        {
                            "start": 1404,
                            "end": 1501
                        },
                        {
                            "start": 1502,
                            "end": 1661
                        },
                        {
                            "start": 1662,
                            "end": 1796
                        },
                        {
                            "start": 1797,
                            "end": 1870
                        },
                        {
                            "start": 1873,
                            "end": 2006
                        },
                        {
                            "start": 2007,
                            "end": 2226
                        },
                        {
                            "start": 2227,
                            "end": 2303
                        },
                        {
                            "start": 2304,
                            "end": 2568
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 529,
                            "end": 550,
                            "matchedPaperCorpusId": "873046"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83740234375
                }
            ],
            "relevance_judgement": 0.83740234375,
            "relevance_judgment_input_expanded": "# Title: Orion-14B: Open-source Multilingual Large Language Models\n# Venue: arXiv.org\n# Authors: Du Chen, Yi Huang, Xiaopu Li, Yongqiang Li, Yongqiang Liu, Haihui Pan, Leichao Xu, Dacheng Zhang, Zhipeng Zhang, Kun Han\n## Abstract\nIn this study, we introduce Orion-14B, a collection of multilingual large language models with 14 billion parameters. We utilize a data scheduling approach to train a foundational model on a diverse corpus of 2.5 trillion tokens, sourced from texts in English, Chinese, Japanese, Korean, and other languages. Additionally, we fine-tuned a series of models tailored for conversational applications and other specific use cases. Our evaluation results demonstrate that Orion-14B achieves state-of-the-art performance across a broad spectrum of tasks. We make the Orion-14B model family and its associated code publicly accessible https://github.com/OrionStarAI/Orion, aiming to inspire future research and practical applications in the field.\n## Data Scheduling\nTraining large language models requires hundreds of billions to trillions of tokens. On the other side, while numerous theoretical and empirical studies have examined the interplay between model size and training data volume, there is no universally accepted methodology for scheduling training data. Considering that humans acquire knowledge in a deliberate order (Evanson et al., 2023), it is plausible that language models might also benefit from a structured learning order when processing training data. Curriculum learning (Bengio et al., 2009) has been suggested as a method to organize the learning process by progressively increasing the complexity of the training data. However, most prior studies have concentrated on sample-level data and smaller datasets. Chen et al. ( 2023) employed a skills-based framework for training data selection and continuous pretraining with a 3B-parameter language model. This approach achieved greater accuracy compared to the baseline method of uniform data source sampling, suggesting the potential efficacy of strategic data scheduling. \n\nIn training the Orion-14B model, we intentionally develop a data scheduling strategy that organizes training data to incrementally increase its complexity. We divide the training data into several stages based on the data sources and their complexity. These stages are differentiated by the mix ratios of data sources. Initial stages primarily include data with common knowledge, such as web pages and news articles. In the subsequent stages, we gradually augment the proportion of data containing more complex knowledge, including textbooks, academic papers, and source code. Additionally, the linguistic diversity of the training data is expanded progressively from English and Chinese to Japanese and Korean. The brief structure of our training data schedule is depicted in Table 3. \n\nTo assess the effectiveness of the data scheduling approach, we monitor the loss on a validation set throughout the training process. This validation set consists of 5,000 documents, each unseen in the   To our knowledge, the training of most prior LLMs utilized fully shuffling the training data, which was then fed into the model in a random sequence. Orion-14B is the first LLM trained with a specific data scheduling strategy. The evaluation results indicate that this model demonstrates impressive performance in language understanding tasks at its early stages and rapidly enhances its capabilities in reasoning and academic tasks in later stages, aligning with our data scheduling policy.",
            "reference_string": "[267095066 | Chen et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining",
            "venue": "International Conference on Learning Representations",
            "year": 2025,
            "reference_count": 44,
            "citation_count": 6,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.06733, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "143827145",
                    "name": "Daouda Sow"
                },
                {
                    "authorId": "2220057323",
                    "name": "Herbert Woisetschl\u00e4ger"
                },
                {
                    "authorId": "35693339",
                    "name": "Saikiran Bulusu"
                },
                {
                    "authorId": "2255363698",
                    "name": "Shiqiang Wang"
                },
                {
                    "authorId": "2254179381",
                    "name": "Hans-Arno Jacobsen"
                },
                {
                    "authorId": "2344954540",
                    "name": "Yingbin Liang"
                }
            ],
            "abstract": "Pretraining large language models (LLMs) on vast and heterogeneous datasets is crucial for achieving state-of-the-art performance across diverse downstream tasks. However, current training paradigms treat all samples equally, overlooking the importance or relevance of individual samples throughout the training process. Existing reweighting strategies, which primarily focus on group-level data importance, fail to leverage fine-grained instance-level information and do not adapt dynamically to individual sample importance as training progresses. In this paper, we introduce novel algorithms for dynamic, instance-level data reweighting aimed at improving both the efficiency and effectiveness of LLM pretraining. Our methods adjust the weight of each training sample based on its loss value in an online fashion, allowing the model to dynamically focus on more informative or important samples at the current training stage. In particular, our framework allows us to systematically devise reweighting strategies deprioritizing redundant or uninformative data, which we find tend to work best. Furthermore, we develop a new theoretical framework for analyzing the impact of loss-based reweighting on the convergence of gradient-based optimization, providing the first formal characterization of how these strategies affect convergence bounds. We empirically validate our approach across a spectrum of tasks, from pretraining 7B and 1.4B parameter LLMs to smaller-scale language models and linear regression problems, demonstrating that our loss-based reweighting approach can lead to faster convergence and significantly improved performance.",
            "corpus_id": 276250237,
            "sentences": [
                {
                    "corpus_id": "276250237",
                    "title": "Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining",
                    "text": "Training Data Re-weighting/Selection for LLMs. Several recent studies (Xie et al., 2023;Chen et al., 2023;Fan et al., 2023;Thakkar et al., 2023) have explored various reweighting techniques to enhance the generalization and efficiency of language models pretraining. For instance, Xie et al. ( 2023) and Fan et al. (2023) optimize the composition of pretraining corpora to achieve better performance across pretraining domains or for out-of-domain generalization. Chen et al. (2023) introduce a framework for ordered skill learning, optimizing data selection based on how effectively it teaches interdependent skills for continual pretraining and fine-tuning regimes. Although effective, these techniques operate at the group level, whereas our work explores reweighting at the instance level, offering finer control over how individual samples are treated based on their loss values. Furthermore, we demonstrate that combining domain-level methods such as DoReMi (Xie et al., 2023) or DoGE (Fan et al., 2023) with our instance-level reweighting methods results in improved performance across multiple domains. Instance-level reweighting has been used in post-training settings of LLMs (Chen et al., 2024;Jiang et al., 2024). Jiang et al. (2024) boost the self-improvement abilities of LLMs by employing sample reweighting to filter out self-generated data that have correct answers but exhibit high distribution shifts. Chen et al. (2024) reweight individual samples during continual training/instruction-tuning to focus on medium-loss samples. In contrast, our work systematically studies the effects of various sample-level, loss-based reweighting strategies on the efficiency and effectiveness of LLMs pretraining. The approach in Fan & Jaggi (2023) offers a curriculum learning framework that prioritizes samples with a higher learnability score, which is precomputed using another auxiliary model similar to DoReMi and DoGE.",
                    "score": 0.4095352482820364,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 6453,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 46
                        },
                        {
                            "start": 47,
                            "end": 266
                        },
                        {
                            "start": 267,
                            "end": 463
                        },
                        {
                            "start": 464,
                            "end": 667
                        },
                        {
                            "start": 668,
                            "end": 884
                        },
                        {
                            "start": 885,
                            "end": 1110
                        },
                        {
                            "start": 1111,
                            "end": 1225
                        },
                        {
                            "start": 1226,
                            "end": 1420
                        },
                        {
                            "start": 1421,
                            "end": 1545
                        },
                        {
                            "start": 1546,
                            "end": 1718
                        },
                        {
                            "start": 1719,
                            "end": 1930
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 88,
                            "end": 106,
                            "matchedPaperCorpusId": "260203057"
                        },
                        {
                            "start": 464,
                            "end": 482,
                            "matchedPaperCorpusId": "260203057"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81494140625
                }
            ],
            "relevance_judgement": 0.81494140625,
            "relevance_judgment_input_expanded": "# Title: Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining\n# Venue: International Conference on Learning Representations\n# Authors: Daouda Sow, Herbert Woisetschl\u00e4ger, Saikiran Bulusu, Shiqiang Wang, Hans-Arno Jacobsen, Yingbin Liang\n## Abstract\nPretraining large language models (LLMs) on vast and heterogeneous datasets is crucial for achieving state-of-the-art performance across diverse downstream tasks. However, current training paradigms treat all samples equally, overlooking the importance or relevance of individual samples throughout the training process. Existing reweighting strategies, which primarily focus on group-level data importance, fail to leverage fine-grained instance-level information and do not adapt dynamically to individual sample importance as training progresses. In this paper, we introduce novel algorithms for dynamic, instance-level data reweighting aimed at improving both the efficiency and effectiveness of LLM pretraining. Our methods adjust the weight of each training sample based on its loss value in an online fashion, allowing the model to dynamically focus on more informative or important samples at the current training stage. In particular, our framework allows us to systematically devise reweighting strategies deprioritizing redundant or uninformative data, which we find tend to work best. Furthermore, we develop a new theoretical framework for analyzing the impact of loss-based reweighting on the convergence of gradient-based optimization, providing the first formal characterization of how these strategies affect convergence bounds. We empirically validate our approach across a spectrum of tasks, from pretraining 7B and 1.4B parameter LLMs to smaller-scale language models and linear regression problems, demonstrating that our loss-based reweighting approach can lead to faster convergence and significantly improved performance.\n## INTRODUCTION\nTraining Data Re-weighting/Selection for LLMs. Several recent studies (Xie et al., 2023;Chen et al., 2023;Fan et al., 2023;Thakkar et al., 2023) have explored various reweighting techniques to enhance the generalization and efficiency of language models pretraining. For instance, Xie et al. ( 2023) and Fan et al. (2023) optimize the composition of pretraining corpora to achieve better performance across pretraining domains or for out-of-domain generalization. Chen et al. (2023) introduce a framework for ordered skill learning, optimizing data selection based on how effectively it teaches interdependent skills for continual pretraining and fine-tuning regimes. Although effective, these techniques operate at the group level, whereas our work explores reweighting at the instance level, offering finer control over how individual samples are treated based on their loss values. Furthermore, we demonstrate that combining domain-level methods such as DoReMi (Xie et al., 2023) or DoGE (Fan et al., 2023) with our instance-level reweighting methods results in improved performance across multiple domains. Instance-level reweighting has been used in post-training settings of LLMs (Chen et al., 2024;Jiang et al., 2024). Jiang et al. (2024) boost the self-improvement abilities of LLMs by employing sample reweighting to filter out self-generated data that have correct answers but exhibit high distribution shifts. Chen et al. (2024) reweight individual samples during continual training/instruction-tuning to focus on medium-loss samples. In contrast, our work systematically studies the effects of various sample-level, loss-based reweighting strategies on the efficiency and effectiveness of LLMs pretraining. The approach in Fan & Jaggi (2023) offers a curriculum learning framework that prioritizes samples with a higher learnability score, which is precomputed using another auxiliary model similar to DoReMi and DoGE.",
            "reference_string": "[276250237 | Sow et al. | 2025 | Citations: 6]"
        },
        {
            "title": "Curriculum Learning: A Survey",
            "venue": "International Journal of Computer Vision",
            "year": 2021,
            "reference_count": 213,
            "citation_count": 359,
            "influential_citation_count": 13,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2101.10382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "40901288",
                    "name": "Petru Soviany"
                },
                {
                    "authorId": "1817759",
                    "name": "Radu Tudor Ionescu"
                },
                {
                    "authorId": "39337007",
                    "name": "Paolo Rota"
                },
                {
                    "authorId": "1703601",
                    "name": "N. Sebe"
                }
            ],
            "abstract": "Training machine learning models in a meaningful order, from the easy samples to the hard ones, using curriculum learning can provide performance improvements over the standard training approach based on random data shuffling, without any additional computational costs. Curriculum learning strategies have been successfully employed in all areas of machine learning, in a wide range of tasks. However, the necessity of finding a way to rank the samples from easy to hard, as well as the right pacing function for introducing more difficult data can limit the usage of the curriculum approaches. In this survey, we show how these limits have been tackled in the literature, and we present different curriculum learning instantiations for various tasks in machine learning. We construct a multi-perspective taxonomy of curriculum learning approaches by hand, considering various classification criteria. We further build a hierarchical tree of curriculum learning methods using an agglomerative clustering algorithm, linking the discovered clusters with our taxonomy. At the end, we provide some interesting directions for future work.",
            "corpus_id": 231709290,
            "sentences": [
                {
                    "corpus_id": "231709290",
                    "title": "Curriculum Learning: A Survey",
                    "text": "transformed into weights and added to the objective function. Ruiter et al. (2020) analyze the behavior of self-supervised neural machine translation systems that jointly learn to select the right training data and to perform translation. In this framework, the two processes are designed in such a fashion that they enable and enhance each other. The authors show that the sampling choices made by these models generate an implicit curriculum that matches the principles of CL: samples are self-selected based on increasing complexity and task-relevance, while also performing a denoising curriculum. Zhao et al. (2020a) introduce a method for generating the right curriculum for neural machine translation. The authors claim that this task highly relies on large quantities of data that are hard to acquire. Hence, they suggest re-selecting influential data samples from the original training set. To discover which examples from the existing data set may further improve the model, the re-selection is designed as a reinforcement learning problem. The state is represented by the features of randomly selected training instances, the action is selecting one of the samples, and the reward is the perplexity difference on a validation set, with the final goal of finding the policy that maximizes the reward. Zhou et al. (2020b) introduce an uncertainty-based curriculum batching approach for neural machine translation. They propose using uncertainty at the data level, for establishing the easy-to-hard ordering, and the model level, to decide the right moment to enhance the training set with more difficult samples. To measure the difficulty of the examples, they start from the intuition that samples with higher crossentropy and uncertainty are more difficult to translate. Thus, the data uncertainty is measured according to its joint distribution, as it is estimated by a language model pre-trained on the training data. On the other hand, the model's uncertainty is evaluated using the variance of the distribution over a Bayesian neural network. Sachan and Xing (2016) propose new heuristics for determining the easiness of examples in an SPL scenario, other than the standard loss function. Aside from the heuristics, the authors highlight the importance of diversity. They measure diversity using the angle between the hyperplanes that the question examples induce in the feature space. Their solution selects a question that is valid according to both criteria, being easy, but also diverse with regards",
                    "score": 0.4146082266682252,
                    "section_title": "Natural Language Processing",
                    "char_start_offset": 87921,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 62,
                            "end": 82,
                            "matchedPaperCorpusId": "222176797"
                        },
                        {
                            "start": 602,
                            "end": 621,
                            "matchedPaperCorpusId": "211102338"
                        },
                        {
                            "start": 1311,
                            "end": 1330,
                            "matchedPaperCorpusId": "220047761"
                        },
                        {
                            "start": 2058,
                            "end": 2080,
                            "matchedPaperCorpusId": "16503693"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8095703125
                }
            ],
            "relevance_judgement": 0.8095703125,
            "relevance_judgment_input_expanded": "# Title: Curriculum Learning: A Survey\n# Venue: International Journal of Computer Vision\n# Authors: Petru Soviany, Radu Tudor Ionescu, Paolo Rota, N. Sebe\n## Abstract\nTraining machine learning models in a meaningful order, from the easy samples to the hard ones, using curriculum learning can provide performance improvements over the standard training approach based on random data shuffling, without any additional computational costs. Curriculum learning strategies have been successfully employed in all areas of machine learning, in a wide range of tasks. However, the necessity of finding a way to rank the samples from easy to hard, as well as the right pacing function for introducing more difficult data can limit the usage of the curriculum approaches. In this survey, we show how these limits have been tackled in the literature, and we present different curriculum learning instantiations for various tasks in machine learning. We construct a multi-perspective taxonomy of curriculum learning approaches by hand, considering various classification criteria. We further build a hierarchical tree of curriculum learning methods using an agglomerative clustering algorithm, linking the discovered clusters with our taxonomy. At the end, we provide some interesting directions for future work.\n## Natural Language Processing\ntransformed into weights and added to the objective function. Ruiter et al. (2020) analyze the behavior of self-supervised neural machine translation systems that jointly learn to select the right training data and to perform translation. In this framework, the two processes are designed in such a fashion that they enable and enhance each other. The authors show that the sampling choices made by these models generate an implicit curriculum that matches the principles of CL: samples are self-selected based on increasing complexity and task-relevance, while also performing a denoising curriculum. Zhao et al. (2020a) introduce a method for generating the right curriculum for neural machine translation. The authors claim that this task highly relies on large quantities of data that are hard to acquire. Hence, they suggest re-selecting influential data samples from the original training set. To discover which examples from the existing data set may further improve the model, the re-selection is designed as a reinforcement learning problem. The state is represented by the features of randomly selected training instances, the action is selecting one of the samples, and the reward is the perplexity difference on a validation set, with the final goal of finding the policy that maximizes the reward. Zhou et al. (2020b) introduce an uncertainty-based curriculum batching approach for neural machine translation. They propose using uncertainty at the data level, for establishing the easy-to-hard ordering, and the model level, to decide the right moment to enhance the training set with more difficult samples. To measure the difficulty of the examples, they start from the intuition that samples with higher crossentropy and uncertainty are more difficult to translate. Thus, the data uncertainty is measured according to its joint distribution, as it is estimated by a language model pre-trained on the training data. On the other hand, the model's uncertainty is evaluated using the variance of the distribution over a Bayesian neural network. Sachan and Xing (2016) propose new heuristics for determining the easiness of examples in an SPL scenario, other than the standard loss function. Aside from the heuristics, the authors highlight the importance of diversity. They measure diversity using the angle between the hyperplanes that the question examples induce in the feature space. Their solution selects a question that is valid according to both criteria, being easy, but also diverse with regards",
            "reference_string": "[231709290 | Soviany et al. | 2021 | Citations: 359]"
        },
        {
            "title": "Domain Adaptation with BERT-based Domain Classification and Data Selection",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2019,
            "reference_count": 27,
            "citation_count": 95,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D19-6109.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/D19-6109, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "47646605",
                    "name": "Xiaofei Ma"
                },
                {
                    "authorId": "2091437540",
                    "name": "Peng Xu"
                },
                {
                    "authorId": "40296541",
                    "name": "Zhiguo Wang"
                },
                {
                    "authorId": "1701451",
                    "name": "Ramesh Nallapati"
                }
            ],
            "abstract": "The performance of deep neural models can deteriorate substantially when there is a domain shift between training and test data. For example, the pre-trained BERT model can be easily fine-tuned with just one additional output layer to create a state-of-the-art model for a wide range of tasks. However, the fine-tuned BERT model suffers considerably at zero-shot when applied to a different domain. In this paper, we present a novel two-step domain adaptation framework based on curriculum learning and domain-discriminative data selection. The domain adaptation is conducted in a mostly unsupervised manner using a small target domain validation set for hyper-parameter tuning. We tested the framework on four large public datasets with different domain similarities and task types. Our framework outperforms a popular discrepancy-based domain adaptation method on most transfer tasks while consuming only a fraction of the training budget.",
            "corpus_id": 208031414,
            "sentences": [
                {
                    "corpus_id": "208031414",
                    "title": "Domain Adaptation with BERT-based Domain Classification and Data Selection",
                    "text": "Our method is inspired by the work on curriculum learning and recent work on data selection for transfer learning. \n\nCurriculum Learning: Curriculum Learning (Bengio et al., 2009) deals with the question of how to use prior knowledge about the difficulty of the training examples, to boost the rate of learning and the performance of the final model. The ranking or weighting of the training examples is used to guide the order of presentation of examples to the learner. The idea is to build a curriculum of progressively harder samples in order to significantly accelerate a neural network's train-ing. While curriculum learning can leverage label information (loss of the model, training progress) (Weinshall and Amir, 2018) to guide data selection, this work assumes no or few labeled data in the new domain. \n\nData Selection: Not all the data points from the source domain are equally important for target domain transfer. Irrelevant source data points only add noise and overfit the training model. Recent work from Ruder and Plank, applied Bayesian optimization to learn a scoring function to rank the source data points. Data selection method was also used by Tsvetkov et al. to",
                    "score": 0.4757819075429176,
                    "section_title": "Related Work",
                    "char_start_offset": 6782,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 114
                        },
                        {
                            "start": 117,
                            "end": 350
                        },
                        {
                            "start": 351,
                            "end": 471
                        },
                        {
                            "start": 472,
                            "end": 604
                        },
                        {
                            "start": 605,
                            "end": 812
                        },
                        {
                            "start": 815,
                            "end": 927
                        },
                        {
                            "start": 928,
                            "end": 1004
                        },
                        {
                            "start": 1005,
                            "end": 1128
                        },
                        {
                            "start": 1129,
                            "end": 1186
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80029296875
                }
            ],
            "relevance_judgement": 0.80029296875,
            "relevance_judgment_input_expanded": "# Title: Domain Adaptation with BERT-based Domain Classification and Data Selection\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Xiaofei Ma, Peng Xu, Zhiguo Wang, Ramesh Nallapati\n## Abstract\nThe performance of deep neural models can deteriorate substantially when there is a domain shift between training and test data. For example, the pre-trained BERT model can be easily fine-tuned with just one additional output layer to create a state-of-the-art model for a wide range of tasks. However, the fine-tuned BERT model suffers considerably at zero-shot when applied to a different domain. In this paper, we present a novel two-step domain adaptation framework based on curriculum learning and domain-discriminative data selection. The domain adaptation is conducted in a mostly unsupervised manner using a small target domain validation set for hyper-parameter tuning. We tested the framework on four large public datasets with different domain similarities and task types. Our framework outperforms a popular discrepancy-based domain adaptation method on most transfer tasks while consuming only a fraction of the training budget.\n## Related Work\nOur method is inspired by the work on curriculum learning and recent work on data selection for transfer learning. \n\nCurriculum Learning: Curriculum Learning (Bengio et al., 2009) deals with the question of how to use prior knowledge about the difficulty of the training examples, to boost the rate of learning and the performance of the final model. The ranking or weighting of the training examples is used to guide the order of presentation of examples to the learner. The idea is to build a curriculum of progressively harder samples in order to significantly accelerate a neural network's train-ing. While curriculum learning can leverage label information (loss of the model, training progress) (Weinshall and Amir, 2018) to guide data selection, this work assumes no or few labeled data in the new domain. \n\nData Selection: Not all the data points from the source domain are equally important for target domain transfer. Irrelevant source data points only add noise and overfit the training model. Recent work from Ruder and Plank, applied Bayesian optimization to learn a scoring function to rank the source data points. Data selection method was also used by Tsvetkov et al. to",
            "reference_string": "[208031414 | Ma et al. | 2019 | Citations: 95]"
        },
        {
            "title": "TAGCOS: Task-agnostic Gradient Clustered Coreset Selection for Instruction Tuning Data",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 59,
            "citation_count": 12,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.15235, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "50561049",
                    "name": "Jipeng Zhang"
                },
                {
                    "authorId": "2314914700",
                    "name": "Yaxuan Qin"
                },
                {
                    "authorId": "2066420772",
                    "name": "Renjie Pi"
                },
                {
                    "authorId": "2249562904",
                    "name": "Weizhong Zhang"
                },
                {
                    "authorId": "2192845956",
                    "name": "Rui Pan"
                },
                {
                    "authorId": "2266465257",
                    "name": "Tong Zhang"
                }
            ],
            "abstract": "Instruction tuning has achieved unprecedented success in NLP, turning large language models into versatile chatbots. However, the increasing variety and volume of instruction datasets demand significant computational resources. To address this, it is essential to extract a small and highly informative subset (i.e., Coreset) that achieves comparable performance to the full dataset. Achieving this goal poses non-trivial challenges: 1) data selection requires accurate data representations that reflect the training samples' quality, 2) considering the diverse nature of instruction datasets, and 3) ensuring the efficiency of the coreset selection algorithm for large models. To address these challenges, we propose Task-Agnostic Gradient Clustered COreset Selection (TAGCOS). Specifically, we leverage sample gradients as the data representations, perform clustering to group similar data, and apply an efficient greedy algorithm for coreset selection. Experimental results show that our algorithm, selecting only 5% of the data, surpasses other unsupervised methods and achieves performance close to that of the full dataset.",
            "corpus_id": 271328390,
            "sentences": [
                {
                    "corpus_id": "271328390",
                    "title": "TAGCOS: Task-agnostic Gradient Clustered Coreset Selection for Instruction Tuning Data",
                    "text": "Instruction tuning has achieved unprecedented success in NLP, turning large language models into versatile chatbots. However, the increasing variety and volume of instruction datasets demand significant computational resources. To address this, it is essential to extract a small and highly informative subset (i.e., Coreset) that achieves comparable performance to the full dataset. Achieving this goal poses non-trivial challenges: 1) data selection requires accurate data representations that reflect the training samples' quality, 2) considering the diverse nature of instruction datasets, and 3) ensuring the efficiency of the coreset selection algorithm for large models. To address these challenges, we propose Task-Agnostic Gradient Clustered COreset Selection (TAGCOS). Specifically, we leverage sample gradients as the data representations, perform clustering to group similar data, and apply an efficient greedy algorithm for coreset selection. Experimental results show that our algorithm, selecting only 5% of the data, surpasses other unsupervised methods and achieves performance close to that of the full dataset.",
                    "score": 0.4016705467405399,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.79638671875
                }
            ],
            "relevance_judgement": 0.79638671875,
            "relevance_judgment_input_expanded": "# Title: TAGCOS: Task-agnostic Gradient Clustered Coreset Selection for Instruction Tuning Data\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Jipeng Zhang, Yaxuan Qin, Renjie Pi, Weizhong Zhang, Rui Pan, Tong Zhang\n## Abstract\nInstruction tuning has achieved unprecedented success in NLP, turning large language models into versatile chatbots. However, the increasing variety and volume of instruction datasets demand significant computational resources. To address this, it is essential to extract a small and highly informative subset (i.e., Coreset) that achieves comparable performance to the full dataset. Achieving this goal poses non-trivial challenges: 1) data selection requires accurate data representations that reflect the training samples' quality, 2) considering the diverse nature of instruction datasets, and 3) ensuring the efficiency of the coreset selection algorithm for large models. To address these challenges, we propose Task-Agnostic Gradient Clustered COreset Selection (TAGCOS). Specifically, we leverage sample gradients as the data representations, perform clustering to group similar data, and apply an efficient greedy algorithm for coreset selection. Experimental results show that our algorithm, selecting only 5% of the data, surpasses other unsupervised methods and achieves performance close to that of the full dataset.\n",
            "reference_string": "[271328390 | Zhang et al. | 2024 | Citations: 12]"
        },
        {
            "title": "On the Role of Corpus Ordering in Language Modeling",
            "venue": "SUSTAINLP",
            "year": 2021,
            "reference_count": 50,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.sustainlp-1.15.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2021.sustainlp-1.15, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2628916",
                    "name": "Ameeta Agrawal"
                },
                {
                    "authorId": "2145771623",
                    "name": "Suresh Singh"
                },
                {
                    "authorId": "144958306",
                    "name": "Lauren Schneider"
                },
                {
                    "authorId": "151134500",
                    "name": "Michael Samuels"
                }
            ],
            "abstract": "Language models pretrained on vast corpora of unstructured text using self-supervised learning framework are used in numerous natural language understanding and generation tasks. Many studies show that language acquisition in humans follows a rather structured simple-to-complex pattern and guided by this intuition, curriculum learning, which enables training of computational models in a meaningful order, such as processing easy samples before hard ones, has been shown to potentially reduce training time. The question remains whether curriculum learning can benefit pretraining of language models. In this work, we perform comprehensive experiments involving multiple curricula strategies varying the criteria for complexity and the training schedules. Empirical results of training transformer language models on English corpus and evaluating it intrinsically as well as after fine-tuning across eight tasks from the GLUE benchmark, show consistent improvement gains over conventional vanilla training. Interestingly, in our experiments, when evaluated on one epoch, the best model following a document-level hard-to-easy curriculum, outperforms the vanilla model by 1.7 points (average GLUE score) and it takes the vanilla model twice as many training steps to reach comparable performance.",
            "corpus_id": 237553761,
            "sentences": [
                {
                    "corpus_id": "237553761",
                    "title": "On the Role of Corpus Ordering in Language Modeling",
                    "text": "Language models pretrained on vast corpora of unstructured text using self-supervised learning framework are used in numerous natural language understanding and generation tasks. Many studies show that language acquisition in humans follows a rather structured simple-to-complex pattern and guided by this intuition, curriculum learning, which enables training of computational models in a meaningful order, such as processing easy samples before hard ones, has been shown to potentially reduce training time. The question remains whether curriculum learning can benefit pretraining of language models. In this work, we perform comprehensive experiments involving multiple curricula strategies varying the criteria for complexity and the training schedules. Empirical results of training transformer language models on English corpus and evaluating it intrinsically as well as after fine-tuning across eight tasks from the GLUE benchmark, show consistent improvement gains over conventional vanilla training. Interestingly, in our experiments, when evaluated on one epoch, the best model following a document-level hard-to-easy curriculum, outperforms the vanilla model by 1.7 points (average GLUE score) and it takes the vanilla model twice as many training steps to reach comparable performance.",
                    "score": 0.4549382824945951,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.79052734375
                },
                {
                    "corpus_id": "237553761",
                    "title": "On the Role of Corpus Ordering in Language Modeling",
                    "text": "In this paper, we investigate several corpus ordering and training schedules as a way of exploring the effectiveness of curriculum learning, specifically with regards to obtaining better models and potentially reducing training time and/or cost, for pretraining transformer language models (English, in this case). Between the two notions of complexity, one computed at sentence-level, and the other at document-level, we find them to perform relatively comparably. In the document-level curriculum, our findings suggest that going from hard to easy training samples may be an effective strategy. Furthermore, iterating over disjoint bins one at a time seems comparably effective to incrementally increasing the training data size. Finally, interestingly, our empirical results on eight down-stream tasks from GLUE benchmark reveal that an ordered corpus yields competitive performance as compared to vanilla training in almost half the number of training steps, as measured over two epochs of training.\n\nMany interesting avenues for future work remain such as devising more efficient corpus ordering algorithms or verifying whether these simple yet effective strategies generalize to different training corpora or model architectures, thus enabling development of sustainable language models. J Peter Kincaid, Robert P Fishburne Jr, Richard L Rogers, and Brad S Chissom. 1975 Tables 7 and 8 present some sample text excerpts from easy and hard bins, as computed by documentlevel complexity and sentence-level complexity, respectively.",
                    "score": 0.5974600784682362,
                    "section_title": "Conclusions and Future Directions",
                    "char_start_offset": 27426,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75830078125
                },
                {
                    "corpus_id": "237553761",
                    "title": "On the Role of Corpus Ordering in Language Modeling",
                    "text": "a curriculum for machine learning models. The intuition behind this approach is to mimic the manner in which humans learn. Training samples are organized by levels of difficulty and training proceeds in steps where the model is first trained on a subset of the corpus at a given difficulty level before being trained on another difficulty level, and so on.\n\nFor our experiments, we pretrain from scratch multiple models of BERT BASE using the WikiText-103 corpus (Merity et al., 2016), a collection of English articles from Wikipedia. We explored standard length-based metrics of complexity as well as document-level complexity using Flesch Reading Ease readability index to organize the corpora into a curriculum. We also investigated two different methods of accessing such an ordered training setone where the bins remain disjoint (BINNED) and another where the bins are incrementally joined together to increase the training data size (STEPPED) over n epochs. An extensive evaluation is conducted in terms of metrics related to not just pretraining but also fine-tuning on eight downstream tasks, specifically the suite of datasets from the GLUE benchmark (Wang et al., 2018).\n\nOur results demonstrate that while sentencelevel and document-level complexity metrics work comparably well, they outperform the vanilla models in all experiments. Furthermore, while easy to hard is a good strategy when complexity is computed at sentence-level, the reverse is true when complexity is measured at document-level. Finally, the most encouraging results suggest that corpus ordering takes significantly less time (measured in terms of training steps) as compared to vanilla training, while yielding comparable results as measured by the average GLUE score.\n\nIn summary, this work makes the following contributions,\n\n\u2022 We propose a novel paradigm defined by document-level metrics of text complexity for ordering a training corpus. \u2022 We explore two strategies for learning from such a curriculum in the context of pretraining of language models. \u2022 We conduct extensive experiments by training several versions of transformer model from scratch and evaluating their performance in terms of metrics computed at both pretraining and fine-tuning stages.",
                    "score": 0.4253602135586808,
                    "section_title": "Introduction",
                    "char_start_offset": 2108,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 1160,
                            "end": 1179,
                            "matchedPaperCorpusId": "5034059"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70751953125
                }
            ],
            "relevance_judgement": 0.79052734375,
            "relevance_judgment_input_expanded": "# Title: On the Role of Corpus Ordering in Language Modeling\n# Venue: SUSTAINLP\n# Authors: Ameeta Agrawal, Suresh Singh, Lauren Schneider, Michael Samuels\n## Abstract\nLanguage models pretrained on vast corpora of unstructured text using self-supervised learning framework are used in numerous natural language understanding and generation tasks. Many studies show that language acquisition in humans follows a rather structured simple-to-complex pattern and guided by this intuition, curriculum learning, which enables training of computational models in a meaningful order, such as processing easy samples before hard ones, has been shown to potentially reduce training time. The question remains whether curriculum learning can benefit pretraining of language models. In this work, we perform comprehensive experiments involving multiple curricula strategies varying the criteria for complexity and the training schedules. Empirical results of training transformer language models on English corpus and evaluating it intrinsically as well as after fine-tuning across eight tasks from the GLUE benchmark, show consistent improvement gains over conventional vanilla training. Interestingly, in our experiments, when evaluated on one epoch, the best model following a document-level hard-to-easy curriculum, outperforms the vanilla model by 1.7 points (average GLUE score) and it takes the vanilla model twice as many training steps to reach comparable performance.\n## Introduction\na curriculum for machine learning models. The intuition behind this approach is to mimic the manner in which humans learn. Training samples are organized by levels of difficulty and training proceeds in steps where the model is first trained on a subset of the corpus at a given difficulty level before being trained on another difficulty level, and so on.\n\nFor our experiments, we pretrain from scratch multiple models of BERT BASE using the WikiText-103 corpus (Merity et al., 2016), a collection of English articles from Wikipedia. We explored standard length-based metrics of complexity as well as document-level complexity using Flesch Reading Ease readability index to organize the corpora into a curriculum. We also investigated two different methods of accessing such an ordered training setone where the bins remain disjoint (BINNED) and another where the bins are incrementally joined together to increase the training data size (STEPPED) over n epochs. An extensive evaluation is conducted in terms of metrics related to not just pretraining but also fine-tuning on eight downstream tasks, specifically the suite of datasets from the GLUE benchmark (Wang et al., 2018).\n\nOur results demonstrate that while sentencelevel and document-level complexity metrics work comparably well, they outperform the vanilla models in all experiments. Furthermore, while easy to hard is a good strategy when complexity is computed at sentence-level, the reverse is true when complexity is measured at document-level. Finally, the most encouraging results suggest that corpus ordering takes significantly less time (measured in terms of training steps) as compared to vanilla training, while yielding comparable results as measured by the average GLUE score.\n\nIn summary, this work makes the following contributions,\n\n\u2022 We propose a novel paradigm defined by document-level metrics of text complexity for ordering a training corpus. \u2022 We explore two strategies for learning from such a curriculum in the context of pretraining of language models. \u2022 We conduct extensive experiments by training several versions of transformer model from scratch and evaluating their performance in terms of metrics computed at both pretraining and fine-tuning stages.\n\n## Conclusions and Future Directions\nIn this paper, we investigate several corpus ordering and training schedules as a way of exploring the effectiveness of curriculum learning, specifically with regards to obtaining better models and potentially reducing training time and/or cost, for pretraining transformer language models (English, in this case). Between the two notions of complexity, one computed at sentence-level, and the other at document-level, we find them to perform relatively comparably. In the document-level curriculum, our findings suggest that going from hard to easy training samples may be an effective strategy. Furthermore, iterating over disjoint bins one at a time seems comparably effective to incrementally increasing the training data size. Finally, interestingly, our empirical results on eight down-stream tasks from GLUE benchmark reveal that an ordered corpus yields competitive performance as compared to vanilla training in almost half the number of training steps, as measured over two epochs of training.\n\nMany interesting avenues for future work remain such as devising more efficient corpus ordering algorithms or verifying whether these simple yet effective strategies generalize to different training corpora or model architectures, thus enabling development of sustainable language models. J Peter Kincaid, Robert P Fishburne Jr, Richard L Rogers, and Brad S Chissom. 1975 Tables 7 and 8 present some sample text excerpts from easy and hard bins, as computed by documentlevel complexity and sentence-level complexity, respectively.",
            "reference_string": "[237553761 | Agrawal et al. | 2021 | Citations: 7]"
        },
        {
            "title": "From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning",
            "venue": "International Conference on Computational Processing of the Portuguese Language",
            "year": 2024,
            "reference_count": 28,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.13229, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2144877093",
                    "name": "Alexandre Alcoforado"
                },
                {
                    "authorId": "2144849309",
                    "name": "Thomas Palmeira Ferraz"
                },
                {
                    "authorId": "2280908993",
                    "name": "Lucas Hideki Okamura"
                },
                {
                    "authorId": "2280909524",
                    "name": "I. Fama"
                },
                {
                    "authorId": "2280922488",
                    "name": "Arnold Moya Lavado"
                },
                {
                    "authorId": "2280909234",
                    "name": "B. Bueno"
                },
                {
                    "authorId": "2280908189",
                    "name": "Bruno Veloso"
                },
                {
                    "authorId": "2280992811",
                    "name": "Anna Helena Reali Costa"
                }
            ],
            "abstract": "A major challenge in Natural Language Processing is obtaining annotated data for supervised learning. An option is the use of crowdsourcing platforms for data annotation. However, crowdsourcing introduces issues related to the annotator's experience, consistency, and biases. An alternative is to use zero-shot methods, which in turn have limitations compared to their few-shot or fully supervised counterparts. Recent advancements driven by large language models show potential, but struggle to adapt to specialized domains with severely limited data. The most common approaches therefore involve the human itself randomly annotating a set of datapoints to build initial datasets. But randomly sampling data to be annotated is often inefficient as it ignores the characteristics of the data and the specific needs of the model. The situation worsens when working with imbalanced datasets, as random sampling tends to heavily bias towards the majority classes, leading to excessive annotated data. To address these issues, this paper contributes an automatic and informed data selection architecture to build a small dataset for few-shot learning. Our proposal minimizes the quantity and maximizes diversity of data selected for human annotation, while improving model performance.",
            "corpus_id": 267200114,
            "sentences": [
                {
                    "corpus_id": "267200114",
                    "title": "From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning",
                    "text": "A major challenge in Natural Language Processing is obtaining annotated data for supervised learning. An option is the use of crowdsourcing platforms for data annotation. However, crowdsourcing introduces issues related to the annotator's experience, consistency, and biases. An alternative is to use zero-shot methods, which in turn have limitations compared to their few-shot or fully supervised counterparts. Recent advancements driven by large language models show potential, but struggle to adapt to specialized domains with severely limited data. The most common approaches therefore involve the human itself randomly annotating a set of datapoints to build initial datasets. But randomly sampling data to be annotated is often inefficient as it ignores the characteristics of the data and the specific needs of the model. The situation worsens when working with imbalanced datasets, as random sampling tends to heavily bias towards the majority classes, leading to excessive annotated data. To address these issues, this paper contributes an automatic and informed data selection architecture to build a small dataset for few-shot learning. Our proposal minimizes the quantity and maximizes diversity of data selected for human annotation, while improving model performance.",
                    "score": 0.40560711164987606,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78662109375
                }
            ],
            "relevance_judgement": 0.78662109375,
            "relevance_judgment_input_expanded": "# Title: From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning\n# Venue: International Conference on Computational Processing of the Portuguese Language\n# Authors: Alexandre Alcoforado, Thomas Palmeira Ferraz, Lucas Hideki Okamura, I. Fama, Arnold Moya Lavado, B. Bueno, Bruno Veloso, Anna Helena Reali Costa\n## Abstract\nA major challenge in Natural Language Processing is obtaining annotated data for supervised learning. An option is the use of crowdsourcing platforms for data annotation. However, crowdsourcing introduces issues related to the annotator's experience, consistency, and biases. An alternative is to use zero-shot methods, which in turn have limitations compared to their few-shot or fully supervised counterparts. Recent advancements driven by large language models show potential, but struggle to adapt to specialized domains with severely limited data. The most common approaches therefore involve the human itself randomly annotating a set of datapoints to build initial datasets. But randomly sampling data to be annotated is often inefficient as it ignores the characteristics of the data and the specific needs of the model. The situation worsens when working with imbalanced datasets, as random sampling tends to heavily bias towards the majority classes, leading to excessive annotated data. To address these issues, this paper contributes an automatic and informed data selection architecture to build a small dataset for few-shot learning. Our proposal minimizes the quantity and maximizes diversity of data selected for human annotation, while improving model performance.\n",
            "reference_string": "[267200114 | Alcoforado et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Towards a Task-Agnostic Model of Difficulty Estimation for Supervised Learning Tasks",
            "venue": "AACL",
            "year": 2020,
            "reference_count": 45,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2020.aacl-srw.3, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2033736232",
                    "name": "Antonio Laverghetta"
                },
                {
                    "authorId": "1716200134",
                    "name": "Jamshidbek Mirzakhalov"
                },
                {
                    "authorId": "2143879",
                    "name": "John Licato"
                }
            ],
            "abstract": "Curriculum learning, a training strategy where training data are ordered based on their difficulty, has been shown to improve performance and reduce training time on various NLP tasks. While much work over the years has developed novel approaches for generating curricula, these strategies are typically only suited for the task they were designed for. This work explores developing a task-agnostic model for problem difficulty and applying it to the Stanford Natural Language Inference (SNLI) dataset. Using the human responses that come with the dev set of SNLI, we train both regression and classification models to predict how many annotators will answer a question correctly and then project the difficulty estimates onto the full SNLI train set to create the curriculum. We argue that our curriculum is effectively capturing difficulty for this task through various analyses of both the model and the predicted difficulty scores.",
            "corpus_id": 227905455,
            "sentences": [
                {
                    "corpus_id": "227905455",
                    "title": "Towards a Task-Agnostic Model of Difficulty Estimation for Supervised Learning Tasks",
                    "text": "Recent advances on natural language processing (NLP) benchmarks have been driven by increasingly sophisticated language models, which are pretrained on enormous amounts of data before use. Refinements of this process has led to increasingly powerful language models, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and more recently T5 (Raffel et al., 2019). Such models are quickly saturating even new tasks that have undergone a rigorous adversarial filtering process (Zellers et al., 2018). However, these downstream performance improvements also require more computational resources and data to train the models, which is not always feasible. Curriculum learning (Elman, 1993), a strategy where the model is trained on easier examples before harder ones, has recently been shown to improve performance and reduce training time on a variety of NLP tasks, especially machine translation (Liu et al., 2020;Wang et al., 2020;Zhou et al., 2020;Xu et al., 2020). \n\nThe success of this research shows that the order in which training data is presented to a model is important, but how to best apply curriculum learning broadly to NLP and how to design effective measures of difficulty to create curricula remain relatively unexplored. Prior approaches either use a hand-crafted measure of difficulty that works well for a particular task or design an architecture that automatically creates the curriculum during training. In either case, the curriculum is formed using some information-theoretic measure of difficulty (semantic distance, feedback from a separate network, etc.), and it is difficult to interpret why they work well for some tasks and not others. In a practical sense, it is seldom clear how to apply a previously investigated curricula directly to another task. \n\nIn this paper, we explore how to address these shortcomings by creating what we call a taskagnostic model of difficulty, which we argue can, in principle, be applied to any supervised learning task. We use this model to investigate what makes a good difficulty measure for curricula beyond how it affects downstream performance to better explain why one curriculum should be preferred over another.",
                    "score": 0.4686646361844679,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 188
                        },
                        {
                            "start": 189,
                            "end": 374
                        },
                        {
                            "start": 375,
                            "end": 509
                        },
                        {
                            "start": 510,
                            "end": 662
                        },
                        {
                            "start": 663,
                            "end": 976
                        },
                        {
                            "start": 979,
                            "end": 1247
                        },
                        {
                            "start": 1248,
                            "end": 1435
                        },
                        {
                            "start": 1436,
                            "end": 1675
                        },
                        {
                            "start": 1676,
                            "end": 1791
                        },
                        {
                            "start": 1794,
                            "end": 1992
                        },
                        {
                            "start": 1993,
                            "end": 2192
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 280,
                            "end": 301,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 486,
                            "end": 508,
                            "matchedPaperCorpusId": "52019251"
                        },
                        {
                            "start": 683,
                            "end": 696,
                            "matchedPaperCorpusId": "2105042"
                        },
                        {
                            "start": 923,
                            "end": 941,
                            "matchedPaperCorpusId": "220045412"
                        },
                        {
                            "start": 941,
                            "end": 959,
                            "matchedPaperCorpusId": "220047761"
                        },
                        {
                            "start": 959,
                            "end": 975,
                            "matchedPaperCorpusId": "220045816"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76953125
                }
            ],
            "relevance_judgement": 0.76953125,
            "relevance_judgment_input_expanded": "# Title: Towards a Task-Agnostic Model of Difficulty Estimation for Supervised Learning Tasks\n# Venue: AACL\n# Authors: Antonio Laverghetta, Jamshidbek Mirzakhalov, John Licato\n## Abstract\nCurriculum learning, a training strategy where training data are ordered based on their difficulty, has been shown to improve performance and reduce training time on various NLP tasks. While much work over the years has developed novel approaches for generating curricula, these strategies are typically only suited for the task they were designed for. This work explores developing a task-agnostic model for problem difficulty and applying it to the Stanford Natural Language Inference (SNLI) dataset. Using the human responses that come with the dev set of SNLI, we train both regression and classification models to predict how many annotators will answer a question correctly and then project the difficulty estimates onto the full SNLI train set to create the curriculum. We argue that our curriculum is effectively capturing difficulty for this task through various analyses of both the model and the predicted difficulty scores.\n## Introduction\nRecent advances on natural language processing (NLP) benchmarks have been driven by increasingly sophisticated language models, which are pretrained on enormous amounts of data before use. Refinements of this process has led to increasingly powerful language models, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and more recently T5 (Raffel et al., 2019). Such models are quickly saturating even new tasks that have undergone a rigorous adversarial filtering process (Zellers et al., 2018). However, these downstream performance improvements also require more computational resources and data to train the models, which is not always feasible. Curriculum learning (Elman, 1993), a strategy where the model is trained on easier examples before harder ones, has recently been shown to improve performance and reduce training time on a variety of NLP tasks, especially machine translation (Liu et al., 2020;Wang et al., 2020;Zhou et al., 2020;Xu et al., 2020). \n\nThe success of this research shows that the order in which training data is presented to a model is important, but how to best apply curriculum learning broadly to NLP and how to design effective measures of difficulty to create curricula remain relatively unexplored. Prior approaches either use a hand-crafted measure of difficulty that works well for a particular task or design an architecture that automatically creates the curriculum during training. In either case, the curriculum is formed using some information-theoretic measure of difficulty (semantic distance, feedback from a separate network, etc.), and it is difficult to interpret why they work well for some tasks and not others. In a practical sense, it is seldom clear how to apply a previously investigated curricula directly to another task. \n\nIn this paper, we explore how to address these shortcomings by creating what we call a taskagnostic model of difficulty, which we argue can, in principle, be applied to any supervised learning task. We use this model to investigate what makes a good difficulty measure for curricula beyond how it affects downstream performance to better explain why one curriculum should be preferred over another.",
            "reference_string": "[227905455 | Laverghetta et al. | 2020 | Citations: 2]"
        },
        {
            "title": "Towards Accelerated Model Training via Bayesian Data Selection",
            "venue": "Neural Information Processing Systems",
            "year": 2023,
            "reference_count": 47,
            "citation_count": 5,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.10544",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.10544, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145114723",
                    "name": "Zhijie Deng"
                },
                {
                    "authorId": "2153522384",
                    "name": "Peng Cui"
                },
                {
                    "authorId": "145254043",
                    "name": "Jun Zhu"
                }
            ],
            "abstract": "Mislabeled, duplicated, or biased data in real-world scenarios can lead to prolonged training and even hinder model convergence. Traditional solutions prioritizing easy or hard samples lack the flexibility to handle such a variety simultaneously. Recent work has proposed a more reasonable data selection principle by examining the data's impact on the model's generalization loss. However, its practical adoption relies on less principled approximations and additional holdout data. This work solves these problems by leveraging a lightweight Bayesian treatment and incorporating off-the-shelf zero-shot predictors built on large-scale pre-trained models. The resulting algorithm is efficient and easy to implement. We perform extensive empirical studies on challenging benchmarks with considerable data noise and imbalance in the online batch selection scenario, and observe superior training efficiency over competitive baselines. Notably, on the challenging WebVision benchmark, our method can achieve similar predictive performance with significantly fewer training iterations than leading data selection methods.",
            "corpus_id": 261048902,
            "sentences": [
                {
                    "corpus_id": "261048902",
                    "title": "Towards Accelerated Model Training via Bayesian Data Selection",
                    "text": "Extensive methods have been proposed to accelerate model training through techniques such as data pruning, coreset selection, curriculum learning, online batch selection, etc. Data pruning explores various metrics, such as EL2N score [33], forgetting score [40], and classification margin [34], to measure individual differences among data points and retains only the hardest examples for model training. However, data pruning still exhibits limitations when dealing with noisy labels, and some of these metrics are computationally expensive. Coreset selection methods also partially address the problem of accelerating model training. In particular, [43] contributes a data scoring mechanism that is robust to the change of scenarios for coreset selection, and [46] makes an in-depth understanding of the catastrophic accuracy drop issue of one-shot coreset selection and contributes a novel solution to it. However, these methods lack the flexibility to prioritize samples with different properties at various training stages. Curriculum learning, as advocated by [1], prioritizes easy points with low label noise before uniformly training on all data points. While this strategy enhances convergence, it fails to address the issue of skipping redundant points already learned. \n\nOnline batch selection methods [26,19,17] tackle the training acceleration problem by selecting hard data identified by high loss or gradient norm. However, they also suffer from a common drawback-high loss can be caused by label noise or ambiguous labels, so prioritizing such data can result in a decline in predictive performance. Compared to the prior art, our method establishes a Bayesian data selection metric and exploits zero-shot predictors to prioritize valuable training data for addressing these issues.",
                    "score": 0.4233872633842616,
                    "section_title": "Related Works",
                    "char_start_offset": 24792,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 404
                        },
                        {
                            "start": 405,
                            "end": 542
                        },
                        {
                            "start": 543,
                            "end": 635
                        },
                        {
                            "start": 636,
                            "end": 908
                        },
                        {
                            "start": 909,
                            "end": 1028
                        },
                        {
                            "start": 1029,
                            "end": 1161
                        },
                        {
                            "start": 1162,
                            "end": 1279
                        },
                        {
                            "start": 1282,
                            "end": 1429
                        },
                        {
                            "start": 1430,
                            "end": 1615
                        },
                        {
                            "start": 1616,
                            "end": 1798
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 234,
                            "end": 238,
                            "matchedPaperCorpusId": "235898952"
                        },
                        {
                            "start": 257,
                            "end": 261,
                            "matchedPaperCorpusId": "55481903"
                        },
                        {
                            "start": 289,
                            "end": 293,
                            "matchedPaperCorpusId": "210932316"
                        },
                        {
                            "start": 651,
                            "end": 655,
                            "matchedPaperCorpusId": "259298636"
                        },
                        {
                            "start": 762,
                            "end": 766,
                            "matchedPaperCorpusId": "253224188"
                        },
                        {
                            "start": 1066,
                            "end": 1069,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 1317,
                            "end": 1320,
                            "matchedPaperCorpusId": "3663876"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7568359375
                }
            ],
            "relevance_judgement": 0.7568359375,
            "relevance_judgment_input_expanded": "# Title: Towards Accelerated Model Training via Bayesian Data Selection\n# Venue: Neural Information Processing Systems\n# Authors: Zhijie Deng, Peng Cui, Jun Zhu\n## Abstract\nMislabeled, duplicated, or biased data in real-world scenarios can lead to prolonged training and even hinder model convergence. Traditional solutions prioritizing easy or hard samples lack the flexibility to handle such a variety simultaneously. Recent work has proposed a more reasonable data selection principle by examining the data's impact on the model's generalization loss. However, its practical adoption relies on less principled approximations and additional holdout data. This work solves these problems by leveraging a lightweight Bayesian treatment and incorporating off-the-shelf zero-shot predictors built on large-scale pre-trained models. The resulting algorithm is efficient and easy to implement. We perform extensive empirical studies on challenging benchmarks with considerable data noise and imbalance in the online batch selection scenario, and observe superior training efficiency over competitive baselines. Notably, on the challenging WebVision benchmark, our method can achieve similar predictive performance with significantly fewer training iterations than leading data selection methods.\n## Related Works\nExtensive methods have been proposed to accelerate model training through techniques such as data pruning, coreset selection, curriculum learning, online batch selection, etc. Data pruning explores various metrics, such as EL2N score [33], forgetting score [40], and classification margin [34], to measure individual differences among data points and retains only the hardest examples for model training. However, data pruning still exhibits limitations when dealing with noisy labels, and some of these metrics are computationally expensive. Coreset selection methods also partially address the problem of accelerating model training. In particular, [43] contributes a data scoring mechanism that is robust to the change of scenarios for coreset selection, and [46] makes an in-depth understanding of the catastrophic accuracy drop issue of one-shot coreset selection and contributes a novel solution to it. However, these methods lack the flexibility to prioritize samples with different properties at various training stages. Curriculum learning, as advocated by [1], prioritizes easy points with low label noise before uniformly training on all data points. While this strategy enhances convergence, it fails to address the issue of skipping redundant points already learned. \n\nOnline batch selection methods [26,19,17] tackle the training acceleration problem by selecting hard data identified by high loss or gradient norm. However, they also suffer from a common drawback-high loss can be caused by label noise or ambiguous labels, so prioritizing such data can result in a decline in predictive performance. Compared to the prior art, our method establishes a Bayesian data selection metric and exploits zero-shot predictors to prioritize valuable training data for addressing these issues.",
            "reference_string": "[261048902 | Deng et al. | 2023 | Citations: 5]"
        },
        {
            "title": "D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 57,
            "citation_count": 34,
            "influential_citation_count": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.07931",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.07931, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "8785371",
                    "name": "Adyasha Maharana"
                },
                {
                    "authorId": "46841632",
                    "name": "Prateek Yadav"
                },
                {
                    "authorId": "2253396640",
                    "name": "Mohit Bansal"
                }
            ],
            "abstract": "Analytical theories suggest that higher-quality data can lead to lower test errors in models trained on a fixed data budget. Moreover, a model can be trained on a lower compute budget without compromising performance if a dataset can be stripped of its redundancies. Coreset selection (or data pruning) seeks to select a subset of the training data so as to maximize the performance of models trained on this subset, also referred to as coreset. There are two dominant approaches: (1) geometry-based data selection for maximizing data diversity in the coreset, and (2) functions that assign difficulty scores to samples based on training dynamics. Optimizing for data diversity leads to a coreset that is biased towards easier samples, whereas, selection by difficulty ranking omits easy samples that are necessary for the training of deep learning models. This demonstrates that data diversity and importance scores are two complementary factors that need to be jointly considered during coreset selection. We represent a dataset as an undirected graph and propose a novel pruning algorithm, D2 Pruning, that uses forward and reverse message passing over this dataset graph for coreset selection. D2 Pruning updates the difficulty scores of each example by incorporating the difficulty of its neighboring examples in the dataset graph. Then, these updated difficulty scores direct a graph-based sampling method to select a coreset that encapsulates both diverse and difficult regions of the dataset space. We evaluate supervised and self-supervised versions of our method on various vision and language datasets. Results show that D2 Pruning improves coreset selection over previous state-of-the-art methods for up to 70% pruning rates. Additionally, we find that using D2 Pruning for filtering large multimodal datasets leads to increased diversity in the dataset and improved generalization of pretrained models.",
            "corpus_id": 263909051,
            "sentences": [
                {
                    "corpus_id": "263909051",
                    "title": "D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning",
                    "text": "CCS enforces a uniform distribution of sample difficulty scores in the coreset, which is beneficial at high pruning rates for providing even coverage over easy and difficult samples. However, at lower pruning rates (or with increasing data budget), difficult training samples yield a lower test loss from deep learning models (Sorscher et al., 2022). The hyperparameters k and \u03b3 in D 2 PRUNING (see Sec. 3) allow flexibility in the distribution of easy/difficult samples in coresets. We find that higher values of \u03b3 and lower value of k in D 2 PRUNING leads to a coreset that is skewed towards more difficult samples and benefits performance at lower pruning rates. Conversely, low \u03b3 and high k lead to an equitable distribution over easy/difficult samples and are more useful for higher pruning rates. See discussion on hyperparameters in Sec.5.2 and qualitative analysis of coresets in Appendix. \n\nResults from the evaluation of various coreset selection methods, including D 2 PRUNING, on NLP datasets are presented in Tab. 2. First, we find that when pretrained language models (PLMs) are finetuned on task-specific datasets, the models do not suffer from a catastrophic decline in performance at high pruning rates, in contrast to models trained from scratch on vision datasets. For IMDB reviews, the performance of finetuned RoBERTa goes from 91.8% at 0% pruning to 81.3% at 90% pruning using random sampling. The performance improves to 87.5% using CCS coreset selection and further improves to 90.3% using D 2 PRUNING. The ANLI dataset has been carefully crafted with an iterative, adversarial human-and-model-in-the-loop process, and hence, is significantly less redundant than conventional NLP datasets. The performance for ANLI falls from 48.8% to 42.8% at 80% pruning using random sampling.",
                    "score": 0.4038242535714923,
                    "section_title": "EXPERIMENTAL SETUP",
                    "char_start_offset": 18305,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 182
                        },
                        {
                            "start": 183,
                            "end": 350
                        },
                        {
                            "start": 351,
                            "end": 483
                        },
                        {
                            "start": 484,
                            "end": 665
                        },
                        {
                            "start": 666,
                            "end": 802
                        },
                        {
                            "start": 803,
                            "end": 897
                        },
                        {
                            "start": 900,
                            "end": 1283
                        },
                        {
                            "start": 1284,
                            "end": 1415
                        },
                        {
                            "start": 1416,
                            "end": 1526
                        },
                        {
                            "start": 1527,
                            "end": 1713
                        },
                        {
                            "start": 1714,
                            "end": 1802
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 326,
                            "end": 349,
                            "matchedPaperCorpusId": "250113273"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.74755859375
                }
            ],
            "relevance_judgement": 0.74755859375,
            "relevance_judgment_input_expanded": "# Title: D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning\n# Venue: arXiv.org\n# Authors: Adyasha Maharana, Prateek Yadav, Mohit Bansal\n## Abstract\nAnalytical theories suggest that higher-quality data can lead to lower test errors in models trained on a fixed data budget. Moreover, a model can be trained on a lower compute budget without compromising performance if a dataset can be stripped of its redundancies. Coreset selection (or data pruning) seeks to select a subset of the training data so as to maximize the performance of models trained on this subset, also referred to as coreset. There are two dominant approaches: (1) geometry-based data selection for maximizing data diversity in the coreset, and (2) functions that assign difficulty scores to samples based on training dynamics. Optimizing for data diversity leads to a coreset that is biased towards easier samples, whereas, selection by difficulty ranking omits easy samples that are necessary for the training of deep learning models. This demonstrates that data diversity and importance scores are two complementary factors that need to be jointly considered during coreset selection. We represent a dataset as an undirected graph and propose a novel pruning algorithm, D2 Pruning, that uses forward and reverse message passing over this dataset graph for coreset selection. D2 Pruning updates the difficulty scores of each example by incorporating the difficulty of its neighboring examples in the dataset graph. Then, these updated difficulty scores direct a graph-based sampling method to select a coreset that encapsulates both diverse and difficult regions of the dataset space. We evaluate supervised and self-supervised versions of our method on various vision and language datasets. Results show that D2 Pruning improves coreset selection over previous state-of-the-art methods for up to 70% pruning rates. Additionally, we find that using D2 Pruning for filtering large multimodal datasets leads to increased diversity in the dataset and improved generalization of pretrained models.\n## EXPERIMENTAL SETUP\nCCS enforces a uniform distribution of sample difficulty scores in the coreset, which is beneficial at high pruning rates for providing even coverage over easy and difficult samples. However, at lower pruning rates (or with increasing data budget), difficult training samples yield a lower test loss from deep learning models (Sorscher et al., 2022). The hyperparameters k and \u03b3 in D 2 PRUNING (see Sec. 3) allow flexibility in the distribution of easy/difficult samples in coresets. We find that higher values of \u03b3 and lower value of k in D 2 PRUNING leads to a coreset that is skewed towards more difficult samples and benefits performance at lower pruning rates. Conversely, low \u03b3 and high k lead to an equitable distribution over easy/difficult samples and are more useful for higher pruning rates. See discussion on hyperparameters in Sec.5.2 and qualitative analysis of coresets in Appendix. \n\nResults from the evaluation of various coreset selection methods, including D 2 PRUNING, on NLP datasets are presented in Tab. 2. First, we find that when pretrained language models (PLMs) are finetuned on task-specific datasets, the models do not suffer from a catastrophic decline in performance at high pruning rates, in contrast to models trained from scratch on vision datasets. For IMDB reviews, the performance of finetuned RoBERTa goes from 91.8% at 0% pruning to 81.3% at 90% pruning using random sampling. The performance improves to 87.5% using CCS coreset selection and further improves to 90.3% using D 2 PRUNING. The ANLI dataset has been carefully crafted with an iterative, adversarial human-and-model-in-the-loop process, and hence, is significantly less redundant than conventional NLP datasets. The performance for ANLI falls from 48.8% to 42.8% at 80% pruning using random sampling.",
            "reference_string": "[263909051 | Maharana et al. | 2023 | Citations: 34]"
        },
        {
            "title": "Sampling Approach Matters: Active Learning for Robotic Language Acquisition",
            "venue": "2020 IEEE International Conference on Big Data (Big Data)",
            "year": 2020,
            "reference_count": 80,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://mdsoar.org/bitstream/11603/20254/1/2011.08021.pdf",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2011.08021, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144339038",
                    "name": "Nisha Pillai"
                },
                {
                    "authorId": "34885007",
                    "name": "Edward Raff"
                },
                {
                    "authorId": "2064957151",
                    "name": "Francis Ferraro"
                },
                {
                    "authorId": "2674440",
                    "name": "Cynthia Matuszek"
                }
            ],
            "abstract": "Ordering the selection of training data using active learning can lead to improvements in learning efficiently from smaller corpora. We present an exploration of active learning approaches applied to three grounded language problems of varying complexity in order to analyze what methods are suitable for improving data efficiency in learning. We present a method for analyzing the complexity of data in this joint problem space, and report on how characteristics of the underlying task, along with design decisions such as feature selection and classification model, drive the results. We observe that representativeness, along with diversity, is crucial in selecting data samples.",
            "corpus_id": 226964591,
            "sentences": [
                {
                    "corpus_id": "226964591",
                    "title": "Sampling Approach Matters: Active Learning for Robotic Language Acquisition",
                    "text": "Ordering the selection of training data using active learning can lead to improvements in learning efficiently from smaller corpora. We present an exploration of active learning approaches applied to three grounded language problems of varying complexity in order to analyze what methods are suitable for improving data efficiency in learning. We present a method for analyzing the complexity of data in this joint problem space, and report on how characteristics of the underlying task, along with design decisions such as feature selection and classification model, drive the results. We observe that representativeness, along with diversity, is crucial in selecting data samples.",
                    "score": 0.41217862564284274,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72509765625
                }
            ],
            "relevance_judgement": 0.72509765625,
            "relevance_judgment_input_expanded": "# Title: Sampling Approach Matters: Active Learning for Robotic Language Acquisition\n# Venue: 2020 IEEE International Conference on Big Data (Big Data)\n# Authors: Nisha Pillai, Edward Raff, Francis Ferraro, Cynthia Matuszek\n## Abstract\nOrdering the selection of training data using active learning can lead to improvements in learning efficiently from smaller corpora. We present an exploration of active learning approaches applied to three grounded language problems of varying complexity in order to analyze what methods are suitable for improving data efficiency in learning. We present a method for analyzing the complexity of data in this joint problem space, and report on how characteristics of the underlying task, along with design decisions such as feature selection and classification model, drive the results. We observe that representativeness, along with diversity, is crucial in selecting data samples.\n",
            "reference_string": "[226964591 | Pillai et al. | 2020 | Citations: 3]"
        },
        {
            "title": "Data Selection Curriculum for Neural Machine Translation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 47,
            "citation_count": 13,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2203.13867",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.13867, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "6838342",
                    "name": "Tasnim Mohiuddin"
                },
                {
                    "authorId": "1755162",
                    "name": "Philipp Koehn"
                },
                {
                    "authorId": "113810201",
                    "name": "Vishrav Chaudhary"
                },
                {
                    "authorId": "2059363961",
                    "name": "James Cross"
                },
                {
                    "authorId": "2116473",
                    "name": "Shruti Bhosale"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                }
            ],
            "abstract": "Neural Machine Translation (NMT) models are typically trained on heterogeneous data that are concatenated and randomly shuffled. However, not all of the training data are equally useful to the model. Curriculum training aims to present the data to the NMT models in a meaningful order. In this work, we introduce a two-stage curriculum training framework for NMT where we fine-tune a base NMT model on subsets of data, selected by both deterministic scoring using pre-trained methods and online scoring that considers prediction scores of the emerging NMT model. Through comprehensive experiments on six language pairs comprising low- and high-resource languages from WMT'21, we have shown that our curriculum strategies consistently demonstrate better quality (up to +2.2 BLEU improvement) and faster convergence (approximately 50% fewer updates).",
            "corpus_id": 247762191,
            "sentences": [
                {
                    "corpus_id": "247762191",
                    "title": "Data Selection Curriculum for Neural Machine Translation",
                    "text": "Curriculum Learning Inspired by human learners, Elman (1993) argues that optimization of neural network training can be accelerated by gradually increasing the difficulty of the concepts. Bengio et al. (2009) were the first to use the term \"curricu-lum learning\" to refer to the easy-to-hard training strategies in the context of machine learning. Using an easy-to-hard curriculum based on increasing vocabulary size in language model training, they achieved performance improvement. Recent work (Jiang et al., 2015;Hacohen and Weinshall, 2019;Zhou et al., 2020a) shows that manoeuvring the sequence of training data can improve both training efficiency and model accuracy. Several studies show the effectiveness of the difficulty-based curriculum learning in a wide range of NLP tasks including task-specific word representation learning (Tsvetkov et al., 2016), natural language understanding tasks (Sachan and Xing, 2016;Xu et al., 2020a), reading comprehension (Tay et al., 2019), and language modeling (Campos, 2021). Several studies show the effectiveness of the difficultybased curriculum learning in a wide range of NLP tasks (Cirik et al., 2016;Liu et al., 2018). \n\nCurriculum Learning in NMT The difficultybased curriculum in NMT was first explored by Kocmi and Bojar (2017) (Zhao et al., 2020). In contrast, our proposed two-stage training framework for NMT fine-tunes the base model from the warm-up stage on selected subsets of data. Our data-selection curriculum training framework is more realistic, resembling the formal education system as discussed in \u00a75.2. \n\nSelf-paced Learning in NMT Here, the model itself measures the difficulty of the training samples to adjust the learning pace (Kumar et al., 2010). In their approach, Wan et al. (2020) first train the NMT model for M passes on the data and cache the translation probabilities to find variance. The lower variance of the translation probabilities of a sample reflects higher confidence. Later, they use the confidence scores as factors to weight the loss to control the model updates.",
                    "score": 0.4181886547777687,
                    "section_title": "Related Work",
                    "char_start_offset": 21431,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 187
                        },
                        {
                            "start": 188,
                            "end": 347
                        },
                        {
                            "start": 348,
                            "end": 483
                        },
                        {
                            "start": 484,
                            "end": 673
                        },
                        {
                            "start": 674,
                            "end": 1022
                        },
                        {
                            "start": 1023,
                            "end": 1172
                        },
                        {
                            "start": 1175,
                            "end": 1305
                        },
                        {
                            "start": 1306,
                            "end": 1446
                        },
                        {
                            "start": 1447,
                            "end": 1575
                        },
                        {
                            "start": 1578,
                            "end": 1725
                        },
                        {
                            "start": 1726,
                            "end": 1871
                        },
                        {
                            "start": 1872,
                            "end": 1963
                        },
                        {
                            "start": 1964,
                            "end": 2061
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 48,
                            "end": 60,
                            "matchedPaperCorpusId": "2105042"
                        },
                        {
                            "start": 188,
                            "end": 208,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 496,
                            "end": 516,
                            "matchedPaperCorpusId": "10891229"
                        },
                        {
                            "start": 516,
                            "end": 544,
                            "matchedPaperCorpusId": "102350936"
                        },
                        {
                            "start": 544,
                            "end": 563,
                            "matchedPaperCorpusId": "227275560"
                        },
                        {
                            "start": 839,
                            "end": 862,
                            "matchedPaperCorpusId": "3963337"
                        },
                        {
                            "start": 901,
                            "end": 924,
                            "matchedPaperCorpusId": "16503693"
                        },
                        {
                            "start": 924,
                            "end": 941,
                            "matchedPaperCorpusId": "220045816"
                        },
                        {
                            "start": 965,
                            "end": 983,
                            "matchedPaperCorpusId": "166228313"
                        },
                        {
                            "start": 1154,
                            "end": 1171,
                            "matchedPaperCorpusId": "51606954"
                        },
                        {
                            "start": 1262,
                            "end": 1284,
                            "matchedPaperCorpusId": "26468344"
                        },
                        {
                            "start": 1285,
                            "end": 1304,
                            "matchedPaperCorpusId": "211102338"
                        },
                        {
                            "start": 1704,
                            "end": 1724,
                            "matchedPaperCorpusId": "1977996"
                        },
                        {
                            "start": 1745,
                            "end": 1762,
                            "matchedPaperCorpusId": "222272176"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71240234375
                },
                {
                    "corpus_id": "247762191",
                    "title": "Data Selection Curriculum for Neural Machine Translation",
                    "text": "In contrast, the online curriculum dynamically selects a subset of the data for each epoch without requiring any external measure. Specifically, it leverages prediction scores of the emerging NMT model which are the training by-product. \n\nFor picking the data subset in the online curriculum, we investigate two approaches of dataselection window -static and dynamic. Even though the size of the data-selection window is constant throughout the training in the static approach, the samples in the selected subset vary from epoch-to-epoch due to the change in their prediction scores. In contrast, we change the dataselection window size in the dynamic approach by either expanding or shrinking. \n\nComprehensive experiments on six language pairs (12 translation directions) comprising lowand high-resource languages from WMT'21 (Akhbardeh et al., 2021) reveal that our curricu-lum strategies consistently demonstrate better performance compared to the baseline trained on all the data (up to +2.2 BLEU). We observe bigger gains in the high-resource pairs compared to the low-resource ones. Interestingly, we find that the online curriculum approaches perform on par with the deterministic approaches while not using any external pretrained models. Our proposed curriculum training approaches not only exhibit better performance but also converge much faster requiring approximately 50% fewer updates.",
                    "score": 0.5506536532278031,
                    "section_title": "Introduction",
                    "char_start_offset": 2071,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 130
                        },
                        {
                            "start": 131,
                            "end": 236
                        },
                        {
                            "start": 239,
                            "end": 367
                        },
                        {
                            "start": 368,
                            "end": 583
                        },
                        {
                            "start": 584,
                            "end": 694
                        },
                        {
                            "start": 697,
                            "end": 1002
                        },
                        {
                            "start": 1003,
                            "end": 1088
                        },
                        {
                            "start": 1089,
                            "end": 1246
                        },
                        {
                            "start": 1247,
                            "end": 1399
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7119140625
                }
            ],
            "relevance_judgement": 0.71240234375,
            "relevance_judgment_input_expanded": "# Title: Data Selection Curriculum for Neural Machine Translation\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Tasnim Mohiuddin, Philipp Koehn, Vishrav Chaudhary, James Cross, Shruti Bhosale, Shafiq R. Joty\n## Abstract\nNeural Machine Translation (NMT) models are typically trained on heterogeneous data that are concatenated and randomly shuffled. However, not all of the training data are equally useful to the model. Curriculum training aims to present the data to the NMT models in a meaningful order. In this work, we introduce a two-stage curriculum training framework for NMT where we fine-tune a base NMT model on subsets of data, selected by both deterministic scoring using pre-trained methods and online scoring that considers prediction scores of the emerging NMT model. Through comprehensive experiments on six language pairs comprising low- and high-resource languages from WMT'21, we have shown that our curriculum strategies consistently demonstrate better quality (up to +2.2 BLEU improvement) and faster convergence (approximately 50% fewer updates).\n## Introduction\nIn contrast, the online curriculum dynamically selects a subset of the data for each epoch without requiring any external measure. Specifically, it leverages prediction scores of the emerging NMT model which are the training by-product. \n\nFor picking the data subset in the online curriculum, we investigate two approaches of dataselection window -static and dynamic. Even though the size of the data-selection window is constant throughout the training in the static approach, the samples in the selected subset vary from epoch-to-epoch due to the change in their prediction scores. In contrast, we change the dataselection window size in the dynamic approach by either expanding or shrinking. \n\nComprehensive experiments on six language pairs (12 translation directions) comprising lowand high-resource languages from WMT'21 (Akhbardeh et al., 2021) reveal that our curricu-lum strategies consistently demonstrate better performance compared to the baseline trained on all the data (up to +2.2 BLEU). We observe bigger gains in the high-resource pairs compared to the low-resource ones. Interestingly, we find that the online curriculum approaches perform on par with the deterministic approaches while not using any external pretrained models. Our proposed curriculum training approaches not only exhibit better performance but also converge much faster requiring approximately 50% fewer updates.\n\n## Related Work\nCurriculum Learning Inspired by human learners, Elman (1993) argues that optimization of neural network training can be accelerated by gradually increasing the difficulty of the concepts. Bengio et al. (2009) were the first to use the term \"curricu-lum learning\" to refer to the easy-to-hard training strategies in the context of machine learning. Using an easy-to-hard curriculum based on increasing vocabulary size in language model training, they achieved performance improvement. Recent work (Jiang et al., 2015;Hacohen and Weinshall, 2019;Zhou et al., 2020a) shows that manoeuvring the sequence of training data can improve both training efficiency and model accuracy. Several studies show the effectiveness of the difficulty-based curriculum learning in a wide range of NLP tasks including task-specific word representation learning (Tsvetkov et al., 2016), natural language understanding tasks (Sachan and Xing, 2016;Xu et al., 2020a), reading comprehension (Tay et al., 2019), and language modeling (Campos, 2021). Several studies show the effectiveness of the difficultybased curriculum learning in a wide range of NLP tasks (Cirik et al., 2016;Liu et al., 2018). \n\nCurriculum Learning in NMT The difficultybased curriculum in NMT was first explored by Kocmi and Bojar (2017) (Zhao et al., 2020). In contrast, our proposed two-stage training framework for NMT fine-tunes the base model from the warm-up stage on selected subsets of data. Our data-selection curriculum training framework is more realistic, resembling the formal education system as discussed in \u00a75.2. \n\nSelf-paced Learning in NMT Here, the model itself measures the difficulty of the training samples to adjust the learning pace (Kumar et al., 2010). In their approach, Wan et al. (2020) first train the NMT model for M passes on the data and cache the translation probabilities to find variance. The lower variance of the translation probabilities of a sample reflects higher confidence. Later, they use the confidence scores as factors to weight the loss to control the model updates.",
            "reference_string": "[247762191 | Mohiuddin et al. | 2022 | Citations: 13]"
        },
        {
            "title": "Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2023,
            "reference_count": 34,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2303.17910",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.17910, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2226017688",
                    "name": "Min Liu"
                },
                {
                    "authorId": "145854784",
                    "name": "Yu Bao"
                },
                {
                    "authorId": "144562857",
                    "name": "Chengqi Zhao"
                },
                {
                    "authorId": "2124946880",
                    "name": "Shujian Huang"
                }
            ],
            "abstract": "Benefiting from the sequence-level knowledge distillation, the Non-Autoregressive Transformer (NAT) achieves great success in neural machine translation tasks. \nHowever, existing knowledge distillation has side effects, such as propagating errors from the teacher to NAT students, which may limit further improvements of NAT models and are rarely discussed in existing research.\nIn this paper, we introduce selective knowledge distillation by introducing an NAT evaluator to select NAT-friendly targets that are of high quality and easy to learn.\nIn addition, we introduce a simple yet effective progressive distillation method to boost NAT performance. \nExperiment results on multiple WMT language directions and several representative NAT models show that our approach can realize a flexible trade-off between the quality and complexity of training data for NAT models, achieving strong performances.\nFurther analysis shows that distilling only 5% of the raw translations can help an NAT outperform its counterpart trained on raw data by about 2.4 BLEU.",
            "corpus_id": 257901096,
            "sentences": [
                {
                    "corpus_id": "257901096",
                    "title": "Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation",
                    "text": "Motivated by the success of curriculum learning (Qian et al. 2021a;Guo et al. 2020;Liu et al. 2020), we further introduce a hard-to-easy learning strategy to improve the performance. Ding et al. (2021b) show that pretraining with raw data can improve the performance of NAT by rejuvenating low-frequency words. To keep the merits of low-mode, they further trained the pretrained model on distilled data. We combine this idea with our data selection method by decreasing the ratio of raw data in the training process. Specifically, the training data for each update can be formulated as: \n\nwhere T k and D k denote the threshold and the set of tuples (X, Y, \u0176 ) for the kth update respectively. T k can be determined by a preset function or feedbacks from the NAT student. In our experiments, we adopt a linear function for T k which is computed as \n\n, where K is the total number of updates, the constants T 0 and T 1 can be determined according to the distribution of score P (score(X, Y )) given a specific NAT evaluator and the raw training data. The whole data selection process can be found in Algorithm 1. This process is an additional stage following standard training procedures for NAT, thus being generic to various data and architectures.",
                    "score": 0.461388956207355,
                    "section_title": "Hard-to-Easy Data Selection",
                    "char_start_offset": 11928,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 182
                        },
                        {
                            "start": 183,
                            "end": 310
                        },
                        {
                            "start": 311,
                            "end": 403
                        },
                        {
                            "start": 404,
                            "end": 516
                        },
                        {
                            "start": 517,
                            "end": 586
                        },
                        {
                            "start": 589,
                            "end": 693
                        },
                        {
                            "start": 694,
                            "end": 771
                        },
                        {
                            "start": 772,
                            "end": 847
                        },
                        {
                            "start": 850,
                            "end": 1049
                        },
                        {
                            "start": 1050,
                            "end": 1111
                        },
                        {
                            "start": 1112,
                            "end": 1249
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 48,
                            "end": 67,
                            "matchedPaperCorpusId": "221150562"
                        },
                        {
                            "start": 67,
                            "end": 83,
                            "matchedPaperCorpusId": "220046693"
                        },
                        {
                            "start": 183,
                            "end": 202,
                            "matchedPaperCorpusId": "229923128"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71142578125
                }
            ],
            "relevance_judgement": 0.71142578125,
            "relevance_judgment_input_expanded": "# Title: Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Min Liu, Yu Bao, Chengqi Zhao, Shujian Huang\n## Abstract\nBenefiting from the sequence-level knowledge distillation, the Non-Autoregressive Transformer (NAT) achieves great success in neural machine translation tasks. \nHowever, existing knowledge distillation has side effects, such as propagating errors from the teacher to NAT students, which may limit further improvements of NAT models and are rarely discussed in existing research.\nIn this paper, we introduce selective knowledge distillation by introducing an NAT evaluator to select NAT-friendly targets that are of high quality and easy to learn.\nIn addition, we introduce a simple yet effective progressive distillation method to boost NAT performance. \nExperiment results on multiple WMT language directions and several representative NAT models show that our approach can realize a flexible trade-off between the quality and complexity of training data for NAT models, achieving strong performances.\nFurther analysis shows that distilling only 5% of the raw translations can help an NAT outperform its counterpart trained on raw data by about 2.4 BLEU.\n## Hard-to-Easy Data Selection\nMotivated by the success of curriculum learning (Qian et al. 2021a;Guo et al. 2020;Liu et al. 2020), we further introduce a hard-to-easy learning strategy to improve the performance. Ding et al. (2021b) show that pretraining with raw data can improve the performance of NAT by rejuvenating low-frequency words. To keep the merits of low-mode, they further trained the pretrained model on distilled data. We combine this idea with our data selection method by decreasing the ratio of raw data in the training process. Specifically, the training data for each update can be formulated as: \n\nwhere T k and D k denote the threshold and the set of tuples (X, Y, \u0176 ) for the kth update respectively. T k can be determined by a preset function or feedbacks from the NAT student. In our experiments, we adopt a linear function for T k which is computed as \n\n, where K is the total number of updates, the constants T 0 and T 1 can be determined according to the distribution of score P (score(X, Y )) given a specific NAT evaluator and the raw training data. The whole data selection process can be found in Algorithm 1. This process is an additional stage following standard training procedures for NAT, thus being generic to various data and architectures.",
            "reference_string": "[257901096 | Liu et al. | 2023 | Citations: 3]"
        },
        {
            "title": "An Empirical Exploration of Curriculum Learning for Neural Machine Translation",
            "venue": "arXiv.org",
            "year": 2018,
            "reference_count": 31,
            "citation_count": 112,
            "influential_citation_count": 14,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.00739, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "49469742",
                    "name": "Xuan Zhang"
                },
                {
                    "authorId": "48387892",
                    "name": "Manish Kumar"
                },
                {
                    "authorId": "3115181",
                    "name": "Huda Khayrallah"
                },
                {
                    "authorId": "38730896",
                    "name": "Kenton Murray"
                },
                {
                    "authorId": "3456371",
                    "name": "Jeremy Gwinnup"
                },
                {
                    "authorId": "3219152",
                    "name": "Marianna J. Martindale"
                },
                {
                    "authorId": "145324163",
                    "name": "Paul McNamee"
                },
                {
                    "authorId": "1800354",
                    "name": "Kevin Duh"
                },
                {
                    "authorId": "2954727",
                    "name": "Marine Carpuat"
                }
            ],
            "abstract": "Machine translation systems based on deep neural networks are expensive to train. Curriculum learning aims to address this issue by choosing the order in which samples are presented during training to help train better models faster. We adopt a probabilistic view of curriculum learning, which lets us flexibly evaluate the impact of curricula design, and perform an extensive exploration on a German-English translation task. Results show that it is possible to improve convergence time at no loss in translation quality. However, results are highly sensitive to the choice of sample difficulty criteria, curriculum schedule and other hyperparameters.",
            "corpus_id": 53295888,
            "sentences": [
                {
                    "corpus_id": "53295888",
                    "title": "An Empirical Exploration of Curriculum Learning for Neural Machine Translation",
                    "text": "We consider a wide range of schedules, based not only on the easy-to-difficult ordering, but also on strategies developed independently from curriculum learning, such as dynamic sampling and boosting (Zhang et al., 2017;van der Wees et al., 2017;Wang et al., 2018). \n\nWe conduct an extensive empirical exploration of curriculum learning on a German-English translation task, implementing all training strategies in the Sockeye NMT toolkit. 1 . Our experiments confirm that curriculum learning can improve convergence speed without loss of translation quality, and show that viewing curriculum learning more flexibly than strictly training on easy samples first has some benefits. We also demonstrate that curriculum learning is highly sensitive to hyperpa-rameters, and no clear single best strategy emerges from the experiments. \n\nIn this sense, our conclusions are both positive and negative: We have confirmed that curriculum learning can be an effective method for training expensive models like those in NMT, but careful design of the specific curriculum hyperparameters is important in practice. \n\n2 Related Work Bengio et al. (2009) coined the term of curriculum learning to refer to techniques that guide the training of learning systems \"by choosing which examples to present and in which order to present them in the learning system\", and hypothesize that training on easier samples first is beneficial. While organizing training samples based on difficulty has been demonstrated in NLP outside of neural models -e.g., Spitkovsky et al. (2010) bootstrap unsupervised dependency parsers by learning from incrementally longer sentences -curriculum learning has gained popularity to address the difficult optimization problem of training deep neural models (Bengio, 2012). Bengio et al. (2009) improve neural language model training using a curriculum based on increasing vocabulary size. More recently, Tsvetkov et al. (2016) improve word embedding training using Bayesian optimization to order paragraphs in the training corpus based on a range of distributional and linguistic features (diversity, simplicity, prototypicality).",
                    "score": 0.463345034867918,
                    "section_title": "Introduction",
                    "char_start_offset": 1954,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 265
                        },
                        {
                            "start": 268,
                            "end": 443
                        },
                        {
                            "start": 444,
                            "end": 679
                        },
                        {
                            "start": 680,
                            "end": 829
                        },
                        {
                            "start": 832,
                            "end": 1101
                        },
                        {
                            "start": 1104,
                            "end": 1413
                        },
                        {
                            "start": 1414,
                            "end": 1779
                        },
                        {
                            "start": 1780,
                            "end": 1895
                        },
                        {
                            "start": 1896,
                            "end": 2137
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 200,
                            "end": 220,
                            "matchedPaperCorpusId": "550225"
                        },
                        {
                            "start": 220,
                            "end": 246,
                            "matchedPaperCorpusId": "7921428"
                        },
                        {
                            "start": 246,
                            "end": 264,
                            "matchedPaperCorpusId": "20639213"
                        },
                        {
                            "start": 1119,
                            "end": 1139,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 1529,
                            "end": 1553,
                            "matchedPaperCorpusId": "1363892"
                        },
                        {
                            "start": 1780,
                            "end": 1800,
                            "matchedPaperCorpusId": "873046"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.708984375
                },
                {
                    "corpus_id": "53295888",
                    "title": "An Empirical Exploration of Curriculum Learning for Neural Machine Translation",
                    "text": "Curriculum learning (Bengio et al., 2009) hypothesizes that choosing the order in which training samples are presented to a learning system can help train better models faster. In particular, presenting samples that are easier to learn from before presenting difficult samples is an intuitively attractive idea, which has been applied in various ways in Machine Learning and Natural Language Processing tasks (Bengio et al., 2009;Tsvetkov et al., 2016;Cirik et al., 2016;Graves et al., 2017, inter alia). \n\nIn this paper, we conduct an empirical exploration of curriculum learning for Neural Machine Translation (NMT). NMT is a good test case for curriculum learning as training is prohibitively slow in the large data conditions required to reach good performance (Koehn and Knowles, 2017). However, designing a curriculum for NMT training is a complex problem. First, it is not clear how to quantify sample difficulty for this task. Second, NMT systems already rely on established data organization methods to deal with the scale and varying length of training samples (Khomenko et al., 2016;Doetsch et al., 2017;Sennrich et al., 2017;Hieber et al., 2017), and it is not clear how a curriculum should interact with these existing design decisions. Kocmi and Bojar (2017) showed that constructing and ordering mini-batches based on sample length or word frequency helps when training for one epoch. It remains to be seen how curricula impact training until convergence. \n\nTo address these issues, we adopt a probabilistic view of curriculum learning that lets us explore a wide range of curricula flexibly. Our approach does not order samples in a deterministic fashion. Instead, each sample has a probability of being selected for training, and this probability changes depending on the difficulty of the sample and on the curriculum's schedule. We explore difficulty criteria based on NMT model scores as well as linguistic properties. We consider a wide range of schedules, based not only on the easy-to-difficult ordering, but also on strategies developed independently from curriculum learning, such as dynamic sampling and boosting (Zhang et al., 2017;van der Wees et al., 2017;Wang et al., 2018).",
                    "score": 0.5457246045803575,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 176
                        },
                        {
                            "start": 177,
                            "end": 504
                        },
                        {
                            "start": 507,
                            "end": 618
                        },
                        {
                            "start": 619,
                            "end": 791
                        },
                        {
                            "start": 792,
                            "end": 862
                        },
                        {
                            "start": 863,
                            "end": 934
                        },
                        {
                            "start": 935,
                            "end": 1249
                        },
                        {
                            "start": 1250,
                            "end": 1399
                        },
                        {
                            "start": 1400,
                            "end": 1470
                        },
                        {
                            "start": 1473,
                            "end": 1607
                        },
                        {
                            "start": 1608,
                            "end": 1671
                        },
                        {
                            "start": 1672,
                            "end": 1847
                        },
                        {
                            "start": 1848,
                            "end": 1938
                        },
                        {
                            "start": 1939,
                            "end": 2204
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 20,
                            "end": 41,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 409,
                            "end": 430,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 765,
                            "end": 790,
                            "matchedPaperCorpusId": "11080756"
                        },
                        {
                            "start": 1071,
                            "end": 1094,
                            "matchedPaperCorpusId": "14333788"
                        },
                        {
                            "start": 1115,
                            "end": 1137,
                            "matchedPaperCorpusId": "905565"
                        },
                        {
                            "start": 1250,
                            "end": 1272,
                            "matchedPaperCorpusId": "26468344"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68798828125
                }
            ],
            "relevance_judgement": 0.708984375,
            "relevance_judgment_input_expanded": "# Title: An Empirical Exploration of Curriculum Learning for Neural Machine Translation\n# Venue: arXiv.org\n# Authors: Xuan Zhang, Manish Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J. Martindale, Paul McNamee, Kevin Duh, Marine Carpuat\n## Abstract\nMachine translation systems based on deep neural networks are expensive to train. Curriculum learning aims to address this issue by choosing the order in which samples are presented during training to help train better models faster. We adopt a probabilistic view of curriculum learning, which lets us flexibly evaluate the impact of curricula design, and perform an extensive exploration on a German-English translation task. Results show that it is possible to improve convergence time at no loss in translation quality. However, results are highly sensitive to the choice of sample difficulty criteria, curriculum schedule and other hyperparameters.\n## Introduction\nCurriculum learning (Bengio et al., 2009) hypothesizes that choosing the order in which training samples are presented to a learning system can help train better models faster. In particular, presenting samples that are easier to learn from before presenting difficult samples is an intuitively attractive idea, which has been applied in various ways in Machine Learning and Natural Language Processing tasks (Bengio et al., 2009;Tsvetkov et al., 2016;Cirik et al., 2016;Graves et al., 2017, inter alia). \n\nIn this paper, we conduct an empirical exploration of curriculum learning for Neural Machine Translation (NMT). NMT is a good test case for curriculum learning as training is prohibitively slow in the large data conditions required to reach good performance (Koehn and Knowles, 2017). However, designing a curriculum for NMT training is a complex problem. First, it is not clear how to quantify sample difficulty for this task. Second, NMT systems already rely on established data organization methods to deal with the scale and varying length of training samples (Khomenko et al., 2016;Doetsch et al., 2017;Sennrich et al., 2017;Hieber et al., 2017), and it is not clear how a curriculum should interact with these existing design decisions. Kocmi and Bojar (2017) showed that constructing and ordering mini-batches based on sample length or word frequency helps when training for one epoch. It remains to be seen how curricula impact training until convergence. \n\nTo address these issues, we adopt a probabilistic view of curriculum learning that lets us explore a wide range of curricula flexibly. Our approach does not order samples in a deterministic fashion. Instead, each sample has a probability of being selected for training, and this probability changes depending on the difficulty of the sample and on the curriculum's schedule. We explore difficulty criteria based on NMT model scores as well as linguistic properties. We consider a wide range of schedules, based not only on the easy-to-difficult ordering, but also on strategies developed independently from curriculum learning, such as dynamic sampling and boosting (Zhang et al., 2017;van der Wees et al., 2017;Wang et al., 2018).\n...\nWe consider a wide range of schedules, based not only on the easy-to-difficult ordering, but also on strategies developed independently from curriculum learning, such as dynamic sampling and boosting (Zhang et al., 2017;van der Wees et al., 2017;Wang et al., 2018). \n\nWe conduct an extensive empirical exploration of curriculum learning on a German-English translation task, implementing all training strategies in the Sockeye NMT toolkit. 1 . Our experiments confirm that curriculum learning can improve convergence speed without loss of translation quality, and show that viewing curriculum learning more flexibly than strictly training on easy samples first has some benefits. We also demonstrate that curriculum learning is highly sensitive to hyperpa-rameters, and no clear single best strategy emerges from the experiments. \n\nIn this sense, our conclusions are both positive and negative: We have confirmed that curriculum learning can be an effective method for training expensive models like those in NMT, but careful design of the specific curriculum hyperparameters is important in practice. \n\n2 Related Work Bengio et al. (2009) coined the term of curriculum learning to refer to techniques that guide the training of learning systems \"by choosing which examples to present and in which order to present them in the learning system\", and hypothesize that training on easier samples first is beneficial. While organizing training samples based on difficulty has been demonstrated in NLP outside of neural models -e.g., Spitkovsky et al. (2010) bootstrap unsupervised dependency parsers by learning from incrementally longer sentences -curriculum learning has gained popularity to address the difficult optimization problem of training deep neural models (Bengio, 2012). Bengio et al. (2009) improve neural language model training using a curriculum based on increasing vocabulary size. More recently, Tsvetkov et al. (2016) improve word embedding training using Bayesian optimization to order paragraphs in the training corpus based on a range of distributional and linguistic features (diversity, simplicity, prototypicality).",
            "reference_string": "[53295888 | Zhang et al. | 2018 | Citations: 112]"
        },
        {
            "title": "Can training neural language models on a curriculum with developmentally plausible data improve alignment with human reading behavior?",
            "venue": "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning",
            "year": 2023,
            "reference_count": 23,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.18761, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268760204",
                    "name": "Aryaman Chobey"
                },
                {
                    "authorId": "2268760018",
                    "name": "Oliver Smith"
                },
                {
                    "authorId": "2268796061",
                    "name": "Anzi Wang"
                },
                {
                    "authorId": "2268760229",
                    "name": "Grusha Prasad"
                }
            ],
            "abstract": "The use of neural language models to model human behavior has met with mixed success. While some work has found that the surprisal estimates from these models can be used to predict a wide range of human neural and behavioral responses, other work studying more complex syntactic phenomena has found that these surprisal estimates generate incorrect behavioral predictions. This paper explores the extent to which the misalignment between empirical and model-predicted behavior can be minimized by training models on more developmentally plausible data, such as in the BabyLM Challenge. We trained teacher language models on the BabyLM\"strict-small\"dataset and used sentence level surprisal estimates from these teacher models to create a curriculum. We found tentative evidence that our curriculum made it easier for models to acquire linguistic knowledge from the training data: on the subset of tasks in the BabyLM challenge suite evaluating models' grammatical knowledge of English, models first trained on the BabyLM data curriculum and then on a few randomly ordered training epochs performed slightly better than models trained on randomly ordered epochs alone. This improved linguistic knowledge acquisition did not result in better alignment with human reading behavior, however: models trained on the BabyLM dataset (with or without a curriculum) generated predictions that were as misaligned with human behavior as models trained on larger less curated datasets. This suggests that training on developmentally plausible datasets alone is likely insufficient to generate language models capable of accurately predicting human language processing.",
            "corpus_id": 265506572,
            "sentences": [
                {
                    "corpus_id": "265506572",
                    "title": "Can training neural language models on a curriculum with developmentally plausible data improve alignment with human reading behavior?",
                    "text": "Curriculum learning (Bengio et al., 2009) refers to training models through a difficulty-based ordering of training examples (i.e. a curriculum), most often \"starting small\" (Elman, 1993) from easy examples before progressing to increasingly difficult sentences. In NLP, curriculum learning has been widely used for Machine Translation (e.g., Platanios et al., 2019), but has also been applied more recently to other natural language understanding tasks (Xu et al., 2020). For a survey see Soviany et al. (2022); Wang et al. (2021). \n\nThere are two steps involved in designing a curriculum: assigning a difficulty score to every training example (\"difficulty measurer\") and using these difficulty scores to determine the order in which training examples are presented to the model (\"training scheduler\") (Wang et al., 2021).",
                    "score": 0.44855477550976,
                    "section_title": "Background",
                    "char_start_offset": 3761,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 262
                        },
                        {
                            "start": 263,
                            "end": 472
                        },
                        {
                            "start": 473,
                            "end": 532
                        },
                        {
                            "start": 535,
                            "end": 824
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 20,
                            "end": 41,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 454,
                            "end": 471,
                            "matchedPaperCorpusId": "220045816"
                        },
                        {
                            "start": 490,
                            "end": 511,
                            "matchedPaperCorpusId": "231709290"
                        },
                        {
                            "start": 513,
                            "end": 531,
                            "matchedPaperCorpusId": "232362223"
                        },
                        {
                            "start": 804,
                            "end": 823,
                            "matchedPaperCorpusId": "232362223"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69384765625
                }
            ],
            "relevance_judgement": 0.69384765625,
            "relevance_judgment_input_expanded": "# Title: Can training neural language models on a curriculum with developmentally plausible data improve alignment with human reading behavior?\n# Venue: Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning\n# Authors: Aryaman Chobey, Oliver Smith, Anzi Wang, Grusha Prasad\n## Abstract\nThe use of neural language models to model human behavior has met with mixed success. While some work has found that the surprisal estimates from these models can be used to predict a wide range of human neural and behavioral responses, other work studying more complex syntactic phenomena has found that these surprisal estimates generate incorrect behavioral predictions. This paper explores the extent to which the misalignment between empirical and model-predicted behavior can be minimized by training models on more developmentally plausible data, such as in the BabyLM Challenge. We trained teacher language models on the BabyLM\"strict-small\"dataset and used sentence level surprisal estimates from these teacher models to create a curriculum. We found tentative evidence that our curriculum made it easier for models to acquire linguistic knowledge from the training data: on the subset of tasks in the BabyLM challenge suite evaluating models' grammatical knowledge of English, models first trained on the BabyLM data curriculum and then on a few randomly ordered training epochs performed slightly better than models trained on randomly ordered epochs alone. This improved linguistic knowledge acquisition did not result in better alignment with human reading behavior, however: models trained on the BabyLM dataset (with or without a curriculum) generated predictions that were as misaligned with human behavior as models trained on larger less curated datasets. This suggests that training on developmentally plausible datasets alone is likely insufficient to generate language models capable of accurately predicting human language processing.\n## Background\nCurriculum learning (Bengio et al., 2009) refers to training models through a difficulty-based ordering of training examples (i.e. a curriculum), most often \"starting small\" (Elman, 1993) from easy examples before progressing to increasingly difficult sentences. In NLP, curriculum learning has been widely used for Machine Translation (e.g., Platanios et al., 2019), but has also been applied more recently to other natural language understanding tasks (Xu et al., 2020). For a survey see Soviany et al. (2022); Wang et al. (2021). \n\nThere are two steps involved in designing a curriculum: assigning a difficulty score to every training example (\"difficulty measurer\") and using these difficulty scores to determine the order in which training examples are presented to the model (\"training scheduler\") (Wang et al., 2021).",
            "reference_string": "[265506572 | Chobey et al. | 2023 | Citations: 5]"
        },
        {
            "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 24,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.07490, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2301165169",
                    "name": "Jisu Kim"
                },
                {
                    "authorId": "2301167177",
                    "name": "Juhwan Lee"
                }
            ],
            "abstract": "The rapid advancement of Large Language Models (LLMs) has improved text understanding and generation but poses challenges in computational resources. This study proposes a curriculum learning-inspired, data-centric training strategy that begins with simpler tasks and progresses to more complex ones, using criteria such as prompt length, attention scores, and loss values to structure the training data. Experiments with Mistral-7B (Jiang et al., 2023) and Gemma-7B (Team et al., 2024) models demonstrate that curriculum learning slightly improves performance compared to traditional random data shuffling. Notably, we observed that sorting data based on our proposed attention criteria generally led to better performance. This approach offers a sustainable method to enhance LLM performance without increasing model size or dataset volume, addressing scalability challenges in LLM training.",
            "corpus_id": 269756933,
            "sentences": [
                {
                    "corpus_id": "269756933",
                    "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
                    "text": "The rapid advancement of Large Language Models (LLMs) has improved text understanding and generation but poses challenges in computational resources. This study proposes a curriculum learning-inspired, data-centric training strategy that begins with simpler tasks and progresses to more complex ones, using criteria such as prompt length, attention scores, and loss values to structure the training data. Experiments with Mistral-7B (Jiang et al., 2023) and Gemma-7B (Team et al., 2024) models demonstrate that curriculum learning slightly improves performance compared to traditional random data shuffling. Notably, we observed that sorting data based on our proposed attention criteria generally led to better performance. This approach offers a sustainable method to enhance LLM performance without increasing model size or dataset volume, addressing scalability challenges in LLM training.",
                    "score": 0.41045944639441756,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67236328125
                }
            ],
            "relevance_judgement": 0.67236328125,
            "relevance_judgment_input_expanded": "# Title: Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning\n# Venue: arXiv.org\n# Authors: Jisu Kim, Juhwan Lee\n## Abstract\nThe rapid advancement of Large Language Models (LLMs) has improved text understanding and generation but poses challenges in computational resources. This study proposes a curriculum learning-inspired, data-centric training strategy that begins with simpler tasks and progresses to more complex ones, using criteria such as prompt length, attention scores, and loss values to structure the training data. Experiments with Mistral-7B (Jiang et al., 2023) and Gemma-7B (Team et al., 2024) models demonstrate that curriculum learning slightly improves performance compared to traditional random data shuffling. Notably, we observed that sorting data based on our proposed attention criteria generally led to better performance. This approach offers a sustainable method to enhance LLM performance without increasing model size or dataset volume, addressing scalability challenges in LLM training.\n",
            "reference_string": "[269756933 | Kim et al. | 2024 | Citations: 10]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "264439064",
            "title": "Irreducible Curriculum for Language Model Pretraining",
            "text": "Automatic data selection and curriculum design for training large language models is challenging, with only a few existing methods showing improvements over standard training. Furthermore, current schemes focus on domain-level selection, overlooking the more fine-grained contributions of each individual training point. It is difficult to apply traditional datapoint selection methods on large language models: most online batch selection methods perform two-times forward or backward passes, which introduces considerable extra costs with large-scale models. To mitigate these obstacles, we propose irreducible curriculum as a curriculum learning algorithm for language model pretraining, which prioritizes samples with higher learnability. Specifically, to avoid prohibitive extra computation overhead, we simulate the sample loss along the main model's training trajectory using a small-scale proxy model. Our experiments on the RedPajama-1B dataset demonstrate a consistent improvement on validation perplexity across all 7 domains compared to random uniform baseline and the anti-curriculum strategy. Our method also reduces the sharpness of the network and illustrates a better 5-shot accuracy on MMLU benchmarks.",
            "score": 0.6515444245883804,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "237553761",
            "title": "On the Role of Corpus Ordering in Language Modeling",
            "text": "In this paper, we investigate several corpus ordering and training schedules as a way of exploring the effectiveness of curriculum learning, specifically with regards to obtaining better models and potentially reducing training time and/or cost, for pretraining transformer language models (English, in this case). Between the two notions of complexity, one computed at sentence-level, and the other at document-level, we find them to perform relatively comparably. In the document-level curriculum, our findings suggest that going from hard to easy training samples may be an effective strategy. Furthermore, iterating over disjoint bins one at a time seems comparably effective to incrementally increasing the training data size. Finally, interestingly, our empirical results on eight down-stream tasks from GLUE benchmark reveal that an ordered corpus yields competitive performance as compared to vanilla training in almost half the number of training steps, as measured over two epochs of training.\n\nMany interesting avenues for future work remain such as devising more efficient corpus ordering algorithms or verifying whether these simple yet effective strategies generalize to different training corpora or model architectures, thus enabling development of sustainable language models. J Peter Kincaid, Robert P Fishburne Jr, Richard L Rogers, and Brad S Chissom. 1975 Tables 7 and 8 present some sample text excerpts from easy and hard bins, as computed by documentlevel complexity and sentence-level complexity, respectively.",
            "score": 0.5974600784682362,
            "section_title": "Conclusions and Future Directions",
            "char_start_offset": 27426,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75830078125
        },
        {
            "corpus_id": "237553761",
            "title": "On the Role of Corpus Ordering in Language Modeling",
            "text": "Pretrained language models are the foundation for achieving impressive results on many natural language processing tasks today (Peters et al., 2018;Devlin et al., 2019;Yang et al., 2019) but they are also prohibitively expensive to train, requiring enormous computing resources to be effective. The exploding demand of computations, together with the resulting massive energy cost (Strubell et al., 2019), pose serious obstacles to the development of new pretrained models, thus, leading to a number of recent research efforts aimed towards addressing this problem by proposing approaches for improving model efficiency  or sample efficiency (Clark et al., 2020), to name just a few.\n\nThe primary method researchers have explored to address this problem is to develop smaller language models Jiao et al., 2020). In this paper we study a complementary approach to simplify pretraining of language models based on corpus ordering via curriculum learning (Bengio et al., 2009). The motivation is that curriculum learning has been shown to help with faster convergence (Guo et al., 2018;Hacohen and Weinshall, 2019) which, in combination with simpler language models, will give us a sustainable platform for future research into language models.\n\nAlthough curriculum learning strategies have been successfully employed in many areas of machine learning, on a wide range of tasks Soviany et al., 2021), little is understood about the role of corpus ordering in the context of pretraining language models with the exception of some notable early works on language modeling (Bengio et al., 2009;Graves et al., 2017). In this paper, we continue this line of investigation by asking the question whether curriculum based pretraining of transformer language models provides any benefits when compared with traditional vanilla training.\n\nIn order to create a curriculum from an unlabeled text corpus for such self-supervised form of learning, one needs to define a measure of complexity. We explore if metrics of text readability difficulty designed for humans can serve as beneficial metric in creating a curriculum for machine learning models. The intuition behind this approach is to mimic the manner in which humans learn. Training samples are organized by levels of difficulty and training proceeds in steps where the model is first trained on a subset of the corpus at a given difficulty level before being trained on another difficulty level, and so on.",
            "score": 0.5616675763465964,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 127,
                    "end": 148,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 148,
                    "end": 168,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 381,
                    "end": 404,
                    "matchedPaperCorpusId": "174802812"
                },
                {
                    "start": 952,
                    "end": 972,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1065,
                    "end": 1083,
                    "matchedPaperCorpusId": "51920640"
                },
                {
                    "start": 1083,
                    "end": 1111,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 1567,
                    "end": 1588,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1588,
                    "end": 1608,
                    "matchedPaperCorpusId": "11137059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58837890625
        },
        {
            "corpus_id": "260203057",
            "title": "Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models",
            "text": "The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training. Using this intuition, our framework formalizes the notion of a skill and of an ordered set of skills in terms of the associated data. First, using both synthetic and real data, we demonstrate that these ordered skill sets exist, and that their existence enables more advanced skills to be learned with less data when we train on their prerequisite skills. Second, using our proposed framework, we introduce an online data sampling algorithm, Skill-It, over mixtures of skills for both continual pre-training and fine-tuning regimes, where the objective is to efficiently learn multiple skills in the former and an individual skill in the latter. On the LEGO synthetic in the continual pre-training setting, Skill-It obtains 36.5 points higher accuracy than random sampling. On the Natural Instructions dataset in the fine-tuning setting, Skill-It reduces the validation loss on the target skill by 13.6% versus training on data associated with the target skill itself. We apply our skills framework on the recent RedPajama dataset to continually pre-train a 3B-parameter LM, achieving higher accuracy on the LM Evaluation Harness with 1B tokens than the baseline approach of sampling uniformly over data sources with 3B tokens.",
            "score": 0.5548131000763677,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8984375
        },
        {
            "corpus_id": "247762191",
            "title": "Data Selection Curriculum for Neural Machine Translation",
            "text": "In contrast, the online curriculum dynamically selects a subset of the data for each epoch without requiring any external measure. Specifically, it leverages prediction scores of the emerging NMT model which are the training by-product. \n\nFor picking the data subset in the online curriculum, we investigate two approaches of dataselection window -static and dynamic. Even though the size of the data-selection window is constant throughout the training in the static approach, the samples in the selected subset vary from epoch-to-epoch due to the change in their prediction scores. In contrast, we change the dataselection window size in the dynamic approach by either expanding or shrinking. \n\nComprehensive experiments on six language pairs (12 translation directions) comprising lowand high-resource languages from WMT'21 (Akhbardeh et al., 2021) reveal that our curricu-lum strategies consistently demonstrate better performance compared to the baseline trained on all the data (up to +2.2 BLEU). We observe bigger gains in the high-resource pairs compared to the low-resource ones. Interestingly, we find that the online curriculum approaches perform on par with the deterministic approaches while not using any external pretrained models. Our proposed curriculum training approaches not only exhibit better performance but also converge much faster requiring approximately 50% fewer updates.",
            "score": 0.5506536532278031,
            "section_title": "Introduction",
            "char_start_offset": 2071,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 236
                },
                {
                    "start": 239,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1399
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7119140625
        },
        {
            "corpus_id": "53295888",
            "title": "An Empirical Exploration of Curriculum Learning for Neural Machine Translation",
            "text": "Curriculum learning (Bengio et al., 2009) hypothesizes that choosing the order in which training samples are presented to a learning system can help train better models faster. In particular, presenting samples that are easier to learn from before presenting difficult samples is an intuitively attractive idea, which has been applied in various ways in Machine Learning and Natural Language Processing tasks (Bengio et al., 2009;Tsvetkov et al., 2016;Cirik et al., 2016;Graves et al., 2017, inter alia). \n\nIn this paper, we conduct an empirical exploration of curriculum learning for Neural Machine Translation (NMT). NMT is a good test case for curriculum learning as training is prohibitively slow in the large data conditions required to reach good performance (Koehn and Knowles, 2017). However, designing a curriculum for NMT training is a complex problem. First, it is not clear how to quantify sample difficulty for this task. Second, NMT systems already rely on established data organization methods to deal with the scale and varying length of training samples (Khomenko et al., 2016;Doetsch et al., 2017;Sennrich et al., 2017;Hieber et al., 2017), and it is not clear how a curriculum should interact with these existing design decisions. Kocmi and Bojar (2017) showed that constructing and ordering mini-batches based on sample length or word frequency helps when training for one epoch. It remains to be seen how curricula impact training until convergence. \n\nTo address these issues, we adopt a probabilistic view of curriculum learning that lets us explore a wide range of curricula flexibly. Our approach does not order samples in a deterministic fashion. Instead, each sample has a probability of being selected for training, and this probability changes depending on the difficulty of the sample and on the curriculum's schedule. We explore difficulty criteria based on NMT model scores as well as linguistic properties. We consider a wide range of schedules, based not only on the easy-to-difficult ordering, but also on strategies developed independently from curriculum learning, such as dynamic sampling and boosting (Zhang et al., 2017;van der Wees et al., 2017;Wang et al., 2018).",
            "score": 0.5457246045803575,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 504
                },
                {
                    "start": 507,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1470
                },
                {
                    "start": 1473,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2204
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 41,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 409,
                    "end": 430,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 765,
                    "end": 790,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 1071,
                    "end": 1094,
                    "matchedPaperCorpusId": "14333788"
                },
                {
                    "start": 1115,
                    "end": 1137,
                    "matchedPaperCorpusId": "905565"
                },
                {
                    "start": 1250,
                    "end": 1272,
                    "matchedPaperCorpusId": "26468344"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68798828125
        },
        {
            "corpus_id": "249097496",
            "title": "The Effect of Task Ordering in Continual Learning",
            "text": "Often inspired by the human classroom, curriculum learning refers to training a model with a curriculum of increasing difficulty [6,[25][26][27]. Typically, the curriculum is over data points, such that individual training examples are sorted into a progression from easy to hard according to some metric. It is a matter of ongoing debate as to whether curriculum learning improves performance [6][7][8]28], though it seems increasingly common in practice when training large state-of-the-art models [28]. Curriculum learning is closely related to both importance sampling [29][30][31][32] and to active learning [33,34]. In our work, we apply the idea of investigating ordering to whole tasks, rather than individual datapoints.",
            "score": 0.5437167357987701,
            "section_title": "Curriculum learning",
            "char_start_offset": 5706,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 729
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 132,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 132,
                    "end": 136,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 136,
                    "end": 140,
                    "matchedPaperCorpusId": "206863670"
                },
                {
                    "start": 140,
                    "end": 144,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 394,
                    "end": 397,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 397,
                    "end": 400,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 400,
                    "end": 403,
                    "matchedPaperCorpusId": "5004002"
                },
                {
                    "start": 403,
                    "end": 406,
                    "matchedPaperCorpusId": "227343966"
                },
                {
                    "start": 500,
                    "end": 504,
                    "matchedPaperCorpusId": "227343966"
                },
                {
                    "start": 577,
                    "end": 581,
                    "matchedPaperCorpusId": "13199836"
                },
                {
                    "start": 581,
                    "end": 585,
                    "matchedPaperCorpusId": "3663876"
                },
                {
                    "start": 613,
                    "end": 617,
                    "matchedPaperCorpusId": "6170752"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.145263671875
        },
        {
            "corpus_id": "237581256",
            "title": "The Trade-offs of Domain Adaptation for Neural Language Models",
            "text": "Intelligent selection (Moore and Lewis, 2010; Axelrod et al., 2011) and contrastive data selection, its extension to neural networks (van der Wees et al., 2017;Wang et al., 2018), have been introduced in the language modeling literature. We show that these methods are closely related to importance sampling, even if their original papers does not mention this link.\n\nIntelligent selection selects training samples from an out-of-domain dataset according to the log-odd between an in-domain language model and an out-of-domain language model. Typically, a binary decision is taken per sentence by comparing the average log-odd to a threshold \u03c4 ,\n\nCompared to importance sampling, the weights are binarized, i.e.\n\nb IntSel D,T (y) = I log w imp D,T (y) > \u03c4 .\n\nThe binarization decision was certainly driven by convenience as most n-gram language models training packages did not support weighted likelihood optimization when intelligent selection was introduced. Binarization also has the advantage of down-weighting extreme positive weight values from large log P (y|\u03b8 T ) due to over-fitting on the small set T . More recently, intelligent selection has been extended to neural models (van der Wees et al., 2017;Wang et al., 2018). Contrastive data selection (Wang et al., 2018) suggests to fine tune the in-domain model log P (y|\u03b8 T ) from logP (y|\u03b8 D ) and also observes that selection scores can efficiently be estimated from a model with a much smaller capacity than the final trained model.\n\nDynamic selection (van der Wees et al., 2017) proposes to increase the selection threshold \u03c4 t as training progresses, gradually transitioning from generic to in-domain training. This gradual adaptation of neural network is related to curriculum learning (Bengio et al., 2009) which studies the ordering of examples and tasks during model training.\n\nIntelligent selection methods have been applied both for unconditional models (language modeling) and conditional models (machine translation). In the conditional case, intelligent selection computes\n\nThis ratio of conditional probabilities is different from the ratio of join probabilities stemming from importance sampling, i.e.\n\nThe two ratio matches when P (x|T ) = P (x",
            "score": 0.5417362610666266,
            "section_title": "Intelligent Selection",
            "char_start_offset": 19062,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 67,
                    "matchedPaperCorpusId": "10766958"
                },
                {
                    "start": 133,
                    "end": 160,
                    "matchedPaperCorpusId": "7921428"
                },
                {
                    "start": 160,
                    "end": 178,
                    "matchedPaperCorpusId": "52156056"
                },
                {
                    "start": 1186,
                    "end": 1213,
                    "matchedPaperCorpusId": "7921428"
                },
                {
                    "start": 1213,
                    "end": 1231,
                    "matchedPaperCorpusId": "52156056"
                },
                {
                    "start": 1260,
                    "end": 1279,
                    "matchedPaperCorpusId": "52156056"
                },
                {
                    "start": 1516,
                    "end": 1543,
                    "matchedPaperCorpusId": "7921428"
                },
                {
                    "start": 1753,
                    "end": 1774,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9013671875
        },
        {
            "corpus_id": "155089817",
            "title": "Curriculum Learning for Domain Adaptation in Neural Machine Translation",
            "text": "We adopt similarity metrics from prior work on data selection to score examples for curriculum learning. Let I be an in-domain corpus, and N be a unlabeled-domain data set. Data selection models rank sentences in N according to a domain similarity measure with respect to I, and choose top n samples from N by a cut-off threshold for further training purpose. We examine two data selection methods, Moore-Lewis method (Moore and Lewis, 2010) and cynical data selection (Axelrod, 2017). \n\nMoore-Lewis Method Each sentence s in N is assigned a cross-entropy difference score, \n\nwhere H I (s) is the per-word cross-entropy of s according to a language model trained on I, and H N (s) is the per-word cross-entropy of s according to a language model trained on a random sample of N with roughly the same size as I. A lower cross-entropy difference indicates that s is more like the in-domain data and less like the unlabeled-domain data. \n\nCynical Data Selection Iteratively select sentence s from N to construct a training corpus that would approximately model I. At each iteration, each sentence is scored by the expected cross-entropy change from adding it to the already selected subset of N . The selected sentence is the one which most decreases H n , the cross-entropy between previously selected n-sentence corpus and I.",
            "score": 0.5410374710905349,
            "section_title": "Domain Similarity Scoring",
            "char_start_offset": 3566,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 485
                },
                {
                    "start": 488,
                    "end": 573
                },
                {
                    "start": 576,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 933
                },
                {
                    "start": 936,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1324
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.912109375
        },
        {
            "corpus_id": "269608596",
            "title": "Granular Syntax Processing with Multi-Task and Curriculum Learning",
            "text": "CL aims to automatically select the most suitable samples for each training step [73]. The curriculum is a sequence of training criteria that rely solely on the data, the model, and the task objective. CL is widely used to select training samples from easy to difficult for efficient learning [74][75][76]. \n\nThe signal for determining whether a sample is appropriate for the current model is considered as curriculum criterion. Traditionally, such criterion is defined as a taskdependent difficulty metric, such as input text length [75,77] and term frequency [77,78]. Contrasting the predefined sample ordering, another common way to dynamically generate curriculum is to use task loss as the signal for teach-student CL [79][80][81], or self-pace learning [74,[82][83][84]. However, these CL criteria are applied to enforce the easy-to-hard ordering of the curriculum. To the best of our knowledge, CL has not been used to address the issue of MTL with nonparallel labeled data.",
            "score": 0.5347384588564086,
            "section_title": "Curriculum Learning",
            "char_start_offset": 15319,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 306
                },
                {
                    "start": 309,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 981
                }
            ],
            "ref_mentions": [
                {
                    "start": 81,
                    "end": 85,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 293,
                    "end": 297,
                    "matchedPaperCorpusId": "35005827"
                },
                {
                    "start": 538,
                    "end": 541,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 561,
                    "end": 565,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 565,
                    "end": 568,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 723,
                    "end": 727,
                    "matchedPaperCorpusId": "53103412"
                },
                {
                    "start": 727,
                    "end": 731,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 731,
                    "end": 735,
                    "matchedPaperCorpusId": "191193864"
                },
                {
                    "start": 759,
                    "end": 763,
                    "matchedPaperCorpusId": "35005827"
                },
                {
                    "start": 763,
                    "end": 767,
                    "matchedPaperCorpusId": "3405508"
                },
                {
                    "start": 771,
                    "end": 775,
                    "matchedPaperCorpusId": "227275456"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.419189453125
        },
        {
            "corpus_id": "53295888",
            "title": "An Empirical Exploration of Curriculum Learning for Neural Machine Translation",
            "text": "More recently, Tsvetkov et al. (2016) improve word embedding training using Bayesian optimization to order paragraphs in the training corpus based on a range of distributional and linguistic features (diversity, simplicity, prototypicality). \n\nWhile curriculum learning often refers to organizing examples from simple to difficult, other data ordering strategies have also shown to be beneficial: Amiri et al. ( 2017) improve the convergence speed of neural models using spaced repetition, a technique inspired by psychology findings that human learners can learn efficiently and effectively by increasing intervals of time between reviews of previously seen materials. \n\nCurriculum design is also a concern when deciding how to schedule learning from samples of different tasks either in a sequence from simpler to more difficult tasks (Collobert and Weston, 2008) or in a multi-task learning framework (Graves et al., 2017;Kiperwasser and Ballesteros, 2018). In this work, we focus on the question of organizing training samples for a single task. \n\nIn NMT, curriculum learning has not yet been explored systematically. In practice, training protocols randomize the order of sentence pairs in the training corpus (Sennrich et al., 2017;Hieber et al., 2017). There are works that speed training up by batching the samples of similar lengths (Khomenko et al., 2016;Doetsch et al., 2017). Such works attempt to improve the computational efficiency, while curriculum learning is supposed to improve the statistical efficiency -fewer batches of training examples are needed to achieve a given performance. Kocmi and Bojar (2017) conducted the first study of curriculum learning for NMT by exploring the impact of several criteria for curriculum design on the training of a Czech-English NMT system for one epoch. They ensure samples within each mini-batch have similar linguistic properties, and order mini-batches based on complexity. They show translation quality can be improved by presenting samples from easy to hard based on sentence length and vocabulary frequency. However, it remains to be seen whether these findings hold when training until convergence. \n\nPrevious work has focused on dynamic sampling strategies, emphasizing training on samples that are expected to be most useful based on model scores or domain relevance.",
            "score": 0.5347009386369855,
            "section_title": "Introduction",
            "char_start_offset": 3850,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 244,
                    "end": 669
                },
                {
                    "start": 672,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1049
                },
                {
                    "start": 1052,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2161
                },
                {
                    "start": 2164,
                    "end": 2332
                }
            ],
            "ref_mentions": [
                {
                    "start": 837,
                    "end": 865,
                    "matchedPaperCorpusId": "2617020"
                },
                {
                    "start": 904,
                    "end": 925,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 1215,
                    "end": 1238,
                    "matchedPaperCorpusId": "905565"
                },
                {
                    "start": 1342,
                    "end": 1365,
                    "matchedPaperCorpusId": "14333788"
                },
                {
                    "start": 1603,
                    "end": 1625,
                    "matchedPaperCorpusId": "26468344"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65283203125
        },
        {
            "corpus_id": "266166181",
            "title": "Byte-ranked Curriculum Learning for BabyLM Strict-small Shared Task 2023",
            "text": "Large language models have been highly successful across a wide variety of tasks in Natural Language Processing. Due to the rapidly increasing model size and training data size, however, the cost to train new models is prohibitively expensive for many researchers. The BabyLM Challenge is a shared task designed to highlight methods for training language models at a smaller scale. These methods may lead to improvements in scaling up training more efficiently, training language models in low-resource settings, and drawing upon the way human children acquire language. \n\nIn this work, the strict-small track allowed our models to use a given dataset containing around ten million words from data sources that a child may encounter when learning language. No tools which used outside data for pretraining were allowed, reducing the ability to use many existing pipelines. This restriction is realistic for many lowresource scenarios in which these tools are lacking. \n\nThis work explores ordering training data by bytes per line for a curriculum learning approach. This measure of difficulty is inspired by the use of byte-based byte-pair-encoding tokenization and is easy to apply without needing any domain knowledge of the dataset. The results show that curriculum learning with this setup obtains improved results on benchmark evaluations when training for a set number of epochs. In settings in which additional tools, data, or computational resources are available, this curriculum setup is easy to apply and further evaluation in those settings is a potential area for future work. \n\nThis work used the Augie High-Performance Computing cluster, funded by award NSF 2018933, at Villanova University.",
            "score": 0.5286360005250246,
            "section_title": "Conclusion",
            "char_start_offset": 24019,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 570
                },
                {
                    "start": 573,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 967
                },
                {
                    "start": 970,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1589
                },
                {
                    "start": 1592,
                    "end": 1706
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3486328125
        },
        {
            "corpus_id": "237553761",
            "title": "On the Role of Corpus Ordering in Language Modeling",
            "text": "We first summarize the process of creating the curriculum and then explain the strategy for training over it. As described in the previous section, sentences or documents are sorted by their complexity score, which are then distributed into nonoverlapping bins, essentially subsets of data (also known as shards ), such that samples in each bin are similar in complexity.\n\nThe training consists of t sequential phases, where t denotes the different points of time during the training, where training samples are fetched only from a subset of bins. For instance, t = 1 may correspond to the first epoch or first n steps, t = 2 may correspond to the second epoch or the next n steps, and so on. In our experiments, t denotes an epoch. A subset consists of one or more bins, and for creating and iterating over these training subsets during training, we explore two different strategies -BINNED and STEPPED.\n\n(i) BINNED: In this variant, the model is trained sequentially on each bin, one at a time. In other words, the model is first trained on the first bin and its state is saved. The training then continues from the saved checkpoint on the next bin, and so on. This is similar to the case where a subset consists of only one bin, and we iterate over it for one epoch.\n\nThe bins themselves can be accessed in order of either increasing difficulty (from easy to medium to hard), an approach that can be intuitively seen as mimicking the way humans learn, or in the reverse order of decreasing difficulty (from hard to medium to easy), a technique shown to benefit machine learning algorithms (Weinshall et al., 2018). In doing so, the question we ask is whether curriculum or anti-curriculum help in the context of language modeling, if at all.\n\nIt is worth mentioning that while the bins are accessed in a pre-defined order (i.e., easy to hard or reverse), the samples within the bins are still randomly selected, thus combining a deterministic schedule with the benefits of randomization that serve neural models well.\n\n(ii) STEPPED: Alternatively, the bins could be accessed cumulatively where the training set progressively increases in size by addition of newer bins while retaining earlier bins (also referred to as Baby Steps curriculum (",
            "score": 0.5279374502128092,
            "section_title": "Corpus Ordering for Pretraining",
            "char_start_offset": 11328,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1592,
                    "end": 1616,
                    "matchedPaperCorpusId": "5004002"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.484619140625
        },
        {
            "corpus_id": "264439064",
            "title": "Irreducible Curriculum for Language Model Pretraining",
            "text": "The evolution of language models demonstrates impressive generation and reasoning abilities from a continuous scaling-up of model size and training corpus [Brown et al., 2020, Chowdhery et al., 2022, Kaplan et al., 2020, Hoffmann et al., 2022], along with very significant growth in computation cost. Besides the quantity [Kaplan et al., 2020] and quality [Lee et al., 2022, Longpre et al., 2023] of training data, several recent works show that the ordering [Chen et al., 2023] and composition [Xie et al., 2023] of data during the training can highly impact performance and efficiency. Xie et al. [2023] proposes DOREMI, which determines the optimal domain mixture to construct the pretraining corpus using auxiliary models; Chen et al. [2023] introduces an online selection scheme, which seeks to dynamically update the mixture of data from each skill-set at each training step. While DOREMI and SKILL-IT demonstrates great downstream performance, their curriculum are built upon the group-level where all the instances within one group (domain/skill) shares the same sampling probability, without looking inside each domains on the sample-specific attributions. \n\nNevertheless, few research have succeeded applying traditional sample-level selection schemes for large langauge model pretraining. Online batch selection methods [Loshchilov and Hutter, 2015, Katharopoulos and Fleuret, 2018, Jiang et al., 2019, Schaul et al., 2015] select hard samples with high loss or high gradient norm, which require a second forward/backward pass. That introduces large extra computation costs when the model size is large, which hurts the scalability. On the other side, Campos [2021] shows that the linguistic-based curriculum learning failed to improve on causal language model pretraining. Schaul et al. [2015] introduces RHO-LOSS, which is an online batch selection scheme based on the gap between current training loss and an irreducible loss term Preprint. Under review.",
            "score": 0.5237703684655631,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1165
                },
                {
                    "start": 1168,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 1968
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 174,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1359,
                    "end": 1392,
                    "matchedPaperCorpusId": "3663876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8525390625
        },
        {
            "corpus_id": "260203057",
            "title": "Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models",
            "text": "For Figure 1: Inspired by how humans acquire knowledge, we hypothesize that LMs best learn skills in a particular order and that this can help improve our understanding and training of LMs. We show that these ordered skill sets exist in real data, which enables skills to be learned with less data given that we train on their prerequisite skills. We then propose SKILL-IT, an online data selection algorithm that learns skills quickly by exploiting their ordering. instance, we partitioned the Alpaca dataset [56] by instruction type-a technique used to capture dataset diversity [62]-but we found that sampling based on instruction types and random sampling resulted in similar model performance, suggesting that not just any existing notion of data groups can characterize skills. \n\nSecond, these definitions of skills must be used to construct sampling distributions to actually improve model training. \n\nTo develop criteria for a data selection algorithm that learns skills efficiently, we identify challenges that naive selection approaches face. The standard approach of random uniform sampling over data fails to learn skills optimally due to not accounting for skill imbalance and ordering. Skills can be distributed unevenly in the data, with more complex skills being rare-for instance, Spanish and question generation (QG) are 5% and 4% of the Natural Instructions dataset [63], respectively, but Spanish QG is only 0.2%. Random sampling also provides no mechanism for taking into account a particular training order and dependency structure on skills. More sophisticated techniques like curriculum learning account for sample-level ordering, but not skills or their dependencies. Our goal framework must account for these issues of imbalance and ordering. \n\nSkill-based framework We define a skill as a unit of behavior that a model can learn using an associated slice of data (Definition 2.1). An ordered skill set is a collection of skills with a directed skills graph that is neither complete nor empty, where an edge from a prerequisite skill to a skill exists if the amount of training it takes to learn the skill can be reduced if the prerequisite skill is also learned (Definition 2.2, Figure 1 left, center). We show that ordered skill sets exist in synthetic and real datasets using this operational definition.",
            "score": 0.5219447886238668,
            "section_title": "Introduction",
            "char_start_offset": 2180,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 906
                },
                {
                    "start": 909,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1768
                },
                {
                    "start": 1771,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2229
                },
                {
                    "start": 2230,
                    "end": 2333
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.916015625
        },
        {
            "corpus_id": "264288956",
            "title": "Harnessing Dataset Cartography for Improved Compositional Generalization in Transformers",
            "text": "For the second setup, we train models using subsets sized at 50% of the original datasets. Along with the hard-to-learn, ambiguous, and easy-to-learn subsets, we construct combined subsets that are also half the size of the original dataset by merging two 33% subsets selected based on the same confidence measure. Specifically, we select 33% of the examples from the more informative subset and allocate the remaining 17% from the other subset, following Swayamdipta et al. (2020). As will be discussed in the next section, our findings demonstrate that hard-to-learn samples have a more pronounced impact on model performance compared to ambiguous and easy-tolearn samples, and thus we consider them as more informative. When combining ambiguous and easyto-learn samples, we consider including a greater number of ambiguous samples than easy-to-learn samples. If the union of these subsets is smaller than half of the whole training data, we randomly add samples to reach the 50% dataset size. Furthermore, we address the out-of-vocabulary (OOV) problem during subset selection by incorporating training samples from the entire dataset if they increase the vocabulary size. On the contrary, we remove the least informative samples that do not reduce the vocabulary size, ensuring consistent sub-set sizes throughout the experiments. The statistics about the subsets obtained from the data maps are provided in Appendix A.4. \n\nIn addition to our subset selection experiments, we explore the potential of leveraging dataset cartography as a criterion for curriculum learning (CL). In particular, we adopt two CL approaches proposed by Hacohen and Weinshall (2019) and Zhang et al. (2019). We experiment with a fixed exponential pacing schedule using default hyperparameters in the former. We set the starting percentage to 4% and increase the scale to 1.9. On the other hand, the second CL method by Zhang et al. (2019) involves sorting examples based on a given criterion and dividing them into 10 equal-sized bins, resulting in a 10-stage curriculum. Within each bin, the examples are further sorted based on their lengths, and then each sorted bin is divided into non-overlapping batches.",
            "score": 0.5174133655970831,
            "section_title": "Experimental Setup",
            "char_start_offset": 13742,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1425
                },
                {
                    "start": 1428,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2191
                }
            ],
            "ref_mentions": [
                {
                    "start": 456,
                    "end": 481,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 1668,
                    "end": 1687,
                    "matchedPaperCorpusId": "155089817"
                },
                {
                    "start": 1900,
                    "end": 1919,
                    "matchedPaperCorpusId": "155089817"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36669921875
        },
        {
            "corpus_id": "53650043",
            "title": "An Empirical Evaluation of Sketched SVD and its Application to Leverage Score Ordering",
            "text": "Given these leverage scores, we use ideas from curriculum learning to generate 3 different orderings of the data based on several different sampling policies \u03c0: (1) dec indicates that the training data is ordered based on strictly decreasing leverage scores. As a result, the most important and diverse training points are seen first. (2) dec, sampling with replacement (dec, swr) indicates that the training data is ordered based on sampling with replacement from a discrete distribution defined by the leverage scores. As a result, the most important and diverse training points are seen first, but with randomness introduced into the order so that the order of training labels are less correlated. Note that sampling with replacement might duplicate datapoints with very high leverage scores and ignore datapoints with very low leverage scores. (3) dec, sampling without replacement (dec, swor) is the same but sampling without replacement. Additionally, we compare to the (4) shuffle baseline where models are trained on shuffled training data.",
            "score": 0.513976623657514,
            "section_title": "Experimental Setup",
            "char_start_offset": 16797,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1048
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1302490234375
        },
        {
            "corpus_id": "266177072",
            "title": "Data Selection Curriculum for Abstractive Text Summarization",
            "text": "To investigate the impact of the number of selected instances, we conducted experiments using varying ratios of selected data from the dataset. The model was trained on the selected data for one epoch, and without early stopping, the latest checkpoint was used for testing. Alternatively, with early stopping, the checkpoint with the highest ROUGE scores was selected. The results, depicted in Figure 2, indicate that the quantity of data does not necessarily correlate with improved performance. Surprisingly, an excessive number of instances may even lead to a degradation in performance. Moreover, we observed that the training process tends to favor easier instances, as optimal performance was achieved with less than 20% of the available instances. \n\nFigure 2: DSC performance with different data ratios. \n\nWe validate our methods on XSum (Narayan et al., 2018) from the same start point of BRIO. We initialize our model using \"google/pegasusxsum\"4 and use the selected 20% instances to train. The BERTScore of our model (52.88) beats BRIO (41.08) by a large margin, so the summary from our model is more similar to the reference than BRIO. We also conducted separate experiments to evaluate each sub-model. MLR represents our novel contrastive ranking loss. DS is shorthand for data selection, which refers to training the model using selected data instead of all instances. CL denotes curriculum learning, which trains the model in the from-easy-to-hard ordering. Table 3 shows that all methods we proposed contribute to the improvement of summarization performance.",
            "score": 0.5092108268806148,
            "section_title": "Selected Data Ratio",
            "char_start_offset": 10142,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 754
                },
                {
                    "start": 757,
                    "end": 810
                },
                {
                    "start": 813,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1574
                }
            ],
            "ref_mentions": [
                {
                    "start": 845,
                    "end": 867,
                    "matchedPaperCorpusId": "215768182"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.572265625
        },
        {
            "corpus_id": "269761286",
            "title": "EfficientTrain++: Generalized Curriculum Learning for Efficient Visual Backbone Training",
            "text": "Curriculum learning is a training paradigm inspired by the organized learning order of examples in human curricula [8], [25], [26].This idea has been widely explored in the context of training deep networks from easier data to harder data [9], [10], [11], [12], [16], [27], [28].Typically, a predefined [8], [29], [30], [31] or automatically-learned [11], [12], [14], [16], [27], [32], [33], [34], [35], [36], [37] difficulty measurer is deployed to differentiate between easier and harder samples, while a scheduler [8], [9], [11], [28] is defined to determine when and how to introduce harder training data.For example, DIHCL [38] and InfoBatch [39] determine the hardness of each sample by leveraging its online training loss or the change in model outputs.MCL [40] adaptively selects a sequence of training subsets by repeatedly solving joint continuous-discrete minimax optimization problems, whose objective combines both a continuous training loss that reflects training set hardness and a discrete submodular promoter of diversity for the chosen subset.CurriculumNet [41] designs a curriculum that measures the difficulty of different data using the distribution density in the feature space obtained by training an initial model with all data.Our work is based on a similar 'starting small' spirit [25] to these methods, but we reformulate the training curriculum as a soft-selection function that uncovers progressively more difficult patterns within each example, rather than performing easier-to-harder data selection.\n\nOur work is also related to curriculum by smoothing [42], curriculum dropout [43] and label-similarity curriculum [44], which do not perform example selection as well.However, our method is orthogonal to them since we propose to reduce the training cost by modifying the model inputs.",
            "score": 0.5058040878476937,
            "section_title": "RELATED WORK",
            "char_start_offset": 7902,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 131,
                    "end": 279
                },
                {
                    "start": 279,
                    "end": 609
                },
                {
                    "start": 609,
                    "end": 760
                },
                {
                    "start": 760,
                    "end": 1061
                },
                {
                    "start": 1061,
                    "end": 1252
                },
                {
                    "start": 1252,
                    "end": 1530
                },
                {
                    "start": 1532,
                    "end": 1699
                },
                {
                    "start": 1699,
                    "end": 1816
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 118,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 120,
                    "end": 124,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 126,
                    "end": 130,
                    "matchedPaperCorpusId": "206863670"
                },
                {
                    "start": 239,
                    "end": 242,
                    "matchedPaperCorpusId": "232362223"
                },
                {
                    "start": 244,
                    "end": 248,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 250,
                    "end": 254,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 256,
                    "end": 260,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 274,
                    "end": 278,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 303,
                    "end": 306,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 308,
                    "end": 312,
                    "matchedPaperCorpusId": "5658192"
                },
                {
                    "start": 314,
                    "end": 318,
                    "matchedPaperCorpusId": "1821165"
                },
                {
                    "start": 320,
                    "end": 324,
                    "matchedPaperCorpusId": "879067"
                },
                {
                    "start": 350,
                    "end": 354,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 356,
                    "end": 360,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 362,
                    "end": 366,
                    "matchedPaperCorpusId": "51876228"
                },
                {
                    "start": 380,
                    "end": 384,
                    "matchedPaperCorpusId": "21323057"
                },
                {
                    "start": 386,
                    "end": 390,
                    "matchedPaperCorpusId": "207216972"
                },
                {
                    "start": 392,
                    "end": 396,
                    "matchedPaperCorpusId": "5004002"
                },
                {
                    "start": 398,
                    "end": 402,
                    "matchedPaperCorpusId": "4321928"
                },
                {
                    "start": 404,
                    "end": 408,
                    "matchedPaperCorpusId": "52114770"
                },
                {
                    "start": 517,
                    "end": 520,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 522,
                    "end": 525,
                    "matchedPaperCorpusId": "232362223"
                },
                {
                    "start": 527,
                    "end": 531,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 533,
                    "end": 537,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 647,
                    "end": 651,
                    "matchedPaperCorpusId": "257427625"
                },
                {
                    "start": 1075,
                    "end": 1079,
                    "matchedPaperCorpusId": "51920640"
                },
                {
                    "start": 1307,
                    "end": 1311,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 1584,
                    "end": 1588,
                    "matchedPaperCorpusId": "220301592"
                },
                {
                    "start": 1609,
                    "end": 1613,
                    "matchedPaperCorpusId": "18331438"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15625
        },
        {
            "corpus_id": "276421299",
            "title": "Task-Informed Anti-Curriculum by Masking Improves Downstream Performance on Text",
            "text": "Our results confirm the observations of Ankner et al. (2024) and Yang et al. (2023), although we reset the masking ratio during training, resulting in a cyclic decaying masking ratio. \n\nCurriculum learning methods specifically designed for text (Gong et al., 2021;Kocmi and Bojar, 2017;Liu et al., 2018Liu et al., , 2020b;;Zhan et al., 2021) are also related to our work. Some of the most popular approaches rely on text length (Nagatsuka et al., 2023) andmodel competence (Platanios et al., 2019) to organize the samples from easy to hard. Recent approaches are based on more complex strategies. For instance, the state-of-the-art curriculum learning method proposed by Poesina et al. (2024) employs data cartography (Swayamdipta et al., 2020) while training a baseline model to obtain the variability and confidence of each sample. The training data is further mapped as easy, ambiguous or hard. The model is then retrained via an easy-to-hard curriculum. To boost performance, the method employs stratified sampling as well as a continuous function to map the data points, resulting in a method called Cart-Stra-CL++. \n\nDifferent from related curriculum and anticurriculum learning methods (Ankner et al., 2024;Poesina et al., 2024;Yang et al., 2023), we design an anti-curriculum strategy for the fine-tuning stage of pre-trained language models, leveraging knowledge about the downstream tasks. Moreover, our novel design leads to superior performance on a range of downstream tasks.",
            "score": 0.5021409114023083,
            "section_title": "A Related Work",
            "char_start_offset": 15024,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 186,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1120
                },
                {
                    "start": 1123,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1488
                }
            ],
            "ref_mentions": [
                {
                    "start": 40,
                    "end": 60,
                    "matchedPaperCorpusId": "258865570"
                },
                {
                    "start": 65,
                    "end": 83,
                    "matchedPaperCorpusId": "251741301"
                },
                {
                    "start": 245,
                    "end": 264,
                    "matchedPaperCorpusId": "237279108"
                },
                {
                    "start": 264,
                    "end": 286,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 286,
                    "end": 302,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 302,
                    "end": 323,
                    "matchedPaperCorpusId": "219260306"
                },
                {
                    "start": 428,
                    "end": 456,
                    "matchedPaperCorpusId": "255221201"
                },
                {
                    "start": 671,
                    "end": 692,
                    "matchedPaperCorpusId": "269921463"
                },
                {
                    "start": 718,
                    "end": 744,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 1193,
                    "end": 1214,
                    "matchedPaperCorpusId": "258865570"
                },
                {
                    "start": 1214,
                    "end": 1235,
                    "matchedPaperCorpusId": "269921463"
                },
                {
                    "start": 1235,
                    "end": 1253,
                    "matchedPaperCorpusId": "251741301"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35009765625
        },
        {
            "corpus_id": "270619569",
            "title": "Improving Zero-Shot Cross-Lingual Transfer via Progressive Code-Switching",
            "text": "The scheduler aims to sample the data and send it to the model trainer for training.The scheduler decides when to sample the harder training data with the training progress.For our PCS, we begin with a temperature of \u03c4 = 0 equivalent to sampling the source language data.Then, the temperature linearly increases by the increment \u03b4 (e.g.\u03b4 = 0.1) every time the validation loss convergence, up to a final temperature of \u03c4 = 1.As the temperature increases, we will generate harder code-switching data.To encourage the model to pay more attention to harder data, we set larger early stopping patience for harder curricula than easier ones.However, we found that training the model on a sequence of CS datasets faces the problem of catastrophic forgetting [Kirkpatrick et al., 2017].As the curriculum stage progresses, code-switching training datasets with varying augmentation levels are sequentially inputted into the model.This results in the modification of weights acquired during the initial curriculum once the model encounters the target of the new curriculum, causing the occurrence of catastrophic forgetting.To mitigate this problem, we design a dynamic curriculum scheduler for the model to revisit previous curricula.Specifically, at the k-th curriculum stage, the scheduler selects the code-switching data\n\nCS for the model training on the following probability:\n\n(5)",
            "score": 0.5011654654051081,
            "section_title": "Scheduler",
            "char_start_offset": 11163,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 84,
                    "end": 173
                },
                {
                    "start": 173,
                    "end": 271
                },
                {
                    "start": 271,
                    "end": 336
                },
                {
                    "start": 336,
                    "end": 424
                },
                {
                    "start": 424,
                    "end": 498
                },
                {
                    "start": 498,
                    "end": 635
                },
                {
                    "start": 635,
                    "end": 778
                },
                {
                    "start": 778,
                    "end": 921
                },
                {
                    "start": 921,
                    "end": 1114
                },
                {
                    "start": 1114,
                    "end": 1225
                },
                {
                    "start": 1225,
                    "end": 1314
                },
                {
                    "start": 1316,
                    "end": 1371
                },
                {
                    "start": 1373,
                    "end": 1376
                }
            ],
            "ref_mentions": [
                {
                    "start": 751,
                    "end": 777,
                    "matchedPaperCorpusId": "266176298"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5966796875
        },
        {
            "corpus_id": "204949786",
            "title": "Distribution Density, Tails, and Outliers in Machine Learning: Metrics and Applications",
            "text": "of which we discussed in Section 4. Next, we review related work in two of these applications: namely, curriculum learning and reducing sample complexity.\n\nCurriculum learning. Based on the observation that the order in which training data is presented to the model can improve performance (e.g., convergence) of optimization during learning and circumvent limitations of the dataset (e.g., data imbalance or noisy labels), curriculum learning seeks to find the best order in which to analyze training data (Bengio et al., 2009). This first effort further hypothesizes that easy-to-classify samples should be presented early in training while complex samples gradually inserted as learning progresses. While Bengio et al. (2009) assumed the existence of hard-coded curriculum labels in the dataset, Chin & Liang (2017) sample an order for the training set by assigning each point a sampling probability proportional to its leverage score-the distance between the point and a linear model fitted to the whole data. Instead, we use metrics that also apply to data that cannot be modeled linearly.\n\nThe curriculum may also be generated online during training, so as to take into account progress made by the learner (Kumar et al., 2010). For instance, Katharopoulos & Fleuret (2017) train an auxiliary LSTM model to predict the loss of training samples, which they use to sample a subset of training points analyzed by the learner at each training iteration. Similarly, (Jiang et al., 2017) have an auxiliary model predict the curriculum. This auxiliary model is trained using the learner's current feature representation of a smaller holdout set of data for which ground-truth curriculum is known.\n\nHowever, as reported in our experiments, training on easy samples is beneficial when the dataset is noisy, whereas training on hard examples is on the contrary more effective when data is clean. These observations oppose self-paced learning (Kumar et al., 2010) with hard example mining (Shrivastava et al., 2016). Several strategies have been proposed to perform better in both settings. Assuming the existence of a holdout set as well, Ren et al. (2018) assign a weight to each training example that characterizes the alignment of both the logits and gradients of the learner on training",
            "score": 0.49964138740815034,
            "section_title": "Related Work",
            "char_start_offset": 30295,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 507,
                    "end": 527,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 708,
                    "end": 728,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.277587890625
        },
        {
            "corpus_id": "11185579",
            "title": "Learning and Knowledge Transfer with Memory Networks for Machine Comprehension",
            "text": "It has been shown in the context of language modelling that presenting the training samples in an easy to hard ordering allows for shielding    the model from very hard samples during training, yielding faster convergence and better models . We investigate a curriculum learning inspired training procedure for memory networks to improve performance on the three subsets of the CNN dataset described below.",
            "score": 0.4965038189549217,
            "section_title": "Improve Learning Procedure",
            "char_start_offset": 11251,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64306640625
        },
        {
            "corpus_id": "209862398",
            "title": "Learning From Multiple Experts: Self-paced Knowledge Distillation for Long-tailed Classification",
            "text": "Following the spirit of curriculum learning which mimics the human learning process, three questions need to be answered: 1) how to evaluate the difficulty of each instance, 2) how to select or unselect a sample 3) how to design an automatic self-paced schedule from easy to hard samples. \n\nFor the first question, we use the expert models' output confidence for each instance as an indication for instance difficulty. Given a training instance (x i , y i ), suppose its ground-truth class y i falls into the l-th subset S l , i.e. y i \u2208 S l , then we take the corresponding l-th expert model and use its output prediction for class y i as confidence score, denoted as p i . In this way, we can obtain the confidence score for all the instances in the training set. \n\nFor the second question, we adopt a soft selection method for instance selection. For instance (x i , y i ), we replace the cross entropy loss with a weighted version: \n\nis the selection weight at k-th epoch. A higher value of v i (close to 1) indicates a soft selection of i-th instance, while a smaller value indicates a soft unselection of that instance. \n\nFinally, to answer the third question, we design an automatic curriculum to determine the value of v (k) i , so that the instances are selected from easy to hard. The simplest approach is to sort the instances using their confidence score p i obtained by the expert models. However, different from traditional curriculum learning scenarios, our longtailed classification problem involves both many-shot and low-shot categories, where low-shot instances tend to have lower confidence scores than many-shot instances. When sorted by the confidence score, the low-shot samples tend to be classified as hard examples and are not selected at first, which we do not expect to happen. To deal with such scenarios, instead of sorting across the whole training set, we sort instances according to their confidence scores within each cardinality-adjacent subset. To be more specific, given the expert output confidence, v k i should be determined by three factors 1) the expert confidence p i , 2) current epoch k 3) the cardinality-adjacent subset S l the i-th instance belongs to.",
            "score": 0.49604797344477036,
            "section_title": "Self-paced Instance Selection",
            "char_start_offset": 15179,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 288
                },
                {
                    "start": 291,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 765
                },
                {
                    "start": 768,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 935
                },
                {
                    "start": 938,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1125
                },
                {
                    "start": 1128,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2200
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1727294921875
        },
        {
            "corpus_id": "259370648",
            "title": "HuCurl: Human-induced Curriculum Discovery",
            "text": "(Erhan et al., 2010;Wu et al., 2021), which limits the curriculum space that can be examined for finding effective curricula.\n\nThis paper advocates a re-imagining of CL paradigms by introducing and formalizing the task of curriculum discovery, which aims to find effective curricula for a given model and dataset over a curriculum space. The present work specifically focuses on determining when and in which difficulty order text data samples should be learned for effective training of NLP systems. We propose a framework that employs prior knowledge about sample difficulty, such as entropy in human annotations, to inform an effective and flexible sample weighting scheme for curriculum discovery. The framework is capable of discovering optimal curricula (within the space of its weight functions) for any given model and dataset by optimizing the weight functions and adjusting the difficulty group of data samples as training progresses. The discovered curricula provide useful insights about datasets and models, such as the relative importance of different groups of samples for models or knowledge dependency among samples. We illustrate that the proposed framework has the potential to encompass some of the existing CL approaches.\n\nExperimental results show that (a): the topperforming discovered curricula for the same model and dataset can be fundamentally dissimilar in their training strategies, indicating that effective training can be achieved in multiple ways; (b): the discovered curricula are often non-monotonic and greatly differ from the known strategies reported in existing literature, indicating that existing curricula, including easy-to-hard transition curricula, are at the risk of underperforming; and (c): the curricula discovered on small datasets and models perform exceptionally well on larger datasets and models respectively, illustrating the transferability of the discovered curricula. The paper presents a new curriculum learning approach that unlike existing approaches can discover multiple high-performing (and often diverse) curricula for each given NLP model and dataset, provide interpretable curricula in terms of sample difficulty, and encompass some of the existing curriculum learning approaches. 1",
            "score": 0.49176362480557634,
            "section_title": "Introduction",
            "char_start_offset": 1796,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 36,
                    "matchedPaperCorpusId": "227343966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.423583984375
        },
        {
            "corpus_id": "272761648",
            "title": "Joint Imbalance Adaptation for Radiology Report Generation",
            "text": "where \u2663V \u2663 is the vocabulary size. Rank(p, p[z]) assigns a rank to p in descending order and identifies the position of p[z] within this ranking. k ranges from 0 to 1 under regularization with \u2663V \u2663. A higher value of k indicates that the sample is more difficult. Then, we feed the difficulty information to the next step, Training Scheduler. \n\nTraining scheduler aims to automatically leverage imbalance effects by selecting training samples via the difficulty measurers. Our goal is to increase the number of easier samples when the performance decreases and vice versa. According to our goal, we design our scheduler function, c(s t ) as following: \n\n, where s is the average performance of all training samples, measuring the model's learning ability. t is the training step. Given decreasing performance as an example, \n\nst\u22121 will be negative. During the process, the ratio 1 \u2212 (st\u2212st\u22121) \n\nst\u22121 > 1 will allow the model to include more easy training data than the last step c(s t\u22121 ). When the performance increase, the scheduler feed less easy samples to the model and reduce the over-fitting on these samples. After multiple epochs of training, harder samples receive more training iterations than easier samples. In this way, we can alleviate the the challenge from imbalanced tokens and labels in radiology report generation task. To start our curriculum learning, we record the samples' average performance of the last two regular training epochs as s 0 and s 1 , where we empirically initialize c(s 0 ) as 1.",
            "score": 0.4914619355610971,
            "section_title": "Radgraph",
            "char_start_offset": 8027,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 34
                },
                {
                    "start": 35,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 342
                },
                {
                    "start": 345,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 823
                },
                {
                    "start": 826,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 892
                },
                {
                    "start": 895,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1519
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36669921875
        },
        {
            "corpus_id": "250391006",
            "title": "On Curriculum Learning for Commonsense Reasoning",
            "text": "Curriculum learning (Elman, 1993;Bengio et al., 2009) is an alternative to the typical uniform random sampling of training data and is motivated by the gradual progression of human learning from easier to difficult concepts (see Figure 1). In the machine learning paradigm, a 'teacher' ranks the training samples from easy to difficult and introduces them to the 'learner' in that order. Dodge et al. (2020) show that randomly initialized training data orders can lead to large variance in model performance on the GLUE benchmark (Wang et al., 2018). In light of such evidence, we seek to answer: Does a meaningful data order such as a curriculum based on model confidence or dataset distribution outperform a random data order? Such experiments have been carried out for some NLP tasks like machine translation (Platanios et al., 2019) and natural language understanding (Xu et al., 2020) with positive outcomes. While large pretrained models (PTLMs) have been achieving high performance on such tasks, their commonsense reasoning abilities have been limited. Moreover, the process of commonsense acquisition in humans has been shown to be informative for developing algorithms to accomplish the same in machines (Zhu et al., 2020). Hence, we study the effect of a human-like curricu-lum learning to improve the finetuning of PTLMs for commonsense reasoning tasks. \n\nTo impose structure on the data order for sampling training mini-batches, we adopt paced curriculum learning by transfer as proposed in Hacohen and Weinshall (2019). In this method, a pacing function determines the speed at which the ranked data is introduced to the model during training. Ranking of the training dataset is performed using outputs from a pretrained network which has been finetuned on the target dataset using a random training order. We refer to this approach as fixed curriculum learning. During human acquisition of skill sets, a student can benefit from a curriculum that is continuously adjusted by the teacher according to the learning progress of the student. Hence, we also investigate adaptive curriculum learning for commonsense reasoning tasks. The initial data order imposed by the teacher model is updated at regular intervals during training by taking the learner model's current state into account (Kong et al., 2021).",
            "score": 0.489124133419034,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1365
                },
                {
                    "start": 1368,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2141
                },
                {
                    "start": 2142,
                    "end": 2319
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 33,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 33,
                    "end": 53,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 530,
                    "end": 549,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 812,
                    "end": 836,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 872,
                    "end": 889,
                    "matchedPaperCorpusId": "220045816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4453125
        },
        {
            "corpus_id": "3963337",
            "title": "Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning",
            "text": "It is well established that in language acquisition, there are robust patterns in the order by which phenomena are acquired. For example, prototypical concepts are acquired earlier; concrete words tend to be learned before abstract ones (Rosch, 1978). The acquisition of lexical knowledge in artificial systems proceeds differently. In general, models will improve during the course of parameter learning, but the time course of acquisition is not generally studied beyond generalization error as a function of training time or data size. We revisit this issue of choosing the order of learning-curriculum learning-framing it as an optimization problem so that a rich array of factors-including nuanced measures of difficulty, as well as prototypicality and diversity-can be exploited. \n\nPrior research focusing on curriculum strategies in NLP is scarce, and has conventionally been following a paradigm of \"starting small\" (Elman, 1993), i.e., initializing the learner with \"simple\" examples first, and then gradually increasing data complexity (Bengio et al., 2009;Spitkovsky et al., 2010). In language modeling, this preference for increasing complexity has been realized by curricula that increase the entropy of training data by growing the size of the training vocabulary from frequent to less frequent words (Bengio et al., 2009). In unsupervised grammar induction, an effective curriculum comes from increasing length of training sentences as training progresses (Spitkovsky et al., 2010). These case studies have demonstrated that carefully designed curricula can lead to better results. However, they have relied on heuristics in selecting curricula or have followed the intuitions of human and animal learning (Kail, 1990;Skinner, 1938). Had different heuristics been chosen, the results would have been different. In this paper, we use curriculum learning to create improved word representations. However, rather than testing a small number of curricula, we search for an optimal curriculum using Bayesian optimization. A curriculum is defined to be the ordering of the training instances, in our case it is the ordering of paragraphs in which the representation learning model reads the corpus. We use a linear ranking function to conduct a systematic exploration of interacting factors that affect curricula of representation learning models.",
            "score": 0.48475673192149915,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 785
                },
                {
                    "start": 788,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2207
                },
                {
                    "start": 2208,
                    "end": 2356
                }
            ],
            "ref_mentions": [
                {
                    "start": 237,
                    "end": 250,
                    "matchedPaperCorpusId": "15633758"
                },
                {
                    "start": 924,
                    "end": 937,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 1046,
                    "end": 1067,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1067,
                    "end": 1091,
                    "matchedPaperCorpusId": "1363892"
                },
                {
                    "start": 1315,
                    "end": 1336,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1471,
                    "end": 1496,
                    "matchedPaperCorpusId": "1363892"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59716796875
        },
        {
            "corpus_id": "267740312",
            "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
            "text": "Instruction-tuning language models has become a crucial step in aligning them for general use. Typically, this process involves extensive training on large datasets, incurring high training costs. In this paper, we introduce a novel training data selection based on the learning percentage of the samples. We assert that current language models possess the capability to autonomously select high-quality training data, leading to comparable or improved performance compared to training on the entire dataset. Our experiments span different-sized models, revealing that this characteristic holds for models ranging from 1B (small) to 13B (large) in size. Moreover, we demonstrate an interesting finding that the data hardness transfers across model sizes, and a smaller 350M model can effectively curate high-quality training data with hard samples for a larger 13B model, resulting in an equally or superior instruction-tuned model compared to training on the complete dataset. Utilizing open-sourced OPT and Llama-2 models up to 13B in size, two publicly available instruction-tuning training datasets and evaluated by both automatic metrics&humans, our paper introduces a novel approach to training data selection, showcasing a more efficient alternative.",
            "score": 0.4838030793742177,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90869140625
        },
        {
            "corpus_id": "270371045",
            "title": "MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models",
            "text": "Nevertheless, the computational cost remains prohibitive for model-aware data selection, which requires tracking the evolving data preferences of pretraining models on the fly. \n\nOn the other hand, many researchers have proposed curriculum learning strategies that dynamically adjust the data distribution in the pretraining [11; 24; 47; 51; 56; 62]. ELECTRA-style models [11] incorporated curriculum learning in their pretraining process by synchronously training the model with an auxiliary generator, which provided more and more difficult training signals for the discriminator. This implicit curriculum significantly improved the pretraining efficiency on denoising language models [4; 11; 19; 39; 65]. Other methods have explicitly designed the curriculum for pretraining data selection, such as decreasing gradient norms [56], least certainty [24; 51], and increasing expertise [62], demonstrating the benefits of adapting the pretraining data signal according to the model's ever-changing preferences.",
            "score": 0.4829560418233687,
            "section_title": "Introduction",
            "char_start_offset": 5964,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 179,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 1009
                }
            ],
            "ref_mentions": [
                {
                    "start": 372,
                    "end": 376,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 828,
                    "end": 832,
                    "matchedPaperCorpusId": "264935129"
                },
                {
                    "start": 885,
                    "end": 889,
                    "matchedPaperCorpusId": "267681974"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8916015625
        },
        {
            "corpus_id": "254974314",
            "title": "Improving Automated Program Repair with Domain Adaptation",
            "text": "Learning. Curriculum Learning is basically \"Full Fine Tuning\" with two important differences. First, the order of training samples is not random. Second, the amount of training data is increased in each epoch. We are inspired by the methods described in 2.2.1 and implement our own method of Curriculum Learning based on APR's assumptions. These assumptions are: (a) we do not change the pretrained model training scheme or data, and (b) we do not directly use data from the general corpus in the adaptation. This is consistent with the assumptions of other DA methods. That is we take the pretrained model and only change the adaptation scheme. \n\nWe introduce a curriculum that ranks the training samples based on difficulty or distance. In each epoch, the model trains on the easier samples first and then learns the harder ones. We also start with a small portion of training samples and increase it in each epoch. The idea is to gradually adapt the model to the target data, also known as \"soft adaptation\". The rate of the samples fed to the model is as Table 1. \n\nThe result of Curriculum Learning hugely depends on the type of the used curriculum. We propose the following three types of curriculum. \n\n\u2022 Code Length: Longer code lines are harder to fix for the model [57]. \n\n\u2022 Model Confidence: The model returns a confidence score for each prediction. The lower the score, the harder the code line is. \u2022 Similarity to General Data: More similar a data point is to the general data, the easier it is to fix. We note that the last curriculum requires access to the source (general) dataset but does not change the training scheme of the pretrained model. We use CodeBERT [31] as the tool to extract embeddings from samples. Then, we measure the cosine similarity [81] between the vectors of the source dataset and each sample of the target project.",
            "score": 0.48232933991831034,
            "section_title": "Curriculum",
            "char_start_offset": 28788,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1067
                },
                {
                    "start": 1070,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1206
                },
                {
                    "start": 1209,
                    "end": 1279
                },
                {
                    "start": 1282,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1854
                }
            ],
            "ref_mentions": [
                {
                    "start": 1769,
                    "end": 1773,
                    "matchedPaperCorpusId": "260972090"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06903076171875
        },
        {
            "corpus_id": "220045816",
            "title": "Curriculum Learning for Natural Language Understanding",
            "text": "Natural Language Understanding (NLU), which requires machines to understand and reason with human language, is a crucial yet challenging problem. Recently, language model (LM) pre-training has achieved remarkable success in NLU. Pre-trained LMs learn universal language representations from large-scale unlabeled data, and can be simply finetuned with a few adjustments to adapt to various NLU tasks, showing consistent and significant improvements in these tasks (Radford et al., 2018;Devlin et al., 2018). \n\nWhile lots of attention has been devoted to designing better pre-training strategies (Yang et al., 2019;Liu et al., 2019;Raffel et al., 2019), it is also valuable to explore how to more effectively solve downstream NLU tasks in the fine-tuning stage. Most current approaches perform fine-tuning in a straightforward manner, i.e., all training examples are treated equally and presented in a completely random order during training. However, even in the same NLU task, the training examples could vary significantly in their difficulty levels, with some easily solvable by simple lexical clues while others requiring sophisticated reasoning. Table 1 shows some examples from the SST-2 sentiment classification task (Socher et al., 2013), which identifies sentiment polarities (positive or negative) of movie reviews. The easy cases can be solved directly by identifying sentiment words such as \"comfortable\" and \"unimaginative\", while the hard ones further require reasoning with negations or verb qualifiers like \"supposedly\" and \"occasionally\". Extensive research suggests that presenting training examples in a meaningful order, starting from easy ones and gradually moving on to hard ones, would benefit the learning process, not only for humans but also for machines (Skinner, 1958;Elman, 1993;Peterson, 2004;Krueger and Dayan, 2009). \n\nSuch an organization of learning materials in human learning procedure is usually referred to as Curriculum. In this paper, we draw inspiration from similar ideas, and propose our approach for arranging a curriculum when learning NLU tasks. Curriculum Learning (CL) is first proposed by (Bengio et al., 2009) in machine learning area, where the definition of easy examples is established ahead, and an easy-to-difficult curriculum is arranged accordingly for the learning procedure.",
            "score": 0.47981318543406015,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1848
                },
                {
                    "start": 1851,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2091
                },
                {
                    "start": 2092,
                    "end": 2333
                }
            ],
            "ref_mentions": [
                {
                    "start": 1224,
                    "end": 1245,
                    "matchedPaperCorpusId": "990233"
                },
                {
                    "start": 1796,
                    "end": 1808,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 1808,
                    "end": 1823,
                    "matchedPaperCorpusId": "34687176"
                },
                {
                    "start": 1823,
                    "end": 1847,
                    "matchedPaperCorpusId": "206863670"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64404296875
        },
        {
            "corpus_id": "202734083",
            "title": "Data Ordering Patterns for Neural Machine Translation: An Empirical Study",
            "text": "We present the experimental results using different data ordering patterns of the training data in table 1. Randomly shuffling the data once before training can achieve a comparable BLEU score to the default technique used for seq2seq model training of randomly shuffling the training data every epoch, but the model with shuffling every epoch achieves a considerably lower test perplexity than with a single shuffle of the training data. A simple curriculum learning based approach of sorting the source language sentences based on their lengths performs considerably inferior to a randomly sorted ordering. Sorting based on the target language sentences length performs better than sorting based on the source language sentences lengths but is still not comparable to the random shuffling performance. This can possibly because of the optimizer getting stuck at a local optima and converging earlier rather than finding the global optima as is evidenced by the small number of epochs required for convergence. \n\nUsing data ordering patterns sorted on metrics like perplexity and BLEU outperforms the default approach of randomly shuffling the data. Perplexity and BLEU are the most commonly used estimators of the complexity of a sentence pair to be translated correctly by a translation model. A sentence pair having a lower perplexity or BLEU score than another from a pre-trained model implies that the first sentence pair was easier for the model to translate than the latter. From the table it is empirically observed that using an ascending order sorted approach performs the best and gets an improvement in BLEU score of 1.7 points from the default setting. We conjecture this is because the model first accesses less complex examples followed by more complex examples in line with the idea of curriculum learning. \n\nThe interesting observation made here is that a descending order sorted training data schedule based on perplexity or BLEU also outperforms the default setting of random shuffling though the model accesses training data samples from more complex to less complex. This idea is slightly in contrast to the curriculum learning approach of providing the model with training examples in increasing order of complexity.",
            "score": 0.47756414133368763,
            "section_title": "Results",
            "char_start_offset": 7295,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 1011
                },
                {
                    "start": 1014,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1823
                },
                {
                    "start": 1826,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2239
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.348388671875
        },
        {
            "corpus_id": "208031414",
            "title": "Domain Adaptation with BERT-based Domain Classification and Data Selection",
            "text": "Our method is inspired by the work on curriculum learning and recent work on data selection for transfer learning. \n\nCurriculum Learning: Curriculum Learning (Bengio et al., 2009) deals with the question of how to use prior knowledge about the difficulty of the training examples, to boost the rate of learning and the performance of the final model. The ranking or weighting of the training examples is used to guide the order of presentation of examples to the learner. The idea is to build a curriculum of progressively harder samples in order to significantly accelerate a neural network's train-ing. While curriculum learning can leverage label information (loss of the model, training progress) (Weinshall and Amir, 2018) to guide data selection, this work assumes no or few labeled data in the new domain. \n\nData Selection: Not all the data points from the source domain are equally important for target domain transfer. Irrelevant source data points only add noise and overfit the training model. Recent work from Ruder and Plank, applied Bayesian optimization to learn a scoring function to rank the source data points. Data selection method was also used by Tsvetkov et al. to",
            "score": 0.4757819075429176,
            "section_title": "Related Work",
            "char_start_offset": 6782,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 117,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1186
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80029296875
        },
        {
            "corpus_id": "278550228",
            "title": "Curriculum Learning for Few-Shot Domain Adaptation in CT-Based Airway Tree Segmentation",
            "text": "In order to investigate how the ordering of the training data affects performance, we explore multiple curriculum learning strategies for initial network training. In each case, the CL configuration is kept similar, but training examples are ordered differently. The common configuration is organized around 3 CL Batches of increasing size (expressed below as a percentage of the full training set): \n\n\u2022 CL Batch n\u00b01 of size 15% and trained on for 20 epochs. \n\n\u2022 CL Batch n\u00b02 of size 40% and trained on for 70 epochs. \n\n\u2022 CL Batch n\u00b03 of size 45% and trained on for 110 epochs. \n\nBatches are not nested but each includes a small portion of examples from previous ones (15% of initial size). This overlap ensures a smoother transition between batches, discouraging the network from forgetting previously seen examples. We explore three CL orderings: \n\n\u2022 Vanilla CL: Training data is introduced in order of increasing complexity, from an Easy to a Hard batch, following a traditional curriculum-based approach. \u2022 Mixed CL: Data is still ordered from easy to hard, but each batch includes an additional 15% of harder examples. \u2022 Reverse CL: The order is reversed compared to vanilla. \n\nWe compare CL against a control No CL approach in which training samples are introduced in a random order.",
            "score": 0.47424693345032287,
            "section_title": "Full Training on Source Dataset",
            "char_start_offset": 5707,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 458
                },
                {
                    "start": 461,
                    "end": 517
                },
                {
                    "start": 520,
                    "end": 577
                },
                {
                    "start": 580,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 848
                },
                {
                    "start": 851,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1180
                },
                {
                    "start": 1183,
                    "end": 1289
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0872802734375
        },
        {
            "corpus_id": "264832986",
            "title": "Bandit-Driven Batch Selection for Robust Learning under Label Noise",
            "text": "As applications increasingly demand larger and more complex deep learning models, the need for efficient training strategies has become paramount. One way to accelerate training and potentially improve model performance is through the use of Curriculum Learning (CL) and adaptive batch selection. These techniques optimize learning by selectively focusing on data samples that are intrinsically rich and informative at the most appropriate stages of the learning process. Such strategies not only accelerate convergence but also enhance the model's ability to generalize [19,20,27]. \n\nWhile many methods use difficulty metrics to select easy, hard, or uncertain instances for training [31], a key area lies in handling noisy or mislabeled datasets [28]. This domain is particularly important for two reasons: a) the impact of batch selection strategies is easily measured, leading to more insightful conclusions; and b) it addresses the prevalent real-world scenarios where data is often sourced from the web [18] or crowdsourced [7], and a large portion is considered \"unclean\". Sample selection strategies using auxiliary Deep Neural Networks (DNN) effectively mitigate the impact of noisy or mislabeled data. However, these approaches incur substantial computational overhead, limiting their scalability [9,13,17,33]. While alternative methods like SELFIE [26] offer computational efficiency, they are under-explored and rely on steps like re-labeling for optimal performance. Meanwhile, the literature on CL and batch selection offers numerous methods for efficient sample selection across diverse domains [8,19]. This paper introduces a novel approach that synergizes insights from the CL and batch selection literature to enhance efficient sampling schemes, specifically targeting scenarios with prevalent label noise. Our methodology aims to achieve superior performance without the computational burden often associated with deploying additional DNNs, thereby striking a balance between efficacy and computational efficiency. Unlike traditional CL approaches that focus on individual instances or tasks, our method refines the feedback loop from each training iteration to optimize the selected batch. This approach is particularly relevant for tackling the challenges posed by the increasing computational complexity and diversity of machine learning applications across various domains.",
            "score": 0.47405204047315724,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2209
                },
                {
                    "start": 2210,
                    "end": 2396
                }
            ],
            "ref_mentions": [
                {
                    "start": 575,
                    "end": 578,
                    "matchedPaperCorpusId": "8432394"
                },
                {
                    "start": 578,
                    "end": 581,
                    "matchedPaperCorpusId": "208158432"
                },
                {
                    "start": 685,
                    "end": 689,
                    "matchedPaperCorpusId": "232362223"
                },
                {
                    "start": 748,
                    "end": 752,
                    "matchedPaperCorpusId": "220546541"
                },
                {
                    "start": 1030,
                    "end": 1033,
                    "matchedPaperCorpusId": "57246310"
                },
                {
                    "start": 1307,
                    "end": 1310,
                    "matchedPaperCorpusId": "52065462"
                },
                {
                    "start": 1359,
                    "end": 1363,
                    "matchedPaperCorpusId": "174800904"
                },
                {
                    "start": 1610,
                    "end": 1613,
                    "matchedPaperCorpusId": "11137059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4755859375
        },
        {
            "corpus_id": "259370648",
            "title": "HuCurl: Human-induced Curriculum Discovery",
            "text": "We introduce an effective curriculum learning framework that employs prior knowledge about sample difficulty in its training paradigm for curriculum discovery. The proposed framework initially partitions its input data into several groups of increasing difficulty, defines parameterized func-  Table 2. tions to weight sample losses in each difficulty group, moves samples across difficulty groups based on their learning progress, and enables tuning the parameters of the weight function to discover novel curricula. We demonstrate that this framework is capable of representing several categories of curriculum learning approaches. The task of curriculum discovery alleviates the limitations imposed by selecting a single curriculum strategy, and instead, focuses on finding and analyzing different curricula that work equally-well for a given model and dataset. In addition, the discovered curricula provide insight into how different portions of the dataset contribute toward learning at different stages of training a model, which, in turn, provide knowledge about the learning dynamics of different models. The task of curriculum discovery could be costly on large datasets, in particular, when the goal is to find optimal curricula for different models and datasets. To mitigate the computational : Confidence assignment to samples in our datasets by three CL approaches. The x-axis is the epoch number, and y-axis is the average weight assigned to samples of each difficulty group. Blue (solid) is easy, orange (dashed) is medium, and green (dash-dot) is hard. The shaded area is the 95% CI over the datasets with five random seeds each. The curves are monotonic for most parts, and can be approximated by our framework.\n\ncost, we show that it is possible to rapidly discover a curriculum on a small subset of the dataset (or a smaller version of the model with significantly less number of parameters) and apply the resulting curriculum to the full dataset.\n\nThere are several promising areas for future work. These include approaches for learning new difficulty indicators from data (e.g., linguistic difficulty including lexical, syntactic and semantic difficulty), prioritizing medium level instances and those with greatest progress during training, and developing challenge datasets that contain diverse data samples with different levels of difficulty. Finally, investigating diverse curricula that are suitable for general use and across datasets through curriculum discovery and generalization is a promising area for research.",
            "score": 0.4721701504174693,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 27435,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.280029296875
        },
        {
            "corpus_id": "227227757",
            "title": "Dynamic Curriculum Learning for Low-Resource Neural Machine Translation",
            "text": "Curriculum learning (Bengio et al., 2009) is motivated by the learning strategy of biological organisms which orders the training samples in an easy-to-hard manner (Elman, 1993). Benefited from organized training, the neural network explores harder samples effectively utilizing the previous knowledge learned from easier samples. Weinshall et al. (2018) demonstrate curriculum learning speeds up the learning process, especially at the beginning of training. Curriculum learning has been applied to several tasks, including language modeling (Bengio et al., 2009), image classification (Weinshall et al., 2018), and human attribute analysis (Wang et al., 2019). \n\nCurriculum learning has recently shown to train large-scale translation tasks efficiently and effectively by controlling the way of feeding samples. Kocmi and Bojar (2017) construct mini-batch contains sentences similar in length and linguistic phenomena, then organize the order by increased complexity in one epoch. Zhang et al. (2018) group the training samples into shards based on model-based and linguistic difficulty criteria, then train with hand-crafted curriculum schedules. Platanios et al. (2019) propose competence-based curriculum learning that select training samples based on sample difficulty and model competence. Kumar et al. (2019) use reinforcement learning to learn the curriculum automatically. Liu et al. (2020) propose a norm-based curriculum learning method based on the norm of word embedding to improve the efficiency of training an NMT system. Zhou et al. (2020) propose uncertainty-aware curriculum learning. To the best of our knowledge, this is the first comprehensive discussion of curriculum learning in a low-resource setup. \n\nOn the other hand, curriculum learning is similar to data selection and data sampling methods. More similar work is that Wang et al. (2018) propose a dynamic sampling method that calculates the decline of loss during training to improve the NMT training efficiency. They start training from the full training set and then gradually decrease. This is contrary to the idea of curriculum learning.",
            "score": 0.470865849631407,
            "section_title": "Curriculum Learning",
            "char_start_offset": 4953,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 662
                },
                {
                    "start": 665,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1724
                },
                {
                    "start": 1727,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2068
                },
                {
                    "start": 2069,
                    "end": 2121
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 41,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 331,
                    "end": 354,
                    "matchedPaperCorpusId": "5004002"
                },
                {
                    "start": 543,
                    "end": 564,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 587,
                    "end": 611,
                    "matchedPaperCorpusId": "5004002"
                },
                {
                    "start": 642,
                    "end": 661,
                    "matchedPaperCorpusId": "58981386"
                },
                {
                    "start": 814,
                    "end": 836,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 1150,
                    "end": 1173,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 1297,
                    "end": 1316,
                    "matchedPaperCorpusId": "67855673"
                },
                {
                    "start": 1383,
                    "end": 1400,
                    "matchedPaperCorpusId": "219260306"
                },
                {
                    "start": 1538,
                    "end": 1556,
                    "matchedPaperCorpusId": "220047761"
                },
                {
                    "start": 1848,
                    "end": 1866,
                    "matchedPaperCorpusId": "20639213"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59716796875
        },
        {
            "corpus_id": "261557260",
            "title": "Epi-Curriculum: Episodic Curriculum Learning for Low-Resource Domain Adaptation in Neural Machine Translation",
            "text": "We follow the general curriculum learning framework of difficulty measure and training scheduler (Wang et al., 2021b), where difficulty measure determines the relative \"difficulty\" of each data sample and training scheduler decides the sequence of data subsets throughout the training process. Additionally, curriculum learning has been approved as an effective method for data cleaning, and many works have shown that the performance can be improved on denoised data (Wang et al., 2017(Wang et al., , 2019)). Thus, in order to let the model focus on the high-relevant in-domain corpus, curriculum learning is also applied to filter the data. \n\nWe evaluate our Epi-Curriculum on English-German (En-De) and English-Romanian (En-Ro) translation tasks with 10 and 9 different domains. There are 5 seen domains used for training a teacher model, and then individually fine-tuning on the 5 seen and the rest unseen domains. BLEU score (Papineni et al., 2002;Post, 2018) is reported and the experimental results show that Epi-Curriculum improves the model's robustness and adaptability on both seen and unseen domains. For instance, it outperforms the baselines by 1.37 -3.64 on the En-De task and 1.73 -3.32 on the En-Ro task. We further demonstrate the model's robustness to domain shift, where they improve the baseline by 2.55 and 2.59 BLEU scores, respectively. Our contributions mainly lie in three aspects: \n\n\u2022 We propose a novel episodic training framework to handle the model's robustness to domain shift. This is the first work that simulates the situation of unexpected statistics during training for domain adaptation in NMT. \n\n\u2022 Curriculum learning is applied to our training framework. Our curriculum learning not only guides the model from easy to difficult tasks but also denoises the training data. \n\n\u2022 We evaluate Epi-Curriculum on two language pairs with ten and nine different domains, empirically showing the strength of our proposed approach. \n\n2 Related Work",
            "score": 0.469901840475601,
            "section_title": "Introduction",
            "char_start_offset": 3566,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1407
                },
                {
                    "start": 1410,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1631
                },
                {
                    "start": 1634,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1809
                },
                {
                    "start": 1812,
                    "end": 1958
                },
                {
                    "start": 1961,
                    "end": 1975
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 117,
                    "matchedPaperCorpusId": "232362223"
                },
                {
                    "start": 468,
                    "end": 486,
                    "matchedPaperCorpusId": "1054586"
                },
                {
                    "start": 486,
                    "end": 508,
                    "matchedPaperCorpusId": "174798125"
                },
                {
                    "start": 930,
                    "end": 953,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 953,
                    "end": 964,
                    "matchedPaperCorpusId": "13751870"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48193359375
        },
        {
            "corpus_id": "273811618",
            "title": "Does the Definition of Difficulty Matter? Scoring Functions and their Role for Curriculum Learning",
            "text": "Despite the initial motivation through CL, we have so far only investigated SFs in isolation their alignment of difficulty orderings. This section focuses on the practical implications of SD orderings for CL. We perform experiments as described in Section III-E, based on the ensemble SD orderings obtained from Section IV-B4. This approach aims to provide further insights into how the performance of a model changes when we alter the data it is exposed to based on different scoring functions. \n\n1) Different orderings and pacing functions: We start the experimental investigation of CL with very coarse differences in the training setup. While we did notice differences in the agreement of difficulty values across SFs and training settings, the results of Section IV-B provide enough evidence to assume that either of the SFs is a suitable estimator for a distinction between the easiest and the most difficult samples within a dataset. Moreover, to limit computational demands, we exclude the CVLoss and FIT SFs from the subsequent performance experiments, as a correlation above 80 % is noted to CumAcc for seed-based ensembles across both datasets. \n\nIn our first experiments, we evaluate how model performance is impacted when examples are presented in the intended curriculum ordering (easy-to-hard; CL), the reverse ordering (hard-to-easy; ACL), and a completely random ordering (RCL). \n\nFor a robust evaluation across the different orderings, we choose the seed-based ensembles for all SFs. For training, we select the EfficientNet-B0 architecture in combination with the best baseline training setup, as preliminary experiments indicated that this configuration also shows high performance in combination with different pacing functions. We evaluate each SF across four PFs (logarithmic, root, linear, exponential), starting with an initial training dataset size of b = 20 % and a saturation on the full dataset after a = 50 % and a = 80 % of the training iterations. Following the approach of [5], we incorporate new training samples in a class-balanced manner, such that the training subsets remain balanced throughout training, assuming the full dataset is balanced. Analogous to [15], we also delay the introduction of new samples until all examples in the current training subset have been used at least once.",
            "score": 0.46966331144337625,
            "section_title": "C. Curriculum Learning",
            "char_start_offset": 37249,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 495
                },
                {
                    "start": 498,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1155
                },
                {
                    "start": 1158,
                    "end": 1395
                },
                {
                    "start": 1398,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2181
                },
                {
                    "start": 2182,
                    "end": 2326
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10797119140625
        },
        {
            "corpus_id": "227905455",
            "title": "Towards a Task-Agnostic Model of Difficulty Estimation for Supervised Learning Tasks",
            "text": "Recent advances on natural language processing (NLP) benchmarks have been driven by increasingly sophisticated language models, which are pretrained on enormous amounts of data before use. Refinements of this process has led to increasingly powerful language models, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and more recently T5 (Raffel et al., 2019). Such models are quickly saturating even new tasks that have undergone a rigorous adversarial filtering process (Zellers et al., 2018). However, these downstream performance improvements also require more computational resources and data to train the models, which is not always feasible. Curriculum learning (Elman, 1993), a strategy where the model is trained on easier examples before harder ones, has recently been shown to improve performance and reduce training time on a variety of NLP tasks, especially machine translation (Liu et al., 2020;Wang et al., 2020;Zhou et al., 2020;Xu et al., 2020). \n\nThe success of this research shows that the order in which training data is presented to a model is important, but how to best apply curriculum learning broadly to NLP and how to design effective measures of difficulty to create curricula remain relatively unexplored. Prior approaches either use a hand-crafted measure of difficulty that works well for a particular task or design an architecture that automatically creates the curriculum during training. In either case, the curriculum is formed using some information-theoretic measure of difficulty (semantic distance, feedback from a separate network, etc.), and it is difficult to interpret why they work well for some tasks and not others. In a practical sense, it is seldom clear how to apply a previously investigated curricula directly to another task. \n\nIn this paper, we explore how to address these shortcomings by creating what we call a taskagnostic model of difficulty, which we argue can, in principle, be applied to any supervised learning task. We use this model to investigate what makes a good difficulty measure for curricula beyond how it affects downstream performance to better explain why one curriculum should be preferred over another.",
            "score": 0.4686646361844679,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 976
                },
                {
                    "start": 979,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1791
                },
                {
                    "start": 1794,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2192
                }
            ],
            "ref_mentions": [
                {
                    "start": 280,
                    "end": 301,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 486,
                    "end": 508,
                    "matchedPaperCorpusId": "52019251"
                },
                {
                    "start": 683,
                    "end": 696,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 923,
                    "end": 941,
                    "matchedPaperCorpusId": "220045412"
                },
                {
                    "start": 941,
                    "end": 959,
                    "matchedPaperCorpusId": "220047761"
                },
                {
                    "start": 959,
                    "end": 975,
                    "matchedPaperCorpusId": "220045816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76953125
        },
        {
            "corpus_id": "253734400",
            "title": "In-sample Curriculum Learning by Sequence Completion for Natural Language Generation",
            "text": "Curriculum learning (CL) proposed by Bengio et al. (2009) provides performance improvements on a number of machine learning tasks. It mimics the learning process of humans by training models with samples in a more meaningful order, i.e., from the easy ones to the hard ones. Therefore, ranking training samples by difficulty lies in the core of CL, which is also the key challenge when it's applied to natural language generation (NLG) tasks.\n\nPrevious work on CL for NLG focuses on measuring the difficulty of training samples in two ways. One is to resort to human-crafted rules based on various linguistic features and human observations (Liu et al., 2018;Kocmi and Bojar, 2017). The other uses models either trained from outside data or the same data but in previous epochs/steps (Zhou et al., 2020;Kumar et al., 2019;Shen and Feng, 2020). Either way seeks to produce a numeric score for each training sample relying on domain expertise so that it can be ranked, making it difficult to generalize to different tasks. For * The corresponding author. example, summarization focuses more on generating concise outputs while style transfer emphasizes style changes. So the former should pay attention to the ratio between the lengths of the output and the input (the more compressed the more difficult), while the latter should focus on differences in style between the input and output (the more different the more difficult). Designing a comprehensive or universal scoring function is difficult or even impossible under this definition of CL.\n\nIn this paper, we propose an alternative to sample-wise CL, which we call in-sample CL (ICL). ICL re-orders the learning sequence within the sample. One particular ICL re-ordering strategy which we find effective is to predict the last few tokens given a long prefix first from the original output, and then gradually increase the number of tokens at the end while shortening the prefix, to create an easy-to-hard training order. Such a curriculum learning strategy focuses more on the difficulty of language generation itself, leading to a better generalization ability among tasks.\n\nActually, we are not the first to propose the idea of ICL. Liang et al. (2021) introduced the notion of \"token-wise curriculum learning(TCL)\". Illustrations",
            "score": 0.46772271691521816,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 57,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 641,
                    "end": 659,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 659,
                    "end": 681,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 784,
                    "end": 803,
                    "matchedPaperCorpusId": "220047761"
                },
                {
                    "start": 822,
                    "end": 842,
                    "matchedPaperCorpusId": "218470266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.461181640625
        },
        {
            "corpus_id": "257901096",
            "title": "Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation",
            "text": "Our method follows the idea of exposing raw data to NAT, but is different from theirs by introducing an NAT evaluator to evaluate each raw translation. By changing the ratio of raw sentences in the training data, we can configure the complexity of data in the training process and benefit more from raw data by exposing some raw translations for a longer period. \n\nCurriculum Learning Our work adopts a hard-to-easy strategy in training NAT models by decreasing the ratio of raw data in the training process, which is contrary to curriculum learning (Bengio et al. 2009) in spirits. Curriculum learning methods train machine learning models from easy to hard data, but (Braun, Neil, and Liu 2017) showed that learning from hard to easy can be effective. They conducted experiments on automatic speech recognition systems and use signal-to-noise ratio (SNR) to create hard-to-easy curriculum. Compared with the opposite ranking of the examples from easy to hard, the hard-to-easy strategy provides better results.",
            "score": 0.46629951563418015,
            "section_title": "Related Work",
            "char_start_offset": 25895,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 362
                },
                {
                    "start": 365,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1012
                }
            ],
            "ref_mentions": [
                {
                    "start": 550,
                    "end": 569,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 669,
                    "end": 696,
                    "matchedPaperCorpusId": "14928979"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.419677734375
        },
        {
            "corpus_id": "388785",
            "title": "Learning What Data to Learn",
            "text": "Training data plays a critical role in machine learning. The data selection strategy along the training process could significantly impact the performance of the learned model. For example, an appropriate strategy of removing redundant data can lead to a better model by using less computational efforts. For another example, previous studies on curriculum learning (Bengio et al., 2009) and self-paced learning (Kumar et al., 2010) reveal some principles of how tailoring data based on its 'hardness' can favor the model training; that is, easy data instances are important at the early age of model training, while at later age, harder training examples tend to be more effective to improve the model, since easy ones bring minor changes. \n\nThese facts reveal that how to feed training samples into a machine learning system is nontrivial and feeding data in a totally random order is not always a good choice. To explore a better data selection strategy for training, previous works including curriculum learning (CL) and self-paced learning (SPL) adopt simple heuristic rules, such as shuffling the sequence length to train language model (Bengio et al., 2009), or abandoning training instances whose loss values are larger than a human-defined threshold (Kumar et al., 2010;Jiang et al., 2014a). Such human-defined rules are a little restricted to certain tasks and cannot be generalized to broader learning scenarios, since different learning tasks may yield different optimal data selection rules, and even one learning task may need data with various properties to optimize at different training stages. Therefore it remains an open problem how to automatically and dynamically allocate appropriate training data at different stages of machine learning? \n\nTo find a solution to the above problem, we design twofold intuitive principles: on one hand, the data selection strategy should be general enougg, such that it can be naturally applied to different learning scenarios without further particularly human-designed efforts; on the other hand, the strategy should be forward-looking, in that its choice at every step along the training leads to better long-term reward, rather than temporarily fitting to current stage. \n\nFollowing these principles, we propose a new data selection framework, based on deep reinforcement learning (DRL).",
            "score": 0.46563423949429217,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 57,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 740
                },
                {
                    "start": 743,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1761
                },
                {
                    "start": 1764,
                    "end": 2229
                },
                {
                    "start": 2232,
                    "end": 2346
                }
            ],
            "ref_mentions": [
                {
                    "start": 366,
                    "end": 387,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 412,
                    "end": 432,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 1143,
                    "end": 1164,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1259,
                    "end": 1279,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 1279,
                    "end": 1299,
                    "matchedPaperCorpusId": "207216972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88134765625
        },
        {
            "corpus_id": "267897619",
            "title": "Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach",
            "text": "Curriculum learning was first proposed by Bengio et al. (2009) under the assumption that learning with reordering from \"easy\" samples to \"hard\" samples would boost performance. It has been applied in various applications, including neural machine translation (Zhou et al., 2020;Platanios et al., 2019;Zhou et al., 2020;Wang et al., 2018), relation extraction (Huang and Du, 2019), reading comprehension (Tay et al., 2019), natural language understanding (Xu et al., 2020) and named entity recognition (Jafarpour et al., 2021;Lobov et al., 2022;Wenjing et al., 2021). \n\nSeveral studies aim to adopt curriculum learning philosophy for textual data and propose various difficulty-scoring functions and curriculum schedulers. Some methods measure sample difficulty with features derived from lexical statistics, e.g., sentence length and word rarity (Platanios et al., 2019;Jafarpour et al., 2021), where longer sentences and rarer words are considered \"hard\". Others use features from pre-trained language models (Zhou et al., 2020;Wang et al., 2018;Liu et al., 2020). Most schedulers select samples with difficulty scores lower than a threshold (Platanios et al., 2019). While Zhou et al. (2020) design a sample selecting function based on model uncertainty. Our approach, unique in applying tokenlevel curriculum learning to DS-NER tasks, diverges from common sentence-level methods by utilizing Transformer-based models like BERT for context-aware token-specific predictions and gradient learning.",
            "score": 0.4650270943138714,
            "section_title": "Appendix .1 Curriculum Learning",
            "char_start_offset": 27802,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 566
                },
                {
                    "start": 569,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1497
                }
            ],
            "ref_mentions": [
                {
                    "start": 42,
                    "end": 62,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 259,
                    "end": 278,
                    "matchedPaperCorpusId": "220047761"
                },
                {
                    "start": 301,
                    "end": 319,
                    "matchedPaperCorpusId": "220047761"
                },
                {
                    "start": 319,
                    "end": 337,
                    "matchedPaperCorpusId": "52156056"
                },
                {
                    "start": 359,
                    "end": 379,
                    "matchedPaperCorpusId": "202764484"
                },
                {
                    "start": 403,
                    "end": 421,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 454,
                    "end": 471,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 501,
                    "end": 525,
                    "matchedPaperCorpusId": "236486249"
                },
                {
                    "start": 525,
                    "end": 544,
                    "matchedPaperCorpusId": "252819279"
                },
                {
                    "start": 544,
                    "end": 565,
                    "matchedPaperCorpusId": "237101139"
                },
                {
                    "start": 870,
                    "end": 893,
                    "matchedPaperCorpusId": "236486249"
                },
                {
                    "start": 1010,
                    "end": 1029,
                    "matchedPaperCorpusId": "220047761"
                },
                {
                    "start": 1029,
                    "end": 1047,
                    "matchedPaperCorpusId": "52156056"
                },
                {
                    "start": 1047,
                    "end": 1064,
                    "matchedPaperCorpusId": "219260306"
                },
                {
                    "start": 1175,
                    "end": 1193,
                    "matchedPaperCorpusId": "220047761"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61083984375
        },
        {
            "corpus_id": "11185579",
            "title": "Learning and Knowledge Transfer with Memory Networks for Machine Comprehension",
            "text": "Curriculum learning makes use of the fact that model performance can be significantly improved if the training samples are not presented randomly but in such a way so as to make the learning task gradually more difficult by presenting examples in an easy to hard ordering . Such a training procedure allows the learner to waste less time with noisy or hard to predict data when the model is not ready to incorporate such samples. However, what remains unanswered and is left as a matter of further exploration is how to devise an effective strategy for a given task?\n\nIn this work, we formulate a curriculum strategy to train a memory network for machine comprehension. Formally, we rank training tuples (q, S, C, s) from easy to hard based on the normalized word frequency for passage, question, and context initially; using the score function (SF) mentioned in Equation 4 (i.e. easier passages have more frequent words). The training data is then divided into a fixed number of chapters, with each successive chapter resulting in addition of more difficult tuples. The model is then trained sequentially on each chapter with the final chapter containing the complete training data. The presence of both the number of chapters and the fixed number of epochs per chapter makes such a strategy flexible and allows to be tailored to different data after optimizing the like other hyper-parameters.\n\n(an \u00d7 log(\u00e2n(P, D))+\n\n(1 \u2212 an) \u00d7 log(1 \u2212\u00e2n(P, D)) \u00d7 1(en, c(n) \u00d7 epc)\n\nThe loss function used for curriculum inspired training varies with epoch number; as mentioned in Equation 5. Note, in Equation 5, en and c(n) represents the current epoch number and chapter number for n th tuple assigned using rank allocated based on SF mentioned in Equation 4 respectively. epc, P , D, and 1 is the number of epochs per chapter, model parameters, training set, and an indicator function which is one if first argument is >= the second argument or else zero; respectively.",
            "score": 0.46462408331221816,
            "section_title": "CIT: Curriculum Inspired Training",
            "char_start_offset": 15435,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.444091796875
        },
        {
            "corpus_id": "260438678",
            "title": "Reverse Stable Diffusion: What prompt was used to generate this image?",
            "text": "Since we employ a novel curriculum learning regime to boost the performance of the studied models, we can also consider work on curriculum learning as related. \n\nThe research community has extensively utilized this learning paradigm across a range of domains, including both computer vision (Bengio et al., 2009;Croitoru et al., 2024;Ionescu et al., 2016;Shi and Ferrari, 2016;Soviany et al., 2021;Chen and Gupta, 2015;Sinha et al., 2020;Zhang et al., 2021a) and natural language processing (Croitoru et al., 2024;Liu et al., 2018;Platanios et al., 2019). However, given the unique nature of each application, distinct data organization approaches have been developed to ensure optimal results. For example, in vision, the number of objects in the image is one criterion (Soviany et al., 2021;Shi and Ferrari, 2016), while, in natural language processing, both word frequency (Liu et al., 2018) and sequence length (Kocmi and Bojar, 2017;Tay et al., 2019;Zhang et al., 2021c) are utilized. Other contributions tried to avoid estimating sample difficulty by implementing curriculum learning on the model itself (Jarca et al., 2024;Karras et al., 2018;Sinha et al., 2020;Croitoru et al., 2024), or by selecting the samples dynamically, based on the performance of the model (Kumar et al., 2010;Jiang et al., 2015). Different from related approaches based on ordering data samples according to their difficulty (Bengio et al., 2009;Soviany et al., 2021;Shi and Ferrari, 2016), we propose to employ a novel approach to assess the difficulty level. More specifically, we utilize the mean cosine similarity between the prompt embedding produced by the model and the ground-truth embedding vector, measured at various stages of the standard training process.",
            "score": 0.46356361648713607,
            "section_title": "Related work",
            "char_start_offset": 13864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 162,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1751
                }
            ],
            "ref_mentions": [
                {
                    "start": 291,
                    "end": 312,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 334,
                    "end": 355,
                    "matchedPaperCorpusId": "879067"
                },
                {
                    "start": 355,
                    "end": 377,
                    "matchedPaperCorpusId": "6954583"
                },
                {
                    "start": 377,
                    "end": 398,
                    "matchedPaperCorpusId": "208138033"
                },
                {
                    "start": 438,
                    "end": 458,
                    "matchedPaperCorpusId": "239016453"
                },
                {
                    "start": 514,
                    "end": 531,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 531,
                    "end": 554,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 771,
                    "end": 793,
                    "matchedPaperCorpusId": "208138033"
                },
                {
                    "start": 793,
                    "end": 815,
                    "matchedPaperCorpusId": "6954583"
                },
                {
                    "start": 876,
                    "end": 894,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 915,
                    "end": 938,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 938,
                    "end": 955,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 955,
                    "end": 975,
                    "matchedPaperCorpusId": "233433844"
                },
                {
                    "start": 1130,
                    "end": 1150,
                    "matchedPaperCorpusId": "3568073"
                },
                {
                    "start": 1272,
                    "end": 1292,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 1408,
                    "end": 1429,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1429,
                    "end": 1450,
                    "matchedPaperCorpusId": "208138033"
                },
                {
                    "start": 1450,
                    "end": 1472,
                    "matchedPaperCorpusId": "6954583"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4384765625
        },
        {
            "corpus_id": "53295888",
            "title": "An Empirical Exploration of Curriculum Learning for Neural Machine Translation",
            "text": "We consider a wide range of schedules, based not only on the easy-to-difficult ordering, but also on strategies developed independently from curriculum learning, such as dynamic sampling and boosting (Zhang et al., 2017;van der Wees et al., 2017;Wang et al., 2018). \n\nWe conduct an extensive empirical exploration of curriculum learning on a German-English translation task, implementing all training strategies in the Sockeye NMT toolkit. 1 . Our experiments confirm that curriculum learning can improve convergence speed without loss of translation quality, and show that viewing curriculum learning more flexibly than strictly training on easy samples first has some benefits. We also demonstrate that curriculum learning is highly sensitive to hyperpa-rameters, and no clear single best strategy emerges from the experiments. \n\nIn this sense, our conclusions are both positive and negative: We have confirmed that curriculum learning can be an effective method for training expensive models like those in NMT, but careful design of the specific curriculum hyperparameters is important in practice. \n\n2 Related Work Bengio et al. (2009) coined the term of curriculum learning to refer to techniques that guide the training of learning systems \"by choosing which examples to present and in which order to present them in the learning system\", and hypothesize that training on easier samples first is beneficial. While organizing training samples based on difficulty has been demonstrated in NLP outside of neural models -e.g., Spitkovsky et al. (2010) bootstrap unsupervised dependency parsers by learning from incrementally longer sentences -curriculum learning has gained popularity to address the difficult optimization problem of training deep neural models (Bengio, 2012). Bengio et al. (2009) improve neural language model training using a curriculum based on increasing vocabulary size. More recently, Tsvetkov et al. (2016) improve word embedding training using Bayesian optimization to order paragraphs in the training corpus based on a range of distributional and linguistic features (diversity, simplicity, prototypicality).",
            "score": 0.463345034867918,
            "section_title": "Introduction",
            "char_start_offset": 1954,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 268,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 829
                },
                {
                    "start": 832,
                    "end": 1101
                },
                {
                    "start": 1104,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2137
                }
            ],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 220,
                    "matchedPaperCorpusId": "550225"
                },
                {
                    "start": 220,
                    "end": 246,
                    "matchedPaperCorpusId": "7921428"
                },
                {
                    "start": 246,
                    "end": 264,
                    "matchedPaperCorpusId": "20639213"
                },
                {
                    "start": 1119,
                    "end": 1139,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1529,
                    "end": 1553,
                    "matchedPaperCorpusId": "1363892"
                },
                {
                    "start": 1780,
                    "end": 1800,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.708984375
        },
        {
            "corpus_id": "270357326",
            "title": "Diversified Batch Selection for Training Acceleration",
            "text": "Coreset selection (Mirzasoleiman et al., 2020;Xin et al., 2024), also known as data pruning, aims to create a smaller subset (coreset) of the original data that captures essential patterns for efficient model training.Various metrics like the entropy score (Coleman et al., 2020), EL2N score (Paul et al., 2021), forgetting score (Toneva et al., 2019), and classification margin (Pleiss et al., 2020), are used to measure individual differences among data points.Yet, selecting samples with the highest scores can lead to diversity issues, especially at high pruning rates, resulting in performance degradation (Xia et al., 2023).Zheng et al. (2023);Xia et al. (2023) propose strategies of selecting samples with intermediate scores or with diverse scores, yielding promising results under high pruning rates.However, coreset selection faces limitations in prioritizing samples with diverse properties during different training stages.Moreover, the acceleration benefits are noticeable only when the coreset is repeatedly used to train various models, as the data selection relies on a full data trained model.\n\nCurriculum learning (Bengio et al., 2009) seeks to enhance model performance with minimal computational costs by prioritizing \"easy\" samples before uniformly training on the entire dataset (Jiang et al., 2015;Sinha et al., 2020;Zhou et al., 2020).Although curriculum learning can improve model convergence, it may not efficiently reduce training expenses.And they fall short in addressing the challenge of skipping redundant points that have already been learned.2023) achieve notable speedup by leveraging additional reference models to select valuable samples.However, their practical applications are restricted by the availability of well-performing reference models.Compared to prior methods, which score and select data in a sample-wise manner, our reference-model-free Di-vBS, excels in selecting high-quality and diverse samples by optimizing the overall orthogonalized representativeness of the subset after removing inter-sample redundancy.\n\nAcceleration of the Selection Process.Except for uni-form sampling, online batch selection methods generally require an additional forward pass for each batch.",
            "score": 0.46324123571490533,
            "section_title": "Related Work",
            "char_start_offset": 20954,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 218,
                    "end": 463
                },
                {
                    "start": 463,
                    "end": 630
                },
                {
                    "start": 630,
                    "end": 809
                },
                {
                    "start": 809,
                    "end": 935
                },
                {
                    "start": 935,
                    "end": 1110
                },
                {
                    "start": 1112,
                    "end": 1359
                },
                {
                    "start": 1359,
                    "end": 1467
                },
                {
                    "start": 1467,
                    "end": 1575
                },
                {
                    "start": 1575,
                    "end": 1674
                },
                {
                    "start": 1674,
                    "end": 1783
                },
                {
                    "start": 1783,
                    "end": 2062
                },
                {
                    "start": 2064,
                    "end": 2102
                },
                {
                    "start": 2102,
                    "end": 2223
                }
            ],
            "ref_mentions": [
                {
                    "start": 18,
                    "end": 46,
                    "matchedPaperCorpusId": "211259075"
                },
                {
                    "start": 46,
                    "end": 63,
                    "matchedPaperCorpusId": "265445587"
                },
                {
                    "start": 292,
                    "end": 311,
                    "matchedPaperCorpusId": "235898952"
                },
                {
                    "start": 611,
                    "end": 629,
                    "matchedPaperCorpusId": "259298636"
                },
                {
                    "start": 630,
                    "end": 650,
                    "matchedPaperCorpusId": "253224188"
                },
                {
                    "start": 650,
                    "end": 667,
                    "matchedPaperCorpusId": "259298636"
                },
                {
                    "start": 1132,
                    "end": 1153,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1301,
                    "end": 1321,
                    "matchedPaperCorpusId": "10891229"
                },
                {
                    "start": 1321,
                    "end": 1340,
                    "matchedPaperCorpusId": "220301592"
                },
                {
                    "start": 1340,
                    "end": 1358,
                    "matchedPaperCorpusId": "220047761"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34228515625
        },
        {
            "corpus_id": "277667729",
            "title": "2D-Curri-DPO: Two-Dimensional Curriculum Learning for Direct Preference Optimization",
            "text": "Curriculum Learning (CL) [19], [20] is a training strategy where data samples are presented in a meaningful order, typically from easy to hard, to control and optimize the information flow during learning. This principle, inspired by human learning, has shown benefits like faster convergence and improved generalization in both humans and machines [20]- [22]. CL has been widely adopted in various NLP tasks, including language modeling [23], [24], reading comprehension [25], question answering [26], [27], and machine translation [28], [29]. \n\nThe application of CL to LLM alignment, particularly within preference optimization frameworks, is an emerging area. [12] introduced Curri-DPO, demonstrating the effectiveness of ordering multiple preference pairs based on their difficulty within an iterative DPO setting. This showed the importance of structured data presentation beyond simply pooling multiple pairs. Concurrent work by [30] also explores curriculum ideas, focusing on self-alignment bootstrapping for supervised finetuning. To the best of our knowledge, our work is the first to propose and systematically evaluate a two-dimensional curriculum for DPO, considering both prompt difficulty and preference pair difficulty.",
            "score": 0.46302957450150994,
            "section_title": "B. Curriculum Learning in AI Alignment",
            "char_start_offset": 6745,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 544
                },
                {
                    "start": 547,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1236
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 29,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 31,
                    "end": 35,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 349,
                    "end": 353,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 355,
                    "end": 359,
                    "matchedPaperCorpusId": "206863670"
                },
                {
                    "start": 438,
                    "end": 442,
                    "matchedPaperCorpusId": "59067608"
                },
                {
                    "start": 444,
                    "end": 448,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 472,
                    "end": 476,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 497,
                    "end": 501,
                    "matchedPaperCorpusId": "16503693"
                },
                {
                    "start": 503,
                    "end": 507,
                    "matchedPaperCorpusId": "44130298"
                },
                {
                    "start": 533,
                    "end": 537,
                    "matchedPaperCorpusId": "155089817"
                },
                {
                    "start": 664,
                    "end": 668,
                    "matchedPaperCorpusId": "268364003"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.378173828125
        },
        {
            "corpus_id": "258378160",
            "title": "Curricular Next Conversation Prediction Pretraining for Transcript Segmentation",
            "text": "Curriculum learning (Bengio et al., 2009) proposes that models observing training samples in an increasing order of difficulty have an advantage over models observing samples in an otherwise random order. Motivated by this, we introduce a curriculum to Advanced NCP pretraining. The pretraining samples from Advanced NCP are divided into two distinct sets -\"similar\" to downstream task (or \"harder\" since, in general, segmentation is a harder task than NCP) and \"dissimilar\" to the segmentation task (or \"easier\"). In order to estimate the similarity or dissimilarity of the NCP pretraining samples to the segmentation task, we use a classification model trained for the segmentation task and use it to predict labels for the pretraining instances. We refer to this as the \"Auxiliary model\" and classify a pretraining sample as \"similar\" if the Auxiliary model correctly predicts its label and vice versa. In the spirit of curriculum learning, we divide the pretraining into two steps. First training on the \"dissimilar\" or \"easy\" (from the perspective of segmentation) samples followed by the \"similar\" or \"hard\" samples. This order makes sure that the model has smoother transition between the two tasks that are semantically close but different. Figure 6 illustrates the Curricular NCP process. Table 4 shows examples of Dissimilar NCP and Similar NCP from the SliceCast-Podcast dataset. All the examples are labeled 0 in their respective tasks. \n\nWhile the Auxiliary Model can be any classification model trained on the segmentation task dataset, we use a model that is additionally pretrained on Advanced NCP data. The Auxiliary model is tested on Advanced NCP samples. While these samples were used during the pretraining of the Auxiliary model, after finetuning on the segmentation task model might not predict the same labels it observed during the pretraining. In our experiments with the SliceCast-Podcast dataset (described in Section 5.1) we indeed observe that 64.4% samples are miss-classified (hence, \"dissimilar\") and 35.6% are correctly classified (hence, \"similar\").",
            "score": 0.4614695825471899,
            "section_title": "Curricular Next Conversation Prediction",
            "char_start_offset": 11142,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1448
                },
                {
                    "start": 1451,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2084
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 41,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.257568359375
        },
        {
            "corpus_id": "257901096",
            "title": "Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation",
            "text": "Motivated by the success of curriculum learning (Qian et al. 2021a;Guo et al. 2020;Liu et al. 2020), we further introduce a hard-to-easy learning strategy to improve the performance. Ding et al. (2021b) show that pretraining with raw data can improve the performance of NAT by rejuvenating low-frequency words. To keep the merits of low-mode, they further trained the pretrained model on distilled data. We combine this idea with our data selection method by decreasing the ratio of raw data in the training process. Specifically, the training data for each update can be formulated as: \n\nwhere T k and D k denote the threshold and the set of tuples (X, Y, \u0176 ) for the kth update respectively. T k can be determined by a preset function or feedbacks from the NAT student. In our experiments, we adopt a linear function for T k which is computed as \n\n, where K is the total number of updates, the constants T 0 and T 1 can be determined according to the distribution of score P (score(X, Y )) given a specific NAT evaluator and the raw training data. The whole data selection process can be found in Algorithm 1. This process is an additional stage following standard training procedures for NAT, thus being generic to various data and architectures.",
            "score": 0.461388956207355,
            "section_title": "Hard-to-Easy Data Selection",
            "char_start_offset": 11928,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 586
                },
                {
                    "start": 589,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 847
                },
                {
                    "start": 850,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1249
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 67,
                    "matchedPaperCorpusId": "221150562"
                },
                {
                    "start": 67,
                    "end": 83,
                    "matchedPaperCorpusId": "220046693"
                },
                {
                    "start": 183,
                    "end": 202,
                    "matchedPaperCorpusId": "229923128"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71142578125
        },
        {
            "corpus_id": "253157803",
            "title": "Contextual Similarity is More Valuable Than Character Similarity: An Empirical Study for Chinese Spell Checking",
            "text": "The core idea of curriculum learning is to train models from easy to hard [8]. And the key to curriculum learning is to design a mechanism to measure the difficulty of samples. Benefiting from this mechanism, we naturally use the contextual similarity of characters as the metric for measuring the sample's difficulty, so as to organize the scattered training samples into ordered samples for model training. Specifically, we train the model in the order from samples with low contextual similarity to samples with high contextual similarity. Hence, the model achieves better performance than only using the traditional character similarity of confusion set. Moreover, our curriculum learning framework is model-agnostic so that it brings stable improvements for most existing CSC models. \n\nThe contributions of our work are summarized as: (1) We empirically verify that contextual similarity is more valuable than character similarity in the CSC task, which is instructive for future works. analyses to demonstrate the effectiveness of our proposed method.",
            "score": 0.46104854170800486,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2074,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 788
                },
                {
                    "start": 791,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1057
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.459228515625
        },
        {
            "corpus_id": "267094971",
            "title": "DsDm: Model-Aware Dataset Selection with Datamodels",
            "text": "When selecting data for training large-scale models, standard practice is to filter for examples that match human notions of data quality. Such filtering yields qualitatively clean datapoints that intuitively should improve model behavior. However, in practice the opposite can often happen: we find that selecting according to similarity with\"high quality\"data sources may not increase (and can even hurt) performance compared to randomly selecting data. To develop better methods for selecting data, we start by framing dataset selection as an optimization problem that we can directly solve for: given target tasks, a learning algorithm, and candidate data, select the subset that maximizes model performance. This framework thus avoids handpicked notions of data quality, and instead models explicitly how the learning process uses train datapoints to predict on the target tasks. Our resulting method greatly improves language model (LM) performance on both pre-specified tasks and previously unseen tasks. Specifically, choosing target tasks representative of standard LM problems and evaluating on diverse held-out benchmarks, our selected datasets provide a 2x compute multiplier over baseline methods.",
            "score": 0.4602536121049412,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9541015625
        },
        {
            "corpus_id": "53434984",
            "title": "Exploiting Class Learnability in Noisy Data",
            "text": "Previous methods that oversee the data selection process for training classifiers have had a variety of goals. Many models are interested in minimizing time to convergence. (Graves et al. 2017) present a curriculum learner that generates an ordering of training tasks based on task difficulty, the intuition being that easy examples help models earlier in training, while harder examples are more appropriate later. Our work follows this same core idea, yet the problem we aim to solve is fundamentally different. While they train a model to complete a pre-determined set of tasks, we learn to focus on tasks, i.e classes, that are easiest to learn for a model. Like us, (Ruder and Plank 2017) are interested in manipulating the selection of training examples, though their goal is to choose examples based on suitability for transfer learning. In contrast our approach is more exploratory, as the only criteria for selecting classes is the resultant performance of a given model. Finally, (Fan et al. 2017) propose to supervise model training with a deep reinforcement learning algorithm. However, rather than actively select batches of training data, they train a filter to ignore certain examples within a given mini-batch at each training step.",
            "score": 0.45969034953348575,
            "section_title": "Learning What to Learn",
            "char_start_offset": 5023,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 173,
                    "end": 192,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 990,
                    "end": 1007,
                    "matchedPaperCorpusId": "388785"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5341796875
        },
        {
            "corpus_id": "155089817",
            "title": "Curriculum Learning for Domain Adaptation in Neural Machine Translation",
            "text": "We identify two general types of curriculum learning strategy. The deterministic curriculum (c.f. Kocmi and Bojar ( 2017)) trains on a fixed order of samples based on their scores (e.g. \"easyto-hard\" or \"more similar to less\"). While simple to motivate, this may not always perform well because neural methods benefit from randomization in the minibatches and multiple epochs. In contrast, the probabilistic curriculum (Bengio et al., 2009) works by dividing the training procedure into distinct phases. Each phase creates a random sample from the entire pool of data, but earlier phases sample the \"easier\" or \"more similar\" sentence with higher probability.. Since each phase can be viewed as creating a new training dataset, all the well-tested tricks of the trade for neural network optimization can be employed. \n\nIn this paper, we use the same probabilistic curriculum strategy and code base1 as Zhang et al. (2018). The main difference here is the application to domain adaptation. The proposed strategy is summarized as follows: \n\n\u2022 Sentences are first ranked by similarity scores and then distributed evenly into shards, such that each shard contains samples with similar similarity criteria values.",
            "score": 0.45904528267411154,
            "section_title": "Curriculum Learning Training Strategy",
            "char_start_offset": 4932,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1036
                },
                {
                    "start": 1039,
                    "end": 1208
                }
            ],
            "ref_mentions": [
                {
                    "start": 419,
                    "end": 440,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22802734375
        },
        {
            "corpus_id": "252280753",
            "title": "Cold-Start Data Selection for Better Few-shot Language Model Fine-tuning: A Prompt-based Uncertainty Propagation Approach",
            "text": "We study cold-start data selection for text classification with c classes formulated as follows: Given a pool of unlabeled samples D u = {x j } U j=1 and an empty training set D l = \u2205, we aim to fine-tune a pre-trained language model M denoted as f (\u2022; \u03b8) under limited labeling budget |B| interactively: In each round, we use an acquisition function F(\u2022) to query b samples denoted as Q from D u . Next, the acquired samples are labeled and moved from D u to D l . Then we fine-tune the pre-trained language model f (\u2022; \u03b8) with D l to maximize the performance on downstream classification tasks. The above steps can either be one-round (Chang et al., 2021;Hacohen et al., 2022) (b = |B| in this case) or repeated for multiple rounds (Yuan et al., 2020) (b = |B|/|Rounds|) until reaching the budget |B|.",
            "score": 0.4557219806117095,
            "section_title": "Problem Formulation",
            "char_start_offset": 8461,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 803
                }
            ],
            "ref_mentions": [
                {
                    "start": 637,
                    "end": 657,
                    "matchedPaperCorpusId": "235755110"
                },
                {
                    "start": 657,
                    "end": 677,
                    "matchedPaperCorpusId": "246634642"
                },
                {
                    "start": 734,
                    "end": 753,
                    "matchedPaperCorpusId": "224724415"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8447265625
        },
        {
            "corpus_id": "237553761",
            "title": "On the Role of Corpus Ordering in Language Modeling",
            "text": "Language models pretrained on vast corpora of unstructured text using self-supervised learning framework are used in numerous natural language understanding and generation tasks. Many studies show that language acquisition in humans follows a rather structured simple-to-complex pattern and guided by this intuition, curriculum learning, which enables training of computational models in a meaningful order, such as processing easy samples before hard ones, has been shown to potentially reduce training time. The question remains whether curriculum learning can benefit pretraining of language models. In this work, we perform comprehensive experiments involving multiple curricula strategies varying the criteria for complexity and the training schedules. Empirical results of training transformer language models on English corpus and evaluating it intrinsically as well as after fine-tuning across eight tasks from the GLUE benchmark, show consistent improvement gains over conventional vanilla training. Interestingly, in our experiments, when evaluated on one epoch, the best model following a document-level hard-to-easy curriculum, outperforms the vanilla model by 1.7 points (average GLUE score) and it takes the vanilla model twice as many training steps to reach comparable performance.",
            "score": 0.4549382824945951,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79052734375
        },
        {
            "corpus_id": "273502726",
            "title": "Scalable Data Ablation Approximations for Language Models through Modular Training and Merging",
            "text": "Our method's efficiency advantage is greater and more straightforward to calculate for the experiments in \u00a75.1, since we sample partitions directly from \"base components\" P \u2032 \u2208 C.  (Dodge et al., 2021;Gao et al., 2020;Computer, 2023;Penedo et al., 2023;Soldaini et al., 2024). The individual components of these mixtures and their relative sizes are typically chosen based on some intrinsic measure of data quality as it is often prohibitively expensive to perform thorough data ablations to create mixtures optimizing for downstream performance. \n\nEfficient data selection Given that data ablations on large language models is expensive, one class of approaches relies on approximating them on smaller models. Relevant work studies scaling laws for model parameters vs training tokens (Hoffmann et al., 2022;Biderman et al., 2023), empirical effects of including or excluding different sources of data (Longpre et al., 2023), and the effects of training over multiple epochs vs new training tokens (Muennighoff et al., 2023). Previous work has also explored improving domain-specific fit via continued pre-training (Gururangan et al., 2020), predicting domain fit using lexical features (Reid et al., 2022), or improving general test-time adaptation via dynamic data selection, either by distributionally robust optimization with a small proxy model (Oren et al., 2019;Xie et al., 2023) or online using a multi-armed bandit approach (Albalak et al., 2023). Additional previous works aim to adapt to known downstream tasks via data selection, including at the individual example level (Wang et al., 2020) or even by explicitly fine-tuning models on many tasks (Aghajanyan et al., 2021). Notable recent and concurrent work proposes scalable influence functions, traditionally used at only very small scales, as a method for selecting better training data (Choe et al., 2024;Yu et al., 2024). Another concurrent preliminary work (Thrush et al., 2024) proposes a training-free method of selecting data to improve performance on downstream tasks.",
            "score": 0.45392564772692223,
            "section_title": "Efficiency analysis",
            "char_start_offset": 26315,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 2042
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 201,
                    "matchedPaperCorpusId": "237568724"
                },
                {
                    "start": 233,
                    "end": 253,
                    "matchedPaperCorpusId": "268096300"
                },
                {
                    "start": 809,
                    "end": 831,
                    "matchedPaperCorpusId": "257921893"
                },
                {
                    "start": 999,
                    "end": 1025,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 1116,
                    "end": 1141,
                    "matchedPaperCorpusId": "216080466"
                },
                {
                    "start": 1188,
                    "end": 1207,
                    "matchedPaperCorpusId": "252907924"
                },
                {
                    "start": 1351,
                    "end": 1370,
                    "matchedPaperCorpusId": "202541009"
                },
                {
                    "start": 1434,
                    "end": 1456,
                    "matchedPaperCorpusId": "265658930"
                },
                {
                    "start": 1660,
                    "end": 1685,
                    "matchedPaperCorpusId": "231718729"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87158203125
        },
        {
            "corpus_id": "237553761",
            "title": "On the Role of Corpus Ordering in Language Modeling",
            "text": "We train from scratch and fine-tune the following models:\n\n\u2022 Training can go from easy to hard (EASY) or hard to easy (HARD).\n\n\u2022 The criteria for determining complexity of text is specified at (a) sentence-level using length (LENGTH), or (b) document-level derived from FRE scores (FRE) or TTR scores (TTR).\n\n\u2022 For BINNED, the training set consists of any one bin during a phase, whereas for STEPPED learning, the bins are cumulatively added to the training set at each phase.\n\n\u2022 Finally, the standard way of training where the original corpus is used as is, without any ordered curriculum, serves as our baseline denoted as the VANILLA model.",
            "score": 0.45345522207340644,
            "section_title": "Model Combinations",
            "char_start_offset": 16241,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.328369140625
        },
        {
            "corpus_id": "235097217",
            "title": "Training Language Models under Resource Constraints for Adversarial Advertisement Detection",
            "text": "We ablate the effects of curriculum learning based on increasing difficulty using models trained in two control conditions. (a) Anti-curriculum learning (B_T L ACL ) using scoring function f = \u2212f where harder samples are fed first and (b) random curriculum (B_T L RCL ) where scoring function randomly scores the training samples. As seen from the Table 3 anti-curriculum and random curriculum are not as effective as the curriculum of increasing hardness. Further, random scoring function results in significant degradation of performance when compared to approaches employing a curriculum. Similar trends are observed for respective models trained in FR as well. \n\nWe further conduct ablations to rule out any other factors contributing to the gain in recall from curriculum based on lexical similarity. We perform two other experiments where we train the model in similar manner but feed the languages in reverse lexical similarity order(B_M L REV LEX ) and random order (B_M L RAN D LEX ). However, in both the experiments we feed the target language at the end to minimise domain shift. We see that the model trained in the lexical similarity order beats the performance of the other two models in Table 3. We validate statistical significance of gains from both lexical and hardness curricula using the Mc-Nemar's Test (Dietterich, 1998;McNemar, 1947) (Raschka, 2018). The gains through both curriculum are statistically significant as p-value is < 0.05 for both DE and FR.",
            "score": 0.4529419399774848,
            "section_title": "Ablations",
            "char_start_offset": 19528,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 664
                },
                {
                    "start": 667,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1479
                }
            ],
            "ref_mentions": [
                {
                    "start": 1343,
                    "end": 1356,
                    "matchedPaperCorpusId": "46226024"
                },
                {
                    "start": 1358,
                    "end": 1373,
                    "matchedPaperCorpusId": "53237711"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29150390625
        },
        {
            "corpus_id": "271745069",
            "title": "Prioritize Alignment in Dataset Distillation",
            "text": "In section 2.1, we show using data selection to filter out unmatched samples could alleviate the misalignment caused in Information Extraction step. According to previous work [11], TM-based methods prefer easy information and choose to match only early trajectories when IPC is small. Conversely, hard information is preferred by high IPCs and they match only late trajectories. Hence, we should use easy samples to train early trajectories, while late trajectories should be trained with hard samples. To realize this efficiently, we first use the data selection method to measure the difficulty of samples contained in the target dataset. Then, during training expert trajectories, a scheduler is implemented to gradually incorporate hard samples into the training set while excluding easier ones. \n\nDifficulty Scoring Function Identifying the difficulty of data for DNNs to learn has been well studied in data selection area [17,18,31,42]. For simplicity consideration, we use Error L2-Norm (EL2N) score [35] as the metric to evaluate the difficulty of training examples (other metrics can also be chosen, see Section 4.3.2). Specifically, let x and y denote a data point and its label, respectively. \n\nThen, the EL2N score can be calculated by: \n\n(3) where p(w t , x) = \u03c3(f (w t , x)) is the output of a model f at training step t transformed into a probability distribution. In consistent with [42], samples with higher EL2N scores are considered as harder samples in this paper. \n\nScheduler The scheduler can be divided into the following stages. Firstly, the hardest samples are removed from the training set, ensuring that it exclusively comprises data meeting a predetermined initial ratio (IR). Then, during training expert trajectories, samples are gradually added to the training set in order of increasing difficulty. After incorporating all the data into the training set, the scheduler will begin to remove easy samples from the target dataset. Unlike the gradual progression involved in adding data, the action of reducing data is completed in a single operation, since now the model has been trained on simple samples for a sufficient time. ( Our method consistently outperforms prior matching-based methods.",
            "score": 0.4511287701521892,
            "section_title": "Filtering Information Extraction",
            "char_start_offset": 7561,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 800
                },
                {
                    "start": 803,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1204
                },
                {
                    "start": 1207,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1485
                },
                {
                    "start": 1488,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2158
                },
                {
                    "start": 2159,
                    "end": 2226
                }
            ],
            "ref_mentions": [
                {
                    "start": 929,
                    "end": 933,
                    "matchedPaperCorpusId": "235421747"
                },
                {
                    "start": 933,
                    "end": 936,
                    "matchedPaperCorpusId": "229339854"
                },
                {
                    "start": 936,
                    "end": 939,
                    "matchedPaperCorpusId": "211259075"
                },
                {
                    "start": 939,
                    "end": 942,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 1008,
                    "end": 1012,
                    "matchedPaperCorpusId": "235898952"
                },
                {
                    "start": 1400,
                    "end": 1404,
                    "matchedPaperCorpusId": "250113273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6591796875
        },
        {
            "corpus_id": "254043443",
            "title": "Impact of Strategic Sampling and Supervision Policies on Semi-Supervised Learning",
            "text": "During this period (prior to the use of labels), the model is trained under unsupervised loss only. figures2 (e) and (f) are examples of this policy where e 0 > 0 and e f = e 0 for (f). Curriculum supervision policy. As indicated earlier, we also explore a curriculum-based strategy as our supervision policy. Curriculum learning is often compared to human learning behaviour, where we start learning with an easy task and slowly learn more complex tasks. However, in machine learning settings, all the available data are generally used at the same time to train the model. Curriculum learning was popularized by [35] in the context of machine learning. Here, pre-defined criteria are used to select easy samples at the beginning of the training and slowly introduce harder samples. Similar to the incremental supervision policy, the curriculum supervision policy also increases the number of labelled samples gradually. However, this policy uses a particular criterion to identify the order in which the labelled data is injected. Here, we utilize the pre-trained encoder to rank the samples. We define an 'easy' sample as having the least entropy in the final prediction of the pre-trained model, while the hard samples are defined as those with higher entropy. All the policies depicted in Figure 2 are also explored with the curriculum policy.",
            "score": 0.450952530000151,
            "section_title": "B. Problem Formulation",
            "char_start_offset": 23795,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1347
                }
            ],
            "ref_mentions": [
                {
                    "start": 613,
                    "end": 617,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2578125
        },
        {
            "corpus_id": "266977266",
            "title": "The Unreasonable Effectiveness of Easy Training Data for Hard Tasks",
            "text": "How can we train models to perform well on hard test data when hard training data is by definition difficult to label correctly? This question has been termed the scalable oversight problem and has drawn increasing attention as language models have continually improved. In this paper, we present the surprising conclusion that current pretrained language models often generalize relatively well from easy to hard data, even performing as well as oracle models finetuned on hard data. We demonstrate this kind of easy-to-hard generalization using simple finetuning methods like in-context learning, linear classifier heads, and QLoRA for seven different measures of datapoint hardness, including six empirically diverse human hardness measures (like grade level) and one model-based measure (loss-based). Furthermore, we show that even if one cares most about model performance on hard data, it can be better to collect easy data rather than hard data for finetuning, since hard data is generally noisier and costlier to collect. Our experiments use open models up to 70b in size and four publicly available question-answering datasets with questions ranging in difficulty from 3rd grade science questions to college level STEM questions and general-knowledge trivia. We conclude that easy-to-hard generalization in LMs is surprisingly strong for the tasks studied. Our code is available at: https://github.com/allenai/easy-to-hard-generalization",
            "score": 0.4501144038959938,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66162109375
        },
        {
            "corpus_id": "251719464",
            "title": "DiscrimLoss: A Universal Loss for Hard Samples and Incorrect Samples Discrimination",
            "text": "Given data with label noise (i.e., incorrect data), deep neural networks would gradually memorize the label noise and impair model performance. To relieve this issue, curriculum learning is proposed to improve model performance and generalization by ordering training samples in a meaningful (e.g., easy to hard) sequence. Previous work takes incorrect samples as generic hard ones without discriminating between hard samples (i.e., hard samples in correct data) and incorrect samples. Indeed, a model should learn from hard samples to promote generalization rather than overfit to incorrect ones. In this article, we address this problem by appending a novel loss function DiscrimLoss, on top of the existing task loss. Its main effect is to automatically and stably estimate the importance of easy samples and difficult samples (including hard and incorrect samples) at the early stages of training to improve the model performance. Then, during the following stages, DiscrimLoss is dedicated to discriminating between hard and incorrect samples to improve the model generalization. Such a training strategy can be formulated dynamically in a self-supervised manner, effectively mimicking the main principle of curriculum learning. Experiments on image classification, image regression, text sequence regression, and event relation reasoning demonstrate the versatility and effectiveness of our method, particularly in the presence of diversified noise levels.",
            "score": 0.44972844253849287,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.348876953125
        },
        {
            "corpus_id": "221660842",
            "title": "Curriculum Learning with Diversity for Supervised Computer Vision Tasks",
            "text": "Training artificial intelligence using curriculum approaches, from easy to hard, can lead to improved results in a wide range of tasks [1,6,7,8,9,15,16,18,19,21,22,27,31,35]. Still, it is not simple to determine which samples are easy or hard, and the available metrics are usually task-dependent. Another challenge of CL is finding the right curriculum schedule, i.e. how fast to add more difficult examples to training, and how to introduce the right amount of harder samples at the right time to positively influence convergence. In this section, we present our approach for estimating difficulty and our curriculum sampling strategies.",
            "score": 0.44921919931894283,
            "section_title": "Methodology",
            "char_start_offset": 11629,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 639
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 138,
                    "matchedPaperCorpusId": "11590585"
                },
                {
                    "start": 138,
                    "end": 140,
                    "matchedPaperCorpusId": "10364203"
                },
                {
                    "start": 140,
                    "end": 142,
                    "matchedPaperCorpusId": "8105909"
                },
                {
                    "start": 144,
                    "end": 146,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 146,
                    "end": 149,
                    "matchedPaperCorpusId": "51876228"
                },
                {
                    "start": 149,
                    "end": 152,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 152,
                    "end": 155,
                    "matchedPaperCorpusId": "11423732"
                },
                {
                    "start": 155,
                    "end": 158,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 158,
                    "end": 161,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 161,
                    "end": 164,
                    "matchedPaperCorpusId": "19805513"
                },
                {
                    "start": 164,
                    "end": 167,
                    "matchedPaperCorpusId": "6954583"
                },
                {
                    "start": 167,
                    "end": 170,
                    "matchedPaperCorpusId": "397556"
                },
                {
                    "start": 170,
                    "end": 173,
                    "matchedPaperCorpusId": "5004002"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.087890625
        },
        {
            "corpus_id": "265506572",
            "title": "Can training neural language models on a curriculum with developmentally plausible data improve alignment with human reading behavior?",
            "text": "Curriculum learning (Bengio et al., 2009) refers to training models through a difficulty-based ordering of training examples (i.e. a curriculum), most often \"starting small\" (Elman, 1993) from easy examples before progressing to increasingly difficult sentences. In NLP, curriculum learning has been widely used for Machine Translation (e.g., Platanios et al., 2019), but has also been applied more recently to other natural language understanding tasks (Xu et al., 2020). For a survey see Soviany et al. (2022); Wang et al. (2021). \n\nThere are two steps involved in designing a curriculum: assigning a difficulty score to every training example (\"difficulty measurer\") and using these difficulty scores to determine the order in which training examples are presented to the model (\"training scheduler\") (Wang et al., 2021).",
            "score": 0.44855477550976,
            "section_title": "Background",
            "char_start_offset": 3761,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 532
                },
                {
                    "start": 535,
                    "end": 824
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 41,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 454,
                    "end": 471,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 490,
                    "end": 511,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 513,
                    "end": 531,
                    "matchedPaperCorpusId": "232362223"
                },
                {
                    "start": 804,
                    "end": 823,
                    "matchedPaperCorpusId": "232362223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69384765625
        },
        {
            "corpus_id": "32493672",
            "title": "An Extensive Evaluation of Filtering Misclassified Instances in Supervised Classification Tasks",
            "text": "In this paper we presented a method for ordering the instances in a data set by complexity (hardness ordering). A hardness ordering uses instance hardness to order the instances in a data set based on the their likelihood of being misclassified. The hardness ordering allows a learning algorithm to focus on the most informative instances. \n\nWe integrated the hardness ordering for a data set into the learning process in curriculum learning. One of the main shortcomings of curriculum learning is the lack of a method for developing curricula. As curriculum learning is a relatively new approach, we examined using a hardness ordering as a general approach to implement curriculum learning. We examined curriculum learning on a set of 52 UCI data sets using MLPs and DTs and compared curriculum learning with filtering and boosting. \n\nOur exploration with curriculum learning has shed interesting, and unexpected, light that curriculum learning performs strikingly similar to filtering and boosting in MLPs. The similarity of curriculum learning in MLPs to filtering is somewhat expected. Elman (1993) pointed out that one of the reasons starting small is so important is due to backpropagation's inflexibility of learning late in the learning process. As training progresses, weights become rigid and only very small changes are made during training. As complex instances are not trained on in curriculum learning until late in the learning process, they will only have a very minor (if any) affect on the trained model, especially with a large number of training epochs between adding more instances. Thus, the harder instances are not expected to have a large impact on the final model. \n\nFor the DTs, the hope was that by starting with easier instances, more appropriate attribute splits would be chosen early in the learning process. As more difficult (and possibly noisy) instances are added later they would have less of an effect on the tree. DTs may not be an ideal candidate for curriculum learning because they are somewhat robust to noise due to using entropy to choose which attribute to split on and because pruning helps avoid overfitting the data. \n\nThe claim that curriculum learning is well suited for non-convex problems may or may not be true. One difficult aspect of this claim is determining the convexity of a dataset. Curriculum learning has an intuitive motivation and has proven to work well in specific past applications.",
            "score": 0.4472407816061653,
            "section_title": "Conclusions",
            "char_start_offset": 31955,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 339
                },
                {
                    "start": 342,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 833
                },
                {
                    "start": 836,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1690
                },
                {
                    "start": 1693,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2164
                },
                {
                    "start": 2167,
                    "end": 2264
                },
                {
                    "start": 2265,
                    "end": 2342
                },
                {
                    "start": 2343,
                    "end": 2449
                }
            ],
            "ref_mentions": [
                {
                    "start": 1090,
                    "end": 1102,
                    "matchedPaperCorpusId": "2105042"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11920166015625
        },
        {
            "corpus_id": "266166181",
            "title": "Byte-ranked Curriculum Learning for BabyLM Strict-small Shared Task 2023",
            "text": "Large language models (LLMs) have received much attention from researchers and the general public in recent years (Devlin et al., 2018;Liu et al., 2019;Brown et al., 2020;Chowdhery et al., 2022;Hoffmann et al., 2022). One distinguishing aspect of these recent models is an explosion in the size of the models and a corresponding massive increase in training data to train these large models. In particular, the Chinchilla (Hoffmann et al., 2022) work suggests that model size and training tokens should be scaled at the same rate. To demonstrate the importance of the amount of training data used to train a model, Chinchilla was trained with 1.4 trillion training tokens, nearly five times the size of the training data for other LLMs at the time. \n\nThe result was an improvement on a number of downstream tasks. \n\nWhile large models perform very well on a large variety of tasks, they also come with many drawbacks. These models require large amounts of computing resources beyond what is available to many researchers. Additionally, the amount of data used to train these models is not currently available in the majority of the world's languages. In an effort to investigate language modeling abilities and training strategies in data-limited situations, the BabyLM challenge restricts the amount of data available to models (Warstadt et al., 2023). \n\nOne approach to improve training speed and improve downstream performance is by providing training data in a specific order. In particular, gradually increasing the difficulty of the training samples provided to the model is known as curriculum learning (Elman, 1993). Human children learning language follow a similar exposure to language. Speech directed at babies is far simpler than speech directed at adults and written language data follows the same trend. The motivation behind curriculum learning is to treat a neural network in a similar manner and allow it to learn from easier training samples before being presented with more difficult training samples. \n\nThe approach taken in this current work is to apply curriculum learning in a data restricted setting, without incorporating outside knowledge or data, to see its impact on training. The preprocessing steps are kept the same across models presented to reduce their effect on the ability to compare across training runs. A byte-level byte-pairencoding tokenization is used across all models presented.",
            "score": 0.4471999628241521,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 748
                },
                {
                    "start": 751,
                    "end": 813
                },
                {
                    "start": 816,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1353
                },
                {
                    "start": 1356,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 2021
                },
                {
                    "start": 2024,
                    "end": 2205
                },
                {
                    "start": 2206,
                    "end": 2342
                },
                {
                    "start": 2343,
                    "end": 2423
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 171,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1329,
                    "end": 1352,
                    "matchedPaperCorpusId": "266182457"
                },
                {
                    "start": 1610,
                    "end": 1623,
                    "matchedPaperCorpusId": "2105042"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53369140625
        },
        {
            "corpus_id": "265466325",
            "title": "Surf-D: Generating High-Quality Surfaces of Arbitrary Topologies Using Diffusion Models",
            "text": "To enhance the learning performance on diverse shapes with arbitrary topologies, we implement curriculum learning instead of training directly on the entire shape dataset holistically. Curriculum learning involves training machine learning models in a meaningful order, from easy to challenging samples, known as hard sample mining. This approach can enhance performance without incurring additional computational costs [2,70]. \n\nCurriculum training involves measuring performance and establishing an easy-to-hard scheduler. In this paper, we use a simple yet effective performance metric, which is the training loss L latent . We select samples with lower losses as incoming training data in each iteration. This continues until all data is included. Specifically, we first randomly sample k shapes from the dataset, then gradually add new difficult samples according to their loss values, resulting in an easyto-hard gradual learning process. The pseudo-code can be expressed as Algo. 1. The influence of the proposed learning strategy is included in the Appendix. M \u2190 train(M, E * , C) 7: end for",
            "score": 0.4465234888647307,
            "section_title": "Curriculum Learning Strategy",
            "char_start_offset": 11249,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 427
                },
                {
                    "start": 430,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1099
                }
            ],
            "ref_mentions": [
                {
                    "start": 420,
                    "end": 423,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 423,
                    "end": 426,
                    "matchedPaperCorpusId": "231709290"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.051361083984375
        },
        {
            "corpus_id": "26468344",
            "title": "Curriculum Learning and Minibatch Bucketing in Neural Machine Translation",
            "text": "It should be noted that all results presented so far are observed after one epoch of curriculum training. It is questionable what would be the best way of subsequent training. \n\nWe considered two options, see As the corresponding curve in Figure 3 however shows, the model is probably already quite fixed in the current optimum and we do not see any further improvement on the test set. Their proposed method orders buckets in an alternating way: first in increasing order by length, then decreasing order, then again increasing order etc. This way the buckets of different length are periodically revisited. With this approach, the authors got a speedup in the training time and also obtained better performance results. Bengio et al. (2009) use curriculum learning for a neural language model, not a full NMT system. They trained the network by iteratively increasing the vocabulary size, starting with the vocabulary of 5000 and increasing by 5000 each epoch. Each epoch used only sentences with words available in the current restricted vocabulary. The last epoch thus used all examples. This curriculum lead to a statistically significant improvement in the performance of the model. Graves et al. (2017) automatically select examples during multitask learning. The method evaluates training signals from the neural network and uses them to focus on specific subtasks to accelerate the training process of the main task. The authors noted that uniformly sampling from the training data is a strong baseline.",
            "score": 0.44622509366571794,
            "section_title": "Continuing the Curriculum",
            "char_start_offset": 16826,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 175
                },
                {
                    "start": 178,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1512
                }
            ],
            "ref_mentions": [
                {
                    "start": 722,
                    "end": 742,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58837890625
        },
        {
            "corpus_id": "271874686",
            "title": "Evaluating Fine-Tuning Efficiency of Human-Inspired Learning Strategies in Medical Question Answering",
            "text": "Training large language models (LLMs) is costly, both in terms of data collection efforts [28,38] and computational resources [15,16]. To mitigate these costs, recent research has focused on dataefficient training methods that optimise data ordering [33]. One promising approach is human-inspired learning strategies, which organise data based on human learning principles [40,37]. Among these strategies, curriculum learning, where samples are arranged from easiest to hardest, is particularly popular [13,37]. This method has proven effective in language tasks such as knowledge acquisition [21], natural language reasoning [24], and information retrieval [32]. \n\nWhile curriculum learning has shown promise, the broader potential of human-inspired data ordering for fine-tuning remains unclear. Specifically, previous studies have focused on (i) a narrower set of models [24,37], with fewer comparisons across multiple models; (ii) limited evaluation in domain-specific applications, especially in high-stakes areas like medical question answering; and (iii) primarily using one set of data labels, with less emphasis on comparing human-versus machine-defined labels to reflect different perspectives on data ordering. To address these gaps, we conduct a comprehensive evaluation of human-inspired learning strategies for fine-tuning in medical question answering, testing models of various sizes using both human-and LLM-defined question difficulty labels. We focus on the medical question-answering domain because of the scarcity of high-quality medical questions needed to train effective medical LLMs, highlighting the importance of efficient fine-tuning strategies [22,25]. Our findings offer empirical insights into the efficiency and generalisability of these strategies for fine-tuning LLMs. Specifically, we find that: \u2022 Human-inspired learning yields moderate improvements over random shuffling. These strategies result in the best accuracy gain of 1.81% and an average gain of 1.02% across datasets, with interleaved strategies providing the best average results. \n\n\u2022 Human-inspired learning lacks generalisation across model-data combinations. The best strategy varies across model-dataset combinations, suggesting caution when generalising the effects of any one strategy to other models based on single-model results. \n\n\u2022 LLM-defined difficulty outperforms human labels in curriculum-based learning.",
            "score": 0.44616478710399965,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2077
                },
                {
                    "start": 2080,
                    "end": 2158
                },
                {
                    "start": 2159,
                    "end": 2334
                },
                {
                    "start": 2337,
                    "end": 2416
                }
            ],
            "ref_mentions": [
                {
                    "start": 377,
                    "end": 380,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 507,
                    "end": 510,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 626,
                    "end": 630,
                    "matchedPaperCorpusId": "250391006"
                },
                {
                    "start": 874,
                    "end": 878,
                    "matchedPaperCorpusId": "250391006"
                },
                {
                    "start": 878,
                    "end": 881,
                    "matchedPaperCorpusId": "220045816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44970703125
        },
        {
            "corpus_id": "211102338",
            "title": "Reinforced Curriculum Learning on Pre-trained Neural Machine Translation Models",
            "text": "al. 2017) and further to Transformer models (Vaswani et al. 2017).\n\nHowever, since obtaining accurately labeled training samples in machine translation is often time-consuming and requires expert knowledge, an important question in NMT is how to best utilize a limited number of available training samples, perhaps with different lengths, qualities, and noise levels. Recently, the application of curriculum learning is also studied for NMT. (Platanios et al. 2019) propose to feed data to an NMT model in an easy-to-difficult order and characterize the \"difficulty\" of a training example by the sentence length and the rarity of words that appear in it. Other than using the straightforward difficulty or complexity as a criterion for curriculum design,  propose a method to calculate the noise level of a training example Figure 1: Illustration of the curriculum learning process. The RL policy \u03bc is used to select samples D S from the training set D T . The selected data D S is used to update a pre-trained NMT model p \u03b8 (y|x).\n\nwith the help of an additional trusted clean dataset and train an NMT model in a noise-annealing curriculum.\n\nA limitation of the existing curriculum learning methods for NMT is that they only address the batch selection issue in a \"learn-from-scratch\" scenario. Unfortunately, training an NMT model is a time-consuming task and sometimes could take up to several weeks (van der Wees, Bisazza, and Monz 2017), depending on the amount of data available. In most practical and commercial cases, a pre-trained model often already exists, while re-training it from scratch with a new ordering of batches is a waste of time and resources. In this paper, we study curriculum learning for NMT from a new perspective, that is given a pre-trained model and the dataset used to train it, to re-select a subset of useful samples from the existing dataset to further improve the model. Unlike the easy-to-difficult insights in traditional curriculum learning (Bengio et al. 2009), (Platanios et al. 2019), our idea is analogous to classroom training where a student first attends classes to learn general subjects with equal weights and then carefully reviews a subset of selected subjects to strengthen his/her weak aspects or to el",
            "score": 0.44555045016229944,
            "section_title": "Introduction",
            "char_start_offset": 1879,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 44,
                    "end": 65,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 442,
                    "end": 465,
                    "matchedPaperCorpusId": "85498775"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6337890625
        },
        {
            "corpus_id": "231709290",
            "title": "Curriculum Learning: A Survey",
            "text": "the curriculum scheduler and the performance measure. The scheduler is responsible for deciding when to update the curriculum in order to use the pace that gives the highest overall performance. Depending on the applied methodology, the scheduler may consider a linear pace or a logarithmic pace. Additionally, in self-paced learning, the scheduler can take into consideration the current performance level to find the right pace. When applying CL over data (see Fig. 1a), a difficulty criterion is employed in order to rank the examples from easy to hard. Next, a selection method determines which examples should be used for training at the current time. Curriculum over tasks works in a very similar way. In Fig. 1b, we observe that CL at the model level does not require a difficulty criterion. Instead, it requires the existence of a model capacity curriculum. This sets how to change the architecture or the parameters of the model to which all the training data is fed.\n\nOn another note, we remark that continuation methods can be seen as curriculum learning performed over the performance measure P (Pathak and Paffenroth 2019). However, this connection is not typically mentioned in literature. Moreover, continuation methods (Allgower and Georg 2003;Richter and DeCarlo 1983;Chow et al. 1991) were studied long before curriculum learning appeared (Bengio et al. 2009). Research on continuation methods is therefore considered an independent field of study (Allgower and Georg 2003;Chow et al. 1991), not necessarily bound to its applications in machine learning (Richter and DeCarlo 1983), as would be the case for curriculum learning.\n\nWe propose a generic formulation of curriculum learning that should encompass the equivalent forms of curriculum presented above. Algorithm 1 illustrates the common steps involved in the curriculum training of a model M on a data set E. It requires the existence of a curriculum criterion C, i.e., a methodology of how to determine the ordering, and a level l at which to apply the curriculum, e.g., data level, model level, or performance measure level. The traditional curriculum learning approach enforces an easy-to-hard re-ranking of the examples, with the criterion, or the difficulty metric, being task-dependent, such as the use of shape complexity (a) General framework for data-level curriculum learning.\n\n(b) General framework",
            "score": 0.4453404574465746,
            "section_title": "Curriculum Learning",
            "char_start_offset": 10795,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1107,
                    "end": 1135,
                    "matchedPaperCorpusId": "211227830"
                },
                {
                    "start": 1260,
                    "end": 1285,
                    "matchedPaperCorpusId": "27466684"
                },
                {
                    "start": 1285,
                    "end": 1302,
                    "matchedPaperCorpusId": "123092335"
                },
                {
                    "start": 1357,
                    "end": 1376,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1491,
                    "end": 1508,
                    "matchedPaperCorpusId": "123092335"
                },
                {
                    "start": 1572,
                    "end": 1598,
                    "matchedPaperCorpusId": "27466684"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09893798828125
        },
        {
            "corpus_id": "267095066",
            "title": "Orion-14B: Open-source Multilingual Large Language Models",
            "text": "Training large language models requires hundreds of billions to trillions of tokens. On the other side, while numerous theoretical and empirical studies have examined the interplay between model size and training data volume, there is no universally accepted methodology for scheduling training data. Considering that humans acquire knowledge in a deliberate order (Evanson et al., 2023), it is plausible that language models might also benefit from a structured learning order when processing training data. Curriculum learning (Bengio et al., 2009) has been suggested as a method to organize the learning process by progressively increasing the complexity of the training data. However, most prior studies have concentrated on sample-level data and smaller datasets. Chen et al. ( 2023) employed a skills-based framework for training data selection and continuous pretraining with a 3B-parameter language model. This approach achieved greater accuracy compared to the baseline method of uniform data source sampling, suggesting the potential efficacy of strategic data scheduling. \n\nIn training the Orion-14B model, we intentionally develop a data scheduling strategy that organizes training data to incrementally increase its complexity. We divide the training data into several stages based on the data sources and their complexity. These stages are differentiated by the mix ratios of data sources. Initial stages primarily include data with common knowledge, such as web pages and news articles. In the subsequent stages, we gradually augment the proportion of data containing more complex knowledge, including textbooks, academic papers, and source code. Additionally, the linguistic diversity of the training data is expanded progressively from English and Chinese to Japanese and Korean. The brief structure of our training data schedule is depicted in Table 3. \n\nTo assess the effectiveness of the data scheduling approach, we monitor the loss on a validation set throughout the training process. This validation set consists of 5,000 documents, each unseen in the   To our knowledge, the training of most prior LLMs utilized fully shuffling the training data, which was then fed into the model in a random sequence. Orion-14B is the first LLM trained with a specific data scheduling strategy. The evaluation results indicate that this model demonstrates impressive performance in language understanding tasks at its early stages and rapidly enhances its capabilities in reasoning and academic tasks in later stages, aligning with our data scheduling policy.",
            "score": 0.4447512572062485,
            "section_title": "Data Scheduling",
            "char_start_offset": 13523,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1082
                },
                {
                    "start": 1085,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1870
                },
                {
                    "start": 1873,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2226
                },
                {
                    "start": 2227,
                    "end": 2303
                },
                {
                    "start": 2304,
                    "end": 2568
                }
            ],
            "ref_mentions": [
                {
                    "start": 529,
                    "end": 550,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83740234375
        },
        {
            "corpus_id": "244119160",
            "title": "Combining Curriculum Learning and Knowledge Distillation for Dialogue Generation",
            "text": "Inspired by the human learning process, curriculum learning (Bengio et al., 2009) is proposed as a machine learning strategy by feeding training instances to the model from easy to hard. It has been applied to many NLP tasks. To name a few, (Sachan and Xing, 2016) propose and study other heuristics that define a measure of easiness and learn the curriculum by selecting samples using this measure. More recently, (Wang et al., 2019) learns a multi-Domain curriculum for neural machine translation. Xu et al. (2020) uses curriculum Learning to distinguish easy examples from difficult ones for natural language understanding by reviewing the trainset in a crossed way. Our paper is quite different from theirs because we arrange a hierarchical curriculum based on the above two aspects (data and model) for the distillation model.",
            "score": 0.4442501532911895,
            "section_title": "Curriculum Learning in NLP",
            "char_start_offset": 6547,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 831
                }
            ],
            "ref_mentions": [
                {
                    "start": 60,
                    "end": 81,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 241,
                    "end": 264,
                    "matchedPaperCorpusId": "16503693"
                },
                {
                    "start": 415,
                    "end": 434,
                    "matchedPaperCorpusId": "174798125"
                },
                {
                    "start": 500,
                    "end": 516,
                    "matchedPaperCorpusId": "220045816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62255859375
        },
        {
            "corpus_id": "251493126",
            "title": "Comparison and Analysis of New Curriculum Criteria for End-to-End ASR",
            "text": "Commonly, neural networks that use a variant of stochastic gradient descent for their weight re-parameterization, randomly shuffle the data before training. When curriculum learning is applied, the data points are not randomly arranged but rather ordered. Let us denote our training set as X and define f : X \u2192 R as a scoring function. The consequent ordering of the examples will happen in so that i < j for data points xi and xj (i.e. the i-th element is considered easier than the j-th one) if f (xi) < f (xj). Choosing a good scoring function is the primary task CL tries to solve. f has to meaningfully encode the acquired knowledge with respect to the notion of difficulty [13]. \n\nTo test our hypothesis regarding the importance of updating the CL scores during training, we have experimented with a range of scoring functions, which are discussed below. \n\nDuration-based: A metadata-based approach where the training examples are sorted based on their duration (a shorter duration implies an easier example). This approach is the current state-of-the-art solution [14]. \n\nLoss-based: We store the loss values of each utterance and at the end of each epoch we re-order the examples based on these values. The rationale behind this solution is that high loss values mean that the model needs to change its parameters more than in the case of lower loss values. Thus we assume that higher loss corresponds to greater difficulty. Examples with the same loss values are sorted with respect to their duration. \n\nMetric-based: After calculating the loss, a decoding step can be performed and a score can be assigned to the corresponding data points based on the word error rates (WER) produced by the model under training. Using the WER directly to assess the difficulty has the advantage that CL now relies on the metric used in the final evaluation of the models. Similar to the lossbased solution, the training examples with the same scores are ordered based on a combination of the prediction confidences and the durations. We assume that high WER values imply harder examples. \n\nLoss/Metric-based with uniform Mixing: A regularized version of the previous two methods. In particular, after acquiring the ordered training set, we cut it into three parts (easy, medium and hard level examples).",
            "score": 0.4425937381295047,
            "section_title": "Scoring Function",
            "char_start_offset": 5598,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1076
                },
                {
                    "start": 1079,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1510
                },
                {
                    "start": 1513,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2081
                },
                {
                    "start": 2084,
                    "end": 2173
                },
                {
                    "start": 2174,
                    "end": 2297
                }
            ],
            "ref_mentions": [
                {
                    "start": 679,
                    "end": 683,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 1071,
                    "end": 1075,
                    "matchedPaperCorpusId": "4556070"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.410400390625
        },
        {
            "corpus_id": "259370648",
            "title": "HuCurl: Human-induced Curriculum Discovery",
            "text": "Annotation information has been extensively used by previous research in NLP to devise strategies for further data collection (Yang et al., 2019;Dligach et al., 2010), model improvement and annotation analysis (Zaidan and Eisner, 2008;Paun et al., 2018), pruning and weighting samples for better learning (Yang et al., 2019), or efficient use of monetary funds (Dligach et al., 2010). Recent studies show consistent positive correlation between difficulty of samples to the model and their level of human agreement (Nie et al., 2020a;Zaidan and Eisner, 2008;Yang et al., 2019). Building on these findings, we aim to utilize such prior knowledge about sample difficulty to develop a curriculum learning (CL) framework that is capable of discovering effective curricula for NLP tasks.\n\nA curriculum is a planned sequence of learning materials and an effective one can improve training of NLP systems (Settles and Meeder, 2016;Amiri et al., 2017;Zhang et al., 2019;Lalor and Yu, 2020;Kreutzer et al., 2021;Agrawal and Carpuat, 2022;Maharana and Bansal, 2022). CL seeks to improve model generalizability by ordering samples for training based on their latent difficulty (Bengio et al., 2009). Recent work reported efficiency and effectiveness gains through CL (Jiang et al., 2018;Castells et al., 2020;Zhou et al., 2020), especially in cases of harder tasks and limited or noisy data (Wu et al., 2021).\n\nExisting CL approaches are designed to learn a single curriculum that works best for a given model and dataset. However, effective training could be achieved in multiple ways. In addition, existing approaches quantify sample difficulty through model behavior during training. Although efficient and effective, model behavior can be affected by initialization and training dynamics (Erhan et al., 2010;Wu et al., 2021), which limits the curriculum space that can be examined for finding effective curricula.\n\nThis paper advocates a re-imagining of CL paradigms by introducing and formalizing the task of curriculum discovery, which aims to find effective curricula for",
            "score": 0.4418549187772618,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 126,
                    "end": 145,
                    "matchedPaperCorpusId": "159040584"
                },
                {
                    "start": 210,
                    "end": 235,
                    "matchedPaperCorpusId": "635609"
                },
                {
                    "start": 235,
                    "end": 253,
                    "matchedPaperCorpusId": "58535743"
                },
                {
                    "start": 305,
                    "end": 324,
                    "matchedPaperCorpusId": "159040584"
                },
                {
                    "start": 515,
                    "end": 534,
                    "matchedPaperCorpusId": "222177851"
                },
                {
                    "start": 534,
                    "end": 558,
                    "matchedPaperCorpusId": "635609"
                },
                {
                    "start": 558,
                    "end": 576,
                    "matchedPaperCorpusId": "159040584"
                },
                {
                    "start": 898,
                    "end": 924,
                    "matchedPaperCorpusId": "14155276"
                },
                {
                    "start": 943,
                    "end": 962,
                    "matchedPaperCorpusId": "155089817"
                },
                {
                    "start": 962,
                    "end": 981,
                    "matchedPaperCorpusId": "226226711"
                },
                {
                    "start": 981,
                    "end": 1003,
                    "matchedPaperCorpusId": "238856825"
                },
                {
                    "start": 1029,
                    "end": 1055,
                    "matchedPaperCorpusId": "250391006"
                },
                {
                    "start": 1256,
                    "end": 1276,
                    "matchedPaperCorpusId": "51876228"
                },
                {
                    "start": 1298,
                    "end": 1316,
                    "matchedPaperCorpusId": "227275560"
                },
                {
                    "start": 1380,
                    "end": 1397,
                    "matchedPaperCorpusId": "227343966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.499267578125
        },
        {
            "corpus_id": "270737831",
            "title": "Dynamic Data Pruning for Automatic Speech Recognition",
            "text": "Instance-wise pruning aims to remove entire audio sequences based on a given score Ht.Several methods have been proposed to calculate the score of each instance, such as the loss values [11] and uncertainty [19].The calculated score Ht is then used to determine which instances to preserve based on pruning criteria.For instance, the score distribution Ht can be used to identify and retain either easy or hard instances, where easy instances are those with lower scores and hard instances are those with higher scores.In this study, we select the loss values L of each instance z as the corresponding score, as these values can be obtained without extra cost during training and reflect the learning status of the instances.Moreover, its effectiveness has been verified in [11,21].Therefore, we explore a variety of instance-wise pruning methods tailored for ASR training:\n\nEasy.We prioritize the training of models on instances classified as \"easy\", which are identified by their lower scores, opting to exclude those with the highest scores.Figure 1 (a) illustrates an example of this method.\n\nHard.Conversely, we focus on incorporating \"hard\" instances for training, effectively sidelining those instances that are scored lower based on score distribution Ht. Figure 1 (b) provides an example for this method.\n\nEasy2hard.Inspired by curriculum learning strategies [22,23], which train their models by progressively showing harder examples, we propose a novel selection strategy that dynamically schedules the presentation of instances to the model during training.Thus, at every checkpoint, (1 \u2212 \u03f5) k points with progressively increasing difficulty are kept, and \u03f5 k points are randomly selected from the remaining dataset.Here \u03f5 is used to strategically schedule the presentation of easy or hard instances to the model.It is worth noting that \u03f5 starts at 1 and gradually linearly decreases during training, effectively altering the selection strategy over time, as shown in Figure 1 (c).",
            "score": 0.44126249947993046,
            "section_title": "Instance-wise pruning",
            "char_start_offset": 4778,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 86,
                    "end": 212
                },
                {
                    "start": 212,
                    "end": 316
                },
                {
                    "start": 316,
                    "end": 519
                },
                {
                    "start": 519,
                    "end": 725
                },
                {
                    "start": 725,
                    "end": 782
                },
                {
                    "start": 782,
                    "end": 873
                },
                {
                    "start": 875,
                    "end": 880
                },
                {
                    "start": 880,
                    "end": 1044
                },
                {
                    "start": 1044,
                    "end": 1095
                },
                {
                    "start": 1097,
                    "end": 1102
                },
                {
                    "start": 1102,
                    "end": 1313
                },
                {
                    "start": 1315,
                    "end": 1325
                },
                {
                    "start": 1325,
                    "end": 1568
                },
                {
                    "start": 1568,
                    "end": 1727
                },
                {
                    "start": 1727,
                    "end": 1824
                },
                {
                    "start": 1824,
                    "end": 1992
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 211,
                    "matchedPaperCorpusId": "259108479"
                },
                {
                    "start": 778,
                    "end": 781,
                    "matchedPaperCorpusId": "18592533"
                },
                {
                    "start": 1368,
                    "end": 1372,
                    "matchedPaperCorpusId": "227343966"
                },
                {
                    "start": 1372,
                    "end": 1375,
                    "matchedPaperCorpusId": "231709290"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64501953125
        },
        {
            "corpus_id": "208175668",
            "title": "Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine Translation",
            "text": "Humans usually learn better when the curriculums are organized from easy to hard. Inspired by that, Bengio et al. (2009) propose curriculum learning, a machine learning training strategy that feeds training instances to the model from easy to hard. Most works on curriculum learning focus on determining the order of data (Lee and Grauman 2011;Sachan and Xing 2016) or tasks (Pentina, Sharmanska, and Lampert 2015;Sarafianos et al. 2017). In our setting, we design curriculums for neither data samples nor tasks, but the training mechanisms. This way, we make the fine-tuning process smoother and ensure a soft landing from the AT models to NAT models.",
            "score": 0.44035490080814155,
            "section_title": "Curriculum Learning",
            "char_start_offset": 7395,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 120,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 322,
                    "end": 344,
                    "matchedPaperCorpusId": "15615355"
                },
                {
                    "start": 344,
                    "end": 365,
                    "matchedPaperCorpusId": "16503693"
                },
                {
                    "start": 375,
                    "end": 414,
                    "matchedPaperCorpusId": "8502955"
                },
                {
                    "start": 414,
                    "end": 437,
                    "matchedPaperCorpusId": "4725956"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.297119140625
        },
        {
            "corpus_id": "231846815",
            "title": "Does the Order of Training Samples Matter? Improving Neural Data-to-Text Generation with Curriculum Learning",
            "text": "To conclude, we show that the sample order does indeed matter when taking into account model competence during training. Further, we demonstrate that the proposed metrics are effective in speeding up model convergence. Given that curriculum learning can be combined with pretty much any neural architecture, we recommend the use of curriculum learning for data-to-text generation. We believe this work offers insights into the annotation process of data with text labels where reduced number of labels are needed.\n\n\"Foundations of Perspicuous Software Systems\". We sincerely thank the anonymous reviewers for their insightful comments that helped us to improve this paper.",
            "score": 0.44011808948332987,
            "section_title": "Conclusion",
            "char_start_offset": 11366,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29541015625
        },
        {
            "corpus_id": "269762735",
            "title": "Data Quality-based Gradient Optimization for Recurrent Neural Networks",
            "text": "The sample quality-based methods for model training can be divided into two categories: removing-based and reordering-based. \n\n(1) Removing-based methods. These methods discard the so-called low-quality samples based on a valuation method [15,16]. Mainstream approaches include data shapely [23], influence function [24], gradient-based influence function [16], and so on. These methods are associated with image classification tasks. Besides, some researchers propose to utilize sample feature to assess sample quality, such as accuracy, completeness, consistency and so on [25,26]. For natural language data, the evaluation is based on simplicity and comprehensibility. (2) Reordering-based methods are known as the curriculum learning (CL) methods. Bengio mentioned in his original work that the basic idea of such methods is to first train the model with easy samples, and then gradually increase the difficult samples until the whole training datasets [27]. The automatic curriculum learning method which measures sample quality based on training loss is popular, with self-paced learning (SPL) and teacher-student learning (TSL) approaches receiving extensive attention [28,29]. Removing-based methods directly delete samples, but it is difficult to determine how many samples should be deleted. Although reordering-based methods adjust the learning order of samples, the disadvantage of such methods is they ignore the cases where those hard-to-learn samples still affect model learning, especially in the later stages of the learning phase. Our proposed DQGO enables direct evaluation of sample quality during model training, providing advantages over removing-based and reordering-based methods.",
            "score": 0.4399467708504851,
            "section_title": "Sample quality evaluation",
            "char_start_offset": 3269,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 127,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1704
                }
            ],
            "ref_mentions": [
                {
                    "start": 239,
                    "end": 243,
                    "matchedPaperCorpusId": "102350503"
                },
                {
                    "start": 243,
                    "end": 246,
                    "matchedPaperCorpusId": "195218427"
                },
                {
                    "start": 291,
                    "end": 295,
                    "matchedPaperCorpusId": "235719065"
                },
                {
                    "start": 316,
                    "end": 320,
                    "matchedPaperCorpusId": "13193974"
                },
                {
                    "start": 356,
                    "end": 360,
                    "matchedPaperCorpusId": "195218427"
                },
                {
                    "start": 575,
                    "end": 579,
                    "matchedPaperCorpusId": "235244755"
                },
                {
                    "start": 579,
                    "end": 582,
                    "matchedPaperCorpusId": "232203938"
                },
                {
                    "start": 957,
                    "end": 961,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1176,
                    "end": 1180,
                    "matchedPaperCorpusId": "232362223"
                },
                {
                    "start": 1180,
                    "end": 1183,
                    "matchedPaperCorpusId": "10891229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46533203125
        },
        {
            "corpus_id": "264405787",
            "title": "Two-Stage Triplet Loss Training with Curriculum Augmentation for Audio-Visual Retrieval",
            "text": "Conventional neural network training sampled random minibatches from the entire training dataset. This study [4] investigates the effect of curriculum learning, which employs non-uniform mini-batch sampling. It achieves this by organizing training examples based on difficulty and generating a progression of mini-batches with increasing levels of difficulty. The work [7] offers a nuanced training strategy by incorporating curriculum learning into the loss function and dynamically adjusts the importance of easy and hard samples at different stages of training. Initially, it places emphasis on easy samples and gradually shifts towards prioritizing harder ones as training advances. The work [3] proposes a curriculum learning strategy that guides contrastive learning through a progressive, easy-to-difficult learning process. They leverage the augmented data sequence and set them in an easy-to-difficult order. Then, conduct contrastive learning via the elaborately designed curriculum. Curriculum learning is also employed in a two-step process in the study of the works [11]. Firstly, it evaluates the difficulty level of each training instance and assesses the current model's proficiency. Then, based on this assessment, it strategically selects an appropriate batch of training instances, ensuring a progressive, step-by-step exposure to the training data. \n\nAlthough the utilization of this specific loss function for curriculum learning has limited attention and reports, the selection of triplets significantly influences model performance in cross-modal retrieval tasks. This paper studies the application of curriculum learning for the incremental training of various types of triplets.",
            "score": 0.4398010899249929,
            "section_title": "B. Curriculum Learning in Cross-Modal Retrieval",
            "char_start_offset": 5701,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1368
                },
                {
                    "start": 1371,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1703
                }
            ],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 112,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 369,
                    "end": 372,
                    "matchedPaperCorpusId": "209050760"
                },
                {
                    "start": 696,
                    "end": 699,
                    "matchedPaperCorpusId": "240230775"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2568359375
        },
        {
            "corpus_id": "272694496",
            "title": "Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement",
            "text": "In this paper, we present a novel approach to selecting a subset of data and optimizing the fine-tuning of language models. Our method involved a scalable sampling technique that maximizes diversity and efficiency in subset selection. Through our proposed k-means-quality (kMQ) algorithm and iterative selection process, we demonstrated significant performance improvements over strong baselines while maintaining a limited training budget. Our contributions include an efficient instruction selection algorithm, the release of our encoded instruction dataset, and a systematic analysis of our method's effectiveness across a range of tasks. Our method outperforms existing baselines, achieving up to 7% improvement in a wide range of evaluation tasks. \n\nBy addressing the challenge of optimal instruct data selection, our work paves the way for more efficient and effective finetuning of language models, making them more accessible and affordable for deployment, especially in resource-constrained settings. We believe that our findings will contribute significantly to the ongoing research in language model optimization and their real-world applications. \n\n7 Limitations and Future Work \n\nWhile our proposed method has shown promising results, there are a few limitations to consider. \n\nOur evaluation focused on a specific set of tasks, and future work can aim to validate our method's effectiveness across a broader range of language models and tasks, including data selection in the pre-training stage and alignment (Yu et al., 2024;Muldrew et al., 2024). Furthermore, our iterative selection process relies on early training signals, and we only presented this as a pilot study to encourage further research. Future work could explore alternative model feedback mechanisms to refine the selected instruction data subsets, especially in mitigating the potential for reward hacking in the iterative refinement process (Pan et al., 2024). \n\nFinally, while we considered diversity and difficulty crucial factors, other characteristics of instruction data could be explored to enhance the finetuning process further. Addressing these limitations and extending this research will contribute to more robust and adaptable language models, capable of excelling in a wide range of real-world applications. \n\nBroader Impact If the data selection process fails to capture important aspects of the full dataset, it could lead to biased or inconsistent outputs from the finetuned models.",
            "score": 0.4396506628741437,
            "section_title": "Conclusion",
            "char_start_offset": 26229,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 752
                },
                {
                    "start": 755,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1158
                },
                {
                    "start": 1161,
                    "end": 1190
                },
                {
                    "start": 1193,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1943
                },
                {
                    "start": 1946,
                    "end": 2119
                },
                {
                    "start": 2120,
                    "end": 2303
                },
                {
                    "start": 2306,
                    "end": 2481
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9375
        },
        {
            "corpus_id": "270620248",
            "title": "Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models",
            "text": "Supervised fine-tuning (SFT) leverages small amounts of instruction-pair data to unlock large language models' instruction-following capabilities and improve generalization across various tasks (Radford et al., 2019;Wei et al., 2022;Singh et al., 2023;Zhang et al., 2023;Albalak et al., 2024). Recent research highlights that data quality is more critical than data quantity for effective fine-tuning (Nakkiran et al., 2020;Shumailov et al., 2024;Zhou et al., 2024;Jindal et al., 2024). As a result, several data curation techniques have been proposed, such as data selection (Chen et al., 2024;Li et al., 2024b), data evolution (Wang et al., 2023;Xu et al., 2023), and data reflection (Mukherjee et al., 2023;Yin et al., 2023). Data selection, in particular, involves choosing a high-quality subset from a candidate dataset based on specific selection criteria, enhancing the model's performance while improving training efficiency by reducing the number of samples. Unlike data augmentation or polishing, it focuses on selecting inherently higherquality samples, as shown in Figure 1. However, despite the rapid development of data selection methods, there is currently no unified framework for systematically guiding and comparing these methods, as experimental settings vary widely across studies. Although some surveys (Bommasani et al., 2021;Albalak et al., 2024;Wang et al., 2024) have reviewed data curation techniques in the contexts of pretraining, fine-tuning, and reinforcement learning, they generally provide high-level overviews and lack in-depth discussions  of the fine-tuning stage. This gap makes it difficult for researchers to conduct focused, sustained studies on this crucial phase. To address this, our survey provides a fine-grained review of data selection methods for instruction fine-tuning LLMs, rethinking existing approaches, proposing a unified comparison method, and outlining key trends and challenges in the field.",
            "score": 0.4396506628741437,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1949
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 233,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 233,
                    "end": 252,
                    "matchedPaperCorpusId": "266163375"
                },
                {
                    "start": 271,
                    "end": 292,
                    "matchedPaperCorpusId": "268032975"
                },
                {
                    "start": 401,
                    "end": 424,
                    "matchedPaperCorpusId": "207808916"
                },
                {
                    "start": 424,
                    "end": 447,
                    "matchedPaperCorpusId": "271448069"
                },
                {
                    "start": 447,
                    "end": 465,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 576,
                    "end": 595,
                    "matchedPaperCorpusId": "259937133"
                },
                {
                    "start": 595,
                    "end": 612,
                    "matchedPaperCorpusId": "261076515"
                },
                {
                    "start": 629,
                    "end": 648,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 686,
                    "end": 710,
                    "matchedPaperCorpusId": "259075316"
                },
                {
                    "start": 710,
                    "end": 727,
                    "matchedPaperCorpusId": "258841263"
                },
                {
                    "start": 1324,
                    "end": 1348,
                    "matchedPaperCorpusId": "237091588"
                },
                {
                    "start": 1348,
                    "end": 1369,
                    "matchedPaperCorpusId": "268032975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9599609375
        },
        {
            "corpus_id": "250391006",
            "title": "On Curriculum Learning for Commonsense Reasoning",
            "text": "Commonsense reasoning tasks follow a standard paradigm of finetuning pretrained language models on the target task data, where samples are introduced to the model in a random order during training. However, recent research suggests that data order can have a significant impact on the performance of finetuned models for natural language understanding. Hence, we examine the effect of a human-like easy-to-difficult curriculum during finetuning of language models for commonsense reasoning tasks. We use paced curriculum learning to rank data and sample training mini-batches with increasing levels of difficulty from the ranked dataset during finetuning. Further, we investigate the effect of an adaptive curriculum, i.e., the data ranking is dynamically updated during training based on the current state of the learner model. We use a teacher model to measure difficulty of each sample and experiment with three measures based on question answering probability, variability and out-of-distribution. To understand the effectiveness of curriculum learning in various scenarios, we apply it on full model fine-tuning as well as parameter-efficient prompt-tuning settings. Our results show that fixed as well as adaptive curriculum learning significantly improve performance for five commonsense reasoning tasks, i.e., SocialIQA, CosmosQA, CODAH, HellaSwag, WinoGrande in both tuning settings. Further, we find that prioritizing the difficult samples in the tail end of training improves generalization to unseen in-domain data as well as out-of-domain data. Our work provides evidence and encourages research into curriculum learning for commonsense reasoning.",
            "score": 0.4381956874562452,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6201171875
        },
        {
            "corpus_id": "245124549",
            "title": "Do Data-based Curricula Work?",
            "text": "In the last years state-of-art results in natural language processing (NLP) are often obtained with Transformer-like architectures based on the selfattention mechanism (Vaswani et al., 2017) such as BERT (Devlin et al., 2019), GPT-3 (Brown et al., 2020), T5 (Raffel et al., 2020), which could have billions of parameters. Due to many parameters, these architectures require lots of time and hardware resources to be trained.\n\nCurriculum learning (CL) is one of the popular methods to reduce training time and increase the resulting quality of the model. Inspired by the importance of adequately ordering information when teaching humans (Avrahami et al., 1997), curriculum learning increases the difficulty of training samples shown to the model over time (Elman, 1993). Previous studies have demonstrated that curriculum learning significantly impacts training time and quality in different machine learning domains, such as computer vision (Soviany, 2020) and reinforcement learning (Narvekar et al., 2020).\n\nIn NLP, some results hint that CL might be beneficial (Platanios et al., 2019;Xu et al., 2020;Kocmi and Bojar, 2017); however, these results are not as optimistic as in reinforcement learning setup.\n\nWe suggest dividing recent research in curriculum learning into two main categories: task-driven curriculum and data-driven curriculum. The idea of the task-driven curriculum was inspired by human behavior. First, the model learns how to solve a simple task, and then the difficulty is gradually increased. This type of curriculum proposed by Bengio et al. (2009) is considered to be classical, and a majority of curriculum-related results are obtained in this framework. Alternatively to the taskdriven curriculum, some curricula try to use some form of filtering or sorting of training data that could facilitate learning a model on a given task. We suggest calling these curricula data-driven and distinguishing them from the classical task-based approach.\n\nThis paper attempts to understand when datadriven curriculum learning works for transformerbased language models. Generally, data-driven curriculum learning is organized in two steps: first, estimating the complexity for the elements that comprise the dataset; second, designing a sampling strategy, thus",
            "score": 0.43722520990159053,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.462158203125
        },
        {
            "corpus_id": "268364003",
            "title": "Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences",
            "text": "Curriculum is a training paradigm that seeks to present data samples in a meaningful manner, thus controlling and optimizing the type of information a model has access to at each training step (Elman, 1993;Bengio et al., 2009). Previous works have shown success of learning from easy to hard examples in humans and machine (Peterson, 2004;Krueger and Dayan, 2009;Bengio et al., 2009). It has also been extensively used in NLP tasks such as language modelling (Choudhury et al., 2017;Xu et al., 2020), reading comprehension (Tay et al., 2019), question answering (Sachan andXing, 2016, 2018) and machine translation (Zhang et al., 2019;Lu and Zhang, 2021). The only application of curriculum learning to LLM alignment is in concurrent work (Wang et al., 2024) where they perform selfalignment bootstrapping for supervised fine-tuning. To the best of our knowledge, we are the first to apply curriculum learning to the DPO framework.",
            "score": 0.4369812164942084,
            "section_title": "Curriculum Learning",
            "char_start_offset": 6168,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 931
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 206,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 206,
                    "end": 226,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 323,
                    "end": 339,
                    "matchedPaperCorpusId": "34687176"
                },
                {
                    "start": 339,
                    "end": 363,
                    "matchedPaperCorpusId": "206863670"
                },
                {
                    "start": 363,
                    "end": 383,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 459,
                    "end": 483,
                    "matchedPaperCorpusId": "59067608"
                },
                {
                    "start": 483,
                    "end": 499,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 523,
                    "end": 541,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 615,
                    "end": 635,
                    "matchedPaperCorpusId": "155089817"
                },
                {
                    "start": 635,
                    "end": 654,
                    "matchedPaperCorpusId": "237605036"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.386962890625
        },
        {
            "corpus_id": "271710435",
            "title": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models",
            "text": "In this work, we aim to unify a wide array of data assessment and selection methods under the context of instruction tuning of LLMs. As revealed from the probabilistic view (John and Draper, 1975;Murphy, 2012;Albalak et al., 2024), the statistical patterns inherent in datasets determines the modeling performance. The overall evaluation of instruction datapoints not only deciphers the distribution in various aspects (e.g., composition, task, and domain) and also help cherry-pick the most beneficial subsets for higher performance with less training cost. Through this survey, we demonstrate that: 1) existing resourceful data assessment methods can be categorized into three main perspectives: quality, diversity, and importance (see Fig. 1). 2) a systematic view of selection methods can be unified even they more or less exhibit coupling with the assessment techniques (see Fig. 2). It is noted that quality, diversity, and importance might be used interchangeably without strict discrimination in previous studies. But here we provide a rationalized organization taxonomy for structured elaboration. Despite the goal of being comprehensive, the present survey only provides details of certain typical, representative methods to avoid being tediously long. We hope the in-depth explanations and discussions on the selected methods provide insights into developing robust data assessment and selection pipelines for further studies. \n\n1.1 Related Surveys (Liu et al., 2024d) studies the mainstream datasets for building LLMs, including the pre-training corpora, instruction tuning datasets, preference datasets, evaluation benchmarks, and traditional NLP datasets. (Albalak et al., 2024) presents a systematic overview of constructing the data pipeline for language models. Any selection method, either via distribution matching or diversification, can be composed of: 1) utility function; 2) selection mechanism. During different stages of the pipeline (e.g., language filtering, data quality, domain knowledge, deduplication, toxic and explicit content removal, and data mixing), the selection method should be adjusted according to different selection objectives. (Wang et al., 2024a) focuses on the data preparation for instruction tuning.",
            "score": 0.4361205550587199,
            "section_title": "Introduction",
            "char_start_offset": 1432,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1437
                },
                {
                    "start": 1440,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2171
                },
                {
                    "start": 2172,
                    "end": 2248
                }
            ],
            "ref_mentions": [
                {
                    "start": 173,
                    "end": 196,
                    "matchedPaperCorpusId": "120997070"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.943359375
        },
        {
            "corpus_id": "276575595",
            "title": "Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert Models",
            "text": "There is an extensive body of work on data selection and mixture optimization for pretraining language models. Albalak et al. (2024) offer a comprehensive recent survey. Approaches for data selection and cleaning consider different granularities of data, such as token-level, sample-level (individual documents or sentences can be selected or weighted), and group-level (where we consider samples in large groups assumed to have common characteristics, often derived from meta-data such as the web domain (like Wikipedia) or source collection name (such as C4). \n\nClosest to our focus is work selecting or sampling data at the level of large sample groups, often termed domains. We will limit our overview to methods optimizing such group-level data mixture sampling rates. Data mixture sampling rates can be static over the course of model training, or dynamic, forming a curriculum over sampling rates which could for example facilitate faster progress through learning easier skills first. Dynamic mix-tures for pre-training have been considered in e.g. (Albalak et al., 2023;Piergiovanni et al., 2023); we focus on static mixtures in this work. \n\nOnline optimization of domain mixture rates through proxies DOGE (Fan et al., 2024) presents an efficient method to optimize data mixture rates through a first-order bi-level optimization approach. DOGE showed successful optimization of the average hedlout domain loss through proxies of size 124M parameters and smaller, with compute cost 2x the cost of training a single proxy model. Our approach is simpler to implement as it does not require changes in the training algorithm for language models, and also offers the possibility to derive optimal weights for a set of different criteria while reusing the same proxy models. DOREMI (Xie et al., 2023) also proposes an online method which optimizes a loss derived from training data, and has similar computational requirements to those of DOGE. For comparison, we train full-scale models with mixture rates optimized through DOGE and DOREMI and report results in Section 4.",
            "score": 0.43587983836893207,
            "section_title": "Related work",
            "char_start_offset": 5936,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 561
                },
                {
                    "start": 564,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1148
                },
                {
                    "start": 1151,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2076
                }
            ],
            "ref_mentions": [
                {
                    "start": 1216,
                    "end": 1234,
                    "matchedPaperCorpusId": "264439382"
                },
                {
                    "start": 1786,
                    "end": 1804,
                    "matchedPaperCorpusId": "258741043"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90380859375
        },
        {
            "corpus_id": "220045816",
            "title": "Curriculum Learning for Natural Language Understanding",
            "text": "With the great success of pre-trained language models, the pretrain-finetune paradigm now becomes the undoubtedly dominant solution for natural language understanding (NLU) tasks. At the fine-tune stage, target task data is usually introduced in a completely random order and treated equally. However, examples in NLU tasks can vary greatly in difficulty, and similar to human learning procedure, language models can benefit from an easy-to-difficult curriculum. Based on this idea, we propose our Curriculum Learning approach. By reviewing the trainset in a crossed way, we are able to distinguish easy examples from difficult ones, and arrange a curriculum for language models. Without any manual model architecture design or use of external data, our Curriculum Learning approach obtains significant and universal performance improvements on a wide range of NLU tasks.",
            "score": 0.4356966265626985,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6376953125
        },
        {
            "corpus_id": "236912520",
            "title": "Curriculum learning for language modeling",
            "text": "Seeking to represent natural language, researchers have found language models (LM) with Sesame Street-inspired names [1] [2] [3] to be incredibly effective methods of producing language representations (LR). These LM's have leverage transfer learning by training on a large text corpus to learn a good representation of language which can then be used in a down steam task like Question Answering or Entity Resolution. While these LMs have shown to be excellent methods to enable language understanding, the ability to train these models is becoming increasingly computationally expensive [4]. Since model performance is closely tied to the size of training data, model size, and compute used to train [5] the bulk of existing research has focused on scaling these aspects without much focus on increasing efficiency of training. Seeking to explore what methods could be used to make LM training more efficient we study the effect of curriculum learning by training ELMo with a wide variety of curricula. Curriculum learning (CL) is a training methodology which applies structure to a models training data. CL has been studied broadly in natural language processing and has been very successful in domains like Neural Machine Translation (NMT) where CL based models are able to train faster and produce better results [6] [7] [8] than unstructured, stochastic sampling. Focusing on LMs, Xu et al. [9] showed that CL can be used in LM finetuning as a way to improve task performance. Despite an abundance of work exploring CL and LMs to the best of our knowledge we are the first to examine the effect of curriculum learning in LM pre-training and transfer performance. To evaluate the effect of CL on LMs we train ELMo with a variety of curricula on the wikitext-2 and wikitext-103 [10] without modification of training time or model hyperparameters. We evaluate model performance on the pre-training task and on the GLUE Benchmark [11] building on the work of Competence Based Curriculum Learning [12] by modifying training sampler within the LM to produce a dataset with gradually increasing difficulty 2 . The contributions of our work are: \n\n\u2022 Exploration of the effects of curriculum learning for language modeling finding no clear improvement to models that use curriculum methods for training.",
            "score": 0.43523617573526885,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2143
                },
                {
                    "start": 2146,
                    "end": 2300
                }
            ],
            "ref_mentions": [
                {
                    "start": 589,
                    "end": 592,
                    "matchedPaperCorpusId": "174802812"
                },
                {
                    "start": 1326,
                    "end": 1329,
                    "matchedPaperCorpusId": "215746703"
                },
                {
                    "start": 1397,
                    "end": 1400,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 1932,
                    "end": 1936,
                    "matchedPaperCorpusId": "5034059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56591796875
        },
        {
            "corpus_id": "259370648",
            "title": "HuCurl: Human-induced Curriculum Discovery",
            "text": "Existing CL approaches are designed to learn a single curriculum that works best for a given model and dataset. They estimate sample difficulty through model behavior during training, quantified by the instantaneous loss Wu et al., 2021), consistency in instantaneous loss , moving average of loss (Jiang et al., 2018;Zhou et al., 2020), transformations of loss (Amiri et al., 2017;Castells et al., 2020;Chen et al., 2021;Vakil and Amiri, 2022), loss regularization (Kumar et al., 2010;Jiang et al., 2015;Castells et al., 2020), or learnable per-sample confidence (Shu et al., 2021;Saxena et al., 2019;Jiang et al., 2018). In terms of data ordering, subsampling approaches sample the easiest or hardest instances at every training iteration (Bengio et al., 2009;Kumar et al., 2010;Guo et al., 2018;Platanios et al., 2019;, sample weighting techniques weight instances according to their estimated difficulty (Kumar et al., 2010;Jiang et al., 2015Jiang et al., , 2018Yang et al., 2019;Castells et al., 2020;Zhou et al., 2020), and sample pruning techniques filter hard or noisy instances from data prior to training (Northcutt et al., 2021). Sub-sampling methods can be cumulative, exclusive or a combination of both. Cumulative approaches add new samples to the ones that have been previously used  Figure 1: The model defines a difficulty score based on prior knowledge about sample difficulty and assigns samples to k difficulty groups before training, e.g., easy, medium, and hard for k = 3. A curriculum is defined for each difficulty group, which dynamically weights sample losses according to their difficulty groups. Each curriculum is defined by a pair of parameters (r, s) that will be optimized to discover an optimized curriculum based on sample difficulty and model behavior.\n\nfor training (Guo et al., 2018;, while exclusive approaches create a new subset of the data at every training stage (Bengio et al., 2009;Zhou and Bilmes, 2018). In addition, previous research",
            "score": 0.434537875659482,
            "section_title": "Related Work",
            "char_start_offset": 4061,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 221,
                    "end": 237,
                    "matchedPaperCorpusId": "227343966"
                },
                {
                    "start": 298,
                    "end": 318,
                    "matchedPaperCorpusId": "51876228"
                },
                {
                    "start": 318,
                    "end": 336,
                    "matchedPaperCorpusId": "227275560"
                },
                {
                    "start": 422,
                    "end": 444,
                    "matchedPaperCorpusId": "248863268"
                },
                {
                    "start": 466,
                    "end": 486,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 486,
                    "end": 505,
                    "matchedPaperCorpusId": "10891229"
                },
                {
                    "start": 564,
                    "end": 582,
                    "matchedPaperCorpusId": "237366380"
                },
                {
                    "start": 582,
                    "end": 602,
                    "matchedPaperCorpusId": "209438234"
                },
                {
                    "start": 602,
                    "end": 621,
                    "matchedPaperCorpusId": "51876228"
                },
                {
                    "start": 762,
                    "end": 781,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 798,
                    "end": 821,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 908,
                    "end": 928,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 928,
                    "end": 946,
                    "matchedPaperCorpusId": "10891229"
                },
                {
                    "start": 946,
                    "end": 966,
                    "matchedPaperCorpusId": "51876228"
                },
                {
                    "start": 966,
                    "end": 984,
                    "matchedPaperCorpusId": "159040584"
                },
                {
                    "start": 1006,
                    "end": 1024,
                    "matchedPaperCorpusId": "227275560"
                },
                {
                    "start": 1115,
                    "end": 1139,
                    "matchedPaperCorpusId": "207870256"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.191162109375
        },
        {
            "corpus_id": "53295888",
            "title": "An Empirical Exploration of Curriculum Learning for Neural Machine Translation",
            "text": "The curriculum's schedule defines the order in which samples of different difficulty classes are presented to the learning system. A curriculum's phase is the period between two curriculum updates. 5 For NMT models, it is natural to come up with the idea of first presenting easy samples to the models, as suggested by Bengio et al. (2009). In the following sections, we refer to this as the default schedule. We also introduce four variants of the default schedule (Figure 4) which lets us explore different trade-offs. The colored blocks are shards of different difficulty levels (see figure 2). Within a sub-figure, each row represents a phase, and shards in that row are accessible shards based on the curriculum. Training starts from the first row and goes through the following rows in succession. Hence, at each phase only subsets of the training data and certain difficulty classes are available. Note that shards (and the samples within them) are shuffled as described in Section 5.3. \n\n\u2022 default Shards are sorted by increasing level of difficulty. Training begins with the easiest shard and harder shards will be included in subsequent phases. \n\n\u2022 reverse Shards are sorted in descending order of difficulty. Training begins with the hardest shard and easier shards will be included in subsequence phases. \n\n\u2022 boost A copy of the hardest shard is added to the training set, after the model has processed shards of all difficulty classes. \n\n\u2022 reduce Once all shards have been visited, we start removing shards from training one at the end of each phase, starting with the easiest. \n\nOnce a fixed number of shards have been removed (2 in our case), we add them back. This reduce and add-back procedure will be iteratively continued until the training converges. The effect is that the model gets to look at harder shards more often. \n\n\u2022 noshuffle Same as default except that shards are never shuffled; that is, they are always presented to the model in ascending order of difficulty (Samples within shards are shuffled as usual). \n\nThe reverse schedule tests the assumption that presenting easy examples first helps learning. It remains unclear if we should start with the easier sentences and move to more difficult ones, or if perhaps some of the difficult sentences are too hard for the model to learn and we should focus on straightforward sentences at the end.",
            "score": 0.4343548027492181,
            "section_title": "Curriculum Schedule",
            "char_start_offset": 14271,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 993
                },
                {
                    "start": 996,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1154
                },
                {
                    "start": 1157,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1316
                },
                {
                    "start": 1319,
                    "end": 1448
                },
                {
                    "start": 1451,
                    "end": 1590
                },
                {
                    "start": 1593,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1841
                },
                {
                    "start": 1844,
                    "end": 2038
                },
                {
                    "start": 2041,
                    "end": 2134
                },
                {
                    "start": 2135,
                    "end": 2374
                }
            ],
            "ref_mentions": [
                {
                    "start": 319,
                    "end": 339,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33447265625
        },
        {
            "corpus_id": "260438678",
            "title": "Reverse Stable Diffusion: What prompt was used to generate this image?",
            "text": "Curriculum learning is a method to train neural models inspired by how humans learn (Soviany et al., 2022). It involves organizing the data samples from easy to hard, thus training models on gradually more difficult samples. In our case, we propose to train the neural networks on samples with progressively higher levels of labeling noise. In the beginning, when the weights are randomly initialized, feeding easier-to-predict (less noisy) examples, then gradually introducing harder ones, can help convergence and stabilize training. \n\nGiven that our outputs are embeddings in a vector space, we harness the cosine similarity between the generated prompt and the ground-truth prompt during training. Thus, we propose a two-stage learning procedure. In the first phase, we train the network for a number of epochs using the conventional learning regime, storing the cosine similarity of each sample after every epoch. The only goal of the first (preliminary) training phase is to quantify the difficulty of learning each training sample. The difficulty score of a training example is computed as the mean of the cosine similarities for the respective example. Generalizing the observations of Swayamdipta et al. (2020), we conjecture that the resulting difficulty score is proportional to the amount of noise, essentially quantifying the level of misalignment between the input image x i and the corresponding text prompt y i . To support our conjecture, we show examples of easy, medium and hard images in Figure 9. For the second training phase, we reset the training environment and split the training data into three chunks, such that each chunk represents a different difficulty level: easy, medium and hard. Finally, we train the model again until convergence in the same number of steps as before, by gradually introducing each data chunk, starting with the easiest one and progressing to the hardest one. The model still gets to see the whole data set, but it ends up spending less time learning noisy examples. \n\nWe consider two alternative data splitting heuristics. The first one is to divide the data set into three equally-sized chunks, inspired by Ionescu et al. (2016). The second splitting heuristic involves setting two threshold values for the cosine similarity and splitting the data set according to these thresholds.",
            "score": 0.4342245643144764,
            "section_title": "Method",
            "char_start_offset": 21921,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 535
                },
                {
                    "start": 538,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2020
                },
                {
                    "start": 2023,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2185
                },
                {
                    "start": 2186,
                    "end": 2338
                }
            ],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 106,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 1194,
                    "end": 1219,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 2163,
                    "end": 2184,
                    "matchedPaperCorpusId": "879067"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.130615234375
        },
        {
            "corpus_id": "260203057",
            "title": "Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models",
            "text": "We show that ordered skill sets exist in synthetic and real datasets using this operational definition. Interestingly, the existence of these ordered skill sets unveils that one can learn a skill quickly not by training solely on that skill, but on a mixture of that skill and prerequisite skills. For instance, in Figure 3 we observe that Spanish QG can be learned more efficiently when the model also learns English QG and Spanish-we can achieve 4% lower validation loss than training on only Spanish QG over a fixed budget of overall training steps. \n\nNext, given an ordered skill set to train on, we use our framework to propose methods for how to select data so that the LM learn skills faster: skill-stratified sampling and an online generalization, SKILL-IT. We address the issue of unevenly distributed skills in datasets by proposing skill-stratified sampling, a simple approach that allows us to explicitly optimize for learning skills by uniformly sampling relevant skills (such as a target skill and its prerequisite skills in fine-tuning). Skill-stratified sampling uses the construction of the ordered skill set but is static, which does not incorporate the ordering as training proceeds and results in oversampling skills that may be already learned early on in training. We address this issue by proposing an online data selection algorithm, SKILL-IT, for selecting mixtures of training skills that allocates more weight towards learning skills that are not yet learned or towards prerequisite influential skills (Figure 1 right). SKILL-IT is derived from an online optimization problem over the training skills for minimizing loss on a set of evaluation skills given a fixed budget of data and the skills graph. SKILL-IT is inspired by online mirror descent and can be adapted for continual pre-training, fine-tuning, or out-of-domain evaluation depending on the relationship between the evaluation skill set and the training skill set. \n\nWe evaluate SKILL-IT on synthetic and real datasets at two model scales, 125M and 1.3B parameters. For the continual pre-training setting, we show on the LEGO synthetic [72] that we obtain a 35.8 point improvement in accuracy over randomly selecting training data and curriculum learning [3].",
            "score": 0.4341755079359133,
            "section_title": "Introduction",
            "char_start_offset": 4410,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1953
                },
                {
                    "start": 1956,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2248
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.875
        },
        {
            "corpus_id": "229924288",
            "title": "Dialogue Response Selection with Hierarchical Curriculum Learning",
            "text": "finding matching clues between the context and the positive response. As for IC, it progressively strengthens the model's ability in identifying the mismatch information be-tween the context and negative responses. To order all positive and negative examples, we need to assess millions of possible context-response combinations in the training data. To overcome this computational challenge, we propose to use a fast neural ranking model to assign learning priorities to all training examples based on their pairwise context-response similarity score.\n\nNotably, our proposed learning framework is independent to the choice of matching models. Therefore, for a comprehensive evaluation, we test our approach with three representative matching models, including the latest advance brought by pretrained language models. Results on two benchmark datasets demonstrate that the proposed learning framework leads to remarkable performance improvement across all evaluation metrics.\n\nIn summary, our contributions are: (1) We propose a new hierarchical curriculum learning framework to tackle the task of response selection; and (2) Experimental results on two benchmark datasets demonstrate that our approach can significantly improve the performance of strong matching models, including the state-of-the-art one.",
            "score": 0.43364124271052873,
            "section_title": "Introduction",
            "char_start_offset": 4114,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49462890625
        },
        {
            "corpus_id": "231709290",
            "title": "Curriculum Learning: A Survey",
            "text": "Training machine learning models in a meaningful order, from the easy samples to the hard ones, using curriculum learning can provide performance improvements over the standard training approach based on random data shuffling, without any additional computational costs. Curriculum learning strategies have been successfully employed in all areas of machine learning, in a wide range of tasks. However, the necessity of finding a way to rank the samples from easy to hard, as well as the right pacing function for introducing more difficult data can limit the usage of the curriculum approaches. In this survey, we show how these limits have been tackled in the literature, and we present different curriculum learning instantiations for various tasks in machine learning. We construct a multi-perspective taxonomy of curriculum learning approaches by hand, considering various classification criteria. We further build a hierarchical tree of curriculum learning methods using an agglomerative clustering algorithm, linking the discovered clusters with our taxonomy. At the end, we provide some interesting directions for future work.",
            "score": 0.4330199229554367,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.182373046875
        },
        {
            "corpus_id": "256390295",
            "title": "Confidence-aware calibration and scoring functions for curriculum learning",
            "text": "More specifically, CL starts from the easier data, then gradually increases the complexity of data until the training has used the whole dataset. CL has been widely applied in recent state-of-the-art deep neural networks and has been shown to benefit model generalization in various applications, including histopathology image classification in the medical field [6], contextual difficulty generator for long narratives in natural language understanding [8], multi-modality data synchronization with self-supervised learning [24], and for generative adversarial networks [25]. \n\nOne key issue in designing a Curriculum Learning (CL) framework is to determine how to rank training samples accurately in terms of difficulty level. Suppose in a training set D, x represents the sample and y is the corresponding true label. S is the scoring function that creates subsets in the training set with the ranking threshold \u00b5. The \u00b5 threshold assigns the learning order to training subsets, such that subsets which are used for training earlier are easier for the model to learn. Each subset d{x, y} can be represented as d{x, y} = S(D{x, y}, \u00b5). \n\nIn this work, instead of scoring subsets of training samples, we utilize curriculum criteria to calculate loss from easier to harder tasks over the training epochs. \n\nHuman Uncertainty. When human judgements are subjective, datasets should contain multiple human judgments for samples, to reflect the distribution of responses possible. Such datasets are in reality hard to come by in sufficient scale to deploy neural models. The three we are aware of wer created through crowdsourcing platforms such as Amazon Mechanical Turk [26], where many annotators are asked to label the same data into one or more classes. For instance, the ArtEmis dataset [27] contains the human annotation of visual art with 9 emotion classes; the CIFAR10-H dataset [3] includes 10,000 images rated by crowdsourcing with 10 object classes; and the WikiArt dataset [28] contains rating for both image and text into 20 emotion classes. These datasets have been used to train deep learning models and have proved beneficial for model generalization and performance.",
            "score": 0.4327524538204065,
            "section_title": "RELATED WORK",
            "char_start_offset": 6753,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 577
                },
                {
                    "start": 580,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1138
                },
                {
                    "start": 1141,
                    "end": 1305
                },
                {
                    "start": 1308,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2181
                }
            ],
            "ref_mentions": [
                {
                    "start": 526,
                    "end": 530,
                    "matchedPaperCorpusId": "53280782"
                },
                {
                    "start": 572,
                    "end": 576,
                    "matchedPaperCorpusId": "195513473"
                },
                {
                    "start": 1790,
                    "end": 1794,
                    "matchedPaperCorpusId": "231639297"
                },
                {
                    "start": 1983,
                    "end": 1987,
                    "matchedPaperCorpusId": "21693606"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3818359375
        },
        {
            "corpus_id": "221856706",
            "title": "My Health Sensor, My Classifier \u2013 Adapting a Trained Classifier to Unlabeled End-User Data",
            "text": "At first glance SICO and Curriculum Learning (CL) [7] can appear to be quite similar. In CL, the training \"starts small\" by using a small training set with easy examples identified with the use of a scoring function [8]. Afterwards, CL progressively utilizes more difficult examples which are added to the curriculum. Similarly, we utilize easier examples in terms of domain similarity, and progressively train with harder datapoints. The previous classifier's class probabilities give us a measurement of the datapoints that are easier for the classifier in terms of the classifiers' higher confidence regarding these points (lower entropy). We hypothesize that datapoints that are easier in terms of lower entropy for a classifier trained in a different domain are more likely to be more similar to datapoints from the source domain, in terms of class separation.\n\nHowever, the basic vanilla CL uses a static scoring function during training. SICO utilizes instead a sequence of scoring functions (i.e., h CLi ) that are learned dynamically as the training process continues. The creation of a new scoring function depends on the dataset and the previous scoring function. Additionally, the first scoring function, i.e., h sr c , is independently trained and acts as a Teacher that provides the initial scoring paradigm, in conjunction with the belief criterion used. We compare our method with other more relevant and recent works of CL for DA in Section 3.",
            "score": 0.43269628647811087,
            "section_title": "SICO and Curriculum Learning",
            "char_start_offset": 10673,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 50,
                    "end": 53,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.026763916015625
        },
        {
            "corpus_id": "227238886",
            "title": "Denoising Pre-Training and Data Augmentation Strategies for Enhanced RDF Verbalization with Transformers",
            "text": "We coerce our model to use a WebNLG-based vocabulary during pre-training so that the same model can be straightly fine-tuned without any vocabulary discrepancy. Technically, we load the last checkpoint from pre-trained model and directly start to fine-tune it by reseting the optimizer and setting the new data loader to the WebNLG training set. We also tried without resetting the optimizer and obtained similar performances. \n\nIn addition, we apply a Curriculum Learning (CL) approach for faster convergence and better local minima through the optimization process. CL was proposed by Bengio et al. (2009), motivated by the learning process of humans and animals. The core idea is to gradually increase the complexity of samples seen during training instead of a random selection. In our case, we define complexity as the number of RDF triples in the input, i.e. N . The higher the number of triples, the higher the complexity. We sort the WebNLG pairs such that easier examples come first and then harder examples afterwards. Our curriculum approach differs in that both easier and harder examples are seen within the same epoch, but in a gradually order. In (Bengio et al., 2009), more complex samples are progressively added during training. To witness the effect of increasing complexity while training, we fine-tune our pre-trained models with and without curriculum learning.",
            "score": 0.43233690549271786,
            "section_title": "Fine-tuning Settings",
            "char_start_offset": 20199,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1383
                }
            ],
            "ref_mentions": [
                {
                    "start": 587,
                    "end": 607,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1162,
                    "end": 1183,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2108154296875
        },
        {
            "corpus_id": "251719585",
            "title": "A Syntax Aware BERT for Identifying Well-Formed Queries in a Curriculum Framework",
            "text": "Its inspiration lies in cognitive science theories [9], [10] which propose for humans to acquire a skill, they are first provided easier variants of the task following increased difficulty. Effectiveness of Curriculum Learning has been explored in Natural Language Understanding tasks such as Question Answering [11] and Natural Answer Generation [12] and more recently in the broad range of GLUE tasks as well [13]. [14] proposed Baby Steps and One Pass curriculum techniques using sentence length as a curriculum strategy for training LSTM [15] on Sentiment Analysis. A tree-structured curriculum ordering based on semantic similarity is proposed by [16]. [17] propose a auxiliary network which is first trained on the dataset and used to calculate difficulty scores for the curriculum ordering. Some of these works [14], [16], [17] have suggested that Baby Steps performs better than One Pass. We perform experiments using both the techniques. While the idea of curriculum remains same across these works, the strategy itself to decide sample ordering is often tough to decide. In this work, we exploit the query well-formedness probability scores given in the query well-formedness dataset [4] to design our curriculum ordering. In doing so we forego the experiments required to choose a curriculum ordering and furthermore, build a strategy of difficulty which agrees with the perspective of annotators and domain experts. \n\nThe overall contributions of our paper are as follows: \n\n\u2022 We establish a new state-of-the-art performance in query well-formedness identification utilising BERT. \u2022 Proposed framework utilizes query well-formedness probability scores to determine a curriculum ordering for training. Samples are ordered in this strategy based on how difficult is it to differentiate them between wellformed or poorly-formed. \u2022 Proposed architecture also infuses syntactic formation in the form of Parts-of-Speech in a manner that helps the model achieves better performance. In Section II, we explain previous SotA approaches which tackle Query well-formedness identification and also, curriculum learning for text classification works. Section III explains the query well-formedness dataset. Section IV explains our Approach. Section V provides our architecture in a detailed manner including the curriculum algorithm. Section VI lists down the baselines for comparison and other recent architectures proposed for this task. Section VII explains our results and Section VIII concludes our work.",
            "score": 0.43211647260261943,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2156,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1427
                },
                {
                    "start": 1430,
                    "end": 1484
                },
                {
                    "start": 1487,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2149
                },
                {
                    "start": 2150,
                    "end": 2205
                },
                {
                    "start": 2206,
                    "end": 2239
                },
                {
                    "start": 2240,
                    "end": 2332
                },
                {
                    "start": 2333,
                    "end": 2438
                },
                {
                    "start": 2439,
                    "end": 2508
                }
            ],
            "ref_mentions": [
                {
                    "start": 56,
                    "end": 60,
                    "matchedPaperCorpusId": "206863670"
                },
                {
                    "start": 312,
                    "end": 316,
                    "matchedPaperCorpusId": "16503693"
                },
                {
                    "start": 347,
                    "end": 351,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 411,
                    "end": 415,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 542,
                    "end": 546,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 652,
                    "end": 656,
                    "matchedPaperCorpusId": "21814386"
                },
                {
                    "start": 824,
                    "end": 828,
                    "matchedPaperCorpusId": "21814386"
                },
                {
                    "start": 1194,
                    "end": 1197,
                    "matchedPaperCorpusId": "52111971"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3505859375
        },
        {
            "corpus_id": "53295888",
            "title": "An Empirical Exploration of Curriculum Learning for Neural Machine Translation",
            "text": "Finally, we address the question of how to draw mini-batches from the training data which has been sharded based on difficulty. Current stateof-the-art NMT model implementations bucket the training samples based on source and target length. Mini-batches are then drawn from these buckets, which are shuffled at each epoch. One way of drawing mini-batches while conditioning on difficulty is to sort the training samples by difficulty and to then draw these deterministically starting from the easiest to the most difficult sample. However, this loses the benefits gained by shuffling the data at each epoch. \n\nInstead, our work uses a strategy similar to the work of Bengio et al. (2009). We organize samples into shards6 according to the univariate classification results (Section 5.1) and allow further bucketing by sentence length within each shard. Samples within each shard are shuffled at each epoch, ensuring that we draw random mini-batches of the same difficulty. \n\nGiven shards of different difficulty levels, we follow these steps for training: \n\n\u2022 The curriculum's schedule defines which shards are available for training. We call these the visible shards for this phase of curriculum training. \n\n\u2022 These shards are then shuffled (except when we use the noshuffle schedule)7 so that the model is trained using random levels of difficulty (in contrast to always using easy to hard). \n\n\u2022 The samples within each shard are shuffled and bucketed by length. Mini-batches are drawn from these buckets. \n\n\u2022 When the curriculum update frequency is reached (defined in terms of number of batches), the curriculum's schedule is updated. For example, this may imply that we include more difficult shards in training in the next phase. In cases where the total number of examples in these shards is smaller than the curriculum update frequency, we repeat the previous step until the update frequency has been achieved. \n\n\u2022 After all available shards are visible to the model, training continues until validation perplexity does not improve for 32 checkpoints. The NMT model has then converged.",
            "score": 0.4313087351752507,
            "section_title": "Training Strategy",
            "char_start_offset": 17870,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 607
                },
                {
                    "start": 610,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 972
                },
                {
                    "start": 975,
                    "end": 1055
                },
                {
                    "start": 1058,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1206
                },
                {
                    "start": 1209,
                    "end": 1393
                },
                {
                    "start": 1396,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1507
                },
                {
                    "start": 1510,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1918
                },
                {
                    "start": 1921,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2093
                }
            ],
            "ref_mentions": [
                {
                    "start": 667,
                    "end": 687,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26806640625
        },
        {
            "corpus_id": "267740312",
            "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
            "text": "In this paper, we delve into the measurement of sample difficulty from the model's perspective. Drawing inspiration from the learning order metric in (Mekala et al., 2022a), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered. We then select the most difficult sample subsets based on this ranking and instruction-tune a language model. Our experiments involve two instruction-tuning datasets, Alpaca-Data (Taori et al., 2023), and Dolly (Conover et al., 2023), with performance measured using automated metrics such as Al-pacaEval (Li et al., 2023b) and human evaluation. \n\nOur main findings indicate that language models can autonomously select training data, achieving performance equal to or better than training on the entire dataset. Furthermore, this characteristic scales across different model sizes, ranging from smaller ones (1B) to larger ones (13B) \n\n1 in parameters. As the size of the language model increases, we observe a consistent reduction in the minimum amount of data needed to surpass the performance of a model trained on the entire dataset. Interestingly, we observe that the data hardness also transfers across models, meaning samples considered difficult by smaller models are similarly challenging for larger models. Moreover, we note that this transferability improves with the size of the smaller model, eventually achieving comparable quality, beyond a size threshold, to that attained by selfselection conducted by larger models. Our study employs open-sourced models such as OPT (Zhang et al., 2022) and Llama-2 (Touvron et al., 2023) to support these findings. The remainder of the paper is structured as follows: initially, we describe the experimental setup encompassing the language models, the datasets employed, and the evaluation metrics utilized (section 2). Subsequently, we present our learning percentage-based difficulty metric and analyze it in detail (section 3). Following this, we optimize the proposed metric and introduce an equally effective, approximate, and faster metric (section 4). Ultimately, we analyze the challenging data identified through this metric (section 5).",
            "score": 0.43100096255817705,
            "section_title": "arXiv:2402.10430v1 [cs.CL] 16 Feb 2024",
            "char_start_offset": 1531,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 775
                },
                {
                    "start": 778,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1064
                },
                {
                    "start": 1067,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 2002
                },
                {
                    "start": 2003,
                    "end": 2113
                },
                {
                    "start": 2114,
                    "end": 2241
                },
                {
                    "start": 2242,
                    "end": 2329
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 172,
                    "matchedPaperCorpusId": "249060677"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9140625
        },
        {
            "corpus_id": "3405508",
            "title": "Self Paced Deep Learning for Weakly Supervised Object Detection",
            "text": "Many recent studies have shown that selecting a subset of \"good\" samples for training a classifier can lead to better results than using all the samples [16,17,24,27,29]. A pioneering work in this direction is the curriculum learning approach proposed in [2]. The authors show that suitably sorting the training samples, from the easiest to the most difficult, and iteratively training a classifier starting with a subset of easy samples (which is progressively augmented with more and more difficult samples), can be useful to find better local minima. In [5], easy and difficult images (taken from datasets known to be more or less \"difficult\") are provided for training a Convolutional Neural Network (CNN) in order to learn generic CNN features using webly annotated data. In [30], different and progressively more complex CNNs are trained for a segmentation task, using more and more difficult data samples together with the output of the previously learned nets. It is worth noting that in this and in all the other curriculum-learning-based approaches, the order of the samples is provided by an external teacher (thus it is a supervised metadatum), usually involving human domain-specific expertise. Since our goal is to let the net select as positives those BBs which are most likely to be correct without using human intervention, manually selecting the \"good\" BBs is approximately equivalent to a standard supervised scenario in which BB-level ground truth is provided. \n\nCurriculum learning was extended to self-paced learning in [15]. The main difference between the two paradigms is that in self-paced learning the order of the samples is automatically computed and is a priori unknown. The selection of the \"easy\" sample set for training is, generally speaking, untractable (it is a subset selection problem). The solution proposed in [15] is based on a continuous relaxation of the problem's constraints and on the optimization of a Structured SVM, with the assumption that both the objective function and the regularizer are convex functions. Supancic et al. \n\n[27] adopt a similar framework in a tracking by detection scenario and train a detector using a subset of video frames, showing that this selection is important to avoid drifting.",
            "score": 0.4307905589578087,
            "section_title": "Related work",
            "char_start_offset": 5103,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1480
                },
                {
                    "start": 1483,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2075
                },
                {
                    "start": 2078,
                    "end": 2257
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 160,
                    "matchedPaperCorpusId": "15615355"
                },
                {
                    "start": 160,
                    "end": 163,
                    "matchedPaperCorpusId": "1554998"
                },
                {
                    "start": 166,
                    "end": 169,
                    "matchedPaperCorpusId": "15602948"
                },
                {
                    "start": 255,
                    "end": 258,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1542,
                    "end": 1546,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 1850,
                    "end": 1854,
                    "matchedPaperCorpusId": "1977996"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36669921875
        },
        {
            "corpus_id": "271212779",
            "title": "Curriculum Learning for Small Code Language Models",
            "text": "In order to explore whether using Curriculum Learning can improve the performance of decoderonly language models trained on code, we adopt the following methodology (Presented in Figure 1) :\n\nWe first generate data (consisting of code snippets followed by their outputs) focusing on a subset of the Python programming language, which allows us to reduce the vocabulary size (section 4).\n\nWe then assess the difficulty of the generated code snippets using our proposed code difficulty metric, which we refer to as the Overall Metric (OM) (section 3) and split the data into three levels -easy, medium, and hard.Next, the models are trained on different Curriculum Learning schedules (section 5).Finally, we evaluate the performance of the models based on token-level and line-level code completion as well as code execution, and compare them to a baseline model trained on all levels of data shuffled together (section 7).Additionally, to investigate the effect of Curriculum Learning on larger pretrained models, we finetuned Code Llama 7B (Rozi\u00e8re et al., 2024) using our best Curriculum Learning schedule and compared it with a baseline finetuning approach where all levels of data are shuffled together (section 7).",
            "score": 0.43059003813745883,
            "section_title": "Overview",
            "char_start_offset": 4043,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 192,
                    "end": 386
                },
                {
                    "start": 388,
                    "end": 610
                },
                {
                    "start": 610,
                    "end": 694
                },
                {
                    "start": 694,
                    "end": 921
                },
                {
                    "start": 921,
                    "end": 1218
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1884765625
        },
        {
            "corpus_id": "271064746",
            "title": "Entropy Law: The Story Behind Data Compression and LLM Performance",
            "text": "Data is the cornerstone of large language models (LLMs), but not all data is useful for model learning. Carefully selected data can better elicit the capabilities of LLMs with much less computational overhead. Most methods concentrate on evaluating the quality of individual samples in data selection, while the combinatorial effects among samples are neglected. Even if each sample is of perfect quality, their combinations may be suboptimal in teaching LLMs due to their intrinsic homogeneity or contradiction. In this paper, we aim to uncover the underlying relationships between LLM performance and data selection. Inspired by the information compression nature of LLMs, we uncover an ``entropy law'' that connects LLM performance with data compression ratio and first-epoch training loss, which reflect the information redundancy of a dataset and the mastery of inherent knowledge encoded in this dataset, respectively. Through both theoretical deduction and empirical evaluation, we find that model performance is negatively correlated to the compression ratio of training data, which usually yields a lower training loss. Based on the findings of the entropy law, we propose a quite efficient and universal data selection method named \\textbf{ZIP} for training LLMs, which aim to prioritize data subsets exhibiting a low compression ratio. Based on a multi-stage algorithm that selects diverse data in a greedy manner, we can obtain a good data subset with satisfactory diversity. Extensive experiments have been conducted to validate the entropy law and the superiority of ZIP across different LLM backbones and alignment stages. We also present an interesting application of entropy law that can detect potential performance risks at the beginning of model training.",
            "score": 0.4304753103877838,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85693359375
        },
        {
            "corpus_id": "231709290",
            "title": "Curriculum Learning: A Survey",
            "text": "In the first part of this section, we focus on the general curriculum learning solutions that have been tested in multiple domains. Two of the main works in this category are the papers that first formulated the vanilla curriculum learning and the self-paced learning paradigms. These works highly influenced the progress of easy-to-hard learning strategies and led to multiple approaches, which have been successfully employed in all domains and in a wide range of tasks. Bengio et al. (2009) introduce a set of easy-to-hard learning strategies for automatic models, referred to as curriculum learning (CL). The idea of presenting the examples in a meaningful order, starting from the easiest samples, then gradually introducing more complex ones, was inspired by the way humans learn. To show that automatic models benefit from such a training strategy, achieving faster convergence, while finding a better local minimum, the authors conduct multiple experiments. They start with toy experiments with a convex criterion in order to analyze the impact of difficulty on the final result. They find that, in some cases, easier examples can be more informative than more complex ones. Additionally, they discover that feeding a perceptron with the samples in increasing order of difficulty performs better than the standard random sampling approach or than a hardto-easy methodology (anti-curriculum). Next, they focus on shape recognition, generating two artificial data sets: Basic-Shapes and GeomShapes, with the first one being designed to be easier, with less variability in terms of shape. They train a neural network on the easier set until a switch epoch when they start training on the GeomShapes set. The evaluation is conducted only on the difficult data, with the curriculum approach generating better results than the standard training method. The methodology above can be considered an adaptation of transfer learning, where the network was pretrained on a similar, but easier, data set. Finally, the authors conduct language modeling experiments for predicting the best word which could follow a sequence of words in correct English. The curriculum strategy is built by iterating over Wikipedia and selecting the most frequent 5000 words from the vocabulary at each step. This vocabulary enhancement method compares favorably to conventional training. Still, their experiments are constructed in a way that enables the easy and the difficult examples to be easily separated. In practice, finding a way to rank the training examples can be a complex task",
            "score": 0.4301995630256954,
            "section_title": "Multi-Domain Approaches",
            "char_start_offset": 29136,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 473,
                    "end": 493,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.303955078125
        },
        {
            "corpus_id": "273229005",
            "title": "Emergent properties with repeated examples",
            "text": "Prior works have studied the role of data reuse in language models. 2024) undertake a controlled study on synthetic language data in the context of knowledge retrieval and find that knowledge augmentation -repeated inclusion of reformulated variants -of a small subset of the data leads to performance improvement; an effect somewhat akin to what we observe in two-set training. \n\nOur work is related to, but different from, curriculum learning (CL) (Bengio et al., 2009;Wang et al., 2022), where training data is presented in a meaningful order, usually from \"easy\" to \"hard\" samples. Two-set training differs from curriculum learning in at least two important ways: in CL, datasets are curated, our subsets are completely random; in CL, the training distribution shifts over time, while our subsets are static. Our ablations show that curating the repeated set, or changing it over time, as in CL, brings no improvement in performance (and may even have an adverse effect). \n\nLastly, our work touches upon the expansive area of out-of-distribution (OOD) generalization (Gulrajani & Lopez-Paz, 2021;Lopez-Paz, 2025), which studies generalization when train and test distributions differ. Curiously, while our two-set approach increases the frequency of some training examples, because the repeated set is chosen at random, the training set remains distributionally equivalent to the test set. Thus, our study falls outside the usual framework of OOD studies.",
            "score": 0.42962937301320664,
            "section_title": "BACKGROUND AND RELATED WORK",
            "char_start_offset": 7521,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 378
                },
                {
                    "start": 381,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1459
                }
            ],
            "ref_mentions": [
                {
                    "start": 450,
                    "end": 471,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 471,
                    "end": 489,
                    "matchedPaperCorpusId": "232362223"
                },
                {
                    "start": 1071,
                    "end": 1100,
                    "matchedPaperCorpusId": "220347682"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38671875
        },
        {
            "corpus_id": "231709290",
            "title": "Curriculum Learning: A Survey",
            "text": ", or performance measure level. The traditional curriculum learning approach enforces an easy-to-hard re-ranking of the examples, with the criterion, or the difficulty metric, being task-dependent, such as the use of shape complexity (a) General framework for data-level curriculum learning.\n\n(b) General framework for model-level curriculum. Algorithm 1 General curriculum learning algorithm M -a machine learning model; E -a training data set; P -performance measure; n -number of iterations / epochs; C -curriculum criterion / difficulty measure; l -curriculum level; S -curriculum scheduler; 1: for t \u2208 1, 2, ..., n do 2:\n\np \u2190 P(M) 3:\n\nif S(t, p) = true then 4:\n\nM, E, P \u2190 C(l, M, E, P) 5: end if 6: E * \u2190 select(E) 7:\n\nM \u2190 train(M, E * , P) 8: end for for images (Bengio et al. 2009;Duan et al. 2020), grammar properties for text (Kocmi and Bojar 2017;Liu et al. 2018) and signal-to-noise ratio for audio (Braun et al. 2017;Ranjan and Hansen 2017). Nevertheless, more general methods can be applied when generating the curriculum, e.g., supervising the learning by a teacher network (teacher-student) (Jiang et al. 2018;Wu et al. 2018;Kim and Choi 2018) or taking into consideration the learning progress of the model (selfpaced learning) (Kumar et al. 2010;Jiang et al. 2014bJiang et al. , 2015Zhao et al. 2015;Zhang et al. 2015a). The easy-to-hard ordering can also be applied when multiple tasks are involved, determining the best order to learn the tasks to maximize the final result (Pentina et al. 2015;Zhang et al. 2017b;Lotter et al. 2017;Sarafianos et al. 2017;Florensa et al. 2017;Matiisen et al. 2019). A special type of methodology is when the curriculum is applied at the model level, adapting various elements of the model during its training (Morerio et al. 2017;Karra",
            "score": 0.428540967731345,
            "section_title": "Curriculum Learning",
            "char_start_offset": 12865,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 768,
                    "end": 788,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 788,
                    "end": 805,
                    "matchedPaperCorpusId": "213005467"
                },
                {
                    "start": 835,
                    "end": 857,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 857,
                    "end": 873,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 910,
                    "end": 929,
                    "matchedPaperCorpusId": "14928979"
                },
                {
                    "start": 929,
                    "end": 952,
                    "matchedPaperCorpusId": "19805513"
                },
                {
                    "start": 1106,
                    "end": 1125,
                    "matchedPaperCorpusId": "51876228"
                },
                {
                    "start": 1125,
                    "end": 1140,
                    "matchedPaperCorpusId": "53103412"
                },
                {
                    "start": 1244,
                    "end": 1263,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 1263,
                    "end": 1281,
                    "matchedPaperCorpusId": "9686483"
                },
                {
                    "start": 1281,
                    "end": 1300,
                    "matchedPaperCorpusId": "10891229"
                },
                {
                    "start": 1300,
                    "end": 1317,
                    "matchedPaperCorpusId": "15188879"
                },
                {
                    "start": 1317,
                    "end": 1336,
                    "matchedPaperCorpusId": "15229758"
                },
                {
                    "start": 1493,
                    "end": 1514,
                    "matchedPaperCorpusId": "8502955"
                },
                {
                    "start": 1514,
                    "end": 1533,
                    "matchedPaperCorpusId": "260497341"
                },
                {
                    "start": 1533,
                    "end": 1552,
                    "matchedPaperCorpusId": "33313018"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0548095703125
        },
        {
            "corpus_id": "232222906",
            "title": "Learning Policies for Multilingual Training of Neural Machine Translation Systems",
            "text": "Curriculum learning (Bengio et al., 2009;Elman, 1993;Rohde and Plaut, 1994) hypothesizes that presenting training samples in a meaningful order to machine learners during training may help improve model quality and convergence speed. In the field of Neural Machine Translation (NMT) most curricula are hand designed e.g., fine-tuning (Luong and Manning, 2015;Freitag and Al-Onaizan, 2016) and data selection (Moore and Lewis, 2010;Axelrod et al., 2011;Duh et al., 2013;Durrani et al., 2016). Another common curriculum is one based on ordering samples from easy to hard using linguistic features and auxiliary model scores (Zhang et al., 2018(Zhang et al., , 2019) ) but these are hard to tune, relying to extensive trial and error to find the right hyperparameters. Attempts to learn a curriculum jointly with the NMT training setup (Kumar et al., 2019) can suffer from observation sparsity, where a single training run does not provide enough training samples for an external agent to learn a good curriculum policy. \n\nOur NMT task of choice in this paper will be lowresource multi-lingual NMT (MNMT). While standard NMT systems typically deal with a language pair, the source and the target, an MNMT model may have multiple languages as source and/or target. Most large-scale MNMT models are trained using some form of model parameter sharing (Johnson et al., 2017;Aharoni et al., 2019;Arivazhagan et al., 2019;Bapna and Firat, 2019). The notion of how input data should be presented to the MNMT system during training only finds prominence in the case of low-resource MNMT. A typical lowresource task will try to leverage a high-resource language pair to aid the training of an NMT system for a low-resource (very small or no parallel data available) and related language-pair of interest.",
            "score": 0.4283690286497398,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 1017
                },
                {
                    "start": 1020,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1792
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 41,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 53,
                    "end": 75,
                    "matchedPaperCorpusId": "961169"
                },
                {
                    "start": 334,
                    "end": 359,
                    "matchedPaperCorpusId": "14646537"
                },
                {
                    "start": 408,
                    "end": 431,
                    "matchedPaperCorpusId": "8170227"
                },
                {
                    "start": 431,
                    "end": 452,
                    "matchedPaperCorpusId": "10766958"
                },
                {
                    "start": 452,
                    "end": 469,
                    "matchedPaperCorpusId": "2030497"
                },
                {
                    "start": 469,
                    "end": 490,
                    "matchedPaperCorpusId": "1169193"
                },
                {
                    "start": 641,
                    "end": 665,
                    "matchedPaperCorpusId": "155089817"
                },
                {
                    "start": 833,
                    "end": 853,
                    "matchedPaperCorpusId": "67855673"
                },
                {
                    "start": 1345,
                    "end": 1367,
                    "matchedPaperCorpusId": "260464809"
                },
                {
                    "start": 1367,
                    "end": 1388,
                    "matchedPaperCorpusId": "67855815"
                },
                {
                    "start": 1413,
                    "end": 1435,
                    "matchedPaperCorpusId": "202660912"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.658203125
        },
        {
            "corpus_id": "256416071",
            "title": "MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning",
            "text": "MILO Framework: In this study, we introduce MILO, a model-agnostic subset selection framework designed for efficient model training and tuning. MILO employs submodular measures [13,23], which capture higher-order interactions between data samples for subset selection. We utilize pre-trained large language models [55] and pre-trained vision transformers [25] as feature encoders due to their zero-shot feature encoding capabilities. These encoders compute the sample metrics in a nominal amount of time, and this computation is independent of the downstream model, rendering MILO model-agnostic. Figure 3 provides a visual representation of MILO for model training, comprising two steps: a) A pre-processing step that involves the selection of multiple subsets from the training dataset using \"Stochastic-Greedy Exploration (SGE)\" (refer to Section 3.1.1), and the construction of a probability distribution over the entire dataset for subset sampling through \"Weighted Random Exploration (WRE)\" (refer to Section 3.1.2); b) A model training scheme that involves training the model on a curriculum of easy-to-hard subsets (refer to Section 3.1.3) using the subsets selected and the probability distribution constructed in the pre-processing step. We also present a class-wise partitioning trick to significantly minimize the memory footprint of MILO (refer to Section 3.2).",
            "score": 0.42822084476839545,
            "section_title": "Contributions",
            "char_start_offset": 4500,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1374
                }
            ],
            "ref_mentions": [
                {
                    "start": 355,
                    "end": 359,
                    "matchedPaperCorpusId": "230435805"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49658203125
        },
        {
            "corpus_id": "231979380",
            "title": "Analyzing Curriculum Learning for Sentiment Analysis along Task Difficulty, Pacing and Visualization Axes",
            "text": "Learning in humans has always been a systematic approach of handling the fundamentals first and then learning incrementally harder concepts. Cognitive Science has established that this leads to a clearer, robust understanding and the most efficient learning (Krueger and Dayan, 2009;Avrahami et al., 1997). Indeed, something similar can be applied while training neural networks. (Bengio et al., 2009) show that Curriculum Learning (CL) -sampling data based on increasing order of difficulty leads to quicker generalization. (Weinshall *The authors contributed equally to the work. et al., 2018) also demonstrate that CL increases the rate of convergence at the beginning of training. Their CL strategy involved sorting the training data based on transfer learning from another network trained on a larger dataset. The idea of reordering samples has been explored in various approaches. In this paper, we evaluate the \"easiness\" with a network and train the samples on another network. Hence, even in our case, we shall pick easier points regarding a target hypothesis then train another network that optimizes its current hypothesis. This idea has been suggested by previous works as well. (Hacohen and Weinshall, 2019;Weinshall et al., 2018) (Cirik et al., 2016) proposed Baby Steps and One Pass curriculum techniques using sentence length as a curriculum strategy for training LSTM (Hochreiter and Schmidhuber, 1997) on Sentiment Analysis. A tree-structured curriculum ordering based on semantic similarity is proposed by (Han and Myaeng, 2017). (Rao et al., 2020) propose an auxiliary network that is first trained on the dataset and used to calculate difficulty scores for the curriculum order. \n\nCL is also used by NLP in tasks like Question Answering (Sachan andXing, 2016, 2018) and NLG for Answer Generation (Liu et al., 2018). For Sentiment Analysis, (Cirik et al., 2016) propose a strategy derived from sentence length, where smaller sentences are considered easier and are provided first. (Han and Myaeng, 2017) provide a tree-structured curriculum based on semantic similarity between new samples and samples already trained.",
            "score": 0.42818063622396446,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1699
                },
                {
                    "start": 1702,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2138
                }
            ],
            "ref_mentions": [
                {
                    "start": 258,
                    "end": 283,
                    "matchedPaperCorpusId": "206863670"
                },
                {
                    "start": 380,
                    "end": 401,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1385,
                    "end": 1419,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1525,
                    "end": 1547,
                    "matchedPaperCorpusId": "21814386"
                },
                {
                    "start": 1817,
                    "end": 1835,
                    "matchedPaperCorpusId": "51606954"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.347412109375
        },
        {
            "corpus_id": "202573016",
            "title": "Towards Open-Domain Named Entity Recognition via Neural Correction Models",
            "text": "Curriculum learning (Bengio et al. 2009) is a training strategy proposed by Bengio et al. in the context of machine learning. They demonstrate that models can be better trained when the inputs are not randomly presented but organized in a meaningful order, such as from easy to hard. Inspired by this thought, we rank all sentences in the correction dataset from easy to hard and split the correction dataset into three sets which are input into the correction model in order. Specifically, we calculate an F 1 score f i for each sentence in the correction dataset with the corresponding entity label l i and l i (see Figure 3 for the distribution). We remove the sentences whose F 1 scores are lower than 0.1. Then, we rank all the sentences in the correction dataset according to their F 1 score from high to low and split the correction dataset into three sets D 1 , D 2 and D 3 . That means in D 1 , the sentence has more similar labels l i and l i and D 1 is easier for the correction model to learn. Similarly, D 3 is more difficult for the model to learn. We input the three sets from D 1 to D 3 and train our correction model with each set for five epochs.",
            "score": 0.42793920562815724,
            "section_title": "Curriculum Learning",
            "char_start_offset": 11999,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1164
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 39,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.482666015625
        },
        {
            "corpus_id": "215746703",
            "title": "Curriculum Learning Strategies for IR",
            "text": "Neural ranking models are traditionally trained on a series of random batches, sampled uniformly from the entire training set. Curriculum learning has recently been shown to improve neural models\u2019 effectiveness by sampling batches non-uniformly, going from easy to difficult instances during training. In the context of neural Information Retrieval (IR) curriculum learning has not been explored yet, and so it remains unclear (1) how to measure the difficulty of training instances and (2) how to transition from easy to difficult instances during training. To address both challenges and determine whether curriculum learning is beneficial for neural ranking models, we need large-scale datasets and a retrieval task that allows us to conduct a wide range of experiments. For this purpose, we resort to the task of conversation response ranking: ranking responses given the conversation history. In order to deal with challenge (1), we explore scoring functions to measure the difficulty of conversations based on different input spaces. To address challenge (2) we evaluate different pacing functions, which determine the velocity in which we go from easy to difficult instances. We find that, overall, by just intelligently sorting the training data (i.e., by performing curriculum learning) we can improve the retrieval effectiveness by up to 2% (The source code is available at https://github.com/Guzpenha/transformers_cl.).",
            "score": 0.4277230472906088,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1964111328125
        },
        {
            "corpus_id": "53295888",
            "title": "An Empirical Exploration of Curriculum Learning for Neural Machine Translation",
            "text": "Let (x, y) be a bitext example, where x is the source sentence and y is the target reference translation. We use subscripts i to denote the sample index and assume a training set D = {(x i , y i )} i=1,2,...S of size S. Curriculum learning can be formulated in a probabilistic view, where each sentence pair (x i , y i ) has a probability of being selected for training, and this sampling probability changes depending on the difficulty of the example and the curriculum schedule (Bengio et al., 2009). Specifically, we segment the curriculum schedule into distinct phases t which correspond to different time points during training. For instance, t = 1 could be the first N checkpoints, t = 2 is the next N checkpoints, etc. The definition of phases is flexible: alternatively t = 1 may correspond to the first epoch, and t = 2 may correspond to the second epoch (or more). At each phase t, we maintain a multinomial distribution q t i over the examples in D, i.e. S i=1 q t i = 1 and q t i \u2265 0 \u2200i. To implement the curriculum schedule that begins with easy examples, we would start at t = 1 by setting q t i to be high for easy examples and q t i to be low (or zero) for difficult examples. Gradually, for large t, we increase q t i for the more difficult examples. At some point, all examples have equal probability of being selected; this corresponds to the standard training procedure. An illustration of this probabilistic view of curriculum learning is shown in Figure 1. There are two advantages to this probabilistic sampling view of curriculum learning: \n\n1. It is a flexible framework that enables the design of various kinds of curriculum schedules. By specifying different kinds of distributions, one can perform easy-to-difficult training or the reverse difficult-to-easy training. One can default to uniform sampling, which corresponds to standard training with random mini-batches. Many of these variants are described in Section 5.2. \n\n2. It is simple to implement in existing deep learning frameworks, requiring only a modification of the data sampling procedure.",
            "score": 0.4268795056642635,
            "section_title": "A Probabilistic View of Curriculum Learning",
            "char_start_offset": 7024,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1563
                },
                {
                    "start": 1566,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 1950
                },
                {
                    "start": 1953,
                    "end": 2081
                }
            ],
            "ref_mentions": [
                {
                    "start": 480,
                    "end": 501,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.265380859375
        },
        {
            "corpus_id": "237351587",
            "title": "DCUR: Data Curriculum for Teaching via Samples with Reinforcement Learning",
            "text": "The data D (T ) comes from the teacher's experienced samples from its training history. Thus there is a natural ordering in D (T ) , in which earlier data tuples (i.e., smaller time indices) tend to have lower reward on average than later data tuples due to the progression of teacher training. The curricula we test in the paper all rely on restricting the range of time indices in D (T ) which are accessible to the student, and in particular, they result in a range of consecutive time indices. We hypothesized that the stronger performance of curricula that allow the available data in D (T ) to grow over time could be due to the increasing reward (on average) from the data tuples. Thus, we tried first reordering all 1M data tuples in D (T ) to form a newer dataset where all data tuples are ordered by increasing reward, in that the following data indexing relation holds: \n\n\u2200 0 \u2264 i < j < 1M, we have r i \u2264 r j in tuples (s i , a i , r i , s i+1 ) and (s j , a j , r j , s j+1 ). \n\nWe experimented with the two primary curricula from the main part of the paper, C add (t; f = 1M ) and C scale (t; c = 1.00). However, this reordering of the buffer did not result in performance gains over simply keeping the original data ordering in D (T ) , suggesting that there is something more fundamental than just the raw reward. We also tried ranking data tuples according to the episodic reward, and then reordering based on that, but this did not yield gains.",
            "score": 0.4268795056642635,
            "section_title": "B. Reordering Tuples Based on Reward",
            "char_start_offset": 37928,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 987
                },
                {
                    "start": 990,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1460
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.012969970703125
        },
        {
            "corpus_id": "265351926",
            "title": "Complexity-Guided Curriculum Learning for Text Graphs",
            "text": "In Curriculum learning (CL) (Bengio et al., 2009) data samples are scheduled in a meaningful difficulty order, typically from easy to hard, for iterative training. In graph machine learning, Wang et al. (2021) introduced CurGraph, a curriculum learning method designed for sub-graph classification. This X X X X X X X X  2019) proposed a dynamic curriculum approach that automatically assigns confidence scores to samples based on their estimated difficulty. However this model requires additional trainable parameters. To address this limitation, Castells et al. (2020) introduced the SuperLoss framework to calculate optimal confidence scores for each instance using a closed-form solution. In (Vakil and Amiri, 2022), we extended SuperLoss to incorporate trend information at the sample level. We utilized loss trajectories to estimate the emerging difficulty of subgraphs and employed weighted sample losses for data scheduling in order to create effective curricula for training GNNs and understanding their learning dynamics. \n\nCurrent curriculum learning methodologies in NLP rely on data properties, e.g., sentence length, word rarity, or syntactic features (Platanios et al., 2019;Liu et al., 2021), or annotation disagreement (Elgaar and Amiri, 2023); as well as model properties such as training loss and its variations (Graves et al., 2017;Amiri et al., 2017) to sequence data samples for training. Elgaar and Amiri (2023) developed a curriculum discovery framework based on prior knowledge of sample difficulty, utilized annotation entropy and loss values. They concluded that curricula based on easy-to-hard or hard-to-easy transition are often at the risk of under-performing, effective curricula are often non-monotonic, and curricula learned from smaller datasets perform well on larger datasets. \n\nOther instances of curriculum learning for textual data have primarily centered on machine translation and language comprehension. For instance, Agrawal and Carpuat (2022) introduced a framework for training non-autoregressive sequence-tosequence models for text editing.",
            "score": 0.426784613409792,
            "section_title": "Related Work",
            "char_start_offset": 21966,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 1031
                },
                {
                    "start": 1034,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1813
                },
                {
                    "start": 1816,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2087
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 49,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 191,
                    "end": 209,
                    "matchedPaperCorpusId": "235324744"
                },
                {
                    "start": 548,
                    "end": 570,
                    "matchedPaperCorpusId": "227275456"
                },
                {
                    "start": 696,
                    "end": 719,
                    "matchedPaperCorpusId": "248863268"
                },
                {
                    "start": 1166,
                    "end": 1190,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 1190,
                    "end": 1207,
                    "matchedPaperCorpusId": "236460239"
                },
                {
                    "start": 1236,
                    "end": 1260,
                    "matchedPaperCorpusId": "259370648"
                },
                {
                    "start": 1331,
                    "end": 1352,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 1352,
                    "end": 1371,
                    "matchedPaperCorpusId": "1916665"
                },
                {
                    "start": 1411,
                    "end": 1434,
                    "matchedPaperCorpusId": "259370648"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26220703125
        },
        {
            "corpus_id": "7403346",
            "title": "Learning to select data for transfer learning with Bayesian Optimization",
            "text": "Most prior work on data selection for transfer learning focuses on phrase-based machine translation. Typically language models are leveraged via perplexity or cross-entropy scoring to select target data (Moore and Lewis, 2010;Axelrod et al., 2011;Duh et al., 2013;Mirkin and Besacier, 2014). A recent study investigates data selection for neural machine translation (van der Wees et al., 2017). Perplexity was also used to select training data for dependency parsing (S\u00f8gaard, 2011), but has been found to be less suitable for tasks such as sentiment analysis (Ruder et al., 2017). In general, there are fewer studies on data selection for other tasks, e.g., constituent parsing (McClosky et al., 2010), dependency parsing (Plank and van Noord, 2011;S\u00f8gaard, 2011) and sentiment analysis (Remus, 2012). Work on predicting task accuracy is related, but can be seen as complementary (Ravi et al., 2008;Van Asch and Daelemans, 2010). \n\nMany domain similarity metrics have been proposed. Blitzer et al. (2007) show that proxy A distance can be used to measure the adaptability between two domains in order to determine examples for annotation. Van Asch and Daelemans (2010) find that R\u00e9nyi divergence outperforms other metrics in predicting POS tagging accuracy, while Plank and van Noord (2011) observe that topic distribution-based representations with Jensen-Shannon divergence perform best for data selection for parsing. Remus (2012) apply Jensen-Shannon divergence to select training examples for sentiment analysis. Finally, Wu and Huang (2016) propose a similarity metric based on a sentiment graph. We test previously explored similarity metrics and complement them with diversity. \n\nVery recently interest emerged in curriculum learning (Bengio et al., 2009). It is inspired by human active learning by providing easier examples at initial learning stages (e.g., by curriculum strategies such as growing vocabulary size). Curriculum learning employs a range of data metrics, but aims at altering the order in which the entire training data is selected, rather than selecting data.",
            "score": 0.4266271516981342,
            "section_title": "Related work",
            "char_start_offset": 24617,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 930
                },
                {
                    "start": 933,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1686
                },
                {
                    "start": 1689,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2086
                }
            ],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 226,
                    "matchedPaperCorpusId": "8170227"
                },
                {
                    "start": 226,
                    "end": 247,
                    "matchedPaperCorpusId": "10766958"
                },
                {
                    "start": 247,
                    "end": 264,
                    "matchedPaperCorpusId": "2030497"
                },
                {
                    "start": 264,
                    "end": 290,
                    "matchedPaperCorpusId": "15488580"
                },
                {
                    "start": 375,
                    "end": 393,
                    "matchedPaperCorpusId": "7921428"
                },
                {
                    "start": 679,
                    "end": 702,
                    "matchedPaperCorpusId": "10585087"
                },
                {
                    "start": 723,
                    "end": 750,
                    "matchedPaperCorpusId": "7362580"
                },
                {
                    "start": 881,
                    "end": 900,
                    "matchedPaperCorpusId": "9429298"
                },
                {
                    "start": 900,
                    "end": 929,
                    "matchedPaperCorpusId": "7986308"
                },
                {
                    "start": 1140,
                    "end": 1169,
                    "matchedPaperCorpusId": "7986308"
                },
                {
                    "start": 1265,
                    "end": 1291,
                    "matchedPaperCorpusId": "7362580"
                },
                {
                    "start": 1528,
                    "end": 1547,
                    "matchedPaperCorpusId": "17859685"
                },
                {
                    "start": 1743,
                    "end": 1764,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88427734375
        },
        {
            "corpus_id": "257937538",
            "title": "A Review of the Evaluation System for Curriculum Learning",
            "text": "Curriculum learning as a model training strategy was first defined as a sequence of training criteria C [1], where Q t (z) is a re-weighting W t (z) of the original data distribution P(z) and z refers to a random variable in the data set (e.g., (x,y) in supervised learning). \n\nwhich satisfies: (i) Gradually increasing the diversity and information of the training subset. \n\n(ii) Gradually, more samples are added for training W t (z) \u2264 W t+1 (z). (iii) Eventually, the weights of all samples are unified and trained on the whole dataset Q T (z) = P(z). \n\nHowever, with the development of curriculum learning in recent years, more research scholars have extended curriculum learning methods to the task and model levels and even discarded the original three restriction rules. Here we divide curriculum learning into three major stages: difficulty evaluation, training schedule, and model evaluation, which are combined in a general framework according to the objects applied at three major levels: data, task, and model (Figure 3). Let the original dataset be E or task set  Phase 1: Difficulty evaluation. Determine the sample difficulty evaluation metrics for the task, design a difficulty evaluator D to evaluate the difficulty of the samples, and construct a training list L or grouping from easy to difficult, where easy z refers to easier samples and hard z refers to more difficult samples: where the list L is not limited to the order from easy to difficult. (3) Phase 1: Difficulty evaluation. Determine the sample difficulty evaluation metrics for the task, design a difficulty evaluator D to evaluate the difficulty of the samples, and construct a training list L or grouping from easy to difficult, where z easy refers to easier samples and z hard refers to more difficult samples: where the list L is not limited to the order from easy to difficult. L = z easy , . . . , z i , . . . , z hard i<n , z \u2208 E \n\nAlgorithm 1 demonstrates the curriculum learning framework. \n\nStep 1: The dataset E is evaluated for difficulty using a difficulty evaluator D to generate a sample list of increasing difficulty. \n\nStep 2: The sample list is sampled using the training scheduler to generate the initial training set e.",
            "score": 0.42647472010595533,
            "section_title": "Method Definition of Curriculum Learning",
            "char_start_offset": 22523,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 275
                },
                {
                    "start": 278,
                    "end": 373
                },
                {
                    "start": 376,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 554
                },
                {
                    "start": 557,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 1918
                },
                {
                    "start": 1921,
                    "end": 1980
                },
                {
                    "start": 1983,
                    "end": 2115
                },
                {
                    "start": 2118,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 104,
                    "end": 107,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08587646484375
        },
        {
            "corpus_id": "237553761",
            "title": "On the Role of Corpus Ordering in Language Modeling",
            "text": "a curriculum for machine learning models. The intuition behind this approach is to mimic the manner in which humans learn. Training samples are organized by levels of difficulty and training proceeds in steps where the model is first trained on a subset of the corpus at a given difficulty level before being trained on another difficulty level, and so on.\n\nFor our experiments, we pretrain from scratch multiple models of BERT BASE using the WikiText-103 corpus (Merity et al., 2016), a collection of English articles from Wikipedia. We explored standard length-based metrics of complexity as well as document-level complexity using Flesch Reading Ease readability index to organize the corpora into a curriculum. We also investigated two different methods of accessing such an ordered training setone where the bins remain disjoint (BINNED) and another where the bins are incrementally joined together to increase the training data size (STEPPED) over n epochs. An extensive evaluation is conducted in terms of metrics related to not just pretraining but also fine-tuning on eight downstream tasks, specifically the suite of datasets from the GLUE benchmark (Wang et al., 2018).\n\nOur results demonstrate that while sentencelevel and document-level complexity metrics work comparably well, they outperform the vanilla models in all experiments. Furthermore, while easy to hard is a good strategy when complexity is computed at sentence-level, the reverse is true when complexity is measured at document-level. Finally, the most encouraging results suggest that corpus ordering takes significantly less time (measured in terms of training steps) as compared to vanilla training, while yielding comparable results as measured by the average GLUE score.\n\nIn summary, this work makes the following contributions,\n\n\u2022 We propose a novel paradigm defined by document-level metrics of text complexity for ordering a training corpus. \u2022 We explore two strategies for learning from such a curriculum in the context of pretraining of language models. \u2022 We conduct extensive experiments by training several versions of transformer model from scratch and evaluating their performance in terms of metrics computed at both pretraining and fine-tuning stages.",
            "score": 0.4253602135586808,
            "section_title": "Introduction",
            "char_start_offset": 2108,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1160,
                    "end": 1179,
                    "matchedPaperCorpusId": "5034059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70751953125
        },
        {
            "corpus_id": "253581382",
            "title": "EfficientTrain: Exploring Generalized Curriculum Learning for Training Visual Backbones",
            "text": "Curriculum learning is a training paradigm inspired by the organized learning order of examples in human cur-ricula [23,24,8]. This idea has been widely explored in the context of training deep networks from easier data to harder data [25,11,16,12,26,27,9,10]. Typically, a pre-defined [8,28,29,30] or automatically-learned [25,31,32,11,33,16,14,34,35,12,36] difficulty measurer is deployed to differentiate between easier and harder samples, while a scheduler [8,11,26,9] is defined to determine when and how to introduce harder training data. Our method is also based on the 'starting small' spirit [23], but we always leverage all the training data simultaneously. Our work is also related to curriculum by smoothing [37] and curriculum dropout [38], which do not perform example selection as well. However, our method is orthogonal to them since we reduce the training cost by modifying the model inputs, while they regularize deep features during training (e.g., via anti-aliasing smoothing or feature dropout). \n\nProgressive or modularized training. Deep networks can be trained efficiently by increasing the model size during training, e.g., a growing number of layers [39,40,41], a growing width [42], or a dynamically changed network connection topology [43,44,45]. These methods are mainly motivated by that smaller models are more efficient to train at earlier epochs. This idea is also explored in language models [46,47], recommendation systems [48] and graph ConvNets [49]. Locally supervised learning, which trains different model components using tailored objectives, is a promising direction as well [50,51,52]. \n\nA similar work to us is progressive learning (PL) [53], which down-samples the images to save the training cost.",
            "score": 0.4252331876813046,
            "section_title": "Related Work",
            "char_start_offset": 4320,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 1016
                },
                {
                    "start": 1019,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1628
                },
                {
                    "start": 1631,
                    "end": 1743
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 120,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 120,
                    "end": 123,
                    "matchedPaperCorpusId": "206863670"
                },
                {
                    "start": 123,
                    "end": 125,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 235,
                    "end": 239,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 239,
                    "end": 242,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 245,
                    "end": 248,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 248,
                    "end": 251,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 251,
                    "end": 254,
                    "matchedPaperCorpusId": "252367384"
                },
                {
                    "start": 256,
                    "end": 259,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 286,
                    "end": 289,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 289,
                    "end": 292,
                    "matchedPaperCorpusId": "5658192"
                },
                {
                    "start": 295,
                    "end": 298,
                    "matchedPaperCorpusId": "879067"
                },
                {
                    "start": 324,
                    "end": 328,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 328,
                    "end": 331,
                    "matchedPaperCorpusId": "21323057"
                },
                {
                    "start": 331,
                    "end": 334,
                    "matchedPaperCorpusId": "207216972"
                },
                {
                    "start": 334,
                    "end": 337,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 337,
                    "end": 340,
                    "matchedPaperCorpusId": "5004002"
                },
                {
                    "start": 343,
                    "end": 346,
                    "matchedPaperCorpusId": "51876228"
                },
                {
                    "start": 346,
                    "end": 349,
                    "matchedPaperCorpusId": "4321928"
                },
                {
                    "start": 349,
                    "end": 352,
                    "matchedPaperCorpusId": "52114770"
                },
                {
                    "start": 352,
                    "end": 355,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 355,
                    "end": 358,
                    "matchedPaperCorpusId": "8432394"
                },
                {
                    "start": 461,
                    "end": 464,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 464,
                    "end": 467,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 467,
                    "end": 470,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 601,
                    "end": 605,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 720,
                    "end": 724,
                    "matchedPaperCorpusId": "220301592"
                },
                {
                    "start": 1176,
                    "end": 1180,
                    "matchedPaperCorpusId": "14124313"
                },
                {
                    "start": 1180,
                    "end": 1183,
                    "matchedPaperCorpusId": "2343056"
                },
                {
                    "start": 1183,
                    "end": 1186,
                    "matchedPaperCorpusId": "3568073"
                },
                {
                    "start": 1204,
                    "end": 1208,
                    "matchedPaperCorpusId": "6702706"
                },
                {
                    "start": 1263,
                    "end": 1267,
                    "matchedPaperCorpusId": "395154"
                },
                {
                    "start": 1267,
                    "end": 1270,
                    "matchedPaperCorpusId": "218774922"
                },
                {
                    "start": 1270,
                    "end": 1273,
                    "matchedPaperCorpusId": "233204639"
                },
                {
                    "start": 1426,
                    "end": 1430,
                    "matchedPaperCorpusId": "174799716"
                },
                {
                    "start": 1430,
                    "end": 1433,
                    "matchedPaperCorpusId": "225066769"
                },
                {
                    "start": 1458,
                    "end": 1462,
                    "matchedPaperCorpusId": "234482104"
                },
                {
                    "start": 1482,
                    "end": 1486,
                    "matchedPaperCorpusId": "219633710"
                },
                {
                    "start": 1621,
                    "end": 1624,
                    "matchedPaperCorpusId": "59222702"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.226806640625
        },
        {
            "corpus_id": "247518847",
            "title": "An Imitation Learning Curriculum for Text Editing with Non-Autoregressive Models",
            "text": "Training Dynamics To verify that curriculum learning helps our model better exploit its training data, we train EDITOR on x% \u2208 [0, 100] of the data, and compare using random samples with samples ranked by increasing edit distance. samples with oracle edit distance between the input and the output sequence <= 2. This supports the hypothesis that despite adding noise, our approach yields easier examples to train on. The order in which samples are presented matters, as adding batches with larger edit distance (> 63% data) without maintaining the order of the samples converges early. By contrast, the curriculum pacing function adds samples in order of increasing difficulty, allowing the model sufficient training time to learn from new samples while improving overall performance across metrics. We also report the learning curves when training EDITOR on the Newsela dataset in Figure 5. Training with curriculum reduces the overall loss consistently on the development dataset, leading to better generalization.",
            "score": 0.4250485471876378,
            "section_title": "Impact of Curriculum Controlled roll-out",
            "char_start_offset": 23577,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1746826171875
        },
        {
            "corpus_id": "277066612",
            "title": "RAG-RL: Advancing Retrieval-Augmented Generation via RL and Curriculum Learning",
            "text": "The results in Tables 1 and 2 demonstrate that including samples of easier difficulty in the training process can help the model achieve higher performance during evaluation. Given a fixed number of training steps, the metrics in Table 3 shows that the baseline curriculum has consistently has lower F1 scores across all questions regardless of the number of hops required to answer the question correctly. Previous work in the area of self-improvement has shown that LLMs exhibit limited generalizability and that weak-to-strong curricula are effective for helping models generalize beyond their initial training distributions [10]. However, our results suggest that this is not always necessary as the linear curriculum is outperformed by the min-max curriculum in both shuffled and non-shuffled settings. We believe that this is a byproduct of our base model having relatively strong performance on the task before any post-training is applied. We show our base model's Pass@32, which is calculated by taking the maximum F1 score across 32 generations per question, in 4. We find that these scores are comparable to our baseline GRPO, demonstrating relatively strong baseline performance on the task. Curriculum learning strategies where the training sets are ordered from easy to hard have been successfully employed in many areas of machine learning [20]. Table 5 and Figure 3 include an ablation in which we sort the training set from easiest to hardest by sorting both by the number of hops each question requires and by the number of distractor passages, resulting in batches at the end of training containing the maximum number of passages with the greatest number of hops. We compare this against the linear and linear shuffled curricula which are constructed from the same training set but feature different dataset orderings. Although we see that the evaluation scores for both the final answer and citations start lower for the linear shuffled setting, all three curves converge to around the same point towards the end of training. Across all settings, we observe that the F1 scores are highest when the dataset is shuffled randomly, leading to our conclusion that the results do not conclusively support that using a specific ordering during training leads to significant performance gains or declines.",
            "score": 0.4249502443195059,
            "section_title": "Importance of a Including Additional Easier Samples",
            "char_start_offset": 15285,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2317
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.269287109375
        },
        {
            "corpus_id": "266977266",
            "title": "The Unreasonable Effectiveness of Easy Training Data for Hard Tasks",
            "text": "Training on Easy Data? \n\nWe now examine how well models generalize from easy training data to hard test data. \n\nDesign. For each of our hardness measures, we test models on exclusively hard test data (according to that hardness measure), while varying whether they are finetuned on easy or hard data. 3 esults. Surprisingly, Llama-2-70b with ICL shows comparable generalization to hard test data regardless of whether it is fit to easy or hard data (Fig. 4). In fact, across all six hardness measures, the Supervision Gap Recovered is between 70% and 100%. These results are statistically significant, with CIs and p-values shown in Appendix Table 2. Interestingly, for ARC and MMLU, there is no difference in easy vs. hard generalization using ICL. Results are also robust across finetuning methods and additional hardness measures (Appendix Figs. 13,18). With QLoRA, for example, the SGR remains within 70%-100% for ARC, MMLU and StrategyQA. While GSM8k appears to exhibit worse easy-to-hard generalization, we note that easy-to-all generalization is actually equally good to hard-to-all generalization (see Fig. 11). Thus it seems like easy data provides surprisingly good supervision for LMs. These results contrast notably with past work in curriculum learning and compositional generalization (Bengio et al., 2009;Lake and Baroni, 2018). This is likely because models like Llama-2-70b have learned much more during pretraining than models commonly used in work on curriculum learning and compositional generalization. So, it would seem that finetuning these models on relatively small amounts of easy data successfully elicits the relevant task knowledge from the models in a way that is largely invariant to datapoint hardness.",
            "score": 0.4245046455964554,
            "section_title": "RQ2: Can We Do Well on Hard Data by",
            "char_start_offset": 17416,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 25,
                    "end": 109
                },
                {
                    "start": 112,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1734
                }
            ],
            "ref_mentions": [
                {
                    "start": 1299,
                    "end": 1320,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1320,
                    "end": 1342,
                    "matchedPaperCorpusId": "46761158"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.152099609375
        },
        {
            "corpus_id": "258564296",
            "title": "Code Execution with Pre-trained Language Models",
            "text": "To improve the generalization capacity, we follow the curriculum learning strategy during pre-training. Curriculum learning (Bengio et al., 2009) (CL) is a learning strategy that starts from easy instances and then gradually handles harder ones, which imitates the meaningful learning order in human curricula. In our pre-training process, we organize the learning of the Python code execution datasets according to a curriculum that starts with simple instances, i.e. SingleLine data. First, we employ all the 9 million SingleLine transformations to pre-train CodeExecutor until convergence. To achieve a balanced dataset, we then reserve 3 million instances in Sin- 5 Experimental Setup",
            "score": 0.4235518249425724,
            "section_title": "Curriculum Learning",
            "char_start_offset": 14831,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 688
                }
            ],
            "ref_mentions": [
                {
                    "start": 124,
                    "end": 145,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.394775390625
        },
        {
            "corpus_id": "261048902",
            "title": "Towards Accelerated Model Training via Bayesian Data Selection",
            "text": "Extensive methods have been proposed to accelerate model training through techniques such as data pruning, coreset selection, curriculum learning, online batch selection, etc. Data pruning explores various metrics, such as EL2N score [33], forgetting score [40], and classification margin [34], to measure individual differences among data points and retains only the hardest examples for model training. However, data pruning still exhibits limitations when dealing with noisy labels, and some of these metrics are computationally expensive. Coreset selection methods also partially address the problem of accelerating model training. In particular, [43] contributes a data scoring mechanism that is robust to the change of scenarios for coreset selection, and [46] makes an in-depth understanding of the catastrophic accuracy drop issue of one-shot coreset selection and contributes a novel solution to it. However, these methods lack the flexibility to prioritize samples with different properties at various training stages. Curriculum learning, as advocated by [1], prioritizes easy points with low label noise before uniformly training on all data points. While this strategy enhances convergence, it fails to address the issue of skipping redundant points already learned. \n\nOnline batch selection methods [26,19,17] tackle the training acceleration problem by selecting hard data identified by high loss or gradient norm. However, they also suffer from a common drawback-high loss can be caused by label noise or ambiguous labels, so prioritizing such data can result in a decline in predictive performance. Compared to the prior art, our method establishes a Bayesian data selection metric and exploits zero-shot predictors to prioritize valuable training data for addressing these issues.",
            "score": 0.4233872633842616,
            "section_title": "Related Works",
            "char_start_offset": 24792,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1279
                },
                {
                    "start": 1282,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1798
                }
            ],
            "ref_mentions": [
                {
                    "start": 234,
                    "end": 238,
                    "matchedPaperCorpusId": "235898952"
                },
                {
                    "start": 257,
                    "end": 261,
                    "matchedPaperCorpusId": "55481903"
                },
                {
                    "start": 289,
                    "end": 293,
                    "matchedPaperCorpusId": "210932316"
                },
                {
                    "start": 651,
                    "end": 655,
                    "matchedPaperCorpusId": "259298636"
                },
                {
                    "start": 762,
                    "end": 766,
                    "matchedPaperCorpusId": "253224188"
                },
                {
                    "start": 1066,
                    "end": 1069,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1317,
                    "end": 1320,
                    "matchedPaperCorpusId": "3663876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7568359375
        },
        {
            "corpus_id": "529006",
            "title": "Context-Dependent Translation Selection Using Convolutional Neural Network",
            "text": "Curriculum learning, first proposed by Bengio et al. (2009) in machine learning, refers to a sequence of training strategies that start small, learn easier aspects of the task, and then gradually increase the difficulty level. It has been shown that the curriculum learning can benefit the nonconvex training by giving rise to improved generalization and faster convergence. The key point is that the training examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. \n\nFor each positive example (f , \u00ea+ ), we have three types of negative examples according to the difficulty level of distinguishing the positive example from them: \n\n\u2022 Easy: target phrases randomly chosen from the phrase table; \n\n\u2022 Medium: target phrases extracted from the aligned target sentence for other non-overlap source phrases in the source sentence; \n\n\u2022 Difficult: target phrases extracted from other candidates for the same source phrase. \n\nWe want the CDCM model to learn the following semantic information from easy to difficult: \n\n\u2022 the basic semantic similarity between the source sentence and target phrase from the easy negative examples; \n\n\u2022 the general semantic equivalent between the source and target phrase pair from the medium negative examples; \n\n\u2022 the context-dependent semantic similarities for the same source phrase in varying contexts from the difficult negative examples. \n\nAlg. 1 shows the curriculum training algorithm for the CDCM model. We use different portions of the overall training instances for different curriculums (lines 2-11). For example, we only use the training instances that consist of positive examples and easy negative examples in the easy curriculum (lines 5-6). For the latter curriculums, we gradually increase the difficulty level of the training instances (lines 7-12). \n\nFor each curriculum (lines 12-16), we compute the gradient of the loss objective L \u0398 and learn \u0398 using the SGD algorithm. Note that we meanwhile update the word embeddings to better capture the semantic equivalence across languages during training. If the loss function L \u0398 reaches a local minima or the iterations reach the predefined number, we terminate this curriculum.",
            "score": 0.42322875325525844,
            "section_title": "Curriculum Training",
            "char_start_offset": 12621,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 557
                },
                {
                    "start": 560,
                    "end": 721
                },
                {
                    "start": 724,
                    "end": 785
                },
                {
                    "start": 788,
                    "end": 916
                },
                {
                    "start": 919,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1099
                },
                {
                    "start": 1102,
                    "end": 1212
                },
                {
                    "start": 1215,
                    "end": 1325
                },
                {
                    "start": 1328,
                    "end": 1458
                },
                {
                    "start": 1461,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1883
                },
                {
                    "start": 1886,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2134
                },
                {
                    "start": 2135,
                    "end": 2259
                }
            ],
            "ref_mentions": [
                {
                    "start": 39,
                    "end": 59,
                    "matchedPaperCorpusId": "207178999"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58740234375
        },
        {
            "corpus_id": "202734083",
            "title": "Data Ordering Patterns for Neural Machine Translation: An Empirical Study",
            "text": "Curriculum learning has been studied and applied to various Machine Learning and Natural Language Processing tasks (Tsvetkov et al., 2016) (Cirik et al., 2016) (Graves et al., 2017). Most training protocols for Neural Machine Translation randomize the order of sentence pairs in the training corpus (Sennrich et al., 2017) (Hieber et al., 2017). One of the initial studies carried out by Kocmi and Bojar (2017) proposed a curriculum learning based mini bucketing approach using sequence length, number of coordinating conjunctions and word ranks by ensuring that samples within each mini-batch have similar linguistic properties. They show that translation quality can be improved by presenting samples from easy to hard based on sentence length and vocabulary frequency. Zhang et al. (2017)  Our approach differs from these recent works in the main aspect that the model can access the entire training data in each epoch in our approach as compared to other techniques which partition the training data set and provide a different portion of the training data to the model each epoch.",
            "score": 0.42311000295888623,
            "section_title": "Related Work",
            "char_start_offset": 2028,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 1085
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 137,
                    "matchedPaperCorpusId": "3963337"
                },
                {
                    "start": 160,
                    "end": 181,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 299,
                    "end": 321,
                    "matchedPaperCorpusId": "905565"
                },
                {
                    "start": 388,
                    "end": 410,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 772,
                    "end": 791,
                    "matchedPaperCorpusId": "550225"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.353271484375
        },
        {
            "corpus_id": "257937538",
            "title": "A Review of the Evaluation System for Curriculum Learning",
            "text": "For the training scheduler, the timing, weighting, and proportioning methods for the sample scheduling of interest in this paper are not completely separated. In most of the literature, multiple strategies are adopted to be used together. For example, by dividing the samples into different fragments, the timing of adding fragments in each iteration is judged based on the model convergence [93]. Whether used individually or in combination, a training scheduler that is dynamically tuned during the model training process is preferred over a pre-fixed training scheduler. Because the pre-fixed training scheduler is used to manually estimate the learning progress of the model, the pre-set speed or time of adding new samples by this type of training scheduler may not match the current model capability, and the rate and time of adding samples cannot be properly controlled for the phase when the model is rapidly improving its capability or slowly improving. In general, we summarize the following points for the selection of the training scheduler: (i) The root function [3] outperforms the rest of the functions, and the square root function [91] outperforms the rest of the root functions in the velocity method using functions to control sample accessions. (ii) Using evaluation model capability scheduling, the dynamic model capability evaluator [25,97] outperforms the static model capability evaluator [3,144]. (iii) Focusing on the proportion of sample scheduling, dynamic thresholds [41,157] outperform thresholds with fixed parameters [12,32], and multiple metric thresholds [79,114] are more comprehensive than single metric thresholds [12,32]. Conventional curriculum learning (from easy to hard) such as Mixed [12] and Single [45] scheduling outperforms the Reverse Scheduling strategy [89,110]. For loss evaluator design, an approach that uses each stage to evaluate the model's learning progress [69,85] is superior to the approach without evaluation. \n\nRegarding the three main directions of curriculum learning, self-paced learning, and anti-curriculum learning, primitive curriculum learning focuses more on the importance of prior knowledge, while self-paced learning emphasizes the loss in model training, and anti-curriculum learning, in contrast to curriculum learning, follows a hard-to-easy training sequence.",
            "score": 0.4224658176728667,
            "section_title": "Discussion",
            "char_start_offset": 104788,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 1970
                },
                {
                    "start": 1973,
                    "end": 2337
                }
            ],
            "ref_mentions": [
                {
                    "start": 392,
                    "end": 396,
                    "matchedPaperCorpusId": "51920640"
                },
                {
                    "start": 1355,
                    "end": 1359,
                    "matchedPaperCorpusId": "243865335"
                },
                {
                    "start": 1359,
                    "end": 1362,
                    "matchedPaperCorpusId": "226226711"
                },
                {
                    "start": 1496,
                    "end": 1500,
                    "matchedPaperCorpusId": "239016453"
                },
                {
                    "start": 1549,
                    "end": 1553,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 1651,
                    "end": 1655,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 1727,
                    "end": 1731,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 1915,
                    "end": 1919,
                    "matchedPaperCorpusId": "177535"
                },
                {
                    "start": 1919,
                    "end": 1922,
                    "matchedPaperCorpusId": "246015840"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0966796875
        },
        {
            "corpus_id": "247794120",
            "title": "Weakly-supervised Temporal Path Representation Learning with Contrastive Curriculum Learning",
            "text": "Inspired by the human learning principle of starting by learning simple tasks before proceeding to learn increasingly hard tasks, curriculum learning(CL) [34] uses nonuniform sampling of mini-batches according to the order of sample difficulty. Due its great potential to improve sample efficiency for different deep learning models, CL has attracted considerable interest and has found application in different research domains, e.g., computer vision [35]- [37], and natural language processing(NLP) [38]- [40]. However, none of these studies apply CL to path representation learning. PIM [20] is the closet to our paper, in that it proposes a curriculum negative sampling method to enhance the path representation learning. However, PIM focuses on negative sampling generations, but not on training. Xu et al. [40] propose two-staged curriculum learning for NLP, including difficulty evaluation and curriculum arrangement. Inspired by Xu et al. [40], we propose a curriculum learning framework that can evaluate the difficulty levels of data in a training data set automatically, so that models can be trained on increasingly difficult subsets of the training data set. The new framework features two key novelties. (i) Difficulty score computation: In NLP settings, difficulty scores, e.g., accuracy or F1 score, are computed based on strong labels in a supervised setting. In contrast, our difficulty scores are computed based on representation similarities, which do not rely on strong labels. (ii) How the training data is split into metasets: In NLP settings, training data is often split into metasets at random. In contrast, we split the training data based on the lengths of paths. This facilitates distinguishing the difficulty scores of paths.",
            "score": 0.4223699408955474,
            "section_title": "C. Curriculum Learning",
            "char_start_offset": 10301,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1755
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 158,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 452,
                    "end": 456,
                    "matchedPaperCorpusId": "209050760"
                },
                {
                    "start": 458,
                    "end": 462,
                    "matchedPaperCorpusId": "58981386"
                },
                {
                    "start": 501,
                    "end": 505,
                    "matchedPaperCorpusId": "218470266"
                },
                {
                    "start": 507,
                    "end": 511,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 590,
                    "end": 594,
                    "matchedPaperCorpusId": "235457940"
                },
                {
                    "start": 812,
                    "end": 816,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 947,
                    "end": 951,
                    "matchedPaperCorpusId": "220045816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.447021484375
        },
        {
            "corpus_id": "273233719",
            "title": "Efficient Pretraining Data Selection for Language Models via Multi-Actor Collaboration",
            "text": "Efficient data selection is crucial for the pretraining of large language models (LLMs), as the quality of training data significantly impacts the statistical efficiency of the training procedure and the model performance (Brown, 2020;Du et al., 2022;Chowdhery et al., 2023). Recently, we have witnessed numerous approaches, such as filtering high-quality data (Xie et al., 2023b;Wettig et al., 2024), mixing data from multiple domains (Xie et al., 2023a;Liu et al., 2024), and selecting data that optimally boosts downstream task performance dynamically (Engstrom et al., 2024;Yu et al., 2024), which aim to improve data efficiency by prioritizing more informative training samples. However, these methods often operate independently or in isolated settings, limiting their potential when integrated into a collaborative framework. In this work, we want to explore how to effectively, flexibly, and robustly combine these advanced data selection techniques through the dynamic pretraining process, addressing the challenges of optimizing data efficiency for LLM pretraining at scale. Nowadays, various heuristic methods have been proposed to provide measurements for the data samples used during LLM pre-training, aiming to optimize data efficiency by selecting or weighting the most informative training examples. However, we observe that integrating multiple data selection and mixing strategies presents significant challenges due to their inherent conflicts. For example, high-quality data identified by scoring functions may not align with data that strongly impact model performance as measured by influence functions (Engstrom et al., 2024); similar conflicts also exists between other methods -further details are enumerated in \u00a72. These observations actually motivate us to launch a systematic discussion about how to effectively integrate these methods during the dynamic pretraining process that provides superior data efficiency for LLM pretraining. \n\nOn the other hand, effectively integrating these data selection methods into a single framework is much harder to implement than to ask for. In fact, one may have to explore an exponential space to find the optimal combination for different data sampling schemas. Such a heavy burden will be Figure 1: Statistics of the SlimPajama dataset.",
            "score": 0.42151116072472744,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1962
                },
                {
                    "start": 1965,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2228
                },
                {
                    "start": 2229,
                    "end": 2304
                }
            ],
            "ref_mentions": [
                {
                    "start": 235,
                    "end": 251,
                    "matchedPaperCorpusId": "245124124"
                },
                {
                    "start": 251,
                    "end": 274,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 361,
                    "end": 380,
                    "matchedPaperCorpusId": "256627727"
                },
                {
                    "start": 380,
                    "end": 400,
                    "matchedPaperCorpusId": "267681974"
                },
                {
                    "start": 436,
                    "end": 455,
                    "matchedPaperCorpusId": "258741043"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96337890625
        },
        {
            "corpus_id": "250391006",
            "title": "On Curriculum Learning for Commonsense Reasoning",
            "text": "The initial data order imposed by the teacher model is updated at regular intervals during training by taking the learner model's current state into account (Kong et al., 2021). Importantly, we propose to reverse the ranking to a difficulty-to-easy curriculum in ACL, in order to reinforce feedback from the hardto-learn data points, which has been shown to be beneficial for generalization (Swayamdipta et al., 2020). In order to measure difficulty, we explore three different data-sample informativeness scoring methods i.e. Question Answering Probability (QAP) (Zhang and Bansal, 2019), Energy-based Out-of-Distribution Score (Liu et al., 2020) and Cartography-based Variability (Swayamdipta et al., 2020). Our work is most related to Xu et al. (2020) which splits the training data into N meta-datasets, trains N models for computing the curriculum and follows a heuristically designed training regimen. In contrast, we train a single model for computing the curriculum and use Bayesian optimization (Snoek et al., 2012) to find the best pacing of the curriculum for the target dataset, which is more effective than Xu et al. (2020) as we show in Sec. 5.4, besides being computationally efficient. \n\nWe analyze these methods on five commonsense reasoning datasets dealing with various tasks such as reasoning about social interactions (SocialIQA; Sap et al. (2019)), reading comprehension (Cos-mosQA; Huang et al. ( 2019)), natural language inference (HellaSwag; Zellers et al. (2019)), pronoun resolution (WinoGrande; Sakaguchi et al. (2020)) and adversarial commonsense (CODAH; Chen et al. (2019)). We explore curriculum learning in full-model finetuning as parameter-efficient tuning and show significant improvements using curriculum learning on each of these datasets. We also demonstrate that curriculum learning prevents the learner model from over-fitting on the training set, which leads to improved generalization to in-domain and out-of-domain data.",
            "score": 0.4211605374778159,
            "section_title": "Introduction",
            "char_start_offset": 2157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1201
                },
                {
                    "start": 1204,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1964
                }
            ],
            "ref_mentions": [
                {
                    "start": 391,
                    "end": 417,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 564,
                    "end": 588,
                    "matchedPaperCorpusId": "202572810"
                },
                {
                    "start": 629,
                    "end": 647,
                    "matchedPaperCorpusId": "222208700"
                },
                {
                    "start": 682,
                    "end": 708,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 738,
                    "end": 754,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 1004,
                    "end": 1024,
                    "matchedPaperCorpusId": "632197"
                },
                {
                    "start": 1120,
                    "end": 1136,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 1351,
                    "end": 1368,
                    "matchedPaperCorpusId": "128296356"
                },
                {
                    "start": 1467,
                    "end": 1488,
                    "matchedPaperCorpusId": "159041722"
                },
                {
                    "start": 1523,
                    "end": 1546,
                    "matchedPaperCorpusId": "199370376"
                },
                {
                    "start": 1584,
                    "end": 1602,
                    "matchedPaperCorpusId": "104292422"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1514892578125
        },
        {
            "corpus_id": "271710435",
            "title": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models",
            "text": "We aim at finding the most informative subset S b \u2282 S from the entire set S under the given budget |S b | \u2264 b. Mathematically, the selection of S b requires the quantitative evaluation q(\u2022) on each datapoint x i and an elaborated sampling mechanism \u03c0: \n\nwhere \u03c0(\u2022, b) denotes the sampling process with a maximum b datapoints. With respect to the detailed implementation of \u03c0, either an iterative, greedy algorithm or a batch-wise heuristic rule can be adopted for compatibility with q(\u2022). The expected benefits of such selection include: 1) the reduction of noise by ignoring those mislabeled, mismatched instruction-response pairs, 2) the re-balance of data distributions by down-sampling those easy, common, and similar examples while up-sampling hard, rare ones, and 3) the expedition of training in return for efficient iterations of LLMs.",
            "score": 0.42007879352921085,
            "section_title": "Data Assessment and Selection",
            "char_start_offset": 9509,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 251
                },
                {
                    "start": 254,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 843
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8564453125
        },
        {
            "corpus_id": "232233485",
            "title": "Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning",
            "text": "Motivation. Inspired by human and animal learning, curriculum learning (Bengio et al., 2009) posits that neural networks train better when examples are not randomly presented but instead organized in a meaningful order that gradually shows more concepts and complexity. Traditionally, curriculum learning approaches first assume that a range of example difficulty exists in the data and then leverage various heuristics to sort examples by difficulty and train models on progressively harder examples (Bengio et al., 2009;Tsvetkov et al., 2016;Weinshall et al., 2018). A newer school of thought, however, has noted that instead of discovering a curriculum in existing data, data can be intentionally modified to dictate an artificial range of difficulty (Korbar et al., 2018;Ganesh and Corso, 2020)this is the approach we will take here.\n\nOur approach. Unlike data augmentation in computer vision where augmented data undoubtedly resembles original data, in text, data augmentation techniques might introduce linguistic adversity and therefore can be seen as a form of noising (Li et al., 2017;, where noised data is harder to learn from than unmodified original data. As such, we can create an artificial curriculum in the data by leveraging controlled application of data augmentation, starting by training on only original data and then adding augmented data with a higher levels of noising as training progresses. Specifically, we propose two simple schedules.\n\n(1) Two-stage curriculum data augmentation calls for one stage of training with only original data, followed by one stage of training with augmented data of fixed temperature.\n\n(2) Gradual curriculum data augmentation involves one stage of training with only original data, followed by multiple stages of training with augmented data where the temperature of augmented data (i.e., fraction of perturbed tokens) gradually increases each stage.  (Wei and Zou, 2019). Our proposed twostage curriculum (second stage starts at four-thousand updates) trains faster and achieves slightly higher performance compared with standard augmentation while using the same number of updates. Our proposed gradual curriculum (which here linearly increases augmentation temperature \u03c4 by 0.1 at {4, 8, 12, 16, 20}thousand updates) outperforms both standard augmentation and the",
            "score": 0.4193208369195631,
            "section_title": "Curriculum Data Augmentation",
            "char_start_offset": 2351,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 71,
                    "end": 91,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 501,
                    "end": 522,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 522,
                    "end": 544,
                    "matchedPaperCorpusId": "3963337"
                },
                {
                    "start": 544,
                    "end": 567,
                    "matchedPaperCorpusId": "5004002"
                },
                {
                    "start": 754,
                    "end": 775,
                    "matchedPaperCorpusId": "53280782"
                },
                {
                    "start": 1077,
                    "end": 1094,
                    "matchedPaperCorpusId": "17730607"
                },
                {
                    "start": 1910,
                    "end": 1929,
                    "matchedPaperCorpusId": "59523656"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.259765625
        },
        {
            "corpus_id": "247762191",
            "title": "Data Selection Curriculum for Neural Machine Translation",
            "text": "Despite its effectiveness, our proposed data selection curriculum has some limitations: \n\n\u2022 The deterministic curriculum ( \u00a72.1) uses external scoring methods that require pretrained models for selecting the data subset to be used in the model fine-tuning stage. These scoring methods' pretraining incur additional training costs. \u2022 Our online curriculum approach ( \u00a72.2) on the other hand, is free from such additional pretraining costs. Nevertheless, it requires an extra forward propagation step in each of the epochs of the model fine-tuning stage to find the prediction scores of each of the sentence pairs. One possible way to avoid the extra forward propagation step is to cache the prediction scores while calculating the training losses in the previous epoch. However, there will be discrepancies in the prediction scores of samples as the predicted model will not be the same; it will get updated. Even though we did not investigate this avenue in our paper, we believe this can be an interesting research direction to observe the phenomenon.",
            "score": 0.4185848365485682,
            "section_title": "Limitations",
            "char_start_offset": 24471,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 90,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1052
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50048828125
        },
        {
            "corpus_id": "268715365",
            "title": "Better than Random: Reliable NLG Human Evaluation with Constrained Active Sampling",
            "text": "Eight popular NLG metrics are chosen as the automatic metrics set (details are in the Automatic Metric for Preliminary Phase section) of CASF.Due to the small number of samples and features mainly containing automatic metrics' scores, we explore several popular learning methods and recommend choosing Gradient Boosting Decision Tree (GBDT) (Friedman 2001) as the regressor of the Learner.Full experimental results are in the Learner Selection section of Appendix.The loss function is the least squares method (2007), which is commonly used in GBDT.\n\nSystematic Sampler Systematic sampling has advantage of eliminating clustered selection problem and can reduce the risk of favoritism, which meets our motivation.Therefore, we adopt the systematic sampling method (Yates 1948) sorted by relevant signs as the sampling core of CASF.The Systematic Sampler selects representative initial samples and candidate samples according to the quality ranking of samples.Specifically, the Systematic Sampler first divides the N t = N \u2212 |H| samples for the t-th phase into n t buckets according to the given quality ranking {p t (i)} are divided into the same bucket, where e = 0, 1, ..., n t .The samples with quality rank p t = e \u00d7 \u230a Nt nt \u230b are selected as the initial selection samples.And the rest samples in each bucket are candidate samples.\n\nConstrained Controller The proposed Constrained Controller controls the Redundancy of samples and selects one sample from each of the buckets divided by the Systematic Sampler to form a final sample subset (as shown in Figure 3).Since the Systematic Sampler selects initial samples at a regular interval, which makes the distribution of the initial subset align closely with the overall distribution, we aim to preserve the original sampling intervals as much as possible while controlling the Redundancy to maintain the representativeness of the sample subset.\n\nSpecifically, we define objective function Obj as the quality ranking distance between the current sample x i and the initial selection sample in each bucket.We also define violation function Vio to calculate the Redundancy between the current sample x i and the final samples.",
            "score": 0.4185848365485682,
            "section_title": "Constrained Active Sampling Framework",
            "char_start_offset": 11522,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 142,
                    "end": 389
                },
                {
                    "start": 389,
                    "end": 464
                },
                {
                    "start": 464,
                    "end": 549
                },
                {
                    "start": 551,
                    "end": 713
                },
                {
                    "start": 713,
                    "end": 831
                },
                {
                    "start": 831,
                    "end": 959
                },
                {
                    "start": 959,
                    "end": 1181
                },
                {
                    "start": 1181,
                    "end": 1277
                },
                {
                    "start": 1277,
                    "end": 1335
                },
                {
                    "start": 1337,
                    "end": 1566
                },
                {
                    "start": 1566,
                    "end": 1898
                },
                {
                    "start": 1900,
                    "end": 2058
                },
                {
                    "start": 2058,
                    "end": 2177
                }
            ],
            "ref_mentions": [
                {
                    "start": 341,
                    "end": 356,
                    "matchedPaperCorpusId": "39450643"
                },
                {
                    "start": 764,
                    "end": 776,
                    "matchedPaperCorpusId": "60425504"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06536865234375
        },
        {
            "corpus_id": "208158432",
            "title": "Carpe Diem, Seize the Samples Uncertain \"at the Moment\" for Adaptive Batch Selection",
            "text": "Let D = {(x i , y i )|1 \u2264 i \u2264 N } be the entire training dataset composed of a sample x i with its true label y i , where N is the total number of training samples. Then, a straightforward strategy to construct a mini-batch \n\nBecause not all samples have an equal impact on training, many research efforts have been devoted to develop advanced sampling schemes. Bengio et al. (2009) first took easy samples and then gradually increased the difficulty of samples using heuristic rules. Kumar et al. (2010) determined the easiness of the samples using their prediction errors. Recently, Tsvetkov et al. (2016) used Bayesian optimization to learn an optimal curriculum for training dense, distributed word representations. \n\nSachan & Xing (2016) emphasized that the right curriculum must introduce a small number of the samples dissimilar to those previously seen. Fan et al. (2017) proposed a neural data filter based on reinforcement learning to select training samples adaptively. However, it is common for deep learning to emphasize hard samples because of the plethora of easy ones (Katharopoulos & Fleuret, 2018). Loshchilov & Hutter (2016) proposed a difficulty-based sampling scheme, called Online Batch, that uses the rank of the loss computed from previous epochs. Online Batch sorts the previously computed losses of samples in descending order and exponentially decays the sampling probability of a sample according to its rank r. Then, the r-th ranked sample x(r) is selected with the probability Preprint dropping by a factor of exp log(s e )/N , where s e is the selection pressure parameter that affects the probability gap between the most and the least important samples. When normalized to sum to 1.0, the probability P (x(r)|D; s e ) is defined by Eq. (1). It has been reported that Online Batch accelerates the convergence of training but deteriorates the generalization error because of the overfitting to hard training samples (Loshchilov & Hutter, 2016).",
            "score": 0.41852870286868227,
            "section_title": "RELATED WORK",
            "char_start_offset": 4339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 223
                },
                {
                    "start": 226,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 719
                },
                {
                    "start": 722,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1975
                }
            ],
            "ref_mentions": [
                {
                    "start": 362,
                    "end": 382,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 585,
                    "end": 607,
                    "matchedPaperCorpusId": "3963337"
                },
                {
                    "start": 1084,
                    "end": 1115,
                    "matchedPaperCorpusId": "3663876"
                },
                {
                    "start": 1117,
                    "end": 1143,
                    "matchedPaperCorpusId": "5324823"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58447265625
        },
        {
            "corpus_id": "240419611",
            "title": "Diverse Distributions of Self-Supervised Tasks for Meta-Learning in NLP",
            "text": "The methods discussed so far use a static task distribution for learning with tasks sampled i.i.d from this distribution for training. Curriculum learning (Bengio et al., 2009;Graves et al., 2017) instead posits that choosing the order in which instances are presented, with gradually increasing complexity, can enable faster learning and better generalization. We explore whether a curriculum in task sampling is beneficial for meta-learning by proposing a method to sample increasingly difficult tasks during training. To enable this we need a method to propose difficult tasks based on the current state of the model during training.\n\nSince words act as labels in SMLMT, words that are closer in the representational space of the neural model will be more difficult to distinguish, leading to more difficult tasks. On the other hand, nearestneighbors can be too difficult to induce effective learning for a model. This is related to findings in negative sampling in metric learning literature (Schroff et al., 2015;Suh et al., 2019) where using \"too hard\" negatives typically hurts performance.\n\nTo alleviate this problem, we cluster representations computed from the model and uniformly sample words within the same cluster to create difficult but not impossible tasks (similar to the \"static\" clustering approach). Secondly, we adopt an easyto-hard curriculum by controlling the ratio between the harder tasks from the dynamic distribution D t and the easier ones from the static distribution S, consisting of tasks sampled i.i.d from uniform random word sampling or fixed word-clustering. At step t, let \u03bb t be the probability of sampling a task from D t and 1 \u2212 \u03bb t from S. Then the dynamic curriculum is defined by sampling tasks from the following mixture distribution with \u03bb t linearly annealed over the training epochs from 0 to 1:\n\nTo construct D t , we consider the following word (i.e. label) representation for clustering, obtained by the average representation under the model of the masked sentences corresponding to a word:\n\nwhere S(w i ) is the set of all sentences containing the word w i with the word w i masked out (as defined in SMLMT), f \u03b8t (.) is the representation from the neural model for instance x that is fed into the softmax classification layer, and \u03b8 t are the model parameters at step",
            "score": 0.4184830518573048,
            "section_title": "Dynamic Curriculum over Self-Supervised Tasks",
            "char_start_offset": 9757,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 176,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 176,
                    "end": 196,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 996,
                    "end": 1018,
                    "matchedPaperCorpusId": "206592766"
                },
                {
                    "start": 1018,
                    "end": 1035,
                    "matchedPaperCorpusId": "195510985"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54052734375
        },
        {
            "corpus_id": "231709290",
            "title": "Curriculum Learning: A Survey",
            "text": "Other tasks The importance of presenting the data in a meaningful order is highlighted by Spitkovsky et al. (2009) in their unsupervised grammar induction experiments. They use the length of a sentence as a difficulty metric, with longer sentences being harder, suggesting two approaches: \"Baby steps\" and \"Less is more\". \"Baby steps\" shows the superiority of an easy-to-hard training strategy by iterating over the data in increasing order of the sentence length (difficulty) and augmenting the training set at each step. \"Less is more\" matches the findings of Bengio et al. (2009) that sometimes easier examples can be more informative. Here, the authors improve the state of the art while limiting the sentence length to a maximum of 15. Zaremba and Sutskever (2014) apply curriculum learning to the task of evaluating short code sequences of length = a and nesting = b. They use the two parameters as difficulty metrics to enforce a curriculum methodology. Their first procedure is similar to the one in (Bengio et al. 2009), starting with the length = 1 and nesting = 1, while iteratively increasing the values until reaching a and b, respectively. To improve the results of this naive approach, the authors build a mixed technique, where the values for length and nesting are randomly selected from [1, a] and [1, b]. The last method is a combination of the previous two approaches. In this way, even though the model still follows an easy-to-hard strategy, it has access to more difficult examples in the early stages of the training. Tsvetkov et al. (2016) introduce Bayesian optimization to optimize curricula for word representation learning. They compute the complexity of each paragraph of text using three groups of features: diversity, simplicity, and prototypicality. Then, they order the training set according to complexity, generating word representations that are used as features in a subsequent NLP task. Bayesian optimization is applied to determine the best features and weights that maximize performance on the chosen NLP task. Cirik et al. (2016) analyze the effect of curriculum learning on training Long Short-Term Memory (LSTM) networks. For that, they employ two curriculum strategies and two base-lines. The first curriculum approach uses an easy",
            "score": 0.4184784903281243,
            "section_title": "Question answering",
            "char_start_offset": 91685,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 90,
                    "end": 114,
                    "matchedPaperCorpusId": "1900277"
                },
                {
                    "start": 562,
                    "end": 582,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1008,
                    "end": 1027,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1542,
                    "end": 1564,
                    "matchedPaperCorpusId": "3963337"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.387939453125
        },
        {
            "corpus_id": "253734400",
            "title": "In-sample Curriculum Learning by Sequence Completion for Natural Language Generation",
            "text": "Curriculum learning has shown promising improvements in multiple domains by training machine learning models from easy samples to hard ones. Previous works which either design rules or train models for scoring the difficulty highly rely on task-specific expertise, and cannot generalize. Inspired by the \u201ceasy-to-hard\u201d intuition, we propose to do in-sample curriculum learning for natural language generation tasks. Our learning strategy starts training the model to generate the last few words, i.e., do sequence completion, and gradually extends to generate the whole output sequence. Comprehensive experiments show that it generalizes well to different tasks and achieves significant improvements over strong baselines.",
            "score": 0.41845062386241505,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35595703125
        },
        {
            "corpus_id": "258960507",
            "title": "GripRank: Bridging the Gap between Retrieval and Generation via the Generative Knowledge Improved Passage Ranking",
            "text": "The selection of candidate passages has a significant impact on the distillation quality for a passage ranker. One of the alternative approaches is to mind candidate passages serving as hard negatives [15,16,27] for improving the ranking capability. However, in human education, teachers always train students starting with basic (easy) curricula, and then progressively exposure to more advanced (hard) curricula along with students growing up. Students will learn much better if the curricula are organized in a reasonable order. Inspired by curriculum learning [4], we propose a curriculum knowledge distillation mechanism for better distilling the knowledge from GPE to ranker. Concretely, we devise a curriculum passage sampling strategy to realize the knowledge distillation in a curriculum manner, which is achieved by gradually increasing the difficulty of the sampled candidate passages during the distillation procedure. Thus, it challenges the passage ranker to distinguish and order the passages in an easy-to-hard curriculum progressively.\n\nDifficulty Measurement. We define the difficulty of a candidate passage ( \u2260 , \u2208 P) as its similarity to the input query , where the similarity is the normalized dot-product score predicted by the pre-trained passage retriever Q. We sort the passages according to the similarity in ascending order, obtaining a sorted passage pool P = {\u02dc1, \u00b7 \u00b7 \u00b7\u02dc| P | }. Thus, a higher rank of passage means it is more similar to the input query , indicating that it is more difficult for the passage ranker to distinguish it from the golden provenance .\n\nCurriculum Passage Sampling. To realize curriculum knowledge distillation, we sample passages from the sorted passage pool P by progressively increasing the difficulty of sampled candidates. Concretely, we adjust the scale ( \u2a7d |P |) of the sampling space from which the candidate passages are sampled, where = {\u02dc1, \u00b7 \u00b7 \u00b7 ,\u02dc} is a subset of P (i.e., \u2208 P ). The sampling scale can be determined as follows:\n\nwhere 0 ( 0 \u2208 N, 0 \u2a7d |P |) is the scale of warm-up passage pool. 0 and are warm-up steps and total curriculum steps, respectively. \u230a\u00b7\u230b denotes the fraction rounded down.\n\nIntuitively, when the current training steps satisfy \u2264 0 , the candidate passages are sampled from the set {\u02dc1, \u00b7",
            "score": 0.41843849012016987,
            "section_title": "Curriculum Knowledge Distillation.",
            "char_start_offset": 18008,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "237363386"
                },
                {
                    "start": 205,
                    "end": 208,
                    "matchedPaperCorpusId": "250391085"
                },
                {
                    "start": 208,
                    "end": 211,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 564,
                    "end": 567,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1661376953125
        },
        {
            "corpus_id": "245123731",
            "title": "Improving Logical-Level Natural Language Generation with Topic-Conditioned Data Augmentation and Logical Form Generation",
            "text": "These equations present self-training objectives. \n\nCurriculum learning Furthermore, we adopt the curriculum learning (Bengio et al. 2009) strategy by sorting the augmented data in descending order of their corresponding weights. The motivation is that the models can learn from easy augmented data (in which the generated pseudo examples are cleaner) to harder ones. This setting encourages the models to learn from pseudo-parallel data of higher quality in the beginning, and gradually transit to more error-prone ones that have lower weights. We expect that this strategy can better balance the effects of the noisy augmented data. \n\nTraining scheme We adopt a teacher-student training scheme to optimize the two models. At each epoch, we have both a teacher copy and a student copy of each model. During the unsupervised training, the teacher models are frozen to generate pseudo-parallel data for the student models. At the end of each epoch, the teacher models are updated from the corresponding student models. This is similar to the epochlevel Iterative Back-Translation (IBT) scheme presented in (Xu, Niu, and Carpuat 2020;Zhang et al. 2018). We also perform teacher forcing, to fine-tune the models on the clean supervised data S at the end of each epoch. A formal description of the entire framework is illustrated in Appendix.",
            "score": 0.4182131478354503,
            "section_title": "Joint training of Logic2text and LG",
            "char_start_offset": 12559,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 49
                },
                {
                    "start": 52,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1338
                }
            ],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 138,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1105,
                    "end": 1132,
                    "matchedPaperCorpusId": "222177171"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.431884765625
        },
        {
            "corpus_id": "247762191",
            "title": "Data Selection Curriculum for Neural Machine Translation",
            "text": "Curriculum Learning Inspired by human learners, Elman (1993) argues that optimization of neural network training can be accelerated by gradually increasing the difficulty of the concepts. Bengio et al. (2009) were the first to use the term \"curricu-lum learning\" to refer to the easy-to-hard training strategies in the context of machine learning. Using an easy-to-hard curriculum based on increasing vocabulary size in language model training, they achieved performance improvement. Recent work (Jiang et al., 2015;Hacohen and Weinshall, 2019;Zhou et al., 2020a) shows that manoeuvring the sequence of training data can improve both training efficiency and model accuracy. Several studies show the effectiveness of the difficulty-based curriculum learning in a wide range of NLP tasks including task-specific word representation learning (Tsvetkov et al., 2016), natural language understanding tasks (Sachan and Xing, 2016;Xu et al., 2020a), reading comprehension (Tay et al., 2019), and language modeling (Campos, 2021). Several studies show the effectiveness of the difficultybased curriculum learning in a wide range of NLP tasks (Cirik et al., 2016;Liu et al., 2018). \n\nCurriculum Learning in NMT The difficultybased curriculum in NMT was first explored by Kocmi and Bojar (2017) (Zhao et al., 2020). In contrast, our proposed two-stage training framework for NMT fine-tunes the base model from the warm-up stage on selected subsets of data. Our data-selection curriculum training framework is more realistic, resembling the formal education system as discussed in \u00a75.2. \n\nSelf-paced Learning in NMT Here, the model itself measures the difficulty of the training samples to adjust the learning pace (Kumar et al., 2010). In their approach, Wan et al. (2020) first train the NMT model for M passes on the data and cache the translation probabilities to find variance. The lower variance of the translation probabilities of a sample reflects higher confidence. Later, they use the confidence scores as factors to weight the loss to control the model updates.",
            "score": 0.4181886547777687,
            "section_title": "Related Work",
            "char_start_offset": 21431,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1172
                },
                {
                    "start": 1175,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1575
                },
                {
                    "start": 1578,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2061
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 60,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 188,
                    "end": 208,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 496,
                    "end": 516,
                    "matchedPaperCorpusId": "10891229"
                },
                {
                    "start": 516,
                    "end": 544,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 544,
                    "end": 563,
                    "matchedPaperCorpusId": "227275560"
                },
                {
                    "start": 839,
                    "end": 862,
                    "matchedPaperCorpusId": "3963337"
                },
                {
                    "start": 901,
                    "end": 924,
                    "matchedPaperCorpusId": "16503693"
                },
                {
                    "start": 924,
                    "end": 941,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 965,
                    "end": 983,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 1154,
                    "end": 1171,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 1262,
                    "end": 1284,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 1285,
                    "end": 1304,
                    "matchedPaperCorpusId": "211102338"
                },
                {
                    "start": 1704,
                    "end": 1724,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 1745,
                    "end": 1762,
                    "matchedPaperCorpusId": "222272176"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71240234375
        },
        {
            "corpus_id": "195346954",
            "title": "ScreenerNet: Learning Curriculum for Neural Networks",
            "text": "Training a machine learning model with chosen training samples in a certain order improves the speed of learning and is called Curriculum Learning [3]. The curriculum learning recently gain much attention due to the difficulty of training deep models for reinforcement learning [1,7,19]. However, selecting and ordering is a hard-decision and deprives the chances of samples being selected in the later iterations. In addition, the decision criteria are mostly defined by a set of hand-crafted rules; most of them are from classification error or confidence of the original network in the majority of previous work [2,4,7]. For those hand-crafted rules require additional rules to handle the bias that leads to the early rejection of the samples or solution changes to which the model converges. \n\nTo address both the hard decision and early rejection problems, we present a scheme to determine a weight value of every training sample for building a curriculum. This is a generalization to the hard decision by considering all samples at every curriculum update, thus never ignore the samples at any iteration to maximize the efficacy of the curricular learning, similar to [8,17]. Moreover, to discover the rules for curriculum that are beyond our intuition, we propose to learn the curriculum by an attachable neural network, called ScreenerNet, to the original network. The ScreenerNet computes soft decision values for each sample to be selected, motivated from the recent work of efficiently exploring spaces of state, action, and goal through the learning [1,19]. More importantly, the ScreenerNet is jointly trained with the original network in an end-to-end fashion as shown in Figure 1, thus it provides the locally most accurate weights per the original network is being trained. Particularly, the ScreenerNet maps training samples to their influences on the learning of the main task in the form of error or loss value.",
            "score": 0.4178433738624915,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 795
                },
                {
                    "start": 798,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1930
                }
            ],
            "ref_mentions": [
                {
                    "start": 147,
                    "end": 150,
                    "matchedPaperCorpusId": "312693"
                },
                {
                    "start": 281,
                    "end": 283,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 615,
                    "end": 618,
                    "matchedPaperCorpusId": "1820089"
                },
                {
                    "start": 618,
                    "end": 620,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 620,
                    "end": 622,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 1174,
                    "end": 1177,
                    "matchedPaperCorpusId": "10327263"
                },
                {
                    "start": 1177,
                    "end": 1180,
                    "matchedPaperCorpusId": "13022595"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1754150390625
        },
        {
            "corpus_id": "202677548",
            "title": "Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes",
            "text": "At the core of our work is the idea that the way that data samples are provided for training a model can affect speed or capabilities of the model. This field is broad and there are several distinct approaches to achieve it. Notable examples include curriculum learning (Bengio et al., 2009) and self-paced learning (Kumar et al., 2010), where data points for training are selected based on a metric of easiness or hardness. In Bayesian approaches (Klein et al., 2016), the goal is to create sub-samples of data points, whose traits can be extrapolated as the full dataset. \n\nOur work thus differs from the aforementioned methods in the fact that we focus on exploiting valuable but overlooked information from sequences of tokens. We change the way data points are generated from token sequences and extend the expressivity of a model by providing an augmented, and well sorted, sequence of data points. This method has a related effect of a randomized-length backpropagation through time (BPTT) (Merity et al., 2017), which yields different data points between epochs. It also resembles classical text data-augmentation methods, such as data-augmentation using thesaurus (Zhang and LeCun, 2015). \n\nOur method takes a step forward and proposes a systematic and deterministic approach on building data points that provides the needed variety of data points without the need of randomized-length backpropagation through time (BPTT). This has the effect of producing a text-augmentation without the need of using external resources such as a thesaurus, but only requires the dataset itself. Our method uses a concept of overlapped data points, which can be found in many areas such as data-mining (Dong and Pei, 2007), DNA sequencing (Ng, 2017), spectral analysis (Ding et al., 2000), or temporal data (Lane and Brodley, 1999). In language modeling however, this approach of overlapped data points has not yet been fully exploited.",
            "score": 0.4173762740657264,
            "section_title": "Related work",
            "char_start_offset": 5026,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 573
                },
                {
                    "start": 576,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1197
                },
                {
                    "start": 1200,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1929
                }
            ],
            "ref_mentions": [
                {
                    "start": 270,
                    "end": 291,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 316,
                    "end": 336,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 1762,
                    "end": 1781,
                    "matchedPaperCorpusId": "2160403"
                },
                {
                    "start": 1800,
                    "end": 1824,
                    "matchedPaperCorpusId": "12305305"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59375
        },
        {
            "corpus_id": "259448071",
            "title": "Heuristic search of optimal machine teaching curricula",
            "text": "In curriculum learning the order of concepts is determined by the teacher but not the examples for each concept, while in machine teaching it is the examples that are chosen by the teacher to minimise the learning effort, though the concepts are taught in isolation. Curriculum teaching is the natural combination of both, where both concept order and the set of examples can be chosen to minimise the size of the whole teaching session. Yet, this simultaneous minimisation of teaching sets and concept order is computationally challenging, facing issues such as the \u201cinterposition\u201d phenomenon: previous knowledge may be counter-productive. We build on a machine-teaching framework based on simplicity priors that can achieve short teaching sizes for large classes of languages. Given a set of concepts, we identify an inequality relating the sizes of example sets and concept descriptions. This leverages the definition of admissible heuristics for A* search to spot the optimal curricula by avoiding interposition, being able to find the shortest teaching sessions in a more efficient way than an exhaustive search and with the guarantees we do not have with a greedy algorithm. We illustrate these theoretical findings through case studies in a drawing domain, polygonal strokes on a grid described by a simple language implementing compositionality and recursion.",
            "score": 0.4170552687887766,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0489501953125
        },
        {
            "corpus_id": "267897619",
            "title": "Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach",
            "text": "To avoid overfitting of negative samples, we set the model learning process as Positive-Unlabeled (PU) learning, where data labeled with 0 is considered unlabeled rather than non-entity. PU learning assumes that the unlabeled data follows the distribution of the entire dataset (Zhou et al., 2022). \n\nTo fulfill the assumption, we directly incorporate all unlabeled data into the curriculum to prevent too less unlabeled tokens in curricula. Different curriculum partitions are executed solely on the labeled positive data. In this way, only the learning order of positive samples can affect CuPUL performance. \n\nOur curriculum is built on token difficulty scores H, which are mostly \"easy\" as they follow a longtail distribution (Figure 3). Previous work (Platanios et al., 2019;Gnana Sheela and Deepa, 2013) suggests that a curriculum with uniformly ranged difficulty scores might lead to most tokens belonging to the first curriculum, making curriculum learning ineffective. Hence, we propose using a powerlaw selector to construct a more effective curriculum scheduler. \n\nTo build the curricula, we first arrange T u unlabeled tokens followed by T p positive-labeled tokens sorted by their difficulty scores in ascending order. The first curriculum consists of all unlabeled tokens and the first \u03c4 T p labeled positive tokens, where \u03c4 (0 < \u03c4 < 1) is a selective factor. The second curriculum consists of the first \u03c4 2 T tokens from the remaining (1 \u2212 \u03c4 )T p tokens. This selection process continues until the penultimate curriculum. The remaining tokens are placed in the final curriculum. These curricula are denoted as C 1 , C 2 , ..., C \u03b7 : \n\nFor example, suppose T p = 20, T u = 80, \u03c4 = 0.5, and \u03b7 = 3. Then, the first curriculum consists of tokens indexed from 1 to 90 (80 unlabeled tokens and the 10 easiest positive tokens), the second curriculum consists of tokens indexed from 91 to 95, and the last curriculum consists of tokens indexed from 96 to 100.",
            "score": 0.4169241776803664,
            "section_title": "Curriculum Design",
            "char_start_offset": 11678,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 298
                },
                {
                    "start": 301,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 610
                },
                {
                    "start": 613,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1073
                },
                {
                    "start": 1076,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1647
                },
                {
                    "start": 1650,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1966
                }
            ],
            "ref_mentions": [
                {
                    "start": 278,
                    "end": 297,
                    "matchedPaperCorpusId": "248266826"
                },
                {
                    "start": 780,
                    "end": 809,
                    "matchedPaperCorpusId": "45965215"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18701171875
        },
        {
            "corpus_id": "237553761",
            "title": "On the Role of Corpus Ordering in Language Modeling",
            "text": "selected, thus combining a deterministic schedule with the benefits of randomization that serve neural models well.\n\n(ii) STEPPED: Alternatively, the bins could be accessed cumulatively where the training set progressively increases in size by addition of newer bins while retaining earlier bins (also referred to as Baby Steps curriculum (Bengio et al., 2009;Spitkovsky et al., 2010)). Basically, samples of increasing (or decreasing) complexity are added to the training set after each phase t while the samples from the previous phase still remain in the training set. For instance, at the first phase t = 1, only the first bin is presented; at the next phase t = 2, the training set comprises of both the first and second bins; at t = 3, samples from the first, second, and third bins are accessed, and so on until the model iterates over the entire corpus in the last phase. Essentially, the first bin's samples are seen n more times (n = number of bins) than the last bin. So in our case of three bins, the easy samples are iterated over thrice as compared to the samples from the hard bin if following an increasing level of complexity, and vice versa for a schedule of decreasing complexity.\n\nTo sum it up, if we set a phase t to be an epoch, in the BINNED models, all the bins (and by extension, the entire corpus) are/is iterated over just once, whereas in the STEPPED models, since newer bins are progressively added in addition to existing bins in the training set, a process that not only modifies complexity but also increases data size over time, on average, this is similar to iterating over the entire corpus n 2 times.\n\nFinally, the VANILLA model, where the corpus is not sorted or binned in any way and data from the entire corpus is accessed randomly at any phase, serves as the baseline model.",
            "score": 0.41681871566240813,
            "section_title": "Corpus Ordering for Pretraining",
            "char_start_offset": 13233,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 339,
                    "end": 360,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 360,
                    "end": 384,
                    "matchedPaperCorpusId": "1363892"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.360107421875
        },
        {
            "corpus_id": "273350576",
            "title": "Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws",
            "text": "A significant limitation of almost all existing methods (with the exception of Albalak et al. (2023)) is their requirement for additional computational steps beyond standard training. This extra complexity entails substantial modifications to the training pipeline, making it challenging to integrate these methods when other components are varied (e.g., the architecture, tokenizer, or optimizer). The consequence of this usage barrier is that these methods have not been tested widely, making them less popular for those training a new model. To enable broader adoption of data mixture optimization, we argue that it must be implemented in an online fashion, concurrent with model training, without disrupting the standard training process, which means that the data mixture must adapt itself based on the feedback from the model. This is closely related to curriculum learning (Bengio et al., 2009), a training strategy where models are progressively exposed to domains in a curated order. \n\nAt a fundamental level, this work develops and empirically investigates questions about the existence of good pretraining curricula and the feasibility of cheaply identifying them. We first demonstrate in controlled experiments that good curricula can generally be found with more computation and that it is hard to accurately predict larger models' performance with only a few small models. This leads to our main contribution, Adaptive Data Optimization (ADO): an algorithm for adaptively adjusting the data mixture online during training (Figure 1). In experiments on language models up to 1.3B parameters trained on the Pile (Gao et al., 2020), we find that ADO improves performance across a variety of common benchmarks and improves validation loss on SlimPajama (Soboleva et al., 2023) and FineWeb (Penedo et al., 2023), both of which are considered to be higher-quality datasets. Most importantly, ADO achieves these without requiring significant additional computation (less than 0.4% wallclock time for 1.3B), proxy models, or extensive modification to the training pipelines. The drawbacks of small proxy models. At first glance, transferring data selection strategies from smaller to larger models seems plausible, with positive results in specific cases, such as in Mindermann et al. ( 2022) and Xia et al. (2024b).",
            "score": 0.4167215766755078,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1964,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 992
                },
                {
                    "start": 995,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2117
                },
                {
                    "start": 2118,
                    "end": 2322
                }
            ],
            "ref_mentions": [
                {
                    "start": 79,
                    "end": 100,
                    "matchedPaperCorpusId": "265658930"
                },
                {
                    "start": 880,
                    "end": 901,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72216796875
        },
        {
            "corpus_id": "264439364",
            "title": "Understanding Compositional Data Augmentation in Typologically Diverse Morphological Inflection",
            "text": "With this method, we seek to improve the structural diversity in our subsampled in synthetic training dataset DSyn train . Training on diverse subset is crucial, as the SIGMORPHON 2018 shared task dataset is imbalanced in frequency of different morphosyntactic descriptions (MSDs). 8 These imbalances can pose challenges to the model in generalizing to rarer MSDs. To incorporate greater structural diversity, we employ the templatic sampling process proposed by Oren et al. (2021). Specifically, we modify the distribution over MSDs to be closer to uniform in DSyn train . Formally, we sample without replacement from the following distribution: q \u03b1 (X, Y, T) = p(T) \u03b1 / T p(T) \u03b1 where p(T) is the proportion of times that T appears in D Syn train . We consider two cases: \u03b1 = 0 corresponds to sampling MSDs from a uniform distribution (UMT), while \u03b1 = 1 corresponds to sampling tags according to the empirical distribution over MSDs (EMT). \n\nHIGHLOSS. Next, we employ a selection strategy that selects datapoints that have high predictive uncertainty to the initial model M. Spurious correlations between substructures (like Y stem and T; Section 3) will exist in any dataset of bounded size (Gupta et al., 2022;Gardner et al., 2021), and we conjecture that selecting high uncertainty datapoints will efficiently mitigate these correlations. \n\nWe quantify the uncertainty of a synthetic datapoint in D Syn train by computing the negative loglikelihood (averaged over all tokens in the the target Y) for each synthetic datapoint in D Syn train . Next, we select the synthetic datapoints with the highest uncertainty and add them to DSyn train . To thoroughly demonstrate that incorporating predictive uncertainty is important for yielding training examples that counteract the spurious dependencies in the ground-truth training dataset, we benchmark it against another subset selection strategy LOWLOSS. With this method, we instead select synthetic datapoints that the model finds easy, i.e., those with the lowest uncertainty scores.",
            "score": 0.41599617219237417,
            "section_title": "UNIFORM MORPHOLOGICAL TEMPLATE (UMT).",
            "char_start_offset": 14385,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 941
                },
                {
                    "start": 944,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1343
                },
                {
                    "start": 1346,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2036
                }
            ],
            "ref_mentions": [
                {
                    "start": 463,
                    "end": 481,
                    "matchedPaperCorpusId": "237420426"
                },
                {
                    "start": 1194,
                    "end": 1214,
                    "matchedPaperCorpusId": "253254777"
                },
                {
                    "start": 1214,
                    "end": 1235,
                    "matchedPaperCorpusId": "233296459"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43896484375
        },
        {
            "corpus_id": "174798125",
            "title": "Dynamically Composing Domain-Data Selection with Clean-Data Selection by \u201cCo-Curricular Learning\u201d for Neural Machine Translation",
            "text": "To facilitate the definition of co-curricular learning and following (Platanios et al., 2019;Wang et al., 2018b), we define a dynamic data selection function, D \u03c6 \u03bb (t, D), to return the top \u03bb(t) of examples in a dataset D sorted by a scoring function \u03c6 at a training step t. We use \u03bb(t) = 0.5 t/H , (0 < \u03bb \u2264 1), as a pace function to return a selection ratio value that decays over time controlled by a hyper-parameter H.2 During training, D \u03c6 \u03bb (t, D) progressively evolves into smaller subdatasets that are more relevant to the final task using the scoring function. In practice, D \u03c6 \u03bb (t, D ) can be applied on a small buffer D of random examples from the much bigger D, for efficient online training. It may also be desirable to set a floor value on \u03bb(t) to avoid potential data selection bias. This is how we implement a curriculum in experiments. We introduce two different co-curricula below.",
            "score": 0.41595305272907934,
            "section_title": "Curriculum Mini-Batching",
            "char_start_offset": 6381,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 900
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 93,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 93,
                    "end": 112,
                    "matchedPaperCorpusId": "52156056"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3095703125
        },
        {
            "corpus_id": "277781473",
            "title": "DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training",
            "text": "To understand how DUMP dynamically allocates training effort across data distributions, we analyze the sampling patterns induced by its UCB-based curriculum mechanism. Figure 3 shows the cumulative number of samples drawn from each distribution (3 to 14 characters) over the course of training. We observe a clear curriculum-like progression: distributions corresponding to simpler puzzles (e.g., 3-5 characters) are heavily sampled in the early stages of training, while more complex distributions (e.g., 10-14 characters) are gradually introduced and increasingly prioritized as training progresses. This pattern aligns with the model's evolving capacity-early training favors distributions with high initial advantage magnitudes, and as the model saturates on those, DUMP shifts focus to underexplored but learnable distributions. Importantly, this adaptive sampling behavior emerges automatically from empirical advantage signals without requiring manual specification of curriculum order. These results highlight DUMP's ability to construct an implicit, data-driven curriculum that mirrors traditional easy-to-hard strategies, while remaining responsive to online training dynamics.",
            "score": 0.41585627693399585,
            "section_title": "Analyzing the Automated Curriculum by DUMP",
            "char_start_offset": 22388,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1187
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31884765625
        },
        {
            "corpus_id": "259593617",
            "title": "On the Effectiveness of Curriculum Learning in Educational Text Scoring",
            "text": "For instance, when training on imbalance-distributed image data, the Dynamic Curriculum Learning framework proposed by Wang et al. (2019) employed a two-level curriculum schedulers, which consist of a dynamic sampling scheduler that adjusts the data distribution at each time step and balances the importance between the classification loss and the metric learning loss. In a different vein, when performing the task of natural answer generation, Liu et al. (2018) measured text difficulty from the perspective of Grammar (Stanford Parser score3 ) and trained the model first on the simple and lowquality data and then on the complex and high-quality data to gradually learn to generate reliable answers for questions of different complexity, outperforming the state-of-the-art by an average improvement of about 7.5% in terms of accuracy. More worthy of our attention is that, as indicated by Wang, Chen, and Zhu (2021), CL can be particularly useful when dealing with difficult tasks, e.g., those involving the use of higher-order cognitive skills to develop solutions, e.g., the task of assessing student-author responses investigated in our study. \n\nIt should be pointed out that, though most of the existing CL studies posited that a model's performance can be boosted to the maximum degree by adopting the easy-tohard learning order, there have been some studies (Zhang et al. 2018;Pi et al. 2016;Braun, Neil, and Liu 2017) which demonstrated that, in certain cases, the model could be trained in an opposite learning order, i.e., from harder data to easier data (also called anti-curriculum learning (Wang, Chen, and Zhu 2021). For instance, Zhang et al. ( 2018) demonstrated that the hard-to-easy order, compared to its easy-to-hard counterpart, could lead to better model performance in neural machine translation. \n\nThough CL has been demonstrated effective, few studies attempted to investigate its effectiveness in enhancing the assessment of textual responses authored by students in education, which is often deemed as a challenging and high-stake task (Gierl et al. 2014;Beseiso and Alzahrani 2020;Cao et al. 2020), motivating us to design effective CL strategies to further enhance the performance of existing ATS models.",
            "score": 0.4155633866275401,
            "section_title": "Curriculum Learning",
            "char_start_offset": 11622,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1151
                },
                {
                    "start": 1154,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1823
                },
                {
                    "start": 1826,
                    "end": 2237
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 137,
                    "matchedPaperCorpusId": "58981386"
                },
                {
                    "start": 447,
                    "end": 464,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 1369,
                    "end": 1388,
                    "matchedPaperCorpusId": "233873918"
                },
                {
                    "start": 1388,
                    "end": 1403,
                    "matchedPaperCorpusId": "3194147"
                },
                {
                    "start": 1403,
                    "end": 1429,
                    "matchedPaperCorpusId": "14928979"
                },
                {
                    "start": 2067,
                    "end": 2086,
                    "matchedPaperCorpusId": "28321249"
                },
                {
                    "start": 2086,
                    "end": 2113,
                    "matchedPaperCorpusId": "229256901"
                },
                {
                    "start": 2113,
                    "end": 2129,
                    "matchedPaperCorpusId": "220730151"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1900634765625
        },
        {
            "corpus_id": "218470266",
            "title": "CDL: Curriculum Dual Learning for Emotion-Controllable Response Generation",
            "text": "Intuitively, learning from less noisy and evenquality dataset is simpler, but in this task, the data is inherently complicated as there are multiple emotions mixed in it. To better utilize the data, we integrate curriculum learning into the dual learning framework. The core of curriculum learning (Bengio et al., 2009) is to design an evaluation for complexity, and to provide the model with easy samples first, then gradually increase the difficulty. The curriculum is arranged by sorting each sample in training set according to a specific ranking standard. \n\nHere, We reorder samples from easy, i.e., with high accuracy of emotion classification, to hard. We consider the classification accuracy after pretraining as an indicator of the learning order. Another intuitive way is to put emotionless samples (labelled as \"Neural\") first and then emotional ones, however, it exhibits poor performance in our experiments. At training step t, a batch of training samples is obtained from the top f (t) portions of the entire sorted training samples. Following Platanios et al. ( 2019) and Cai et al. (2020), we define the function f (t) as: \n\nwhere c 2 0 is set to 0.01, which means that the model starts training using the 1% easiest training samples, and T is a hyper-parameter that represents the duration of curriculum learning (curriculum length). At the early stage of the training process, the model learns from the samples in the easy part of the curriculum, where there is only one emotion category. As the advance of the curriculum, the difficulty gradually increases, as complex training samples from more different categories appear. After training T batches, training sample of each batch is drawn from the whole training set, which is the same as the conventional training procedure.",
            "score": 0.4155113505004825,
            "section_title": "Curriculum Plausibility",
            "char_start_offset": 8945,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 560
                },
                {
                    "start": 563,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1138
                },
                {
                    "start": 1141,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1795
                }
            ],
            "ref_mentions": [
                {
                    "start": 298,
                    "end": 319,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12493896484375
        },
        {
            "corpus_id": "254350543",
            "title": "A Curriculum Batching Strategy for Automatic ICD Coding with Deep Multi-Label Classification Models",
            "text": "Elman first proposed a \"starting small strategy\" approach to learn a model with a curriculum [27]. Two methods were developed, including an incremental input method and an incremental memory method. The motivation behind these methods was that compound sentences make it a little difficult for neural networks to learn grammatical concepts in the early phases of training. Therefore, these methods make the models learn gradually, by varying the percentage of simple and compound sentences used during training or constraining the model's capacity in the early phases of training [27]. \n\nBengio et al. [28] revisited Elman's approach and formalized the approach of starting training on easy examples first and then gradually introducing more complex examples during the training. This type of approach was named as \"curriculum learning\", and it can achieve significant improvements in generalization. The core of curriculum learning is to develop a curriculum strategy to train the models from simple to complex. \n\nConsequently, we proposed a simple and effective curriculum batching strategy for automatic ICD coding with deep multi-label classification models in order to solve the inconsistency problem mentioned earlier, improve the model generalization ability and avoid the risk of overfitting. The curriculum batching strategy generates and sequentially utilizes three types of batch sets composed of examples from easy-to-learn to hard-to-learn.",
            "score": 0.41539142748519065,
            "section_title": "Curriculum Learning",
            "char_start_offset": 6608,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 585
                },
                {
                    "start": 588,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1012
                },
                {
                    "start": 1015,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1453
                }
            ],
            "ref_mentions": [
                {
                    "start": 93,
                    "end": 97,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 580,
                    "end": 584,
                    "matchedPaperCorpusId": "2105042"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4609375
        },
        {
            "corpus_id": "231709290",
            "title": "Curriculum Learning: A Survey",
            "text": "transformed into weights and added to the objective function. Ruiter et al. (2020) analyze the behavior of self-supervised neural machine translation systems that jointly learn to select the right training data and to perform translation. In this framework, the two processes are designed in such a fashion that they enable and enhance each other. The authors show that the sampling choices made by these models generate an implicit curriculum that matches the principles of CL: samples are self-selected based on increasing complexity and task-relevance, while also performing a denoising curriculum. Zhao et al. (2020a) introduce a method for generating the right curriculum for neural machine translation. The authors claim that this task highly relies on large quantities of data that are hard to acquire. Hence, they suggest re-selecting influential data samples from the original training set. To discover which examples from the existing data set may further improve the model, the re-selection is designed as a reinforcement learning problem. The state is represented by the features of randomly selected training instances, the action is selecting one of the samples, and the reward is the perplexity difference on a validation set, with the final goal of finding the policy that maximizes the reward. Zhou et al. (2020b) introduce an uncertainty-based curriculum batching approach for neural machine translation. They propose using uncertainty at the data level, for establishing the easy-to-hard ordering, and the model level, to decide the right moment to enhance the training set with more difficult samples. To measure the difficulty of the examples, they start from the intuition that samples with higher crossentropy and uncertainty are more difficult to translate. Thus, the data uncertainty is measured according to its joint distribution, as it is estimated by a language model pre-trained on the training data. On the other hand, the model's uncertainty is evaluated using the variance of the distribution over a Bayesian neural network. Sachan and Xing (2016) propose new heuristics for determining the easiness of examples in an SPL scenario, other than the standard loss function. Aside from the heuristics, the authors highlight the importance of diversity. They measure diversity using the angle between the hyperplanes that the question examples induce in the feature space. Their solution selects a question that is valid according to both criteria, being easy, but also diverse with regards",
            "score": 0.4146082266682252,
            "section_title": "Natural Language Processing",
            "char_start_offset": 87921,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 62,
                    "end": 82,
                    "matchedPaperCorpusId": "222176797"
                },
                {
                    "start": 602,
                    "end": 621,
                    "matchedPaperCorpusId": "211102338"
                },
                {
                    "start": 1311,
                    "end": 1330,
                    "matchedPaperCorpusId": "220047761"
                },
                {
                    "start": 2058,
                    "end": 2080,
                    "matchedPaperCorpusId": "16503693"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8095703125
        },
        {
            "corpus_id": "16503693",
            "title": "Easy Questions First? A Case Study on Curriculum Learning for Question Answering",
            "text": "Cognitive science researchers have emphasized the importance of ordering a complex task into a sequence of easy to hard problems. Such an ordering provides an easier path to learning and increases the speed of acquisition of the task compared to conventional learning. Recent works in machine learning have explored a curriculum learning approach called self-paced learning which orders data samples on the easiness scale so that easy samples can be introduced to the learning algo-rithm \ufb01rst and harder samples can be introduced successively. We introduce a number of heuristics that improve upon self-paced learning. Then, we argue that incorporating easy, yet, a diverse set of samples can further improve learning. We compare these curriculum learning proposals in the context of four non-convex models for QA and show that they lead to real improvements in each of them.",
            "score": 0.4145916014196812,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26318359375
        },
        {
            "corpus_id": "231709290",
            "title": "Curriculum Learning: A Survey",
            "text": "to-global training strategy. The main idea is to first train a model on examples from a certain class, then gradually add more clusters to the training set. Each training round completes when the model converges. The group on which the training commences is randomly selected, while for choosing the next clusters, three different selection criteria are employed. The first one randomly picks the new group and the other two sample the most similar or dissimilar clusters to the groups already selected. Empirical results show that the selection criterion does not impact the superior results of the proposed framework. Pentina et al. (2015) introduce CL for multiple tasks to determine the optimal order for learning the tasks to maximize the final performance. As the authors suggest, although sharing information between multiple tasks boosts the performance of learning models, in a realistic scenario, strong relationships can be identified only between a limited number of tasks. This is why a possible optimization is to transfer knowledge only between the most related tasks. Their approach processes multiple tasks in a sequence, sharing knowledge between subsequent tasks. They determine the curriculum by finding the right task order to maximize the overall expected classification performance. Yu et al. (2020) introduce a multi-task curriculum approach for solving the open-set semi-supervised learning task, where out-of-distribution samples appear in unlabeled data. On the one hand, they compute the out-of-distribution score automatically, training the network to estimate the probability of an example of being out-of-distribution. On the other hand, they use easy in-distribution examples from the unlabeled data to train the network to classify in-distribution instances using a semi-supervised approach. Furthermore, to make the process more robust, they employ a joint operation, updating the network parameters and the scores alternately. Guo et al. (2020b) tackle the task of automatically finding effective architectures using a curriculum procedure. They start searching for a good architecture in a small space, then gradually enlarge the space in a multistage approach. The key idea is to exploit the previously learned knowledge: once a fine architecture has been found in the smaller space, a larger, better, candidate subspace that shares common information with the previous space can be discovered. Gong et al. (2016) tackle the semi-supervised image classification task in a curriculum fashion, using multiple",
            "score": 0.41450503084971296,
            "section_title": "Multiple tasks",
            "char_start_offset": 50584,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 620,
                    "end": 641,
                    "matchedPaperCorpusId": "8502955"
                },
                {
                    "start": 1306,
                    "end": 1322,
                    "matchedPaperCorpusId": "220686906"
                },
                {
                    "start": 1962,
                    "end": 1980,
                    "matchedPaperCorpusId": "220514208"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1802978515625
        },
        {
            "corpus_id": "261704344",
            "title": "Aligning Semantic in Brain and Language: A Curriculum Contrastive Method for Electroencephalography-to-Text Generation",
            "text": "3) Curriculum Level: What are the curriculum levels? We conduct preliminary experiments by setting up the number of curriculum levels from 2 to 5 and finally decide to split the \u00c8+ i and \u00c9\u2212 i into 3 levels due to their better performance. In detail, we split the sorted \u00c8+ i into three equal-length parts, including \n\nIn other words, we obtain curriculums of different difficulty according to the length of sorted \u00c8+ i and \u00c9\u2212 i . Fig. 4 shows two examples of contrastive pairs of different difficulties. We can clearly find the easy pair Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply. already satisfies the condition: positive pairs are similar while negative pairs are dissimilar. In contrast, the hard contrastive pair instead follows the condition: positive pairs are dissimilar while negative pairs are similar. \n\n4) Curriculum Scheduler: When to update the curriculum? We adopt a One-Pass scheduler with a linear pace [3] to progressively train the model in an easy-to-hard order. One-Pass scheduler means that training the model only once per curriculum, while linear pace ensures that each curriculum takes the same amount of training time. In detail, when reaching the hard level, given an anchor EEG E i , we select the positive sample and negative sample from E hard+ i and E hard\u2212 i , respectively.",
            "score": 0.4135161677354194,
            "section_title": "B. Curriculum Learning",
            "char_start_offset": 13341,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 53,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 315
                },
                {
                    "start": 318,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1374
                }
            ],
            "ref_mentions": [
                {
                    "start": 988,
                    "end": 991,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.059967041015625
        },
        {
            "corpus_id": "232307174",
            "title": "Token-wise Curriculum Learning for Neural Machine Translation",
            "text": "Neural Machine Translation (NMT) has achieved significant progress in recent years (Sutskever et al., 2014;Bahdanau et al., 2014;Vaswani et al., 2017), mainly in the scenarios where the parallel training corpora are abundant. However, training corpora can be limited in some domains (e.g., spoken language (Cettolo et al., 2015)) and languages (e.g., African languages) due to the high cost of data acquisition. Koehn and Knowles (2017); Lample et al. (2018) show that NMT models do not perform well in such data-limited settings.\n\nTo improve NMT with limited data, researchers resort to large amounts of auxiliary data. One line * Work was done at Microsoft Azure AI. of research leverages the knowledge from highresource parallel corpora. For examples, some works pre-train NMT models on high-resource data, and then fine-tune them on low-resource data (Zoph et al., 2016;Chen et al., 2017;Kocmi and Bojar, 2018;Neubig and Hu, 2018;Nguyen and Chiang, 2017); others train Multilingual or Multitask NMT models jointly on both high-resource and low-resource datasets (Gu et al., 2018a,b;Aharoni et al., 2019;Jiang et al., 2019;Siddhant et al., 2020). The other line exploits high-resource monolingual data as auxiliary data to train NMT models in a semi-supervised manner (Sennrich et al., 2015;Currey et al., 2017;Cheng, 2019).\n\nAside from previous approaches, curriculum learning (Bengio et al., 2009) is proposed to address the data insufficiency issue via utilizing the limited data more efficiently (Zhang et al., 2019b). The idea of curriculum learning is to sample training data in an order of increasing difficulty. The \"easy\" samples of such a curriculum can be beneficial to the training of the models at the early stage. There have been multiple designs of curriculum for NMT in the recent literatures (Zhou et al., 2020;Liu et al., 2020;Ru",
            "score": 0.4134043280542653,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 83,
                    "end": 107,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 129,
                    "end": 150,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 306,
                    "end": 328,
                    "matchedPaperCorpusId": "51755061"
                },
                {
                    "start": 1314,
                    "end": 1326,
                    "matchedPaperCorpusId": "256189"
                },
                {
                    "start": 1381,
                    "end": 1401,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30322265625
        },
        {
            "corpus_id": "232148117",
            "title": "Virtual Normal: Enforcing Geometric Constraints for Accurate and Robust Depth Prediction",
            "text": "We sort the training data by the increasing difficulty and sample a series of mini-batches that exhibit an increasing level of difficulty. Therefore, there are two problems that should be solved: 1) how to construct the curriculum; 2) how to yield a sequence of easy-to-hard mini-batches for the network. Pseudo-code for multi-curriculum algorithm is shown in Algorithm 1. \n\nThree parts of DiverseDepth, i.e., part-fore, part-in and part-out, are termed as X = {D j } P j=0 . Let D j = {(x ij , y ij )|i = 0, ..., N } represents the N data points of the part j, where x ij denotes a single data, y ij is the corresponding label. We train three models, G j , separately on 3 parts as teachers. The absolute relative error (Abs-Rel) is chosen as the scoring function F(\u2022) to evaluate the difficulty of each training sample. If F(G j (x ij ), y ij ) > F(G j (x (i+1)j ), y (i+1)j ), then we define the data (x ij , y ij ) is more difficult to learn. Finally, we sort 3 parts according to the ascending Abs-Rel error and the ranked datasets are C j = {(x ij , y ij )|i = 0, ..., N }. \n\nThe pacing function H(\u2022) determines a sequence of subsets of the dataset so that the likelihood of the easier data would decrease in this sequence, i.e., {S 0j , . . . , S Kj } \u2286 C j , where S kj represents the first H(k, j) elements of C j . From each subset S kj , a sequence of mini-batches {B 0j , ..., B M j |j = 0, 1, 2} are uniformly sampled. Here we utilize the stair-case function as the pacing function, which is determined by the starting sampling percentage p j , the current step k, and the fixed step length I o (the number of iterations in each step).",
            "score": 0.41287200043719646,
            "section_title": "Real Camera System",
            "char_start_offset": 27636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 372
                },
                {
                    "start": 375,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1079
                },
                {
                    "start": 1082,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1648
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.040618896484375
        },
        {
            "corpus_id": "264288956",
            "title": "Harnessing Dataset Cartography for Improved Compositional Generalization in Transformers",
            "text": "We use dataset cartography to examine the impact of training dynamics on curriculum learning. Curriculum learning is a strategy that trains models on instances from easy to hard, based on the assumption that this order facilitates learning. However, we also explore the opposite strategy, which trains models on instances from hard to easy, and compare it with the conventional curriculum learning approach. This way, we can study how different training schedules affect the model performance. \n\nFigure 3 depicts accuracy plots showing the performance of various CL strategies based on (Hacohen and Weinshall, 2019) on the CFQ dataset. The figure legends indicate the ranking scheme and the employed confidence measure. For instance, hardto-learn (Inv PPL) refers to the case where Inv PPL is being used as the confidence measure, and the inclusion of the hard-to-learn samples is prioritized within the curriculum. Our analysis reveals that no single curriculum consistently outperforms others on the CFQ dataset. Exponential pacing leads to stagnant performance in the final 2/7 th of the training process due to surpassing the training size percentages of 33% and 50%. Surprisingly, initiating training with hard-to-learn samples yields superior performance compared to easy-to-learn samples, contrary to common curriculum learn-  ing expectations. This aligns with our previous findings, emphasizing the benefits of starting with challenging examples for improved adaptation. \n\nFigure 4 examines the impact of leveraging data maps within the CL strategy proposed by Zhang et al. (2019) for compositional generalization. The hard-to-learn (BLEU) configuration outperforms the no curriculum strategy, albeit with no notable improvement in convergence speed. This outcome mirrors our observations using the CL framework developed by Hacohen and Weinshall (2019), where initiating training with harder samples leads to better performance. However, the ambiguous configurations perform similarly to no curriculum, while the easy-to-learn configurations yield worse results than the no curriculum approach. \n\nIn Figures 5 and 6, we gain deeper insights into the contributions of dataset cartography. Overall, hard-to-learn (BLEU) emerges as the most effective configuration in the plots.",
            "score": 0.4124432488213232,
            "section_title": "Impact of Cartography-Based Curriculum Learning",
            "char_start_offset": 18134,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 493
                },
                {
                    "start": 496,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1479
                },
                {
                    "start": 1482,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2104
                },
                {
                    "start": 2107,
                    "end": 2197
                },
                {
                    "start": 2198,
                    "end": 2285
                }
            ],
            "ref_mentions": [
                {
                    "start": 1570,
                    "end": 1589,
                    "matchedPaperCorpusId": "155089817"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1326904296875
        },
        {
            "corpus_id": "237091233",
            "title": "Improving Self-supervised Learning with Hardness-aware Dynamic Curriculum Learning: An Application to Digital Pathology",
            "text": "data is presented to the network (Q2). Typically, an easy to hard (i.e., lowest to highest score (s)) strategy is followed to determine the ordering of samples during training. However, in the context of CL, we argue that there exist two main limitations: (i) due to randomness of SGD optimization, the instantaneous hardness of each training sample can vary significantly over consecutive epochs, which may not reflect the true hardness level of a sample over time with the model being trained. This is because the easier samples stay easy throughout training since their loss value is more likely to stay at samples minima; while for hard examples, the loss value is relatively less stable during the early part of the training and gradually stabilizes as we train more on them. Thus the instantaneous hardness level of a sample tends to decrease monotonically during training and cannot be at a fixed level; (ii) further, keeping track of the instantaneous hardness of each sample up-to-date requires extra inference computation over all training samples, which can be computationally challenging for neural networks [21].\n\nThe aforementioned limitations motivated us to propose a \"hardness-aware dynamic curriculum learning (HaDCL)\" approach to dynamically determine the sample's instantaneous hardness level over the gradual course of training. Our proposed approach consists of a dual-stage curriculum training strategy, which we apply during downstream fine-tuning. In the first stage, we focus on easy-tohard samples, and in the second stage, we focus on hardto-very-hard samples for fine-tuning the pretrained SSL model.\n\nIn Curriculum-I (i.e., easy-to-hard) stage, we first initialize the downstream fine-tuning model f f t (\u00b7; \u03b8) with the pretrained SSL model f pre (\u00b7; \u03b8), and compute loss for all\n\nin a mini-batch B using categorical cross-entropy. Next, all B samples within a mini-batch B are sorted in descending order by their loss value \u2113 to obtain a setD. From the sorted setD, we select the top-K samples that constitute the hard examples: top-K = \u03b1 \u00d7 B, where \u03b1 is parameter (0 \u2264 \u03b1 \u2264 1) which denotes the portion of hard samples in a setD",
            "score": 0.4124387098719182,
            "section_title": "Hardness-aware Dynamic Curriculum Learning (HaDCL)",
            "char_start_offset": 13933,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08331298828125
        },
        {
            "corpus_id": "261394835",
            "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
            "text": "Curriculum learning. Curriculum learning, as introduced by Bengio et al. [3], is a strategy aimed at organizing input data or tasks in a meaningful order, from easy to hard, to enhance the overall learning outcome. It consists of two main components: a curriculum criterion [3] and a scheduling function [3]. Approaches in curriculum learning can be categorized into easy-to-hard (standard curriculum) and hard-to-easy (anti-curriculum) paradigms [43]. In the easyto-hard paradigm, tasks are presented to the model in increasing order of complexity [3,9,26,34,40], while the hardto-easy paradigm reverses this order [4,42]. Constructing a curriculum involves using either approaches based on external complexity measures, such as the degree of occlusion and the complexity of the shape [3,16], or self-paced learning techniques [22,27,29,52], in which the neural network dynamically assesses the difficulty of training samples based on their loss. The scheduling function determines when and how to update the training process and can be categorized as discrete or continuous. Discrete schedulers [3,44] sort and divide the data into discrete subsets, according to the curriculum criterion. Conversely, continuous schedulers [22,36] provide a gradually increasing proportion of difficult training samples to the model. Our method incorporates an easy-to-hard continuous scheduler based on the complexity of the pretext task, where the reconstruction error of the model is used as a measure to construct the curriculum. Our novel curriculum learning strategy is deeply integrated within the proposed masking module. The module generates easy masks by hiding tokens with low reconstruction errors and hard masks by hiding tokens with high reconstruction errors. The complexity of the task gradually increases during training from easy-to-hard, and, at some point, the masking module learns to produce extremely hard masks via adversarial training.",
            "score": 0.41242645278388296,
            "section_title": "Related Work",
            "char_start_offset": 10316,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1945
                }
            ],
            "ref_mentions": [
                {
                    "start": 73,
                    "end": 76,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 274,
                    "end": 277,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 304,
                    "end": 307,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 447,
                    "end": 451,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 549,
                    "end": 552,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 552,
                    "end": 554,
                    "matchedPaperCorpusId": "5658192"
                },
                {
                    "start": 554,
                    "end": 557,
                    "matchedPaperCorpusId": "879067"
                },
                {
                    "start": 557,
                    "end": 560,
                    "matchedPaperCorpusId": "8502955"
                },
                {
                    "start": 560,
                    "end": 563,
                    "matchedPaperCorpusId": "3465897"
                },
                {
                    "start": 616,
                    "end": 619,
                    "matchedPaperCorpusId": "14928979"
                },
                {
                    "start": 619,
                    "end": 622,
                    "matchedPaperCorpusId": "2843566"
                },
                {
                    "start": 786,
                    "end": 789,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 789,
                    "end": 792,
                    "matchedPaperCorpusId": "213005467"
                },
                {
                    "start": 828,
                    "end": 832,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 832,
                    "end": 835,
                    "matchedPaperCorpusId": "51876228"
                },
                {
                    "start": 838,
                    "end": 841,
                    "matchedPaperCorpusId": "5004002"
                },
                {
                    "start": 1097,
                    "end": 1100,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1100,
                    "end": 1103,
                    "matchedPaperCorpusId": "1363892"
                },
                {
                    "start": 1225,
                    "end": 1229,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 1229,
                    "end": 1232,
                    "matchedPaperCorpusId": "85498775"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1094970703125
        },
        {
            "corpus_id": "226964591",
            "title": "Sampling Approach Matters: Active Learning for Robotic Language Acquisition",
            "text": "Ordering the selection of training data using active learning can lead to improvements in learning efficiently from smaller corpora. We present an exploration of active learning approaches applied to three grounded language problems of varying complexity in order to analyze what methods are suitable for improving data efficiency in learning. We present a method for analyzing the complexity of data in this joint problem space, and report on how characteristics of the underlying task, along with design decisions such as feature selection and classification model, drive the results. We observe that representativeness, along with diversity, is crucial in selecting data samples.",
            "score": 0.41217862564284274,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72509765625
        },
        {
            "corpus_id": "248006297",
            "title": "Learning to Solve Travelling Salesman Problem with Hardness-adaptive Curriculum",
            "text": "Curriculum learning (Portelas et al. 2020;Wang, Chen, and Zhu 2021) studies how to improve training strategies by manipulating the data distribution according to the model training stage, aiming to help the model train efficiently and obtain better performance (Bengio et al. 2009;Hacohen and Weinshall 2019).\n\nA family of classical methods trains the target model with samples from easy to challenging (Bengio et al. 2009;Kumar, Packer, and Koller 2010;Platanios et al. 2019;Guo et al. 2018;Florensa et al. 2017), mimicking how humans learn. To perform curriculum learning, one has to first measure the hardness of samples and then use a curriculum scheduler to determine when and how to feed data into the model. In many fields, hardness measurement is based on domain knowledge (e.g., sentence length in machine translation (Platanios et al. 2019), signal to noise ratio in speech recognition (Ranjan and Hansen 2017), and others (Tudor Ionescu et al. 2016;Soviany et al. 2020;Wei et al. 2016)). For learning-based TSP, Lisicki et al. (Lisicki, Afkanpour, and Taylor 2020) takes the size of TSP as an indicator of hardness and train the model by gradually increasing the TSP size. In this paper, we focus on TSP instances of the same size but come from different distributions.",
            "score": 0.41135019534921136,
            "section_title": "Curriculum Learning",
            "char_start_offset": 8059,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 42,
                    "matchedPaperCorpusId": "212644660"
                },
                {
                    "start": 261,
                    "end": 281,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 403,
                    "end": 423,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 423,
                    "end": 454,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 476,
                    "end": 492,
                    "matchedPaperCorpusId": "51920640"
                },
                {
                    "start": 940,
                    "end": 960,
                    "matchedPaperCorpusId": "879067"
                },
                {
                    "start": 960,
                    "end": 980,
                    "matchedPaperCorpusId": "204801162"
                },
                {
                    "start": 980,
                    "end": 996,
                    "matchedPaperCorpusId": "1821165"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.297607421875
        },
        {
            "corpus_id": "231709290",
            "title": "Curriculum Learning: A Survey",
            "text": "compare their different answers. The learning is guided by a curriculum that is generated taking into consideration the conflicts between the two networks or the value of the loss function. Tay et al. (2019) propose a generative curriculum pretraining method for solving the problem of reading comprehension over long narratives. They use an easy-to-hard curriculum approach on top of a pointer-generator model which allows the generation of answers even if they do not exist in the context, thus enhancing the diversity of the data. The authors build the curriculum considering two concepts of difficulty: answerability and understandability. The answerability measures whether an answer exists in the context, while understandability controls the size of the document. Xu et al. (2020) attempt to improve the standard \"pre-train then fine-tune\" paradigm which is broadly used in natural language understanding, by replacing the traditional training from the fine-tuning stage, with an easy-to-hard curriculum. To assess the difficulty of an example, they measure the performance of multiple instances of the same model, trained on different shards of the data set, except the one containing the example itself. In this way, they obtain an ordering of the samples which they use in a curriculum batching strategy for training the same model. Penha and Hauff (2019) investigate curriculum strategies for information retrieval, focusing on conversation response ranking. They use multiple difficulty metrics to rank the examples from easy to hard: information spread, distraction in responses, response heterogeneity, and model confidence. Furthermore, they experiment with multiple methods of selecting the data, using a standard batching approach and other continuous sampling methods. Li et al. (2020) propose a label noise-robust curriculum batching strategy for deep paraphrase identification. They use a combination of two predefined metrics in order to create the easy-to-hard batches. The first metric uses the losses of a model trained for only a few iterations. Starting from the intuition that neural networks learn fast from clean samples and slowly from noisy samples, they design the loss-based noise metric as the mean value of a sequence of losses for training examples in the first epochs. The other criterion is the similarity-based noise metric which computes the similarity between the two sentences using the Jaccard similarity coefficient. Chang et al. (2021)",
            "score": 0.4108029154828308,
            "section_title": "Question answering",
            "char_start_offset": 95827,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 207,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 771,
                    "end": 787,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 1787,
                    "end": 1803,
                    "matchedPaperCorpusId": "221659997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.420654296875
        },
        {
            "corpus_id": "247794120",
            "title": "Weakly-supervised Temporal Path Representation Learning with Contrastive Curriculum Learning",
            "text": "Motivated by Xu et al. [40], we decompose curriculum sample generation into two stages: Curriculum Sample Evaluation and Curriculum Sample Selection, as shown in Fig. 6. \n\n1) In curriculum sample evaluation, we assign a difficulty score S i to path p i in the training dataset D. The score reflects the difficulty of the model to learn a good representation w.r.t. path p i . 2) In curriculum sample selection, we aim to partition the training data into different difficulty stages. More specifically, we first sort the training data according to the difficulty scores. Then, we split the sorted training data into a sequence of sorted learning stages {ST i |i = 1, 2, . . . , M } in an easy-to-difficult fashion. Finally, our base model, WSC, is trained according to this curriculum. We detail these two stages in Sections VI-B and VI-C, respectively.",
            "score": 0.41045944639441756,
            "section_title": "A. Overview of Curriculum Learning",
            "char_start_offset": 29131,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 172,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 852
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "220045816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3291015625
        },
        {
            "corpus_id": "269756933",
            "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
            "text": "The rapid advancement of Large Language Models (LLMs) has improved text understanding and generation but poses challenges in computational resources. This study proposes a curriculum learning-inspired, data-centric training strategy that begins with simpler tasks and progresses to more complex ones, using criteria such as prompt length, attention scores, and loss values to structure the training data. Experiments with Mistral-7B (Jiang et al., 2023) and Gemma-7B (Team et al., 2024) models demonstrate that curriculum learning slightly improves performance compared to traditional random data shuffling. Notably, we observed that sorting data based on our proposed attention criteria generally led to better performance. This approach offers a sustainable method to enhance LLM performance without increasing model size or dataset volume, addressing scalability challenges in LLM training.",
            "score": 0.41045944639441756,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67236328125
        },
        {
            "corpus_id": "264439382",
            "title": "DoGE: Domain Reweighting with Generalization Estimation",
            "text": "Data Selection for Language Modeling. Many works show how a rigorously selected training corpus can effectively   However, due to scalability issues, most traditional data selection methods fail to be applicable for pretraining. \n\nClassifier-based data filtering techniques are commonly used to construct a pretraining corpus (Gao et al., 2020;Penedo et al., 2023). Everaert & Potts (2023) propose GIO to select a subset that minimizes the KL-divergence to the target distribution, yet incurs high computation complexity. 2023) propose to build an online resampling curriculum by exploiting the dependency relationship among skills represented by a directed skill graph. While the computation cost for constructing the skill graph limits its applicability to general language model pretraining.",
            "score": 0.41045944639441756,
            "section_title": "Related Work",
            "char_start_offset": 21997,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 228
                },
                {
                    "start": 231,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 794
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88818359375
        },
        {
            "corpus_id": "231846815",
            "title": "Does the Order of Training Samples Matter? Improving Neural Data-to-Text Generation with Curriculum Learning",
            "text": "Neural data-to-text generation has been the subject of much recent research. The task aims at transforming source-side structured data into targetside natural language descriptions (Reiter and Dale, 2000; Barzilay and Lapata, 2005). The process typically involves mini-matches which are randomly sampled with a fixed size from the training set to feed into the model at each training step. In this paper, we apply curriculum learning to this process, which was explored in neural machine translation (Platanios et al., 2019;Zhou et al., 2020), and show how it can help in neural data-to-text generation.\n\nThe main idea in curriculum learning is to present the training data in a specific order, starting from easy examples and moving on to more difficult ones, as the learner becomes more competent. When starting out with easier instances, the risk of getting stuck in local optima early on in training is reduced, since the loss functions in neural models are typically highly non-convex (Bengio et al., 2009). This learning paradigm enables flexible batch configurations by considering the material properties as well as the state of the learner. The idea brings in two potential benefits: (1) It speeds up the convergence and reduces the computational cost.\n\n(2) It boosts the model performance, without having to change the model or add data.\n\nWith the release of large data-to-text datasets (e.g. Wikibio (Lebret et al., 2016), Totto (Parikh et al., 2020), E2E (Novikova et al., 2017)), neural data-to-text generation is now at a point where training speed and the order of samples may begin to make a real difference. We here show the efficacy of curriculum learning with a general LSTM-based sequence-to-sequence model and define difficulty metrics that can assess the training instances, using a sucessful competence function which estimates the model capability during training. Such metrics have not yet been explored in neural data-to-text generation.\n\nIn this paper, we explore the effectiveness of various difficulty metrics and propose a soft edit distance metric, which leads to substantial improvements over other metrics. Crucially, we observe that difficulty metrics that consider data-text samples jointly",
            "score": 0.41000899185218065,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 231,
                    "matchedPaperCorpusId": "9482302"
                },
                {
                    "start": 500,
                    "end": 524,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 524,
                    "end": 542,
                    "matchedPaperCorpusId": "220047761"
                },
                {
                    "start": 990,
                    "end": 1011,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1411,
                    "end": 1432,
                    "matchedPaperCorpusId": "1238927"
                },
                {
                    "start": 1467,
                    "end": 1490,
                    "matchedPaperCorpusId": "19662556"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3251953125
        },
        {
            "corpus_id": "273482581",
            "title": "Preview-based Category Contrastive Learning for Knowledge Distillation",
            "text": "The idea of curriculum learning is inspired by the human learning process. Human usually learns easy knowledge first and then gradually understands more complicated concepts. The concept of curriculum learning is first proposed by [51] and it has been widely adopted for image classification [52], [53] and object detection [54]. Since the curriculum is predetermined by prior knowledge, Kumar et al. [55] presented the self-paced learning algorithm to generate the curriculum by the learner itself. Tang et al. [52] proposed to adaptively choose easy samples in each iteration for dictionary learning. Combining the merits of prior knowledge and self-paced learning, Jiang et al. [56] came up with a learning paradigm called SPCL. More recently, Graves et al. [57] proposed an algorithm for automatically selecting syllabus to improve the efficiency of curriculum selection. Fan et al. [58] incorporated teacher guidance into curriculum learning in which two intelligent agents (called student model and teacher model) interact with each other. At present, curriculum learning is rarely used in knowledge distillation. Zhu et al. [59] combined the knowledge distillation and the curriculum learning to achieve good results in dialogue generation. Tudor et al. [60] presented the human response time for estimating image difficulty. Xiang et al. [61] presented the LFME framework, including self-paced knowledge distillation and curriculum instance selection, which is specially designed for long-tailed classification and unsuitable for general knowledge distillation. Instead of the classic curriculum learning, we further present a novel preview strategy to assist the student model to evaluate the difficulty level of samples, so that it can assign a small weight for these hard examples at the beginning and accept the guidance of the teacher model more easily, which is also consistent with the human learning process.",
            "score": 0.4098268792905259,
            "section_title": "C. Curriculum Learning",
            "char_start_offset": 11866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1924
                }
            ],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 235,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 292,
                    "end": 296,
                    "matchedPaperCorpusId": "2603360"
                },
                {
                    "start": 298,
                    "end": 302,
                    "matchedPaperCorpusId": "10364203"
                },
                {
                    "start": 324,
                    "end": 328,
                    "matchedPaperCorpusId": "52114770"
                },
                {
                    "start": 401,
                    "end": 405,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 512,
                    "end": 516,
                    "matchedPaperCorpusId": "2603360"
                },
                {
                    "start": 761,
                    "end": 765,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 887,
                    "end": 891,
                    "matchedPaperCorpusId": "5933530"
                },
                {
                    "start": 1131,
                    "end": 1135,
                    "matchedPaperCorpusId": "244119160"
                },
                {
                    "start": 1261,
                    "end": 1265,
                    "matchedPaperCorpusId": "879067"
                },
                {
                    "start": 1346,
                    "end": 1350,
                    "matchedPaperCorpusId": "209862398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1607666015625
        },
        {
            "corpus_id": "265351926",
            "title": "Complexity-Guided Curriculum Learning for Text Graphs",
            "text": "Following the core principle of curriculum learning (Bengio et al., 2009), we propose to gradually increase the contribution of harder samples as training progresses. Specifically, we derive the competence function c(t) that determines the top fraction of training samples that the model is allowed to use for training at time step t. We derive a general form of c(t) by assuming that the rate of competencethe rate by which new samples are added to the current training data-is equally distributed across the remaining training time: \n\nwhere t \u2208 [0, 1] is the normalized value of the current training time step, with t = 1 indicating the time after which the learner is fully competent. Solving this differential equation, we obtain: \n\nwhich results in c(t) = 1 \u2212 exp(b)(1 \u2212 t) for some constant b. Assuming the initial competence c(t = 0) is c 0 and final competence c(t = 1) is 1, we obtain the following linear competence function: \n\nWe modify the above function by allowing flexibility in competence so that models can use \n\nwhere \u03b1 > 0 specifies the rate of change for competence during training. As Figure 2 shows, a larger \u03b1 quickly increases competence, allowing the model to use more data after a short initial training with easier samples. We expect such curricula to be more suitable for datasets with lower prevalence of easier samples than harder ones, so that the learner do not spend excessive time on the small set of easy samples at earlier stages of training. On the other hand, a smaller \u03b1 results in a curriculum that allows more time for learning from easier samples. We expect such curricula to be more suitable for datasets with greater prevalence of easier samples, as it provides sufficient time for the learner to assimilate the information content in easier samples before gradually moving to harder ones.",
            "score": 0.4096842940902103,
            "section_title": "Competence for Gradual Inclusion",
            "char_start_offset": 6323,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 534
                },
                {
                    "start": 537,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 734
                },
                {
                    "start": 737,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 935
                },
                {
                    "start": 938,
                    "end": 1027
                },
                {
                    "start": 1030,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1833
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 73,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.310302734375
        },
        {
            "corpus_id": "267681974",
            "title": "QuRating: Selecting High-Quality Data for Training Language Models",
            "text": "Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that can capture human intuitions about data quality. In this paper, we investigate four qualities - writing style, required expertise, facts&trivia, and educational value - and find that LLMs are able to discern these qualities, especially when making pairwise judgments of texts. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity. When we sample using quality ratings as logits over documents, our models obtain lower perplexity and stronger in-context learning performance than baselines. Our best model is based on educational value and performs similarly to a model trained with uniform sampling for 50% more steps. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications.",
            "score": 0.40965251188195867,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94970703125
        },
        {
            "corpus_id": "276250237",
            "title": "Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining",
            "text": "Training Data Re-weighting/Selection for LLMs. Several recent studies (Xie et al., 2023;Chen et al., 2023;Fan et al., 2023;Thakkar et al., 2023) have explored various reweighting techniques to enhance the generalization and efficiency of language models pretraining. For instance, Xie et al. ( 2023) and Fan et al. (2023) optimize the composition of pretraining corpora to achieve better performance across pretraining domains or for out-of-domain generalization. Chen et al. (2023) introduce a framework for ordered skill learning, optimizing data selection based on how effectively it teaches interdependent skills for continual pretraining and fine-tuning regimes. Although effective, these techniques operate at the group level, whereas our work explores reweighting at the instance level, offering finer control over how individual samples are treated based on their loss values. Furthermore, we demonstrate that combining domain-level methods such as DoReMi (Xie et al., 2023) or DoGE (Fan et al., 2023) with our instance-level reweighting methods results in improved performance across multiple domains. Instance-level reweighting has been used in post-training settings of LLMs (Chen et al., 2024;Jiang et al., 2024). Jiang et al. (2024) boost the self-improvement abilities of LLMs by employing sample reweighting to filter out self-generated data that have correct answers but exhibit high distribution shifts. Chen et al. (2024) reweight individual samples during continual training/instruction-tuning to focus on medium-loss samples. In contrast, our work systematically studies the effects of various sample-level, loss-based reweighting strategies on the efficiency and effectiveness of LLMs pretraining. The approach in Fan & Jaggi (2023) offers a curriculum learning framework that prioritizes samples with a higher learnability score, which is precomputed using another auxiliary model similar to DoReMi and DoGE.",
            "score": 0.4095352482820364,
            "section_title": "INTRODUCTION",
            "char_start_offset": 6453,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 47,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1930
                }
            ],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 106,
                    "matchedPaperCorpusId": "260203057"
                },
                {
                    "start": 464,
                    "end": 482,
                    "matchedPaperCorpusId": "260203057"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81494140625
        },
        {
            "corpus_id": "274234433",
            "title": "Curriculum-enhanced GroupDRO: Challenging the Norm of Avoiding Curriculum Learning in Subpopulation Shift Setups",
            "text": "Our Curriculum-enhanced GroupDRO method is formally described in Alg. 1. Prior to our procedure, we separate the training data, based on G, in two main subsets: bias-confirming DB \u2282 D and bias-conflicting DC \u2282 D, as described in Section 4. We then, at line 1, train an ERM model, M * , featuring the same architecture as our final desired model, for a single epoch on the entire training set. The loss of M * with respect to DB and DC is used to sort out the samples from each subset. We store the sorted indices of samples from DC in J at line 3, and the sorted indices of samples from DB, in reversed order, prioritizing the hardest ones, in I at line 2. At line 4, we initialize the percentage of available samples P with a hyperparameter R, which controls the rate of increase for the percentage of available samples at each stage during the procedure. The number of samples available at a given stage of the curriculum is N = \u230a|DC| * P)\u230b, computed at line 6. We define the training subset S for the current stage, by selecting the first N samples from DB and DC, in terms of the sorted indices from I and J respectively, at line 7. \n\nThis selection ensures that the features of what will become, in the last stage of training, the worst represented groups, are readily available at the beginning. It further ensures that it is as hard as possible for the network to associate the biased features with their respective classes. We account for the fact that this selection is likely to slightly imprint the model weights with the opposite bias, by means of providing an equal number of biasconflicting and bias-confirming samples at each training stage, and by leveraging GroupDRO to balance the initial discrepancy in terms of difficulty. Through these measures combined we aim to skim the likelihood of an initial imbalance as much as possible. At each stage, we train the model M on the subset S for E s epochs. We then increase P by R and continue the process while P \u2264 1. When P = 1, the entirety of the bias-conflicting data is available for training, together with an equal amount of biasconfirming data.",
            "score": 0.40932661591195457,
            "section_title": "III. METHOD",
            "char_start_offset": 8226,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1136
                },
                {
                    "start": 1139,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2114
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.041229248046875
        },
        {
            "corpus_id": "254043593",
            "title": "Learning to Learn: How to Continuously Teach Humans and Machines",
            "text": "Curriculum learning refers to learning with a meaningful ordering of training examples, commonly from \"easier\" to \"harder\" data [8,3].\n\nThe efficacy of proposed curricula is evaluated in terms of generalization to test data and convergence speed during training. Previous works in curriculum learning can be categorized into predefined curriculum learning [8,57,12,13] and automatic curriculum learning [63,30,16,22]. Predefined curriculum learning entails designing a data scheduler or a difficulty measure with human priors. These algorithms work well when designed for specific tasks, but generalize poorly to out-of-domain tasks. In contrast, we propose an automatic curriculum designer that can design and rank curricula based on inter-class feature differences.\n\nIn automatic curriculum learning, most works adopt data-driven approaches [30,16,22] and RL-based approaches incorporating student feedback [55,26,15,44,52]. These methods are often deployed in teaching both machines [60,54,23,69,58,49,32,19,46] and humans [55,26,15,44,52]. In image classification settings, curriculum learning approaches are almost exclusively oriented toward measuring intra-class example difficulty. Existing methods specifically focus on a single multi-class object recognition task [65,59,50,23] in which all examples from each class can be trained on multiple times. We deviate from previous studies in examining the order in which classes or tasks are presented to the network, rather than the ordering of training examples within one task.\n\nOne recent study highlighted how the most widely-used curriculum design strategy (increasing difficulty) may not always be optimal, and how anti-curricula (\"harder\" to \"easier\") or random orderings yield comparable results in multi-class image classification settings [65]. The study reported that curriculum effects become stronger when the number of training iterations is limited. Aligned with this constraint, we investigated the effect of curriculum on CL algorithms under stringent online conditions where training is limited to a single pass through the data.",
            "score": 0.4086728795216501,
            "section_title": "Curriculum Learning",
            "char_start_offset": 8008,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 128,
                    "end": 131,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 356,
                    "end": 359,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 359,
                    "end": 362,
                    "matchedPaperCorpusId": "204801162"
                },
                {
                    "start": 362,
                    "end": 365,
                    "matchedPaperCorpusId": "5658192"
                },
                {
                    "start": 403,
                    "end": 407,
                    "matchedPaperCorpusId": "232362223"
                },
                {
                    "start": 413,
                    "end": 416,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 850,
                    "end": 853,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 916,
                    "end": 919,
                    "matchedPaperCorpusId": "155088184"
                },
                {
                    "start": 919,
                    "end": 922,
                    "matchedPaperCorpusId": "235355778"
                },
                {
                    "start": 922,
                    "end": 925,
                    "matchedPaperCorpusId": "116864392"
                },
                {
                    "start": 986,
                    "end": 990,
                    "matchedPaperCorpusId": "879067"
                },
                {
                    "start": 990,
                    "end": 993,
                    "matchedPaperCorpusId": "69357784"
                },
                {
                    "start": 993,
                    "end": 996,
                    "matchedPaperCorpusId": "51920640"
                },
                {
                    "start": 996,
                    "end": 999,
                    "matchedPaperCorpusId": "220301722"
                },
                {
                    "start": 999,
                    "end": 1002,
                    "matchedPaperCorpusId": "208138033"
                },
                {
                    "start": 1002,
                    "end": 1005,
                    "matchedPaperCorpusId": "198953430"
                },
                {
                    "start": 1005,
                    "end": 1008,
                    "matchedPaperCorpusId": "203836976"
                },
                {
                    "start": 1008,
                    "end": 1011,
                    "matchedPaperCorpusId": "19181872"
                },
                {
                    "start": 1011,
                    "end": 1014,
                    "matchedPaperCorpusId": "11009476"
                },
                {
                    "start": 1033,
                    "end": 1036,
                    "matchedPaperCorpusId": "155088184"
                },
                {
                    "start": 1036,
                    "end": 1039,
                    "matchedPaperCorpusId": "235355778"
                },
                {
                    "start": 1039,
                    "end": 1042,
                    "matchedPaperCorpusId": "116864392"
                },
                {
                    "start": 1274,
                    "end": 1278,
                    "matchedPaperCorpusId": "227343966"
                },
                {
                    "start": 1278,
                    "end": 1281,
                    "matchedPaperCorpusId": "2603360"
                },
                {
                    "start": 1281,
                    "end": 1284,
                    "matchedPaperCorpusId": "209438234"
                },
                {
                    "start": 1284,
                    "end": 1287,
                    "matchedPaperCorpusId": "51920640"
                },
                {
                    "start": 1804,
                    "end": 1808,
                    "matchedPaperCorpusId": "227343966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07574462890625
        },
        {
            "corpus_id": "250390527",
            "title": "Crossroads, Buildings and Neighborhoods: A Dataset for Fine-grained Location Recognition",
            "text": "We design two heuristic curricula based on entity length and word complexity considering that many long and complex entities in HarveyNER are naturally diffcult (as shown in Figure 3, the performance of baseline systems is worse on these hard cases). We further assume that the diffculty to learn may not only depend on the inherent diffculty of a type of case but also depend on how commonly seen or how well represented such cases are in the dataset. Therefore, we propose a novel curriculum with a diffculty scoring function that comprehensively considers the two heuristic diffculty metrics as instance frequencies. Empirical results show that all of the curricula can outperform several other baseline systems, and our novel curriculum performs the best. \n\nWe also fnd that different NER-based systems beneft from different curriculum scheduling strate-gies. In our experiments, the normal curriculum (training with easier samples frst) is suitable for training the neural network-based model NCRF++ without pretrained language models, while the anticurriculum (training with harder samples frst) facilitates fne-tuning of the pretrained language model BERT.",
            "score": 0.4085833518473313,
            "section_title": "Introduction",
            "char_start_offset": 3612,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1163
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.445068359375
        },
        {
            "corpus_id": "251719021",
            "title": "From Easy to Hard: A Dual Curriculum Learning Framework for Context-Aware Document Ranking",
            "text": "As we propose two curricula for learning on positive and negative pairs, to validate their effectiveness, we conduct an ablation study by disabling each of them from COCA+DCL (i.e., the sampling is done among all samples). The results are shown in Table 3. We can observe: \n\nFirst, we can see that both curricula are useful. Applying any of them leads to performance improvement. When no curriculum learning is used, we observe large drops in performance. This directly validates our assumption in this paper that learning from easy to hard samples can guide the model in a good learning direction. Second, the curriculum of positive pairs brings slightly higher improvement than that of negative pairs. This suggests that the ability to capture positive matching signals is more critical than being able to discard negative signals. A possible explanation is that positive signals are more focused while the negative ones are diffuse. \n\nFurthermore, to investigate the influence of samples' difficulty changes during the training, we replace the curriculum of negative pairs by only using easy or hard pairs, and the curriculum of positive pairs is disabled to avoid additional influence. The experimental results are shown in the bottom of Table 3. As can be seen, training with only hard negatives is even better than using all negatives (first row in the table). This finding is consistent with existing studies on using hard negatives to facilitate the optimization of dense retrievers [47,60,62]. However, only using easy samples for training makes the performance drop sharply. This is because the easy negatives cannot provide sufficient \"contrastive signals\" for learning the matching between search context and candidate documents. This is also why we design our curriculum of negative pairs as gradually shrinking to only hard negatives (details are presented in Section 3.3.2). Finally, dynamically adjusting the learning difficulty through curriculum is beneficial for model training (e.g., MAP is improved from 0.5698 to 0.5740). This demonstrates again the effectiveness of applying currciulum learning. 4.4.2 Influence of Scoring Models. We proposed two scoring models for  (\u2022, \u2022) -BM25 and BERT. We investigate their impact, and Table 4 shows the results of COCA+DCL.",
            "score": 0.40839646378417327,
            "section_title": "Impact of Both Curricula.",
            "char_start_offset": 32116,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 272
                },
                {
                    "start": 275,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 935
                },
                {
                    "start": 938,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2118
                },
                {
                    "start": 2119,
                    "end": 2124
                },
                {
                    "start": 2125,
                    "end": 2153
                },
                {
                    "start": 2154,
                    "end": 2212
                },
                {
                    "start": 2213,
                    "end": 2284
                }
            ],
            "ref_mentions": [
                {
                    "start": 1491,
                    "end": 1495,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1495,
                    "end": 1498,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1498,
                    "end": 1501,
                    "matchedPaperCorpusId": "233289894"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1239013671875
        },
        {
            "corpus_id": "269741199",
            "title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation",
            "text": "Another They approximated that the learning task's difficulty is inversely proportional to the gap between the superior LM versus the inferior LM prediction of the task. The power of LM is assumed to be a proxy for the human preference, as the human prefers the output of the superior model over the inferior model. The easier samples are defined as the ones with a clear-cut between the predictions of weaker (inferior) and stronger (superior) models. For example, the easy pairs can be generated using (GPT-4 vs. InstructGPT) models and the hard pairs using (ChatGPT and InstructGPT) models. In the proposed curriculum initially, the LM is trained with the easy pairs so that the LM understands the contrastive differences and later the harder-pairs are introduced following a predefined linear function of the step number. They investigated the effect of CL combined with DPO and found that starting with easy pairs and progressing toward hard pairs can significantly improve the performance of DPO. Ge et al., 2023 proposed the Memorization-Based Curriculum (MBC) learning approach for SFT where the samples that the model is less familiar with (or have less knowledge) have priority in the SFT process. Since the objective of SFT is to train the LMs to better understand the instructions, encouraging the LM to focus on less familiar data, as quantified by the memorization, can be beneficial. In MBC learning, perplexity is used as an approximation of the model's memorization for each training record during SFT. Instead of the typical uniform sampling without replacement for the training records in SFT, MBC uses perplexity values to construct a data distribution from which training records are sampled with replacement. This sampling strategy aims to guide the model to learn more from data that it has not memorized well, thereby enhancing its adaptability and performance. They conducted an extensive evaluation by employing GPT-4 as a judge in pairwise comparisons between the outputs generated by the two models. The results demonstrated the superior performance achieved by the proposed training strategy. It is worth mentioning that MBC deviates from conventional curriculum learning, where the training process starts from easier samples towards more challenging ones.",
            "score": 0.4079388473957421,
            "section_title": "I. Related Work",
            "char_start_offset": 85071,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2121
                },
                {
                    "start": 2122,
                    "end": 2286
                }
            ],
            "ref_mentions": [
                {
                    "start": 1003,
                    "end": 1018,
                    "matchedPaperCorpusId": "269718519"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.501953125
        },
        {
            "corpus_id": "57573793",
            "title": "Low-Shot Learning From Imaginary 3D Model",
            "text": "Recently, many studies have shown the benefits of organizing the training examples in a meaningful order (e.g., from simple to complex) for model training. Bengio et al. [1] first proposed a general learning strategy: curriculum learning. They show that suitably sorting the training samples, from the easiest to the most difficult, and iteratively training a classifier starting with a subset of easy samples (which is progressively augmented with more and more difficult samples), can be useful to find better local minima. Note that in this and in all the other curriculum-learningbased approaches, the order of the samples is provided by an external supervisory signal, taking into account human domain-specific expertise. Curriculum learning was extended to self-paced learning by Kumar et al. [15]. They proposed the respective framework, automatically expanding the training pool in an easy-tohard manner by converting the curriculum mechanism into a concise regularization term. Curriculum learning uses human design to organize the examples, and self-paced learning can automatically choose training examples according to the loss. Supancic et al. [26] adopt a similar framework in a tracking scenario and train a detector using a subset of video frames, showing that this selection is important to avoid drifting. Jiang et al. [9] pre-cluster the training data in order to balance the selection of the easiest samples with a sufficient inter-cluster diversity. Pentina et al. [22] propose a method in which a set of learning tasks is automatically sorted in order to allow a gradual sharing of infor-mation among tasks. In Zhang et al.'s [33] model saliency is used to progressively select samples in weakly supervised object detection. In context of visual categorization some of these self-paced learning methods use CNN-based features to represent samples [16] or use a CNN as the classifier directly [24].",
            "score": 0.4076144148534836,
            "section_title": "Self-Paced Learning",
            "char_start_offset": 9190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1919
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 173,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 799,
                    "end": 803,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 1157,
                    "end": 1161,
                    "matchedPaperCorpusId": "10118550"
                },
                {
                    "start": 1337,
                    "end": 1340,
                    "matchedPaperCorpusId": "9686483"
                },
                {
                    "start": 1486,
                    "end": 1490,
                    "matchedPaperCorpusId": "8502955"
                },
                {
                    "start": 1869,
                    "end": 1873,
                    "matchedPaperCorpusId": "8012207"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4296875
        },
        {
            "corpus_id": "246996522",
            "title": "Curriculum Optimization for Low-Resource Speech Recognition",
            "text": "[9] studied the dynamics of deep neural networks trained with dynamic instance hardness and showed that the model revisits harder samples more often due to higher variance in gradient values while easier examples tend to stay in the minima as soon as the minima are reached. However, [8] finds that in different empirical settings both learning harder and easy tasks first can benefit the model. \n\nCurriculum learning has been successfully used in natural language processing tasks such as language modelling [5,10] neural machine translation (NMT) [11,12,13], keyword spotting [14], and speech recognition [15,16]. Most commonly two complexity strategies are employed: model competencebased [11,12,9] and data-driven. Complexity measures for data-driven learning include sentence/utterance length [16], language model score, n-gram size [5,10], word frequency ranking, and sentence norm [13]. Speech processing systems can rely on speech to noise ratio (SNR) as a measure of difficulty by gradually blending more and more noise into clean speech signals [15,14] ditionally, we demonstrate that both an external teacher curriculum and learner's progress are important for low-resource ASR. The teacher curriculum acts as a reliable prior while the student can construct its own curriculum in an automated manner based on progress gains. We experiment with compression ratio and text-based difficulty measures to show that the signal-based prior leads to a more optimal solution. \n\nThe remainder of the paper as organized as follows: Section 2 describes the complexity measures and methods; Sections 3 and 4 discuss the experimental setting and the results; Section 5 summarizes our contributions.",
            "score": 0.4072027449694646,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2165,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 395
                },
                {
                    "start": 398,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1478
                },
                {
                    "start": 1481,
                    "end": 1696
                }
            ],
            "ref_mentions": [
                {
                    "start": 284,
                    "end": 287,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 509,
                    "end": 512,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 512,
                    "end": 515,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 549,
                    "end": 553,
                    "matchedPaperCorpusId": "20639213"
                },
                {
                    "start": 553,
                    "end": 556,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 578,
                    "end": 582,
                    "matchedPaperCorpusId": "231979234"
                },
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "14928979"
                },
                {
                    "start": 611,
                    "end": 614,
                    "matchedPaperCorpusId": "33957080"
                },
                {
                    "start": 692,
                    "end": 696,
                    "matchedPaperCorpusId": "20639213"
                },
                {
                    "start": 696,
                    "end": 699,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 699,
                    "end": 701,
                    "matchedPaperCorpusId": "227275560"
                },
                {
                    "start": 798,
                    "end": 802,
                    "matchedPaperCorpusId": "33957080"
                },
                {
                    "start": 838,
                    "end": 841,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 841,
                    "end": 844,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 1055,
                    "end": 1059,
                    "matchedPaperCorpusId": "14928979"
                },
                {
                    "start": 1059,
                    "end": 1062,
                    "matchedPaperCorpusId": "231979234"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55712890625
        },
        {
            "corpus_id": "265213396",
            "title": "CLIMB: Curriculum Learning for Infant-inspired Model Building",
            "text": "Children acquire language skills from being exposed to an estimated two to seven million words * Equal contribution per year (Gilkerson et al., 2017). The current learning regimes of large language models require disproportionately larger sizes of training data to acquire linguistic generalization capabilities (Zhang et al., 2021). State-of-the-art LMs are typically trained on gigabytes of data gleaned from the World Wide Web, on multiple GPUs continuously for days at a time (Zhao et al., 2023). For example, the Chinchilla language model was trained on a dataset of 1.4 trillion words (Hoffmann et al., 2022). Such large-scale training regimes are economically and ecologically unsustainable, and access to the required computing resources remains out of reach for most academic groups and industry start-ups (Izsak et al., 2021). \n\nTo enable language models to still perform well with limited data, recent work has looked at utilizing smaller, well-curated, and representative corpora (Samuel et al., 2023;Gao et al., 2020) and careful selection of training and model hyper-parameters (Geiping and Goldstein, 2023). 'Zero-shot' and 'few-shot' learning are other dataefficient approaches which can perform well in certain settings but rely on large pre-trained language models (Brown et al., 2020;Wei et al., 2021). These approaches, however, provide engineering solutions to the problem rather than a cognitivelyinspired, compute-efficient framework for training language models from scratch. \n\nConventional pre-training of large language models remains far removed from human language learning: models operate on a predetermined static vocabulary and optimize a monotonous training objective on a randomly shuffled dataset. We conducted experiments to explore more dynamic learning processes that are motivated by the psycholinguistic and language acquisition literature and are set within the machine learning paradigm of curriculum learning (Bengio et al., 2009).",
            "score": 0.4070202315531358,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 836
                },
                {
                    "start": 839,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1499
                },
                {
                    "start": 1502,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1973
                }
            ],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 149,
                    "matchedPaperCorpusId": "4447742"
                },
                {
                    "start": 312,
                    "end": 332,
                    "matchedPaperCorpusId": "226289695"
                },
                {
                    "start": 815,
                    "end": 835,
                    "matchedPaperCorpusId": "233289750"
                },
                {
                    "start": 992,
                    "end": 1013,
                    "matchedPaperCorpusId": "257622986"
                },
                {
                    "start": 1092,
                    "end": 1121,
                    "matchedPaperCorpusId": "255185900"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60546875
        },
        {
            "corpus_id": "268363364",
            "title": "SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models",
            "text": "This paper introduces a data selection method for large language models (LLMs), aiming to enhance the data efficiency in the supervised fine-tuning (SFT) of these models.\n\nPositive Impacts: Our method, by reducing the data requirements for training LLMs, can make fine-tuning LLMs more effective and accessible.This could lead to broader participation in AI research and application development across various fields, including healthcare and education.\n\nNegative Impacts: Our method does not inherently involve or encourage applications with direct negative societal impacts.The focus is on a generic improvement in the field of machine learning, particularly in the training of LLMs.\n\nTable 4. Complete results used for selecting the best reference model for each one-shot data selection baseline.The choice of early-finetuning (step 1000) and end-of-fine-tuning checkpoint follows (Marion et al., 2023).The best results selected for range of mathematical disciplines from early algebra to calculus and linear algebra, covering various types of questions such as multi-step reasoning, arithmetic word problems, and problems from mathematics competitions.This variety ensures a comprehensive assessment across both in-domain and out-domain tasks.\n\nPipeline.We utilize the pipeline provided by (Yue et al., 2023), designed to first determine whether the model can be prompted to generate a code snippet.This code snippet, if successfully generated, should be executable and produce the correct answer when run.This code-based evaluation is also used for Phi models (Li et al., 2023b).In cases where the model does not directly produce a viable code solution, we employ a \"think step-by-step\" prompting strategy (Wei et al., 2022b).This method prompts the model to break down its reasoning process, a technique that has been widely proven",
            "score": 0.4064586197702927,
            "section_title": "Impact Statements",
            "char_start_offset": 23937,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 172,
                    "end": 311
                },
                {
                    "start": 311,
                    "end": 453
                },
                {
                    "start": 455,
                    "end": 576
                },
                {
                    "start": 576,
                    "end": 685
                },
                {
                    "start": 687,
                    "end": 799
                },
                {
                    "start": 799,
                    "end": 906
                },
                {
                    "start": 906,
                    "end": 1156
                },
                {
                    "start": 1156,
                    "end": 1247
                },
                {
                    "start": 1249,
                    "end": 1258
                },
                {
                    "start": 1258,
                    "end": 1403
                },
                {
                    "start": 1403,
                    "end": 1510
                },
                {
                    "start": 1510,
                    "end": 1584
                },
                {
                    "start": 1584,
                    "end": 1731
                },
                {
                    "start": 1731,
                    "end": 1837
                }
            ],
            "ref_mentions": [
                {
                    "start": 1711,
                    "end": 1730,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86669921875
        },
        {
            "corpus_id": "227227757",
            "title": "Dynamic Curriculum Learning for Low-Resource Neural Machine Translation",
            "text": "Zhang et al. ( 2018) define two general types of curriculum learning strategy. The deterministic curriculum (Kocmi and Bojar, 2017) arranges the training samples with fixed order and performs worse due to lacking randomization. The probabilistic curriculum (Platanios et al., 2019) generates a batch uniformly sampled from the training set based on sample difficulty and model competence. The latter generally works well in the previous curriculum learning methods. However, in our preliminary experiments, we find that the vanilla model trained by sampling performs worse or converges slower slightly than the training strategy which trains the model with the whole training set in an epoch. A possible reason is sampling might lead to unbalanced training because some samples are not fully trained due to sampling omission. \n\nOur method dynamically measures the sample difficulty and model competence at each phase, then selects a certain proportion of easier samples to train based on model competence. It ensures that training samples are not missed due to sampling and also retains randomization to avoid overfitting. Algorithm 1 shows the overall training procedure of our method.",
            "score": 0.40633252163709255,
            "section_title": "Training Strategy",
            "char_start_offset": 13943,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 825
                },
                {
                    "start": 828,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1186
                }
            ],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 131,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 257,
                    "end": 281,
                    "matchedPaperCorpusId": "85498775"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46533203125
        },
        {
            "corpus_id": "211102338",
            "title": "Reinforced Curriculum Learning on Pre-trained Neural Machine Translation Models",
            "text": "In the task of curriculum learning, an action represents the process of data selection. (Fang, Li, and Cohn 2017) assume a stream-based setting where data examples come one by one in a stream, and design their action as making a decision on whether or not a single incoming data example should be selected. We argue that our problem setting is actually poolbased instead of stream-based, where a pool of data exists for selection and deciding on the selection of each individual sample per step would be inefficient. In (Kumar et al. 2019), a dataset is split into several bins according to a noise measure of data samples, and the action determines from which bin the next batch of training data should be selected. However, this method highly depends on an effective heuristic criterion for bin formation and is thus hard to generalize. Therefore, we propose our action design which samples a batch of b data examples from D T , computes a score for each example according to the trained policy, and choose the one with the highest score. Correspondingly, in our design, we can easily control the size of D S by varying the batch size b, i.e., |D S | = |D T |/b, since we choose one out of b samples for each batch.\n\nFor the choice of reward signal, we use the performance improvement of the NMT model evaluated on the validation set after it is updated with the selected sample. Perplexity is used as the performance metric instead of BLEU as it is shown to be more consistent and less noisy (So, Le, and Liang 2019). We assign a reward of 0 to unselected samples.",
            "score": 0.40630631082739416,
            "section_title": "Action and Reward",
            "char_start_offset": 16706,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 520,
                    "end": 538,
                    "matchedPaperCorpusId": "67855673"
                },
                {
                    "start": 1495,
                    "end": 1519,
                    "matchedPaperCorpusId": "59523610"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4541015625
        },
        {
            "corpus_id": "265213396",
            "title": "CLIMB: Curriculum Learning for Infant-inspired Model Building",
            "text": "Previous work in curriculum learning typically focuses on difficulty from a data-centric perspective, however, we note that difficulty can arise from (at least) three major elements of training a neural model: the input representation, the data sampling, and the training process. We explore curriculum learning strategies across three distinct dimensions: the vocabulary, the order of training data, and the objective function. \n\nFor machine learning models, instance difficulty is in part influenced by the choice of instance representation. For language models, the representational space is constrained by the vocabulary. We propose a new vocabulary curriculum inspired by Soviany et al. (2022), who discuss linking the curriculum criteria to the observed vocabulary sizes in child development. To the best of our knowledge, this is the first attempt at manipulating the vocabulary available to a language model through curriculum learning. \n\nIn natural language processing models, the order of the training instances can have a strong effect on performance (Schluter and Varab, 2018). Existing approaches to instance-level curriculum learning determine the difficulty of each instance according to a pre-defined static difficulty assessment according to linguistic criteria (Campos, 2021;Kocmi and Bojar, 2017;Liu et al., 2018;Platanios et al., 2019). It has been shown that humans pay more attention to stimuli that are in just the right zone of difficulty for them: neither too easy nor too hard (Kidd et al., 2012). This so-called 'Goldilocks effect' can be modelled by assessing the difficulty of an instance dynamically based on model behaviour (Sachan and Xing, 2016;Lalor and Yu, 2020). Static and dynamic difficulty assessment can be mapped to teacher-centric and learner-centric educational approaches and we compare both variants in our data curriculum experiments. \n\nHuman language learning is guided and enabled to some extent by other agents in the learner's environment (e.g., adult caregivers, siblings) who interact with the learner. In machine learning, such interactions are modelled by the objective function that guides the weight optimization process. The typical 'masked language modelling' (MLM) objective function requires that a model predicts a target token from a pre-defined vocabulary of size N given the surrounding context. Thus standard MLM defines an N -way token classification task.",
            "score": 0.4061843430986396,
            "section_title": "Determining Difficulty",
            "char_start_offset": 6115,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 428
                },
                {
                    "start": 431,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 944
                },
                {
                    "start": 947,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1880
                },
                {
                    "start": 1883,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2177
                },
                {
                    "start": 2178,
                    "end": 2359
                },
                {
                    "start": 2360,
                    "end": 2422
                }
            ],
            "ref_mentions": [
                {
                    "start": 677,
                    "end": 698,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 1062,
                    "end": 1088,
                    "matchedPaperCorpusId": "53081680"
                },
                {
                    "start": 1293,
                    "end": 1315,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 1315,
                    "end": 1332,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 1332,
                    "end": 1355,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 1503,
                    "end": 1522,
                    "matchedPaperCorpusId": "3172352"
                },
                {
                    "start": 1655,
                    "end": 1678,
                    "matchedPaperCorpusId": "16503693"
                },
                {
                    "start": 1678,
                    "end": 1697,
                    "matchedPaperCorpusId": "226226711"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4296875
        },
        {
            "corpus_id": "225066701",
            "title": "A Comprehensive Survey on Curriculum Learning",
            "text": "Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in tool, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing, etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of Difficulty Measurer + Training Scheduler and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. Finally, we present brief discussions on the relationships between CL and other methods, and point out potential future research directions deserving further investigations.",
            "score": 0.4061752784816544,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1385498046875
        },
        {
            "corpus_id": "247404030",
            "title": "Research on Automatic Error Correction Method in English Writing Based on Deep Neural Network",
            "text": "Curriculum learning is similar to the human learning mechanism, that is, learn simple skills first and then learn difficult skills. If the training data are input in a specific order, in other words, the model first learns from simple data and then learns difficult data after the model has a certain ability, which is in line with human intuition. At the same time, from the perspective of machine learning, this method can also avoid falling into a bad local optimal solution prematurely. is can increase the generation speed and speed up the convergence speed and find a better local minimum in the nonconvex training data.  e curriculum learning method is sensitive to hyperparameters, and this paper uses a curriculum learning method with only one adjustable hyperparameter, called competence-based curriculum learning [28].\n\nere are two crucial concepts in the learning strategy method of this course: difficulty and competence. Difficulty represents the difficulty value of a training sample. Its value is determined by the sentence length and the relative word frequency of the word. e calculation is as follows:\n\nwhere s i is the ith sample, N is the length of the sample sentence, f k is the relative word frequency of the kth word in the sample.\n\nCompetence is a value between 0 and 1, which represents the progress of model training and is defined as a function of the model state. Specifically, this method defines the model's ability c(t) at time t as the proportion of training data allowed to be used at time t. e training samples are sorted according to their difficulty, and the model only allows their top c(t) part to be used at time t. Linear function and root function are two calculation methods.\n\nwhere c 0 is the initial value, T is the time step threshold. When the threshold is exceeded, the model is considered to be fully capable, and t is the time step. In this paper, the curriculum learning strategy is applied to the English writing error correction task to achieve the purpose of improving performance. e model is illustrated in Figure 3. e competence of this paper is in addition to the linear form and root form mentioned above. A method for selecting training data based on loss is also proposed. e most intuitive reflection of the strength of the model is the loss of the model. e calculation formula is as follows:",
            "score": 0.40574456451832613,
            "section_title": "Curriculum Learning Strategy.",
            "char_start_offset": 16687,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.252685546875
        },
        {
            "corpus_id": "277103615",
            "title": "Scale Efficient Training for Large Datasets",
            "text": "We propose SeTa, a simple framework for efficient training through dynamic sample selection as shown in Figure 2. At its core, SeTa employs sample-wise loss as an intrinsic difficulty measure, which serves as a model-agnostic and computation-free proxy for learning dynamics. Loss-guided Sample Clustering. To handle the redundancy in large-scale datasets, we first perform uniform downsampling with ratio r \u2208 (0, 1), yielding a subset I = {i 1 , ..., i m } where m = r|D| and i j \u223c U nif orm(1, |D|). \n\nThe sampled instances are then partitioned into k clusters through loss-guided k-means optimization: \n\nwhere C = {c 1 , ..., c k } denotes the cluster centroids and G j represents the sample set in cluster j, l t i denote the loss of sample i at training step t. The optimization iterates through: \n\nUpon convergence, we obtain difficulty-stratified clusters {G 1 , ..., G k } ordered by their centroid values. Sliding Window Selection. To orchestrate optimal learning progression, we introduce a dynamic curriculum mechanism that cyclically exposes the model to samples of increasing difficulty. We establish a window size w = \u2308\u03b1k\u2309, where \u03b1 \u2208 (0, 1] controls the proportion of clusters selected at each iteration. The window position evolves according to: \n\nwhere n represents the current iteration count. This formulation ensures that when s t +w exceeds k, the window position resets to 0, implementing a cyclical curriculum that repeatedly progresses from easy to difficult samples. The selected sample subset is then constructed as: \n\n, where \u03c3(\u2022) represents the permutation that sorts clusters by their difficulty. This cyclic mechanism ensures that the model regularly revisits easier samples while progressively adapting to more challenging ones, maintaining stable optimization dynamics throughout the training process. Partial Annealing. To mitigate potential optimization bias from localized sample selection, we introduce a stochastic partial annealing strategy during the final training phase. Unlike conventional full-dataset annealing [27], we maintain efficiency by applying probabilistic sample selection with a fixed sampling ratio r:",
            "score": 0.40573349273967124,
            "section_title": "Efficient Training",
            "char_start_offset": 8368,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 501
                },
                {
                    "start": 504,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 801
                },
                {
                    "start": 804,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1260
                },
                {
                    "start": 1263,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1541
                },
                {
                    "start": 1544,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2156
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40087890625
        },
        {
            "corpus_id": "267200114",
            "title": "From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning",
            "text": "A major challenge in Natural Language Processing is obtaining annotated data for supervised learning. An option is the use of crowdsourcing platforms for data annotation. However, crowdsourcing introduces issues related to the annotator's experience, consistency, and biases. An alternative is to use zero-shot methods, which in turn have limitations compared to their few-shot or fully supervised counterparts. Recent advancements driven by large language models show potential, but struggle to adapt to specialized domains with severely limited data. The most common approaches therefore involve the human itself randomly annotating a set of datapoints to build initial datasets. But randomly sampling data to be annotated is often inefficient as it ignores the characteristics of the data and the specific needs of the model. The situation worsens when working with imbalanced datasets, as random sampling tends to heavily bias towards the majority classes, leading to excessive annotated data. To address these issues, this paper contributes an automatic and informed data selection architecture to build a small dataset for few-shot learning. Our proposal minimizes the quantity and maximizes diversity of data selected for human annotation, while improving model performance.",
            "score": 0.40560711164987606,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78662109375
        },
        {
            "corpus_id": "266180249",
            "title": "GPT-wee: How Small Can a Small Language Model Really Get?",
            "text": "Curriculum learning looks like an obvious choice when trying to implement usage-based findings in the training process for (smaller) language models. However, this does not seem to work with the simple form of curriculum learning based on prototypicality measures that we used in this paper. For that, several explanations are possible: 1) more advanced curriculum approaches are needed, with different and more directed ways of ordering and optimizing the curriculum, 2) curriculum learning may not be the right choice for small models (it seems that, if at all, it was rather the larger models which showed tendencies of improvement. Also, other options for implementing usage-based accounts might just work better (e.g. models with dynamic structures and growing numbers of nodes). After all, real human neural networks grow and mature while they are constantly shaped and re-shaped by linguistic input and processing. As such, it also remains hard to interpret language models, their parts and their performance on various evaluation suites in a coherent way. The integration of more linguistic factors into the training process needs to be tested in this regard. For example, Yehezkel and Pinter (2023) propose a subword tokenization algorithm that incorporates contextual information and creates vocabularies that seem to align more with classical ideas of morphology. It remains an open question whether such alterations and other linguistic experiments in the training process would also improve the linguistic quality of the generated output.",
            "score": 0.4053864909292487,
            "section_title": "Discussion",
            "char_start_offset": 25004,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1551
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.321533203125
        },
        {
            "corpus_id": "264288956",
            "title": "Harnessing Dataset Cartography for Improved Compositional Generalization in Transformers",
            "text": "Overall, hard-to-learn (BLEU) emerges as the most effective configuration in the plots. Surprisingly, ambiguous (Inv PPL) performs as the second-best configuration in Figure 6, while hard-to-learn (Inv PPL) holds this position in Figure 5. The no curriculum approach ranks third and fourth in these respective plots. Furthermore, the easy-to-learn configurations demonstrate the poorest final performance across both curriculum learning frameworks. \n\nAnalyzing the accuracy plots of curriculum learning, we observe that initiating training with easier examples and gradually progressing to more challenging instances does not lead to accelerated convergence or improved final model performance. \n\nOn the other hand, the subset experiments presented in Tables 2 and 3 show that training models on hard-to-learn examples result in better model performance. Furthermore, the CL results highlight that starting the curriculum with hard-to-learn samples results in enhanced final performance. These findings, combined with the observation that the first unlocked examples are encountered more frequently during training, suggest that the superiority of hard-to-learn curricula over the no curriculum can be attributed to the increased exposure to challenging instances throughout the training process. \n\nTo sum up, our experimental findings highlight the effectiveness of utilizing dataset cartography for training subset selection and curriculum learning in the context of compositional generalization. Our results consistently show that leveraging dataset cartography leads to improved generalization performance. While curriculum learning also contributes to performance enhancement, its impact appears to be smaller compared to the use of dataset cartography for subset selection. 2020) ranks instances based on predictability, suggesting the removal of easy-to-learn examples. However, our research presents contrasting findings. Our experimental analyses show that combining the easy-to-learn category with other categories can improve the generalization performance. In another recent study, Wang et al. (2022) explore the relationship between the generalization performance and the training dynamics in an active learning setting. Their approach revolves around the adaptive selection of samples for labeling to obtain comparable or better performance with less training data. Notably, they discovered a robust correlation between the convergence speed of training and the resulting generalization performance. By leveraging this connection, they propose a strategy to enhance overall generalization performance.",
            "score": 0.4050027520711315,
            "section_title": "Impact of Cartography-Based Curriculum Learning",
            "char_start_offset": 20332,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 448
                },
                {
                    "start": 451,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1296
                },
                {
                    "start": 1299,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2068
                },
                {
                    "start": 2069,
                    "end": 2233
                },
                {
                    "start": 2234,
                    "end": 2379
                },
                {
                    "start": 2380,
                    "end": 2513
                },
                {
                    "start": 2514,
                    "end": 2615
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1556396484375
        },
        {
            "corpus_id": "44130298",
            "title": "Self-Training for Jointly Learning to Ask and Answer Questions",
            "text": "A key challenge in self-training is selecting which unlabeled data sample to label (iwhich generated questions to add to the training set). The selftraining process may erroneously generate some bad or incorrect questions which can sidetrack the learning process. Thus, we implement a question selection oracle which determines which questions to add among the potentially very large set of questions generated by the QG model in each iteration. \n\nTraditional wisdom in self-training (Yarowsky, 1995;Riloff et al., 2003) advises selecting a subset of questions on which the models have the highest confidence. We experiment with this idea, proposing multiple self-training oracles which introduce questions in the order of how confident the QA and QG models are on the new potential question: \n\n\u2022 QG: The QG oracle introduces the question in the order of how confident the QG model is on generating the question. This is calculated by a number of heuristics (described later). \n\n\u2022 QA: The QA oracle introduces the question in the order of how confident the QA model is on answering the question. This too is calculated by some heuristics (described later). \n\n\u2022 QA+QG: The QA+QG oracle introduces a question when both QA and QG models are confident about the question. The oracle computes the minimum confidence of the QA and QG models for a question and introduces questions which have the the highest minimum confidence score. \n\nOur question selection heurisitcs are based on the ideas of curriculum learning and diversity: \n\n1. Curriculum learning (Bengio et al., 2009;Sachan and Xing, 2016a) requires ordering questions on the easiness scale, so that easy questions can be introduced to the learning algorithm first and harder questions can be introduced successively. The main challenge in learning the curriculum is that it requires the identification of easy and hard questions. \n\nIn our setting, such a ranking of easy and hard questions is difficult to obtain. A human judgement of 'easiness' of a question might not correlate with what is easy for our algorithms in its feature and hypothesis space. We explore various heuristics that define a measure of easiness and learn the ordering by selecting questions using this measure.",
            "score": 0.40498179504650034,
            "section_title": "The Question Selection Oracle",
            "char_start_offset": 10458,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 445
                },
                {
                    "start": 448,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 792
                },
                {
                    "start": 795,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 976
                },
                {
                    "start": 979,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1156
                },
                {
                    "start": 1159,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1427
                },
                {
                    "start": 1430,
                    "end": 1524
                },
                {
                    "start": 1527,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1884
                },
                {
                    "start": 1887,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2238
                }
            ],
            "ref_mentions": [
                {
                    "start": 500,
                    "end": 520,
                    "matchedPaperCorpusId": "1164969"
                },
                {
                    "start": 1550,
                    "end": 1571,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1571,
                    "end": 1594,
                    "matchedPaperCorpusId": "16503693"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2313232421875
        },
        {
            "corpus_id": "3963337",
            "title": "Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning",
            "text": "To characterize and quantify a curriculum, we define three categories of features, focusing on various distributional, syntactic, and semantic aspects of training data. We now detail the feature categories along with motivations for feature selection. \n\nDIVERSITY. Diversity measures capture the distributions of types in data. Entropy is the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006;Gimpel et al., 2013) (Rosenzweig, 1995;Magurran, 2013), to economics and social studies (Stirling, 2007). Diversity has been shown effective in related research on curriculum learning in language modeling, vision, and multimedia analysis (Bengio et al., 2009;Jiang et al., 2014). \n\nLet p i and p j correspond to empirical frequencies of word types t i and t j in the training data. Let d ij correspond to their semantic similarity, calculated as the cosine similarity between embeddings of t i and t j learned from the training data. We annotate each paragraph with the following diversity features: We explore the simplicity hypothesis, albeit without prior assumptions on specific ordering of data, and extend it to additional simplicity/complexity measures of training data. Our features are inspired by prior research in second language acquisition, text simplification, and readability assessment (Schwarm and Ostendorf, 2005;Heilman et al., 2007;Pitler and Nenkova, 2008;Vajjala and Meurers, 2012). We use an off-the-shelf syntactic PROTOTYPICALITY. This is a group of semantic features that use insights from cognitive linguistics and child language acquisition. The goal is to characterize the curriculum of representation learning in terms of the curriculum of human language learning. We resort to the Prototype theory (Rosch, 1978), which posits that semantic categories include more central (or prototypical) as well as less prototypical words. For example, in the ANIMAL category, dog is more prototypical than sloth (because dog is more frequent); dog is more prototypical than canine (because dog is more concrete); and dog is more prototypical than bull terrier (because dog is less specific). According to the theory, more prototypical words are acquired earlier.",
            "score": 0.404862402095328,
            "section_title": "Distributional and Linguistic Features",
            "char_start_offset": 8973,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 251
                },
                {
                    "start": 254,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2153
                },
                {
                    "start": 2154,
                    "end": 2224
                }
            ],
            "ref_mentions": [
                {
                    "start": 425,
                    "end": 444,
                    "matchedPaperCorpusId": "2976898"
                },
                {
                    "start": 444,
                    "end": 464,
                    "matchedPaperCorpusId": "2209694"
                },
                {
                    "start": 532,
                    "end": 548,
                    "matchedPaperCorpusId": "10673693"
                },
                {
                    "start": 682,
                    "end": 703,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 703,
                    "end": 722,
                    "matchedPaperCorpusId": "9686483"
                },
                {
                    "start": 1346,
                    "end": 1375,
                    "matchedPaperCorpusId": "17337432"
                },
                {
                    "start": 1375,
                    "end": 1396,
                    "matchedPaperCorpusId": "9055166"
                },
                {
                    "start": 1396,
                    "end": 1421,
                    "matchedPaperCorpusId": "2937659"
                },
                {
                    "start": 1421,
                    "end": 1447,
                    "matchedPaperCorpusId": "10919200"
                },
                {
                    "start": 1773,
                    "end": 1786,
                    "matchedPaperCorpusId": "15633758"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1771240234375
        },
        {
            "corpus_id": "270619559",
            "title": "CEC: A Noisy Label Detection Method for Speaker Recognition",
            "text": "Curriculum learning is a training strategy that mimics the cognitive learning process of humans, where the model starts learning from easy samples and gradually increases difficulty.This method effectively reduces the model's fitting to noisy labels.We apply curriculum learning to hard samples using a predefined retention threshold \u03c4m as detailed in Equation 6.\n\nwhere e1 is the epoch for warm-up, and e2 is the epoch for gradually reducing inconsistent/hard samples for the gradient backpropagation.e3 is the epoch for linearly increasing the difficulty of hard samples.s1 and s2 are the lower and upper bounds of \u03c4m, respectively.m is the current epoch number.s1 and s2 can be determined based on the task scenario or empirical values.\n\nWe use 1 \u2212 sP to denote the difficulty of the sample.Only when 1 \u2212 sP < \u03c4m, the hard sample will be used to update the model parameters.We do not use the loss value as the difficulty of samples because the range of variation in loss values is large, while the range of variation in cosine distance is fixed and unaffected by the data and model framework.Moreover, classifying samples based on cosine distance aligns more intuitively with the actual application of speaker recognition.We tested the performance of the proposed method in real-world scenarios on the VoxCeleb2 [20] dataset.To evaluate the performance under different ratios of noisy labels, we extracted 2000 speakers from the VoxCeleb2 dataset, comprising a total of 162,901 utterances, as clean labels dataset.The remaining speakers were considered as open-set noisy labels, and we simulated the pair noise by randomly adding them to clean dataset according to different Noisy-to-Clean Ratio (NCR) values ranging from 0% to 50%.We used the Original, Extended, and Hard VoxCeleb1 [21] test sets as evaluation sets (Vox1-O, Vox1-E, and Vox1-H).",
            "score": 0.40484512878797424,
            "section_title": "Curriculum learning",
            "char_start_offset": 5418,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 182,
                    "end": 250
                },
                {
                    "start": 250,
                    "end": 363
                },
                {
                    "start": 365,
                    "end": 502
                },
                {
                    "start": 502,
                    "end": 573
                },
                {
                    "start": 573,
                    "end": 634
                },
                {
                    "start": 634,
                    "end": 664
                },
                {
                    "start": 664,
                    "end": 739
                },
                {
                    "start": 741,
                    "end": 794
                },
                {
                    "start": 794,
                    "end": 877
                },
                {
                    "start": 877,
                    "end": 1095
                },
                {
                    "start": 1095,
                    "end": 1225
                },
                {
                    "start": 1225,
                    "end": 1328
                },
                {
                    "start": 1328,
                    "end": 1517
                },
                {
                    "start": 1517,
                    "end": 1735
                },
                {
                    "start": 1735,
                    "end": 1849
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.338623046875
        },
        {
            "corpus_id": "269921463",
            "title": "A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI: The First Romanian Natural Language Inference Corpus",
            "text": "Ro-BERT + Cart-Stra-CL++. By analyzing the distribution of classes in each data cartography group (see Table 3), we observe that the E2L group does not contain any contrastive or entailment instances. Hence, we propose yet another model based on curriculum learning, which constructs stratified easy-to-hard batches. This ensures the diversity of class labels right from the beginning of the training process, avoiding to bias the model towards certain classes. This novel curriculum learning approach is more suitable for imbalanced datasets, which are more prone to be affected by introducing further class biases.",
            "score": 0.4048187528021394,
            "section_title": "Models",
            "char_start_offset": 23013,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 26,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 616
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15673828125
        },
        {
            "corpus_id": "273654515",
            "title": "Sparse Linear Bandits with Blocking Constraints",
            "text": "In niche industrial applications such as Named Entity Recognition (Nguyen et al., 2023) and learning tasks on low-resource languages (Hedderich et al., 2021), obtaining high-quality labels is challenging due to the lack of expert annotators. However, high-quality labels are critical for effective model training (Li et al., 2023), and thus, expert annotators must provide ground truth labels. (Sorscher et al., 2022) demonstrated that selecting high-quality data can reduce the power-law association of test error with dataset size to an exponential law. In annotation-expensive tasks with large volumes of unlabeled data, the challenge is to select a representative subset of datapoints for labeling. In label-scarce tasks, where the number of expert annotators is extremely low, often only one, it is impractical to query the same datapoint multiple times. While crowd-sourcing literature reduces noise by aggregating labels from multiple annotators (Verroios & Garcia-Molina, 2015), the annotation from a single or aggregated expert is considered the final ground truth label in our setting. We term this restriction, where a datapoint cannot be re-queried after annotation, the blocking constraint. Additionally, the annotation budget is typically much smaller than the datapoint embedding dimension. An efficient annotation strategy should be sequential (instead of one-shot) in such cases, as each annotation informs future decisions and helps identify more informative datapoints. \n\nIn addition, data-pruning techniques like coreset selection emphasize selecting hard examples (Maharana et al., 2023), while (Sorscher et al., 2022) justifies this for perceptron learning. Curriculum learning (Bengio et al., 2009) also uses increasingly difficult examples, but defining the 'hardness' of unlabeled data is ambiguous. Hard examples identified by heuristics are often noisy, mislabeled, or outliers (Mindermann et al., 2022). To address this, we propose soliciting annotation difficulty feedback directly from expert annotators.",
            "score": 0.4044975671829291,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1488
                },
                {
                    "start": 1491,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2034
                }
            ],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 87,
                    "matchedPaperCorpusId": "254535870"
                },
                {
                    "start": 133,
                    "end": 157,
                    "matchedPaperCorpusId": "225062337"
                },
                {
                    "start": 394,
                    "end": 417,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 953,
                    "end": 985,
                    "matchedPaperCorpusId": "8422849"
                },
                {
                    "start": 1616,
                    "end": 1639,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 1700,
                    "end": 1720,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.272705078125
        },
        {
            "corpus_id": "268364003",
            "title": "Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences",
            "text": "Motivated by previous works in curriculum learning (Peterson, 2004;Krueger and Dayan, 2009;Bengio et al., 2009), we hypothesize that preference learning would be more effective if training data is arranged in the order of easier to harder preference pairs. Samples with response ratings that are farther apart (e.g., (R i 1 , R i 4 ) -R i 1 with highest rating as chosen and R i 4 with lowest rating as rejected) should make it easier for the preference model to learn distinguishing signals between them within the DPO framework (Rafailov et al., 2023). However, if the chosen and rejected responses have near similar quality (i.e., (R i 1 , R i 2 ) where R i 2 has the 2nd highest rating), it would be harder for the model to learn contrastive signals. \n\nTherefore, we utilize preference pair (R i 1 , R i 4 ) in the initial iteration of DPO training and present more difficult samples as the training progresses with (R i 1 , R i 2 ) being the final preference pair used in the last iteration of training. This way, the model learns to discern samples in increasing order of difficulty. As shown in figure 1, we create 3 preference pairs where the chosen is always the highest rated response and remaining 3 responses are selected as rejected to create 3 preference pairs for each query prompt \n\nWe then rank each pair based on the difference in response quality rating between chosen and rejected as shown in fig. 1. We focus our study on these 3 preference pairs but given 4 responses per prompt, ofcourse 4 2 preference pairs are plausible. We present this variant in section 5 and observe very minimal gains from the expensive training process of utilizing all possible preference pairs. We use the following rating scores to determine the response quality difference in our experiments: \n\n\u2022 GPT-4 score -In UltraFeedback, we compute the difference in GPT-4 (OpenAI, 2023) scores between chosen and rejected for each of the 3 preference pairs. The pair with the highest difference (easier to learn in curriculum learning i.e. (R i 1 , R i 4 )) is used in the first iteration of DPO training. Similarly, preference pairs with 2nd highest rating difference (i.e.",
            "score": 0.40413511535990276,
            "section_title": "Curating and Arranging Multiple Preference Pairs",
            "char_start_offset": 8891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 754
                },
                {
                    "start": 757,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1296
                },
                {
                    "start": 1299,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1794
                },
                {
                    "start": 1797,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2098
                },
                {
                    "start": 2099,
                    "end": 2167
                }
            ],
            "ref_mentions": [
                {
                    "start": 51,
                    "end": 67,
                    "matchedPaperCorpusId": "34687176"
                },
                {
                    "start": 67,
                    "end": 91,
                    "matchedPaperCorpusId": "206863670"
                },
                {
                    "start": 91,
                    "end": 111,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34375
        },
        {
            "corpus_id": "252212319",
            "title": "An Embedding-Based Grocery Search Model at Instacart",
            "text": "We propose a cascade training method, which uses different training sets in two steps. This is similar to curriculum learning, a strategy that trains a model from easier to harder data, imitating the learning order in human curricula [22]. However, we focus more on best utilizing noisy data through scheduling according to data quality instead of difficulty.",
            "score": 0.4040614016662334,
            "section_title": "Curriculum Learning",
            "char_start_offset": 8698,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 359
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14013671875
        },
        {
            "corpus_id": "263909051",
            "title": "D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning",
            "text": "CCS enforces a uniform distribution of sample difficulty scores in the coreset, which is beneficial at high pruning rates for providing even coverage over easy and difficult samples. However, at lower pruning rates (or with increasing data budget), difficult training samples yield a lower test loss from deep learning models (Sorscher et al., 2022). The hyperparameters k and \u03b3 in D 2 PRUNING (see Sec. 3) allow flexibility in the distribution of easy/difficult samples in coresets. We find that higher values of \u03b3 and lower value of k in D 2 PRUNING leads to a coreset that is skewed towards more difficult samples and benefits performance at lower pruning rates. Conversely, low \u03b3 and high k lead to an equitable distribution over easy/difficult samples and are more useful for higher pruning rates. See discussion on hyperparameters in Sec.5.2 and qualitative analysis of coresets in Appendix. \n\nResults from the evaluation of various coreset selection methods, including D 2 PRUNING, on NLP datasets are presented in Tab. 2. First, we find that when pretrained language models (PLMs) are finetuned on task-specific datasets, the models do not suffer from a catastrophic decline in performance at high pruning rates, in contrast to models trained from scratch on vision datasets. For IMDB reviews, the performance of finetuned RoBERTa goes from 91.8% at 0% pruning to 81.3% at 90% pruning using random sampling. The performance improves to 87.5% using CCS coreset selection and further improves to 90.3% using D 2 PRUNING. The ANLI dataset has been carefully crafted with an iterative, adversarial human-and-model-in-the-loop process, and hence, is significantly less redundant than conventional NLP datasets. The performance for ANLI falls from 48.8% to 42.8% at 80% pruning using random sampling.",
            "score": 0.4038242535714923,
            "section_title": "EXPERIMENTAL SETUP",
            "char_start_offset": 18305,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 897
                },
                {
                    "start": 900,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1802
                }
            ],
            "ref_mentions": [
                {
                    "start": 326,
                    "end": 349,
                    "matchedPaperCorpusId": "250113273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74755859375
        },
        {
            "corpus_id": "261582706",
            "title": "Fast FixMatch: Faster Semi-Supervised Learning with Curriculum Batch Size",
            "text": "Curriculum learning improves the performance of deep learning training by ordering training data using certain schemes into a \"curriculum\" [Bengio et al., 2009, Elman, 1993]. Typically, this is achieved by presenting easy samples first and hard samples later. Much progress has been made in this field on the optimal definition of \"easy\" and \"hard\" [Hacohen and Weinshall, 2019, Dogan et al., 2019, Cascante-Bonilla et al., 2020, Sinha et al., 2020, Castells et al., 2020]. We introduce Curriculum Learning based on whether the curriculum depends on the loss, label, feature space, or is fixed or entirely learnable. \n\nLoss-based Curricula. In [Hacohen and Weinshall, 2019], the authors use confidence of the teacher to order training data, where the teacher is either a pre-trained teacher network, or the student itself trained on data without a curriculum. [Cascante-Bonilla et al., 2020] revisits Pseudo-Labeling in SSL by devising a train and re-train curriculum, each time taking the top x% of most confident pseudolabels from 0 to 100 in increments of 20. \n\nLabel-based Curricula. In [Zhang et al., 2021], the authors propose a curriculum threshold for FixMatch, which increases data utilization by lowering the threshold resulting in more pseudolabels. To address training on imbalanced data, [Wang et al., 2019] proposes a curriculum which downsamples or upsamples -depending on majority or minority classes-and a parameter which balances between cross entropy loss and triplet loss. \n\nFeature Space-based Curricula. CurriculumNet [Guo et al., 2018] produces a curriculum based on feature space density. The authors train a model, compute embeddings, then retrain the model from clean samples to noisy samples, where samples with few nearby embeddings are more noisy. Instead of using one-hot labels, [Dogan et al., 2019] improves performance by defining a probability distribution over the classes based on class similarity given by inner product similarity of items of each class. \n\nFixed-Curricula.",
            "score": 0.4036432513262267,
            "section_title": "CURRICULUM LEARNING",
            "char_start_offset": 7812,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 616
                },
                {
                    "start": 619,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1062
                },
                {
                    "start": 1065,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1492
                },
                {
                    "start": 1495,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1991
                },
                {
                    "start": 1994,
                    "end": 2010
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 159,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 448,
                    "end": 472,
                    "matchedPaperCorpusId": "227275456"
                },
                {
                    "start": 1540,
                    "end": 1558,
                    "matchedPaperCorpusId": "51920640"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.115966796875
        },
        {
            "corpus_id": "264439382",
            "title": "DoGE: Domain Reweighting with Generalization Estimation",
            "text": "We provide the implementation and evaluation details of stage-wise curriculum learning in this section. Specifically, we firstly train a 82M proxy model applying DOGE for 10k steps. We then divide the whole training process of the proxy model into K = 2, 3, 10 stages, with 5000, 3333, 1000 training steps in each stage. By average the domain weights by number of steps within each stage, we get the stage-wise sampling weights distribution as Fig. (10). We then train another 124M model from scratch for 10k steps, where we map the stage-wise sampling weights within the corresponding training steps. We compare the validation perplexity between the model trained with stage-wise curriculum and applying a globally-averaged domain weights. The models trained by each curriculum should have seen the same amount of tokens from each domains in expectation. With K = 2, 3, the stage-wise curriculum keeps comparable performance as original DOGE, which applies the global English (en) German (de) French (fr) Spanish (es) Russian (ru) Catalan (ca) 0.073\u00b10.008 0.043\u00b10.008 0.344\u00b10.103 0.516\u00b10.102 0.024\u00b10.0001 Dutch (nl) 0.259\u00b10.003 0.267\u00b10.037 0.176\u00b10.043 0.203\u00b10.025 0.095\u00b10.022 average as the sampling weights throughout the whole training process. It is worth noting that the models learns hard domains (C4, CC, Book) slightly better than the global curriculum, while sacrificing the performance on easier domains (Arxiv, Github). However, with an extremely find-grained curriculum (K = 10), the curriculum severely hurt the performance on all the domains by a large margin. It suggests that given the same set of data covering diverse knowledge fields, the order of training data does impact the language modelling effectiveness, so that we have to carefully determine the granularity of the curriculum.",
            "score": 0.4033033370339543,
            "section_title": "E. Stage-wise Curriculum",
            "char_start_offset": 27181,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1804
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.325439453125
        },
        {
            "corpus_id": "254685579",
            "title": "Efficient Pre-training of Masked Language Model via Concept-based Curriculum Masking",
            "text": "For comparison with a hard-to-easy curriculum, we train the model with the reverse order of curriculum in CCM. Specifically, MLM is used to warmup the hard-to-easy model for 100k steps, subsequently, we train the model from stage 3 to stage 1.\n\nTeacher review. For teacher review, the BERT Base model is used as the teacher. We use the MLM loss from the pre-trained teacher as the difficulty score  and distribute examples according to the measured difficulty.\n\nMasking ratio. For the masking ratio curriculum, we only mask 10% of the first full sequence. Subsequently, we increase the masking ratio linearly to 15% of tokens when 1M is reached. Table 4, all curriculumbased approaches except the reverse curriculum improve generalization performance on various tasks compared to non-curriculum. The hard-to-easy curriculum shows a significant performance degradation. A possible reason could be that concepts added in the last step are too difficult for language models to learn without prior learning any relevant concepts, leading to the degradation of convergence speed. Finally, our CCM outperforms all other approaches in the experiment on the GLUE tasks.",
            "score": 0.40288786454704534,
            "section_title": "Reverse.",
            "char_start_offset": 18916,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43505859375
        },
        {
            "corpus_id": "265213396",
            "title": "CLIMB: Curriculum Learning for Infant-inspired Model Building",
            "text": "We describe our team's contribution to the STRICT-SMALL track of the BabyLM Challenge. The challenge requires training a language model from scratch using only a relatively small training dataset of ten million words. We experiment with three variants of cognitively-motivated curriculum learning and analyze their effect on the performance of the model on linguistic evaluation tasks. In the vocabulary curriculum, we analyze methods for constraining the vocabulary in the early stages of training to simulate cognitively more plausible learning curves. In the data curriculum experiments, we vary the order of the training instances based on i) infant-inspired expectations and ii) the learning behavior of the model. In the objective curriculum, we explore different variations of combining the conventional masked language modeling task with a more coarse-grained word class prediction task to reinforce linguistic generalization capabilities. Our results did not yield consistent improvements over our own non-curriculum learning baseline across a range of linguistic benchmarks; however, we do find marginal gains on select tasks. Our analysis highlights key takeaways for specific combinations of tasks and settings which benefit from our proposed curricula. We moreover determine that careful selection of model architecture, and training hyper-parameters yield substantial improvements over the default baselines provided by the BabyLM challenge.",
            "score": 0.40266217812082056,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.517578125
        },
        {
            "corpus_id": "258823219",
            "title": "Cross-Lingual Supervision improves Large Language Models Pre-training",
            "text": "When training a model on data from multiple sources, the automated curriculum learning paradigm (Graves et al., 2017) can learn the data-sampling schedule while training. In this way we can learn a dynamic lambda, \u03bb t , which is a function of the time step t; concretely, \u03bb t represents the probability of sampling the MT task and 1 \u2212 \u03bb t is the probability of sampling the LM task. Recent work (Kreutzer et al., 2021) has shown promising results when applying this curriculum approach to Machine Translation Systems where the data comes from multiple domains or multiple languages. For example, (Kreutzer et al., 2021) demonstrates that the multi-armed bandits employed by automated curriculum learning perform competitively against several SOTA heuristics on multi-lingual benchmarks. \n\nWe need to find the right reward function. In order to learn the dynamic scheduling of the MT and LM tasks, we need to assign a reward for using a specific task. Suppose that we sample a task \u03c4 \u2208 {MT, LM}; we then obtain a corresponding batch B \u03c4 and perform gradient descent updating the model parameters from \u0398 to \u0398 . The specific choice of \u03c4 has therefore resulted in a parameter change, and we need to measure how useful it was. After bench-marking different utility functions, Kreutzer et al. (2021) recommends to measure the loss reduction L(\u0398) \u2212 L(\u0398 ) on a trusted validation set. However, while in the setup of Kreutzer et al. (2021) there is a clear choice of the validation set, we are interested in pre-training of an LLM that is then applied to down-stream tasks using the in-context learning paradigm. Therefore, it is not trivial to build a validation set representative of all the possible few-shot tasks. In particular, mitigation strategies would be needed to avoid overfitting to a specific selection of tasks. \n\nWe use an intrinsic reward function. In early experiments we contrasted the rewards assigned by each downstream task (e.g. Question Answering) with those assigned by the training tasks and found that the signal from the former was smaller in magnitude and had a bigger variance. We therefore propose to measure rewards intrinsically on the (pre)-training data itself.",
            "score": 0.4026202759489476,
            "section_title": "Evaluation",
            "char_start_offset": 11092,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1817
                },
                {
                    "start": 1820,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2098
                },
                {
                    "start": 2099,
                    "end": 2187
                }
            ],
            "ref_mentions": [
                {
                    "start": 96,
                    "end": 117,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 395,
                    "end": 418,
                    "matchedPaperCorpusId": "238856825"
                },
                {
                    "start": 596,
                    "end": 619,
                    "matchedPaperCorpusId": "238856825"
                },
                {
                    "start": 1271,
                    "end": 1293,
                    "matchedPaperCorpusId": "238856825"
                },
                {
                    "start": 1408,
                    "end": 1430,
                    "matchedPaperCorpusId": "238856825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5341796875
        },
        {
            "corpus_id": "266166181",
            "title": "Byte-ranked Curriculum Learning for BabyLM Strict-small Shared Task 2023",
            "text": "The size of neural language models has increased rapidly over the past several years. This increase in model size has been accompanied by using larger and larger amounts of language data to train them. As these models and training data sizes have grown, the computational resources required to train them has surpassed what is available to many researchers. This work is part of a shared task called the BabyLM Challenge which requires language models to be trained using a restricted amount of training data a small fraction of the size of what large models use. In addition, no pretrained tools can be used. This work presents a curriculum learning approach to this data restricted setting by applying a bytes per line ordering to provided datasets. Throughout training, the average bytes per line is gradually increased by including more datasets as training data. Overall, there is an increase in performance on downstream tasks when using this curriculum learning approach, which provides a basis for potential further exploration of byte-based curriculum learning approaches.",
            "score": 0.4024982055363106,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.402587890625
        },
        {
            "corpus_id": "273350576",
            "title": "Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws",
            "text": "Data curation and selection. For current large language models, compute often poses a greater constraint than data availability, making data selection crucial (Albalak et al., 2024). A widely used approach is data filtering (Soboleva et al., 2023;Penedo et al., 2023;2024), where undesirable data points are removed based on heuristics like perplexity, repetition (Lee et al., 2021), or semantic similarity (Abbas et al., 2023). This filtering process is foundational for constructing most largescale datasets today. After filtering, data is often categorized into subsets or domains (e.g., code, books), and deciding how much data to use from each domain becomes an important step. \n\nTwo primary strategies for data selection are prevalent: one focuses on deciding whether individual data points should be included based on various criteria (Mindermann et al., 2022;Engstrom et al., 2024), and the other uses all available data but samples from different domains with varying probabilities (Xie et al., 2024;Fan et al., 2023;Albalak et al., 2023). While data selection aims to enhance training efficiency, these methods may introduce considerable computational overhead (Xie et al., 2024;Chen et al., 2024;Fan et al., 2023). Moreover, Kaddour et al. (2023) show that under the same computational budget, these methods often fail to surpass standard training, and Wang et al. (2024) proves that the data selection's efficacy depends on the user's utility function. \n\nAnother line of work focuses on selecting pre-training data that aligns more closely with downstream tasks (Kang et al., 2024). Data selection has also been explored for computer vision. For example, Evans et al. (2023) use a small reference model to select data for CLIP, while others propose pruning batches based on diversity criteria (Qin et al., 2024;Hong et al., 2024) to improve training efficiency. \n\nNeural scaling laws.",
            "score": 0.4024982055363106,
            "section_title": "RELATED WORKS",
            "char_start_offset": 28377,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 29,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 682
                },
                {
                    "start": 685,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1464
                },
                {
                    "start": 1467,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1873
                },
                {
                    "start": 1876,
                    "end": 1896
                }
            ],
            "ref_mentions": [
                {
                    "start": 842,
                    "end": 867,
                    "matchedPaperCorpusId": "235743024"
                },
                {
                    "start": 1026,
                    "end": 1047,
                    "matchedPaperCorpusId": "265658930"
                },
                {
                    "start": 1189,
                    "end": 1207,
                    "matchedPaperCorpusId": "260203057"
                },
                {
                    "start": 1574,
                    "end": 1593,
                    "matchedPaperCorpusId": "269605945"
                },
                {
                    "start": 1805,
                    "end": 1823,
                    "matchedPaperCorpusId": "257427625"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9326171875
        },
        {
            "corpus_id": "253116604",
            "title": "Improving Imbalanced Text Classification with Dynamic Curriculum Learning",
            "text": "Recent advances in pre-trained language models have improved the performance for text classification tasks. However, little attention is paid to the priority scheduling strategy on the samples during training. Humans acquire knowledge gradually from easy to complex concepts, and the difficulty of the same material can also vary significantly in different learning stages. Inspired by this insights, we proposed a novel self-paced dynamic curriculum learning (SPDCL) method for imbalanced text classification, which evaluates the sample difficulty by both linguistic character and model capacity. Meanwhile, rather than using static curriculum learning as in the existing research, our SPDCL can reorder and resample training data by difficulty criterion with an adaptive from easy to hard pace. The extensive experiments on several classification tasks show the effectiveness of SPDCL strategy, especially for the imbalanced dataset.",
            "score": 0.4024865190713356,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.494873046875
        },
        {
            "corpus_id": "231979380",
            "title": "Analyzing Curriculum Learning for Sentiment Analysis along Task Difficulty, Pacing and Visualization Axes",
            "text": "(Han and Myaeng, 2017) provide a tree-structured curriculum based on semantic similarity between new samples and samples already trained. (Tsvetkov et al., 2016) suggest a curriculum based on handcrafted semantic, linguistic, syntactic features for word representation learning. \n\nSome of these works (Cirik et al., 2016;Han and Myaeng, 2017;Rao et al., 2020) have suggested that Baby Steps performs better than One Pass. We perform experiments using both techniques. While the idea of curriculum remains the same across these works, the strategy itself to decide sample ordering is often diverse. \n\n2 Axis I: Curriculum Learning: One Pass and Baby Steps \n\nWhile Curriculum Learning as defined by (Bengio et al., 2009) is not constrained by a strict description, later related works (Cirik et al., 2016;Han and Myaeng, 2017;Spitkovsky et al., 2010;Rao et al., 2020) make distinctions between Baby Steps curriculum and One-Pass curriculum. Most of these previous works have also shown the dominance of Baby Steps over One-Pass. Baby Steps and One Pass curriculum can be defined as follows. For every sentence s i \u2208 D, its sentiment is described as y i \u2208 {0, 1, 2, 3, 4}, where i \u2208 {1, 2...n} for n data points in D. For a model f w , its prediction based on s i will be f w (s i ). Loss L is defined on the model prediction and actual output as L(y i , f w (s i )) \n\nand Cost defining the task as C(D, f w ) \n\nHere, curriculum strategy S(s i ) defines an \"easiness\"/\"difficulty\" quotient of sample s i . Furthermore, One Pass makes distinct, mutually exclusive sets of the training data and trains on each one of these sets one by one. This makes it faster as compared to Baby Steps, where data cumulatively increases in each pass. This implies that the model is trained on previous data and the additional harder data. \n\nTo analyze the two methods for executing CL we choose two curriculum strategies (difficulty scoring function). Furthermore, we also experiment with an individual setting explained in following sections. \n\nOur dataset has 5 labels.",
            "score": 0.4023107312721673,
            "section_title": "Introduction",
            "char_start_offset": 2016,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 278
                },
                {
                    "start": 281,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 597
                },
                {
                    "start": 600,
                    "end": 654
                },
                {
                    "start": 657,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1363
                },
                {
                    "start": 1366,
                    "end": 1406
                },
                {
                    "start": 1409,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1818
                },
                {
                    "start": 1821,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2023
                },
                {
                    "start": 2026,
                    "end": 2051
                }
            ],
            "ref_mentions": [
                {
                    "start": 321,
                    "end": 342,
                    "matchedPaperCorpusId": "21814386"
                },
                {
                    "start": 697,
                    "end": 718,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 803,
                    "end": 824,
                    "matchedPaperCorpusId": "21814386"
                },
                {
                    "start": 824,
                    "end": 848,
                    "matchedPaperCorpusId": "1363892"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1727294921875
        },
        {
            "corpus_id": "258947160",
            "title": "Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving",
            "text": "In the field of In-Context Learning (ICL), research primarily focuses on two main areas: (1) the selection of in-context examples, and (2) the arrangement of these examples in the learning context. With regard to the first area, Liu et al. [25] suggest a retrieval-based prompt selection method, offering a thoughtful alternative to random example selection. This method aims to find examples that are semantically similar to a test sample to form related prompts. Building on this idea, Rubin et al. [38] propose an effective retrieval process for prompts, using a pre-trained language model. Sorensen et al. [40] further the exploration by introducing a new way to select prompt templates that don't need labeled examples or direct access to the model. Instead, they choose the template that maximizes the mutual information between the input and the corresponding model output. Su et al. [41] present a two-step framework that is efficient in annotation. It first selects a set of examples from unlabeled data, and then retrieves task examples from the annotated set during testing. Lastly, Agrawal et al. [1] focus on creating strategies specifically for machine translation tasks, emphasizing the importance of the quality and domain of in-context examples, and warning against the negative effects of unrelated noisy examples. Works in the second area examine the significance of the order in which prompts are presented. Zhao et al. [51] point out the instability in few-shot learning caused by the order of training examples and suggest a calibration method to tackle this. Lu et al. [26] delve deeper into this analysis, demonstrating the sensitivity of prompt order in few-shot learning situations. Even though previous efforts have made remarkable progress in either choosing or sequencing in-context examples, our research sets a new precedent by combining both elements. In this paper, we step out of these isolated areas of concentration, looking into an approach based on diffusion models that effectively tackles both the challenges of selection and ordering at the same time.",
            "score": 0.402094099948703,
            "section_title": "In-context Learning",
            "char_start_offset": 22675,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 2092
                }
            ],
            "ref_mentions": [
                {
                    "start": 1440,
                    "end": 1444,
                    "matchedPaperCorpusId": "231979430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54443359375
        },
        {
            "corpus_id": "222176797",
            "title": "Self-Induced Curriculum Learning in Self-Supervised Neural Machine Translation",
            "text": "While sample difficulty is an intuitive measure on which to base a learning schedule, curricula may focus on other metrics such as task-relevance or noise. \n\nTo date, curriculum learning in NMT has had a strong focus on the relevance of training samples to a given translation task, e.g. in domain adaptation. van der Wees et al. (2017) train on increasingly relevant samples while gradually excluding irrelevant ones. They observed an increase in BLEU over a static NMT baseline and a significant speed-up in training as the data size is incrementally reduced. Zhang et al. (2019) adapt an NMT model to a domain by introducing increasingly domain-distant (difficult) samples. This seemingly contradictory behavior of benefiting from both increasingly difficult (domain-distant) and easy (domain-relevant) samples has been analyzed by Weinshall et al. (2018), showing that the initial phases of training benefit from easy samples with respect to a hypothetical competent model (target hypothesis), while also being boosted (Freund and Schapire, 1996) by samples that are difficult with respect to the current state of the model (Hacohen and Weinshall, 2019). In Wang et al. (2019), both domain-relevance and denoising are combined into a single curriculum. \n\nThe denoising curriculum for NMT proposed by Wang et al. (2018) is related to our approach in that they also use online data selection to build the curriculum based on the current state of the model. However, the noise scores for the dataset at each training step depend on fine-tuning the model on a small selection of clean data, which comes with a high computational cost. To alleviate this cost, Kumar et al. (2019) use reinforcement learning on the pre-scored noisy corpus to jointly learn the denoising curriculum with NMT. In Section 3.2 we show that our model exploits its self-supervised nature to perform denoising by selecting parallel pairs with increasing accuracy, without the need of additional noise metrics. \n\nDifficulty-based curricula for NMT that take into account sentence length and vocabulary frequency have been shown to improve translation quality when samples are presented in increasing complexity (Kocmi and Bojar, 2017).",
            "score": 0.4019065117768876,
            "section_title": "Related Work",
            "char_start_offset": 4333,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 158,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1256
                },
                {
                    "start": 1259,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1983
                },
                {
                    "start": 1986,
                    "end": 2208
                }
            ],
            "ref_mentions": [
                {
                    "start": 310,
                    "end": 336,
                    "matchedPaperCorpusId": "7921428"
                },
                {
                    "start": 562,
                    "end": 581,
                    "matchedPaperCorpusId": "155089817"
                },
                {
                    "start": 835,
                    "end": 858,
                    "matchedPaperCorpusId": "5004002"
                },
                {
                    "start": 1023,
                    "end": 1050,
                    "matchedPaperCorpusId": "1836349"
                },
                {
                    "start": 1128,
                    "end": 1157,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 1162,
                    "end": 1180,
                    "matchedPaperCorpusId": "174798125"
                },
                {
                    "start": 1304,
                    "end": 1322,
                    "matchedPaperCorpusId": "52156056"
                },
                {
                    "start": 1659,
                    "end": 1678,
                    "matchedPaperCorpusId": "67855673"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.634765625
        },
        {
            "corpus_id": "225068747",
            "title": "Improved Synthetic Training for Reading Comprehension",
            "text": "To examine the relationship between difficulty and pre-training effectiveness of synthetic examples, we follow the procedure in Section 3.4 to first partition the 4M answerable and 1M unanswerable synthetic examples into bins of different difficulty levels. We experiment with different bin sizes: 250k For bin sizes between 500k and 2M answerable examples (inclusive), we observe that model performance consistently improves from easier bins (examples with smaller losses) to harder bins (examples with larger losses) on both Dev and Test of SQuAD2.0 and NewsQA. We also observe that this pattern, while still present, is weaker and noisier for the 250k bin. Let HARD and EASY denote the hardest and the easiest bin for each bin size. In Table 5, we report the performance difference between \u03b8 HARD\u2192G and \u03b8 EASY\u2192G : the former consistently outperforms the latter across all bin sizes and test conditions. The above results confirm that harder examples are generally better pre-training examples than easier examples. \n\nNext we examine the relative performance of the set of all synthetic examples S (i.e., the largest 4M bin) against the smaller bins. Table 6 shows that in all four test conditions (SQuAD2.0 Dev/Test, NewsQA Dev/Test), higher performance can be achieved by pre-training only on a hard subset of S rather than on all of S. In fact, we see that in all test conditions except NewsQA Test, the hardest 500k bin already provides better pre-training than S, reducing the amount of synthetic examples needed by 87.5%. However, in none of the four test conditions is the hardest 250k bin as good as S, implying that even though hardest examples are the most effective, enough of them must be included in pre-training to ensure sufficient sample diversity. \n\nFinally, the proposed idea of using the hardest subset of examples for pre-training has similarities with curriculum learning (CL) (Bengio et al., 2009), which trains models in an easy-to-hard order so that the hardest examples are used at the end. In Table 7, we show results for pre-training on all examples S with CL.",
            "score": 0.40188658704680635,
            "section_title": "Targeted Synthetic Pre-training",
            "char_start_offset": 16083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1017
                },
                {
                    "start": 1020,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1766
                },
                {
                    "start": 1769,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2089
                }
            ],
            "ref_mentions": [
                {
                    "start": 1900,
                    "end": 1921,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28564453125
        },
        {
            "corpus_id": "274130626",
            "title": "Efficient Alignment of Large Language Models via Data Sampling",
            "text": "Data selection is a well known problem in literature with many algorithms such as filtering, coresets, importance sampling and more working towards the same goal [22]. Here, we present data selection in the domain of LLMs and natural language and how they can enable data efficient training. \n\nLIMA, proposed by [30] employs a small high quality dataset for fine-tuning. They show that by carefully curating only a 1000 data points, they are able to achieve remarkable performance which is generalizable to unseen data as well, thereby suggesting limited high quality tuning data is sufficient. \n\nAnother prominent study by [16] demonstrates that sample quality can reduce the data requirement without compromising the downstream performance. They investigate data engineering strategies in the fine-tuning paradigm from multiple facets to identify the characterises of good instruction tuning data. DEITA, the model family tuned by their proposed strategy to automatically select a complex and high quality dataset achieves comparable performance to open source models while using only a tenth of the data. \n\nThe study by [22], aims to explore data selection strategies for efficient pre-training of LLMs. They investigate methods targeting the coverage and quality of the sampled dataset and propose Density and Ask-LLM sampling focusing on coverage and quality respectively. They carry out extensive evaluation against other samplers and find their methods to be the best in their categories. The models trained on their sampled data achieves comparable or improved performance to the models trained with the full dataset while converging 70% faster. \n\nHence, while there has been many studies focusing on data efficient fine-tuning and pre-tuning, to the best of our knowledge such strategies have not been studied in the alignment paradigm. In this study, we explore how LLM alignment scales with data and identify data engineering and sampling strategies as viable methods to enable efficient alignment. Further, we propose a novel strategy based on information theory to sample a diverse and high quality subsample outperforming other sampling strategies for efficient alignment.",
            "score": 0.40178926447992963,
            "section_title": "A.2 Related Works",
            "char_start_offset": 14183,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 291
                },
                {
                    "start": 294,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1653
                },
                {
                    "start": 1656,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2186
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96337890625
        },
        {
            "corpus_id": "174798125",
            "title": "Dynamically Composing Domain-Data Selection with Clean-Data Selection by \u201cCo-Curricular Learning\u201d for Neural Machine Translation",
            "text": "Evidence of data-quality transfer. Figure 7 visualizes that CL in one domain (e.g., web) may enable CL in another. This is the foundation of our proposed method. To draw the figure, using a random sample of 2000 pairs from WMT training data and some additional in-domain parallel data, we sort examples by tightening the selection ratio according to a true web curriculum. The web curve shows the co-relation between selection ratio and data relevance to web. The same data order appears to yield increasing relevance to other domains, too, with bigger effect on a closer 'news' domain, but smaller effect on 'patent' and 'short' (sentences). \n\nRegularizing data without a teacher. The analysis in Section 5.4 shows that the denoising scoring function and its bootstrapped versions tend to regularize the late curriculum and make the scheduled data easier for small models to learn on. One potential further application of this data property may be in learning a multitask curriculum where regular data may be helpful for multiple task distributions to work together in the same model. This has been achieved by knowledge distillation in existing research (Tan et al., 2019), by regularizing data with a teacher -We could instead regularize data by example selection, without a teacher. We leave this examination for future research. \n\nPace function hyper-parameters. In experiments, we found that data-discarding pace functions seem to work best when they simultaneously decay down to their respective floors. Adaptively adjusting them seems an interesting future work.",
            "score": 0.4017569113666969,
            "section_title": "Discussion",
            "char_start_offset": 19009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 34
                },
                {
                    "start": 35,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1333
                },
                {
                    "start": 1336,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1570
                }
            ],
            "ref_mentions": [
                {
                    "start": 1156,
                    "end": 1174,
                    "matchedPaperCorpusId": "67856276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44677734375
        },
        {
            "corpus_id": "271328390",
            "title": "TAGCOS: Task-agnostic Gradient Clustered Coreset Selection for Instruction Tuning Data",
            "text": "Instruction tuning has achieved unprecedented success in NLP, turning large language models into versatile chatbots. However, the increasing variety and volume of instruction datasets demand significant computational resources. To address this, it is essential to extract a small and highly informative subset (i.e., Coreset) that achieves comparable performance to the full dataset. Achieving this goal poses non-trivial challenges: 1) data selection requires accurate data representations that reflect the training samples' quality, 2) considering the diverse nature of instruction datasets, and 3) ensuring the efficiency of the coreset selection algorithm for large models. To address these challenges, we propose Task-Agnostic Gradient Clustered COreset Selection (TAGCOS). Specifically, we leverage sample gradients as the data representations, perform clustering to group similar data, and apply an efficient greedy algorithm for coreset selection. Experimental results show that our algorithm, selecting only 5% of the data, surpasses other unsupervised methods and achieves performance close to that of the full dataset.",
            "score": 0.4016705467405399,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79638671875
        },
        {
            "corpus_id": "266844145",
            "title": "Data-CUBE: Data Curriculum for Instruction-based Sentence Representation Learning",
            "text": "In this work, we proposed Data-CUBE, a data curriculum method for multi-task instruction-based sentence representation learning. Our core idea is to reduce the cross-task and cross-instance interference risks caused by the randomly ordered tasks and sampled instances with large divergence, through arranging their orders before training. To achieve this, we employed a simulated annealing algorithm to find the optimal task order to minimize the cross-task interference, and assigned all instances per task into easy-to-difficult mini-batches to reduce the cross-instance interference. Experimental results on MTEB sentence representation evaluation tasks have shown that our approach can boost the performance of state-of-the-art baselines. \n\nIn the future, we will apply our method for other tasks and the pre-training process of large language models. Besides, we will also explore a more efficient and effective data curriculum method for large-scale sentence representation learning.",
            "score": 0.4016195494665637,
            "section_title": "Conclusion",
            "char_start_offset": 31275,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 742
                },
                {
                    "start": 745,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 989
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6259765625
        },
        {
            "corpus_id": "278394359",
            "title": "Text2Cypher: Data Pruning using Hard Example Selection",
            "text": "Finally, Yang et al. [20] focus on diversity-aware selection using sparse autoencoders and either greedy-sampling approach (SAE-GreedSelect) or similarity-based sampling (SAE-SimScale) approach. \n\nAlthough data selection or pruning are well-studied in machine learning, their application to natural language to query language tasks, such as Text2SQL and Text2Cypher, remains largely unexplored. SE-HCL [23] applies curriculum learning to the Text2SQL task by training the model progressively, starting with easy instances and gradually moving to more difficult ones. This approach involves iterative steps that begin with simplifying the data, gradually increasing its complexity, and evaluating the difficulty of individual instances. Some Text2SQL datasets, such as Spider [21] and IndDB [11], provide difficulty labels based on SQL constructs like GROUP BY clauses and nested subqueries, where more complex constructs indicate higher difficulty. However, these difficulty annotations are primarily used for analyzing evaluation outputs rather than for data selection. In this work, we explore data pruning for the Text2Cypher task by focusing on hard-example selection based on instance difficulty.",
            "score": 0.40104617892021716,
            "section_title": "RELATED WORK",
            "char_start_offset": 5220,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 197,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1201
                }
            ],
            "ref_mentions": [
                {
                    "start": 402,
                    "end": 406,
                    "matchedPaperCorpusId": "267670285"
                },
                {
                    "start": 775,
                    "end": 779,
                    "matchedPaperCorpusId": "52815560"
                },
                {
                    "start": 790,
                    "end": 794,
                    "matchedPaperCorpusId": "269542469"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6611328125
        },
        {
            "corpus_id": "264146362",
            "title": "A Comprehensive Study of Privacy Risks in Curriculum Learning",
            "text": "Curriculum learning (CL) [6] is designed to emulate the concept of the human learning process. The general idea is to have a curriculum that imposes a structure on the training data so the \"student\" ML models can learn from the easier samples before the harder ones. As a result, training ML models under CL observes a shorter duration of convergence and higher testing accuracy [6], [89], [25], [27]. For example, Weinshall et al. proposed to use transfer learning to build the curriculum and achieved 0.5% to 3.5% higher accuracy than a model trained in the normal setting [89]. CL has gained significant interest from the ML community, powering real-world applications in many domains. Section VII provides a more detailed survey. \n\nBelow, we formalize CL following the definition of Hacohen et al. [27]. Let X = {X i } N i=1 = {(x i , y i )} N i=1 be the training dataset, where N is the number of samples, x i is a data point, and y i is the label of x i . T is the ML model to be trained. Assuming Stochastic Gradient Descent (SGD) is used for optimization, and each training iteration takes a mini-batch of X , and a sequence of M mini-batches B 1 , . . . , B M will be used for each epoch. The standard training procedure will sample X uniformly to generate the mini-batches. Instead, CL uses a difficulty measurer f (X , C) to generate difficult scores for X , and a training scheduler sorts X by the difficult scores in an ascending order ahead of training. C is the curriculum, and we will elaborate on its common options in Section IV-A. A sequence of subsets X \u2032 1 , . . . , X \u2032 M \u2286 X are extracted from X after sorting, and the size of X \u2032 i is determined by a pacing function g(i). A mini-batch B i is sampled uniformly from X \u2032 i . Algorithm 1 summarizes the process. Noticeably, slight changes can be applied (e.g., skip the step of mini-batch sampling), but they should not affect the conclusions drawn from this study.",
            "score": 0.40085226653464734,
            "section_title": "A. Curriculum Learning",
            "char_start_offset": 5728,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 733
                },
                {
                    "start": 736,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1937
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 28,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 379,
                    "end": 382,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 384,
                    "end": 388,
                    "matchedPaperCorpusId": "5004002"
                },
                {
                    "start": 390,
                    "end": 394,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 396,
                    "end": 400,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 575,
                    "end": 579,
                    "matchedPaperCorpusId": "5004002"
                },
                {
                    "start": 802,
                    "end": 806,
                    "matchedPaperCorpusId": "102350936"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.049224853515625
        },
        {
            "corpus_id": "251979721",
            "title": "Efficient Methods for Natural Language Processing: A Survey",
            "text": "Curriculum learning aims to find a data ordering that reduces the number of training steps required to achieve a target performance (Elman, 1993;Bengio et al., 2009). This method does not reduce dataset size, but does improve its utilization. Hence, it is a common approach for improving training efficiency in both pre-training and fine-tuning. Many curriculum learning methods order instances by difficulty, using heuristics such as sentence length. This has yielded improvements for transformer pretraining (Press et al., 2021;Agrawal et al., 2021) as well as fine-tuning on tasks such as question answering (Tay et al., 2019), MT (Zhang et al., 2019), and others (Xu et al., 2020). \n\nA major challenge in curriculum learning is determining pace, i.e., when to progress to more difficult instances. If not chosen carefully, curriculum learning can waste compute on \"easy\" instances. To tackle this, work has investigated adaptive ordering strategies based on current model state, called self-paced learning (Kumar et al., 2010). This has been successfully applied to improve performance Notably, the training stage is divided into two parts: pre-training, which aims to learn generalizable parameters, and fine-tuning, which optimizes these parameters for specific downstream tasks. \n\nFigure 3: Typology of efficient NLP methods. \n\nin MT using model and data uncertainty (Wan et al., 2020;Zhou et al., 2020;Zhao et al., 2020), and in dialog generation with knowledge distillation (Zhu et al., 2021). However, self-paced learning involves large training costs, and disentangling instance ordering from factors such as optimizer choice and batch size is non-trivial (Dodge et al., 2020).",
            "score": 0.4007840759709439,
            "section_title": "Curriculum Learning",
            "char_start_offset": 5720,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 685
                },
                {
                    "start": 688,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1285
                },
                {
                    "start": 1288,
                    "end": 1332
                },
                {
                    "start": 1335,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1688
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 145,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 611,
                    "end": 629,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 634,
                    "end": 654,
                    "matchedPaperCorpusId": "155089817"
                },
                {
                    "start": 667,
                    "end": 684,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 1010,
                    "end": 1030,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 1374,
                    "end": 1392,
                    "matchedPaperCorpusId": "222272176"
                },
                {
                    "start": 1392,
                    "end": 1410,
                    "matchedPaperCorpusId": "220047761"
                },
                {
                    "start": 1410,
                    "end": 1428,
                    "matchedPaperCorpusId": "211102338"
                },
                {
                    "start": 1483,
                    "end": 1501,
                    "matchedPaperCorpusId": "244119160"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.475341796875
        },
        {
            "corpus_id": "225066701",
            "title": "A Comprehensive Survey on Curriculum Learning",
            "text": "For example, in multi-task learning settings [66], [76], the scholars train the model by each time selecting the easiest or most correlated task(s) to the previous task(s). In domain adaption settings, some authors [111] propose to gradually expands the training set from a purely-in-domain subset (which is easier) to a not-so-indomain larger dataset. In RL settings, it becomes a widely adopted strategy to train the RL agent from easy (sub)tasks to harder tasks (until target) [17]. Moreover, other works also extend the definition of CL by adding more dimensions of data characteristics to the \"easy to hard\" strategy for different purposes in applications. For instance, Jiang et al. [33] propose to train \"from easy & diverse to hard\" to avoid overfitting to the same sample group in multi-group event detection tasks. In addition, Wang et al. [98] train the model \"from easy & imbalanced to hard & balanced\" data to alleviate the severe class imbalance in the task of human attribute analysis. \n\nAt a more abstract level, a curriculum can actually be seen as a sequence of data selection or example reweighting along the training process to achieve faster convergence or better generalization, which is beyond the \"easy to hard\" or \"starting small\" principles. This perspective inspires the academic community to bring more connotations to CL definition with new methodologies. As we will discuss in Sec. IV-C, the automatic CL methodologies in \"RL Teacher\" and \"Other Automatic CL\" (mostly) categories could learn to automatically and dynamically select the most suitable examples or tasks (with adjustable loss weights) for the current training step. Interestingly, in some of the works, the best curriculum found by the algorithm is the opposite of traditional CL, i.e., \"hard to easy\" [15], [96] or \"starting big\" (from full dataset to informative subset) [96], [97], [112]. A discussion on this seemingly paradoxical phenomenon will be made in Sec. V.",
            "score": 0.400748035270303,
            "section_title": "II. DEFINITION OF CL",
            "char_start_offset": 6621,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1000
                },
                {
                    "start": 1003,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 1963
                }
            ],
            "ref_mentions": [
                {
                    "start": 45,
                    "end": 49,
                    "matchedPaperCorpusId": "8502955"
                },
                {
                    "start": 51,
                    "end": 55,
                    "matchedPaperCorpusId": "4725956"
                },
                {
                    "start": 480,
                    "end": 484,
                    "matchedPaperCorpusId": "22729745"
                },
                {
                    "start": 689,
                    "end": 693,
                    "matchedPaperCorpusId": "9686483"
                },
                {
                    "start": 850,
                    "end": 854,
                    "matchedPaperCorpusId": "58981386"
                },
                {
                    "start": 1873,
                    "end": 1877,
                    "matchedPaperCorpusId": "220045412"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10430908203125
        },
        {
            "corpus_id": "229924288",
            "title": "Dialogue Response Selection with Hierarchical Curriculum Learning",
            "text": "To measure the difficulty of each training context-response pair (c, r), we adopt a pre-trained ranking model G(\u00b7, \u00b7) (details are presented in \u00a73.4) to calculate its similarity score as G(c, r). Here, a higher score of G(c, r) corresponds to a higher similarity between c and r and vice versa. Then, for each pair (c i , r + i ) \u2208 D, its corpus-level difficulty is defined as (1) f p (t) is computed based on the current step t; and (2) a batch of context-response pairs are uniformly sampled from the training instances whose corpus-level difficulty is lower than f p (t) (shaded area in the example). In this example, T 0 = 2000 and T = 8000; (b): An illustration of the instance-level pacing function, in this case, k 0 = 6, k T = 3 and T = 8000.\n\nA lower difficulty score indicates c + i and r i are more similar to each other thus are easier for the model to learn.\n\nPacing Function During training, to select the training instances with desired difficulty, we resort to a pre-defined corpus-level pacing function, f p (t). Specifically, f p (t) is defined as a function of training steps. At each time step t, the model is only allowed to use the training instances (c, r + ) whose corpus-level difficulty score f d (c, r + ) is lower than f p (t). Starting from easier data instances, the model gradually learns harder cases as the training evolves. In this work, we propose a simple functional form for f p (t) as shown in the following 2 :\n\nAt the warm up stage of training (first T 0 steps), we learn a basic matching model with the easiest part of the training set. Then, the model is allowed to gradually use harder instances. After f p (t) becomes 1.0 (at time step T ), the corpus-level curriculum is completed and the model can freely access the entire dataset. Figure 2(a) depicts an illustration of the proposed corpus-level curriculum.",
            "score": 0.40062880713116544,
            "section_title": "Difficulty Function",
            "char_start_offset": 9046,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44970703125
        },
        {
            "corpus_id": "209862398",
            "title": "Learning From Multiple Experts: Self-paced Knowledge Distillation for Long-tailed Classification",
            "text": "To achieve this goal, we introduce a self-paced weighted scheme based on the performance gap on the validation set between the expert models and the student model. As the student model acquires knowledge from both data and the expert models, the importance weight of knowledge distillation will gradually decrease, and finally the unified student model is able to achieve comparable or even superior performance compared to the experts. From the perspective of self-paced instance selection, given the confidence scores from the Expert models, we re-organize the training data from easy to hard, i.e. from low-confidence to high-confidence. Then we exploit a soft weighted instance selection scheme to conduct such self-paced curriculum, so that easy samples are trained first, then harder samples are added to the training set gradually. This progressive learning curriculum has proved to be beneficial for training deep models [1]. Finally, with the two levels of self-paced learning schemes, the knowledge from the expert models will be gradually transferred to the unified student model. An overview of the LFME framework is shown in Fig 3.",
            "score": 0.40061254257350654,
            "section_title": "Motivation and Metrics for Evaluating Data Imbalance",
            "char_start_offset": 11692,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1144
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.340087890625
        },
        {
            "corpus_id": "252355060",
            "title": "CES-KD: Curriculum-based Expert Selection for Guided Knowledge Distillation",
            "text": "llation (BLKD) [7] on these different levels of curriculum. Table I shows the test accuracy of the student network trained on three different levels of difficulty with three teachers of different capacities. We see that for easy samples the student network (ResNet20) has higher accuracy when guided through distillation by the lowest capacity teacher (ResNet26) within the group of experts. As for the difficult samples, the student acquires better knowledge from the highest capacity teacher model (ResNet56), which validates our assumption in terms of quality of learning. To further study the student's learning efficiency, Figure 3 (a) shows that the student's optimization, on easy samples, converges faster when trained with the guidance of the lowest capacity teacher model (ResNet26) via knowledge distillation. Also in Figure 3 (b) we observe that on difficult samples, the student learns faster (i.e. faster convergence) from the highest capacity network (ResNet56) than from the lower capacity teachers. This comparison validates our hypothesis and motivates our technique regarding the curriculum data-model selection.  B. Methodology of our proposed method Figure 1 presents a global overview of the distillation framework CES-KD. Our method relies on two main steps: the design of the data curriculum and the selection of a single representative expert based on their expertise on a given input data (i.e. images). We provide in Figure 2 details on the distillation pipeline of our method with a back-toback comparison on current TA-based KD methods (TAKD [9] and DGKD [10]). Following the standard CL paradigm, we need to address two main questions: (1) How do we rank the dataset according to an easy-to-hard curriculum? (see subsection III-B1); (2) How do we train a student model using the ranked instances? (See subsection III-B2).\n\n1) Design of the data curriculum: In this work, we adopt the meta-network method described by Hacohen and Weinshall [17] as transfer learning-based scoring function. In particular, we consider a reference model trained on a very large dataset. Then, we fine-tune this reference model on the smaller training dataset",
            "score": 0.4004068272256596,
            "section_title": "A. Hypothesis and Motivation",
            "char_start_offset": 10710,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1571,
                    "end": 1574,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1584,
                    "end": 1588,
                    "matchedPaperCorpusId": "221802641"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09857177734375
        },
        {
            "corpus_id": "231979380",
            "title": "Analyzing Curriculum Learning for Sentiment Analysis along Task Difficulty, Pacing and Visualization Axes",
            "text": "As we saw in previous section, Hard samples are catastrophically forgotten all the time, hence training on these samples again and again will lead to an improvement. Unlike SST-2 and SST-3 which have a huge proportion of easy samples, Hard tasks according have harder samples and hence instead repeated training in a systematic ordering would not have an adverse effect. \n\nFurthermore, (Xu et al., 2020) suggest a similar reasoning for the Task Difficulty hypothesis. They suggest that when learning without curriculum, in the case of harder tasks, the model is confounded and overwhelmed by the presence of hard samples. \n\nHence, laying out the training where the model observes easier samples first is natural to improve performance. However, this reasoning does not explain well why there is an apparent decrement of performance for CL on high performing tasks.",
            "score": 0.40035805989458295,
            "section_title": "Results and Discussions",
            "char_start_offset": 18036,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 370
                },
                {
                    "start": 373,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 621
                },
                {
                    "start": 624,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 864
                }
            ],
            "ref_mentions": [
                {
                    "start": 386,
                    "end": 403,
                    "matchedPaperCorpusId": "220045816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.090576171875
        },
        {
            "corpus_id": "250390527",
            "title": "Crossroads, Buildings and Neighborhoods: A Dataset for Fine-grained Location Recognition",
            "text": "We fnd that applying different curriculum settings (normal curriculum that exposes the easiest examples frst or anti-curriculum that exposes the most diffcult examples frst) results in a large performance difference between the NCRF++ model and the BERT model. As shown in Figure 4, for the NCRF++ model without pretrained language mod- els, the normal curriculum setting yields signifcantly better average F-1 scores across all the three curriculum scoring functions in comparison with the anti-curriculum setting. However, for the pretrained language model BERT, the results are the opposite; using anti-curriculum learning consistently yields better performance than using normal curriculum learning. \n\nOne possible explanation is that the volatile gradients resulting from using anti-curriculum learning can lead to better local minima for a wellpretrained model. Specifcally, the anti-curriculum learning will feed those \"hard\" samples to the model frst, and the gradients from those long-tailed hard cases will cause relatively large fuctuations compared to those from easy instances. BERT is a pretrained language model and the pretrained pa-rameters might constrain the model to some local regions. The fuctuations produced by the \"hard\" samples from the anti-curriculum learning can enable the BERT model to reach other better local minima regions.",
            "score": 0.40012569191832714,
            "section_title": "Curriculum v.s Anti-curriculum",
            "char_start_offset": 21192,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 703
                },
                {
                    "start": 706,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1357
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.365478515625
        },
        {
            "corpus_id": "140295406",
            "title": "Curriculum Learning in Deep Neural Networks for Financial Forecasting",
            "text": "The fourth model applies curriculum learning to the third model. We use the pre-calculated seasonal decomposition to determine a useful batch ordering method to feed into our neural net, and then apply the Baby Steps curriculum algorithm [4,13] defined in Figure 4. \n\nIn words, we train in increasing order of the residual error calculated from the STL decomposition mentioned previously. Once the k batches are defined, we shuffle the datarows within the batch during training. Within each iteration of the rolling window process, we continue the warm-start process by iteratively adding one batch at a time to the training data, running each rolling window iteration p times, where p is the number of epochs chosen such that convergence can be reached. In summary, for each of the s = 1, ..., k batches, we run D train = D train \u222a D s until the end of p epochs of each rolling window iteration. A sample loss function including curriculum learning is shown in Figure 5, where we have experimentally chosen p = 75. \n\nWe note that there are several ways to form the batching described above. In our results, we present batches formed directly from the datarow sort metric calculated as described above, i.e., the datarow-level residual error (using 5 batches, as determined experimentally). However, we have also found good results when batching by segment (with each batch corresponding to one Microsoft segment), where batches are sorted by revenue-weighted segment-level residual error. In all cases, we shuffle datarows within each batch when training. The idea of curriculum learning, as highlighted in Section 1, is to train on easier examples to learn harder ones. As such, we find that running curriculum learning with batches sorted in order of best-fit to least-fit segments yields similar results to those that we find from the datarow-level (uniform-weighting) method we use in this paper. However, we also experimented with curriculum learning using batches sorted in reverse order: least-fit to best-fit segments. This resulted in far better accuracies for certain (smaller-revenue) segments, but worse accuracies for all other segments.",
            "score": 0.40006725508222973,
            "section_title": "LSTM with Curriculum Learning",
            "char_start_offset": 15027,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 265
                },
                {
                    "start": 268,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1015
                },
                {
                    "start": 1018,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2151
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 244,
                    "matchedPaperCorpusId": "1363892"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0266571044921875
        },
        {
            "corpus_id": "231709290",
            "title": "Curriculum Learning: A Survey",
            "text": "commences with the first bin. The easy-to-hard ordering comes from the fact that, in a scene with fewer sound-sources, it is easier to visually localize the sound-makers from the background and align them to the sounds. Huang et al. (2020a) address the task of synthesizing dance movements from music in a curriculum fashion. They use a sequence-to-sequence architecture to process long sequences of music features and capture the correlation between music and dance. The training process starts from a fully guided scheme that only uses ground-truth movements, proceeding with a less guided autoregressive scheme in which generated movements are gradually added. Using this curriculum, the error accumulation of autoregressive predictions at inference is limited. Zhang et al. (2021b) propose a two-stage curriculum learning approach for improving the performance of audio-visual representations learning. Their teacher-student framework based on constrastive learning starts by pre-training the teacher and then jointly training the teacher and the student models, in the first stage. In the second stage, the roles are reversed, with only the student being trained at first, until commencing the training of both networks. Wang et al. (2020a) introduce a curriculum pre-training method for speech translation. They claim that the traditional pre-training of the encoder using speech recognition does not provide enough information for the model to perform well. To alleviate this problem, the authors include in their curriculum pre-training approach a basic course for transcription learning and two more complex courses for utterance understanding and word mapping in two languages. Different from other curriculum methods, they do not rank examples from easy to hard, but design a series of tasks with increased difficulty in order to maximize the learning potential of the encoder.",
            "score": 0.39997190341495026,
            "section_title": "Speech recognition",
            "char_start_offset": 105524,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1226,
                    "end": 1245,
                    "matchedPaperCorpusId": "216035773"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11474609375
        },
        {
            "corpus_id": "246822409",
            "title": "Development and Comparison of Scoring Functions in Curriculum Learning",
            "text": "Curriculum is very often used in human and animal learning [1,2]. Curriculum refers to the ordering of the samples of the subjects to be learned from easy to difficult. The reason for using this order is that it presumably provides better and faster learning. Deep learning is inspired by human learning. However, in its classical form, it uses all samples completely randomly throughout the training. \n\nIt is suggested to start training with easy samples in machine learning algorithms. And the effectiveness of curriculum learning (CL) has been demonstrated in a variety of investigations, including natural language processing (NLP) tasks, image classification and artificial data sets. CL performed better on the test set. It also accelerated the convergence speed [3]. \n\nIn deep neural network training, parameter updates are performed using the entire data set at each epoch. In order to apply the curriculum, first of all, all samples should be ordered (scoring function) and it should be determined at what rate (pacing function) the samples will be presented to the model. For example, let the total training set consist of 3n samples. After the samples are sorted according to a certain criterion, with the easiest n of them, parameter updates are performed as the number of epochs is determined. Then updates are performed for the easiest 2n samples and finally the training is completed using the entire dataset. The scoring function determines which samples will be used first, and the pacing function determines how fast the used subset size changes during training. \n\nIn order to apply CL in deep learning, a curriculum needs to be designed. It brings an additional computational load. Deep learning works better with large datasets. In addition to creating a dataset with a large number of samples, it will be very costly to determine the sample hardness of the dataset. For this reason, automatic curriculum methods for deep learning have been proposed. \n\nFor example, image classification experiments were carried out on artificially created geometric shapes [3]. Shape sizes and object positions are used for sample difficulty. However, such methods are not applicable for all datasets. For this arXiv:2202.06823v2 [cs.LG] 15 Mar 2022 reason, methods that create a curriculum using a different machine learning model have been proposed. Predictions made by a trained model on a training set are used for curriculum.",
            "score": 0.39956371350463443,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 401
                },
                {
                    "start": 404,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1580
                },
                {
                    "start": 1583,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 1970
                },
                {
                    "start": 1973,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2146
                },
                {
                    "start": 2147,
                    "end": 2205
                },
                {
                    "start": 2206,
                    "end": 2233
                },
                {
                    "start": 2234,
                    "end": 2355
                },
                {
                    "start": 2356,
                    "end": 2434
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 62,
                    "matchedPaperCorpusId": "206863670"
                },
                {
                    "start": 769,
                    "end": 772,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 2077,
                    "end": 2080,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28466796875
        },
        {
            "corpus_id": "234338422",
            "title": "Self-Guided Curriculum Learning for Neural Machine Translation",
            "text": "construct the criterion, while others derive criteria from independently pre-trained models like language model (Zhang et al., 2019;Dou et al., 2020;Liu et al., 2020) and word embedding model (Zhou et al., 2020b). Xu et al. (2020) derives criterion from the NMT model in the training process. And according to different way of scheduling the curriculum, these difficulty criteria are apply to either fixed schedule (Cirik et al., 2016) or dynamic one (Platanios et al., 2019;Liu et al., 2020;Xu et al., 2020;Zhou et al., 2020b).\n\nA well-trained NMT model learns an optimal probability distribution mapping sentences from source language to target language, which is expected to be capable of recovering training labels . However if we test on the training set, we can observe inconsistent predictions against the target reference sentences, reflecting the discrepancy between the model distribution and the empirical distribution of training corpus, as Figure 1 illustrated. For a training example, high recovery degree between prediction and target reference sentence means it's easier to be masted by the NMT model, while low recovery degree means it's more difficult (Ding and Tao, 2019;. Taking recovery degree as the difficulty criterion, we propose a CL strategy to schedule curriculum learning with a well-trained vanilla NMT model. We put forward an analogy of this method that a person can schedule a personal and effective curriculum after skimming over the whole textbook, namely self-guided curriculum.\n\nIn this work, we cast recovery degree of each training example as its learning difficulty, enforcing an NMT model to learn from examples with higher recovery degree to the lower ones, and we analyze the coordination of this criterion with fixed and dynamic curriculum schedules. We conduct experiments on widely-used benchmarks, including WMT14 En-De and WMT17 Zh-En. Experimental results demonstrate that our proposed self-guided CL strategy can boost the performance of an NMT model against strong baseline Transformer.",
            "score": 0.39923842747027316,
            "section_title": "Introduction",
            "char_start_offset": 1972,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 132,
                    "matchedPaperCorpusId": "155089817"
                },
                {
                    "start": 132,
                    "end": 149,
                    "matchedPaperCorpusId": "215415842"
                },
                {
                    "start": 149,
                    "end": 165,
                    "matchedPaperCorpusId": "219260306"
                },
                {
                    "start": 192,
                    "end": 212,
                    "matchedPaperCorpusId": "220047761"
                },
                {
                    "start": 214,
                    "end": 230,
                    "matchedPaperCorpusId": "227227757"
                },
                {
                    "start": 451,
                    "end": 475,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 475,
                    "end": 492,
                    "matchedPaperCorpusId": "219260306"
                },
                {
                    "start": 492,
                    "end": 508,
                    "matchedPaperCorpusId": "227227757"
                },
                {
                    "start": 508,
                    "end": 527,
                    "matchedPaperCorpusId": "220047761"
                },
                {
                    "start": 1170,
                    "end": 1190,
                    "matchedPaperCorpusId": "195767109"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33349609375
        },
        {
            "corpus_id": "190000064",
            "title": "Curriculum-based transfer learning for an effective end-to-end spoken language understanding and domain portability",
            "text": "Intuition of curriculum learning is based on the analogy with humans who learn better when concepts to be learnt and examples are presented gradually, from simple ones to more complex ones. The motivation of curriculum learning is that the order the training data is presented, from easy examples to more difficult ones, helps training algorithms, for instance by accelerating the convergence and by guiding the learner towards better local minima [26]. A curriculum learning strategy can also be considered as a special form of transfer learning where the first learnt tasks are used to guide the learner so that it will perform better on the final task [10]. In this study, we aim to hybrid both curriculum learning strategy and more classical transfer learning. \n\nTo train an end-to-end neural model for spoken language understanding that directly takes speech as input, we need both audio recordings and their semantic annotations. A first remark consists on underlining that such training data are usually limited in size, and are probably not large enough to train an effective SLU system. A second remark is about the availability of other resources containing both audio recordings and manual annotations. For any resourced languages, like English or French, such resources exist and their use must be considered to help to train an SLU end-to-end neural model. These resources can be simple audio recordings with manual transcriptions, but can also be audio recordings with manual annotations that express some semantic aspects, not directly related to the final semantic task. For instance, in French several corpora exist that contain annotations on named entities or semantic concepts for different tasks related to goal-oriented human/machine dialogues. In order to take benefit from the existence of these data to train an SLU end-to-end system, we suggest to order these data from the most semantically generic to the most specific ones, and to train successive neural models by reinjecting the weights trained at step t as preinitialized weights at step t + 1, except for the output layer that has to be reinitialized to handle new output symbols. Figure 1 illustrates this approach. First, we consider the most semantically generic data as the ones containing manual transcriptions (cf. only words) of audio recordings. Secondly, we consider the use of audio recordings associated to manual annotations of named entities.",
            "score": 0.39898158816031604,
            "section_title": "Curriculum-based transfer learning",
            "char_start_offset": 7254,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 764
                },
                {
                    "start": 767,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 2163
                },
                {
                    "start": 2164,
                    "end": 2199
                },
                {
                    "start": 2200,
                    "end": 2303
                },
                {
                    "start": 2304,
                    "end": 2336
                },
                {
                    "start": 2337,
                    "end": 2438
                }
            ],
            "ref_mentions": [
                {
                    "start": 448,
                    "end": 452,
                    "matchedPaperCorpusId": "206863670"
                },
                {
                    "start": 655,
                    "end": 659,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47509765625
        },
        {
            "corpus_id": "239998303",
            "title": "How Important is Importance Sampling for Deep Budgeted Training?",
            "text": "Few works exploit a budgeted training paradigm [34]. Instead, many aim to speed up convergence to a given performance using a better sampling strategy or carefully organizing the samples to allow the model to learn faster and generalize better. Others explore how to improve model performance by labeling the most important samples from an unlabeled set [6,46,59] or how to better train DNNs when limited samples per class are available [2,12,64]. None of these works, however, explore the efficiency of these approaches when trained under constraints in the number of iterations allowed, i.e. budgeted training. This section reviews relevant works that aim to improve the computational efficiency of training DNNs. \n\nCurriculum learning (CL) aims to improve model performance by ordering the samples from easy to difficult [7,13,21,56]. Like importance sampling approaches, CL leverages different samples at different stages of training. However, while CL prioritizes easy samples at the beginning of training and includes all of them at the end, importance sampling prioritizes the most difficult subset of samples at each stage of the training. The main drawback of CL is that, in most cases, the order of the samples (curriculum) has to be defined before training, which is already a costly task that requires manually assessing the sample difficulty or transferring knowledge from a pre-trained model. Some approaches remedy this with a simple curriculum [36] or by learning it during training [27]; these methods, however, do not aim to speed up training by ordering the samples, but to improve convergence by weighting the sample contribution to the loss. \n\nCore-set selection approaches aim to find the subset of samples that is most useful [14,40,53] and maintain accuracy despite training on a fraction of the data. The ability of these methods to reduce training cost relies on using smaller training sets, but the benefit is limited since they require a pre-training stage with the full dataset. They do, however, demonstrate that DNNs can achieve peak performance with a fraction of the full dataset.",
            "score": 0.39863958233642205,
            "section_title": "Related work",
            "char_start_offset": 4433,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 53,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1662
                },
                {
                    "start": 1665,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2113
                }
            ],
            "ref_mentions": [
                {
                    "start": 47,
                    "end": 51,
                    "matchedPaperCorpusId": "152282636"
                },
                {
                    "start": 354,
                    "end": 357,
                    "matchedPaperCorpusId": "182953134"
                },
                {
                    "start": 360,
                    "end": 363,
                    "matchedPaperCorpusId": "148571749"
                },
                {
                    "start": 437,
                    "end": 440,
                    "matchedPaperCorpusId": "220713381"
                },
                {
                    "start": 440,
                    "end": 443,
                    "matchedPaperCorpusId": "102351185"
                },
                {
                    "start": 443,
                    "end": 446,
                    "matchedPaperCorpusId": "214743276"
                },
                {
                    "start": 824,
                    "end": 827,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 827,
                    "end": 830,
                    "matchedPaperCorpusId": "198184759"
                },
                {
                    "start": 830,
                    "end": 833,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 833,
                    "end": 836,
                    "matchedPaperCorpusId": "5004002"
                },
                {
                    "start": 1460,
                    "end": 1464,
                    "matchedPaperCorpusId": "47252984"
                },
                {
                    "start": 1499,
                    "end": 1503,
                    "matchedPaperCorpusId": "51876228"
                },
                {
                    "start": 1749,
                    "end": 1753,
                    "matchedPaperCorpusId": "195750622"
                },
                {
                    "start": 1753,
                    "end": 1756,
                    "matchedPaperCorpusId": "211259075"
                },
                {
                    "start": 1756,
                    "end": 1759,
                    "matchedPaperCorpusId": "55481903"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2958984375
        },
        {
            "corpus_id": "258547260",
            "title": "The Role of Data Curation in Image Captioning",
            "text": "Inspired by scheduling in curriculum learning (Bengio et al., 2009;Kumar et al., 2010), we assume that difficult training samples can be automatically identified throughout the training process. We propose to use the captioning model M that is being trained on dataset D to automatically identify such samples. We can readily use this model to calculate the loss of each sample in D at any point in time, such as at the end of each epoch t: \n\nThe samples can be be ranked by their respective losses, providing candidates for samples that may benefit from data curation. In particular, the highest loss samples are targets for our data curation methods. We focus on samples with losses that are either two standard deviations from the mean, or the top X% highest loss samples. The data curation performs dynamic updates to the training dataset \n\nIn this way, the training dataset is dynamically updated at the end of each epoch according to the model's current captioning capability at time t. We empirically observe that without data curation, the high-loss samples remain high-loss during five epochs of training.2",
            "score": 0.39857759446045876,
            "section_title": "Identifying the difficult samples",
            "char_start_offset": 5742,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 440
                },
                {
                    "start": 443,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 842
                },
                {
                    "start": 845,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1115
                }
            ],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 67,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.337158203125
        },
        {
            "corpus_id": "276249966",
            "title": "FRAME: Boosting LLMs with A Four-Quadrant Multi-Stage Pretraining Strategy",
            "text": "In this study, we simply attempt to split the data into two parts based on the size of PPL and PD. Future research can be more detailed, such as subdividing a dimension into three parts or more, to explore more training stages. In addition, we plan to expand the scope of work to verify the applicability of the two-stage pretraining method in more language model architectures, such as Mamba (Gu and Dao, 2023), and Mixture of Experts (Wang et al., 2024). Although our evaluation criteria are already quite comprehensive, it is still possible to extend to a wider range of evaluations, including more detailed domain-specific or interactive tasks, such as the evaluation of analogy reasoning ability (Hu et al., 2023). Despite these potential limitations, we firmly believe that our research provides valuable insights and practical contributions to the academic community. (Shoeybi et al., 2019). Furthermore, we validate our approach on a 1.3B model, using the same settings as the 3B model. We use Ascend 910B NPU for training. For the 3B model, we use 512 NPUs for training, each model takes over 180 hours; For the 1.3B models, we use 96 NPUs, each model takes 6 hours. \n\nIn Table 5 and 6, we present the model configuration of the 1.3B and 3B models. \n\nBaselines In the 3B model setting, we compare FRAME with several baselines: \n\n\u2022 Random: Data is organized in a completely random order, meaning there is no specific sequence or strategy for data input \n\n\u2022 PDPC (Zhang et al., 2025): Utilizes PD as a model-aware curriculum learning indicator, organizing the data sequence through a preference function that aligns with the model's inherent preferences. \n\nAdopts the same smoothing process as FRAME but prioritizes satisfying PD-related constraints to organize the data order. \n\nIn the 1.3B model setting, we also compare FRAME with other baselines: \n\n\u2022 Sequential: Organizes data by directly sorting according to PPL, PD, and QuRating (Wettig et al., 2024), in either ascending or descending order.",
            "score": 0.39857759446045876,
            "section_title": "Limitations and Future Works",
            "char_start_offset": 21383,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1175
                },
                {
                    "start": 1178,
                    "end": 1257
                },
                {
                    "start": 1260,
                    "end": 1335
                },
                {
                    "start": 1338,
                    "end": 1460
                },
                {
                    "start": 1463,
                    "end": 1661
                },
                {
                    "start": 1664,
                    "end": 1784
                },
                {
                    "start": 1787,
                    "end": 1857
                },
                {
                    "start": 1860,
                    "end": 2007
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.294921875
        },
        {
            "corpus_id": "269756933",
            "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
            "text": "By focusing on the complexity of training data and adopting a gradual learning, curriculum learning could lead to the better performance without the necessity for constant increases in model size.\n\nTherefore, we focuses on data-centric strategy training method by curriculumn learning, by examining the impact of performance of model, comparing random shuffled training dataset.\n\nThis study focuses on a data-centric strategy to enhance model performance through the adoption of curriculum learning.By examining the impact of structured training data, this study aims to demonstrate its effectiveness relative to the traditional method of random shuffling.We seek to establish that curriculum learning is a crucial strategy for improving the development process of large language models, potentially achieving better performance without increasing the model parameters or the size of the dataset.",
            "score": 0.39857759446045876,
            "section_title": "Introduction",
            "char_start_offset": 2319,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 198,
                    "end": 378
                },
                {
                    "start": 380,
                    "end": 499
                },
                {
                    "start": 499,
                    "end": 656
                },
                {
                    "start": 656,
                    "end": 896
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6650390625
        },
        {
            "corpus_id": "257220027",
            "title": "DCLP: Neural Architecture Predictor with Curriculum Contrastive Learning",
            "text": "where \n\nwhere t denotes steps (t \u2208 {1, 2, ...T }), g is tanh, \u03c3 = 0.9, and n > 1, k > 1 controls the frequency and range of difficulty decrease respectively, the decreasing will be discussed in the next paragraph. \u03c4 t determines the preference for easy/hard data. For \u03c4 t < 0, larger difficulty corresponds to a smaller selection probability. Conversely, when \u03c4 t > 0, the curriculum favors hard data. Our strategy gradually adjusts \u03c4 t to achieve robust learning. Initially, we set a negative \u03c4 1 to emphasize learning from easy data at the start without being affected by noise and to quickly reach a relatively good ability. Then we gradually increase \u03c4 t to encourage deeper learning from hard data. Finally, we use a positive \u03c4 T , smoothly transitioning from easy to hard data. However, unlike the conventional easy-to-hard curriculum, we introduce a fluctuating term that causes the curriculum to fluctuate in difficulty controlled by n and k. This nonmonotonic increase in difficulty includes intervals of decreasing difficulty. Therefore, the model can revisit simpler data periodically during training. It is necessary since the harder positive data includes more noise, i.e., negative samples that are mistakenly labeled as positive. Training with increasing difficulty often causes the model to focus excessively on the noise, resulting in loss of learned knowledge in the correct data. Intermittent training with simple data can counteract the learned misinformation from the noise, improving training stability, and being more robust. \n\nThe curriculum facilitates the encoder to acquire a stronger ability by increasing data difficulty. Additionally, incorporating easy data prevents overfitting that may occur as difficulty increases, leading to efficient convergence. \n\nAs shown in Algorithm 1, we use the difficulty function to calculate the difficulty of each positive data and sort them according to the scheduler (Line 10). Next, we feed the positive samples into the contrastive learner for training.",
            "score": 0.3981565593954267,
            "section_title": "Curriculum Learning",
            "char_start_offset": 14473,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 5
                },
                {
                    "start": 8,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1548
                },
                {
                    "start": 1551,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1783
                },
                {
                    "start": 1786,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2021
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.179443359375
        },
        {
            "corpus_id": "220249851",
            "title": "Uncertainty-aware Self-training for Text Classification with Few Labels",
            "text": "This may result in gradual drifts from self-training on noisy pseudo-labeled instances [Zhang et al., 2017]. Sample selection leveraging teacher confidence has been studied in curriculum learning [Bengio et al., 2009] and self-paced learning [Kumar et al., 2010] frameworks. These works leverage easiness of the samples to inform a learning schedule like training on easy concepts first followed by complex ones. Since it is hard to assess the \"easiness\" of a sample, especially in deep neural network based architectures, these works rely only on the teacher model loss while ignoring its uncertainties that can be leveraged for sample selection. \n\nIntuitively, if the teacher model already predicts some samples with high confidence, then there is little to gain with self-training if we focus only on these samples. On the other hand, hard examples for which the teacher model has less confidence are hard to rely on for self-training as these could be noisy or too difficult to learn from. In this scenario, the model could benefit from judiciously selecting examples for which the teacher model is uncertain about. However, it is non-trivial to generate uncertainty estimates for non-probabilistic models like deep neural networks. To this end, we leverage recent advances in Bayesian deep learning [Gal and Ghahramani, 2016] to obtain uncertainty estimates of the teacher for pseudo-labeling and improving the self-training process. \n\nOur task and framework overview. We focus on leveraging pre-trained language models for classification with few labeled samples (e.g., K = {20, 30}) per class for training and validation, and large amounts of task-specific unlabeled data. Figure 1(a) shows an overview of the self-training framework for NLU tasks, where augmented data is obtained from hard pseudo-labels from the teacher (e.g., BERT [Devlin et al., 2019]) without accounting for its uncertainty. We extend this framework with three core components in Figure 1(b), namely: (i) Masked model dropout for uncertainty estimation: We adopt MC dropout [Gal and Ghahramani, 2016] as a technique to obtain uncertainty estimates from the pre-trained language model.",
            "score": 0.3980118265990449,
            "section_title": "Introduction",
            "char_start_offset": 2145,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 647
                },
                {
                    "start": 650,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1438
                },
                {
                    "start": 1441,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2164
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 107,
                    "matchedPaperCorpusId": "6212000"
                },
                {
                    "start": 196,
                    "end": 217,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 242,
                    "end": 262,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 1304,
                    "end": 1330,
                    "matchedPaperCorpusId": "160705"
                },
                {
                    "start": 1842,
                    "end": 1863,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63720703125
        },
        {
            "corpus_id": "17896684",
            "title": "Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks",
            "text": "Inspired by the human learning process, Curriculum Learning (Elman, 1993;Bengio et al., 2009) is an algorithm that emphasizes the order of training instances in a computational learning setup. The main idea is that learning easy instances first could be helpful for learning more complex ones later in the training. The first algorithm proposed by Bengio et al. (2009), which we refer as one-pass curriculum, creates disjoint sets of training examples ordered by the complexity and used separately during training. The second algorithm called baby step curriculum uses an incremental approach where groups of more complex examples are incrementally added to the training set (Spitkovsky, Alshawi, and Jurafsky, 2010). These curriculum learning regimens were shown to improve performance in some Natural Language Processing and Computer Vision tasks (Pentina, Sharmanska, and Lampert, 2015;Spitkovsky, Alshawi, and Jurafsky, 2010). \n\nDespite its usefulness, it is still unknown how exactly computational models are affected internally by curriculum learning. An example of computational model particu-larly relevant to Natural Language Processing is Long Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997). LSTM networks have shown competitive performance in several domains such as handwriting recognition (Graves et al., 2009) and parsing (Vinyals et al., 2015). Surprisingly, curriculum learning has not been studied in the context of LSTM networks to our knowledge. Detailed visualizations and analyses of curriculum learning regimens with LSTM will allow us to better understand how models are affected and provides us insights when to use these regimens. Knowing how curriculum learning works, we can design new extensions and understand the nature of tasks most suited for these learning regimens. \n\nIn this paper, we study the effect of curriculum learning on LSTM networks. We created experiments to directly compare two curriculum learning regimens, one-pass and baby step, with two baseline approaches that include the conventional technique of randomly ordering the training samples. We use two benchmarks for our analyses. First, a synthetic task is designed which is similar to several Natural Language Processing tasks where a sequence of symbols are observed and a particular function (e.g.",
            "score": 0.3979610516133899,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 930
                },
                {
                    "start": 933,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1820
                },
                {
                    "start": 1823,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2111
                },
                {
                    "start": 2112,
                    "end": 2151
                },
                {
                    "start": 2152,
                    "end": 2322
                }
            ],
            "ref_mentions": [
                {
                    "start": 60,
                    "end": 73,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 73,
                    "end": 93,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 348,
                    "end": 368,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 675,
                    "end": 716,
                    "matchedPaperCorpusId": "1363892"
                },
                {
                    "start": 849,
                    "end": 889,
                    "matchedPaperCorpusId": "8502955"
                },
                {
                    "start": 889,
                    "end": 929,
                    "matchedPaperCorpusId": "1363892"
                },
                {
                    "start": 1187,
                    "end": 1221,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1323,
                    "end": 1344,
                    "matchedPaperCorpusId": "14635907"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.407470703125
        },
        {
            "corpus_id": "256671675",
            "title": "Reward estimation with scheduled knowledge distillation for dialogue policy learning",
            "text": "Curriculum learning (Bengio et al., 2009) (CL) is a strategy that trains a machine learning model by learning from easy samples and gradually increasing the complexity. It normally consists of two components, i.e. a difficulty measurer (DM) that evaluates the complexity of each data and a training scheduler (TS), which selects data with desired complexity degree for the model to train on. Generally, all training data is sorted by the DM and passed to the TS, which samples relatively simple examples and sends them to the model for training. The TS also decides when to sample harder examples as the training progresses until the whole dataset is included for training. A plenty of researches have exploited its power in a wide range of application scopes, such as supervised learning tasks (El-Bouri et al., 2020;Guo et al., 2018;Platanios et al., 2019), reinforcement learning tasks (Narvekar & Stone, 2019;Qu et al., 2018) and graph learning tasks (Chu et al., 2021;Dong et al., 2021;Wang, Wang et al., 2021). Recent works employed curriculum learning to arrange reasonable schedule for efficient DPL (Liu et al., 2021;Zhao, Huang et al., 2021;Zhu et al., 2021). In this paper, we incorporate a special kind of curriculum learning, self-paced learning, into distillation process to train the student reward estimator more stable and efficiently.",
            "score": 0.3974449898958604,
            "section_title": "Curriculum learning",
            "char_start_offset": 13514,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1352
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 41,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 795,
                    "end": 818,
                    "matchedPaperCorpusId": "220301658"
                },
                {
                    "start": 818,
                    "end": 835,
                    "matchedPaperCorpusId": "51920640"
                },
                {
                    "start": 835,
                    "end": 858,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 889,
                    "end": 913,
                    "matchedPaperCorpusId": "54442967"
                },
                {
                    "start": 913,
                    "end": 929,
                    "matchedPaperCorpusId": "11009476"
                },
                {
                    "start": 955,
                    "end": 973,
                    "matchedPaperCorpusId": "235671564"
                },
                {
                    "start": 973,
                    "end": 991,
                    "matchedPaperCorpusId": "236924384"
                },
                {
                    "start": 1108,
                    "end": 1126,
                    "matchedPaperCorpusId": "236478347"
                },
                {
                    "start": 1151,
                    "end": 1168,
                    "matchedPaperCorpusId": "248112683"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21484375
        },
        {
            "corpus_id": "236460308",
            "title": "Trigger is Not Sufficient: Exploiting Frame-aware Knowledge for Implicit Event Argument Extraction",
            "text": "In this subsection, we view the disparity between the training and test stage from the perspective of learning complexity and introduce our curriculum distillation strategy. Clues in the form of related arguments and their roles are explicitly given for the teacher model to promote reasoning. While for the student model (the inference stage), there are no golden-standard clues, making it challenging for the model to extract the expected argument by relying on associated ones. Intuitively, the training process of the student model is harder than that of the teacher. \n\nInspired by the curriculum theory that a machine learning model could be trained better by feeding data following the easier to harder order, we introduce a curriculum distillation strategy to promote the learning of student model. We utilize the proportion of given arguments to measure the complexity of the learning task and data points in IEAE task. As in Figure 2 (c), at the beginning of the distillation stage, we utilize questions containing oracle knowledge with all related arguments to train the student as a warm-up procedure. Then we gradually reduce the proportion of given arguments and finally transit to using no extra arguments as in a realistic situation. Note that all teacher models utilize oracle knowledge as they are trained throughout the whole process. \n\nDetails of the curriculum distillation strategy are shown in Algorithm 1. I ALL and I are two sets of training instances with all golden-standard arguments and no extra knowledge are used to build questions, respectively. {M T k } 4 k=1 are four wellinformed teacher models trained with diverse templates that capture different categories of oracle knowledge. M S is the student model. For each training step, firstly, we sample a batch of instances following Bernoulli distribution and the probability of selecting an example from the I ALL is a%. Secondly, we cache the hidden state, start and end distribution of the four teachers with I All as input. \n\nFinally, we utilize all cached status from teacher models to simultaneously distill knowledge to student network. As the training stage progresses, the value of a gradually decreases from 100 to 0, leading to the learning difficulty of batches of data from easier to harder. Note that we evaluate the performance of M S using data without extra arguments in questions.",
            "score": 0.397374887417059,
            "section_title": "Curriculum Distillation",
            "char_start_offset": 15636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 571
                },
                {
                    "start": 574,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1352
                },
                {
                    "start": 1355,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 2009
                },
                {
                    "start": 2012,
                    "end": 2125
                },
                {
                    "start": 2126,
                    "end": 2286
                },
                {
                    "start": 2287,
                    "end": 2380
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1165771484375
        },
        {
            "corpus_id": "231709290",
            "title": "Curriculum Learning: A Survey",
            "text": "the best features and weights that maximize performance on the chosen NLP task. Cirik et al. (2016) analyze the effect of curriculum learning on training Long Short-Term Memory (LSTM) networks. For that, they employ two curriculum strategies and two base-lines. The first curriculum approach uses an easy-then-hard methodology, while the second one is a batching method which gradually enhances the training set with more difficult samples. As baselines, the authors choose conventional training based on random data shuffling and an option where, at each epoch, all samples are presented to the network, ordered from easy to hard. Furthermore, the authors analyze CL with regard to the model complexity and available resources. Graves et al. (2017) tackle the problem of automatically determining the path of a neural network through a curriculum to maximize learning efficiency. They test two related setups. In the multiple tasks setup, the challenge is to achieve high results on all tasks, while in the target task setup, the goal is to maximize the performance on the final task. The authors model the curriculum over n tasks as an n-armed bandit, and a syllabus as an adaptive policy seeking to maximize the rewards from the bandit. Subramanian et al. (2017) employ adversarial architectures to generate natural language. They define the curriculum learning paradigm by constraining the generator to produce sequences of gradually increasing lengths as training progresses. Their results show that the curriculum ordering is essential when generating long sequences with an LSTM.\n\nHuang and Du (2019) introduce a collaborative curriculum learning framework to reduce the impact of mislabeled data in distantly supervised relation extraction. In the first step, they employ an internal self-attention mechanism between the convolution operations which can enhance the quality of sentence representations obtained from the noisy inputs. Next, a curriculum methodology is applied to two sentence selection models. These models behave as relation extractors, and collaboratively learn and regularize each other. This mimics the learning behavior of two students that compare their different answers. The learning is guided by a curriculum that is generated taking into consideration the conflicts between the two networks or the value of the loss function. Tay et al. (2019) propose a generative curriculum pretraining method for solving the problem of reading comprehension over long narratives. They use an easy-",
            "score": 0.3968161361305853,
            "section_title": "Question answering",
            "char_start_offset": 93657,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 729,
                    "end": 749,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 1240,
                    "end": 1265,
                    "matchedPaperCorpusId": "397556"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42041015625
        },
        {
            "corpus_id": "260203057",
            "title": "Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models",
            "text": "Data attribution Another perspective on understanding training data is data attribution, which seeks to identify what data is responsible for particular model behaviors. Influence functions [28] and shapley values [16] are two ways to quantify the role of individual samples. Datamodels [23] fit a model to predict behavior given a subset of training data, providing a framework for understanding individual samples as well as dataset counterfactuals. Simfluence [20] fits a Markov process to a set of training trajectories for finer-grained understanding of how data impacts training. We focus on understanding how groups of data associated with skills elicit broader model capabilities, and utilize this understanding to select data for more efficient training. \n\nCurriculum learning Curriculum learning [3] proposes to show the model data in order from easy samples to hard ones. Various criteria have been used to determine hardness, and anticurriculum as well as various pacing functions and mixing rates have been explored [54]. Curriculum learning can also be performed at the group level [60]. More sophisticated approaches include parametrizing each sample with a dynamic importance [52], and also accounting for irrelevant and noisy data [38]. Our approach similarly utilizes a curriculum, but it is defined over a skills graph and does not necessarily align with training on easiest to hardest skills. \n\nHow LMs learn Many different explanations for how LMs learn from data have been proposed. One hypothesis is that there exist discrete, universal building blocks of LM knowledge called quanta, and power law scaling emerges from a learning over a particular distribution of quanta in the right order [37]. Another is that chain of thought reasoning emerges due to local clusters of latent variables that influence each other, which can be validated by studying the LM's ability to do conditional inference given intermediate variables [48]. Others have provided theoretical analysis of how transformers learn topics by studying co-occurrences of words in the training data [34]. Empirically, how models learn is still a mystery-for instance, models trained on code are found to perform fairly well at commensense reasoning [36]. Our work initiates a study on how LMs learn various skills and how to exploit this for better data selection.",
            "score": 0.3966155884751822,
            "section_title": "Related work",
            "char_start_offset": 34443,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 763
                },
                {
                    "start": 766,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1412
                },
                {
                    "start": 1415,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2091
                },
                {
                    "start": 2092,
                    "end": 2241
                },
                {
                    "start": 2242,
                    "end": 2351
                }
            ],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 218,
                    "matchedPaperCorpusId": "102350503"
                },
                {
                    "start": 806,
                    "end": 809,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1029,
                    "end": 1033,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 1192,
                    "end": 1196,
                    "matchedPaperCorpusId": "209438234"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84619140625
        },
        {
            "corpus_id": "258960497",
            "title": "BigTranslate: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages",
            "text": "Prior to data sampling, we employ multilingual vocabulary to segment the entire multilingual parallel corpus. Subsequently, we construct training samples by concatenating the same language sentence pairs. Each sample is comprised of multiple parallel sentence pairs and has a fixed length of 1,024 tokens. This approach ensured the formation of coherent and consistent training samples for subsequent model training. \n\nTo mitigate the issue of the model disproportionately focusing on learning high-resource corpus during the training phase, which could potentially hinder the learning of low-resource languages, we draw inspiration from curriculum learning (Bengio et al., 2009) to propose an incremental approach for multilingual LLMs pre-training. \n\nIn this incremental pre-training method, we gradually expose the model to language pairs in a curriculum-like manner. Initially, the model is exposed to high-resource language pairs, allowing it to establish a solid foundation in those languages. Subsequently, we progressively introduce low-resource language pairs, enabling the model to gradually expand its knowledge and proficiency in these languages. \n\nSpecifically, we follow a three-step approach in our incremental pre-training method, as shown in Figure 2. Firstly, we set the sample interval size and divide language pairs into distinct intervals based on the number of instances for each language pair. Secondly, we calculate the sample mean for all language pairs in each interval. Thirdly, we dynamically measure the moment of adding the language-pair samples next interval according to the sample means in the previous sample interval. In the following part, we detail the three steps. \n\nIn the first step, we set sample interval size, denoted as S. We hypothesize that if the interval size S is below a specific threshold, we can consider that all language pairs in this interval have average resources. Consequently, training the multilingual model on this interval enables the model to acquire the average proficiency in each language. \n\nIn our preliminary experiments, it is observed that an excessively large language interval results in an unbalanced learning distribution among language pairs within the interval. Conversely, setting the interval too small leads to the model focusing on only a few or even just one language within the interval. Additionally, we noticed that highresource and low-resource languages exhibited different sensitivities to interval size.",
            "score": 0.39648866471920274,
            "section_title": "Incremental Multilingual Pre-training",
            "char_start_offset": 13467,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 416
                },
                {
                    "start": 419,
                    "end": 750
                },
                {
                    "start": 753,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1158
                },
                {
                    "start": 1161,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2055
                },
                {
                    "start": 2058,
                    "end": 2237
                },
                {
                    "start": 2238,
                    "end": 2369
                },
                {
                    "start": 2370,
                    "end": 2491
                }
            ],
            "ref_mentions": [
                {
                    "start": 658,
                    "end": 679,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51611328125
        },
        {
            "corpus_id": "44130298",
            "title": "Self-Training for Jointly Learning to Ask and Answer Questions",
            "text": "Curriculum learning is inspired from the incremental nature of human learning and orders training samples on the easiness scale so that easy samples can be introduced to the learning algorithm first and harder samples can be introduced successively. We show that introducing questions in increasing order of hardness leads to improvements over a baseline that introduces questions randomly. \n\nWe use a seq2seq model with soft attention (Sutskever et al., 2014;Bahdanau et al., 2014) for QG and a neural model inspired from Attentive Reader (Hermann et al., 2015;Chen et al., 2016) for QA. However, these can be any QA and QG models. We evaluate our approach on four datasets: SQUAD, MS MARCO, WikiQA and TrecQA. We use a corpus of English Wikipedia as unlabeled text. Our experiments show that the self-training approach leads to significant improvements over a number of established approaches in QA and QG on these benchmarks. On the two answer sentence selection QA tasks: (Wik-iQA and TrecQA), we obtain state-of-the-art.",
            "score": 0.3961783335707892,
            "section_title": "Introduction",
            "char_start_offset": 2067,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 390
                },
                {
                    "start": 393,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1025
                }
            ],
            "ref_mentions": [
                {
                    "start": 436,
                    "end": 460,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 540,
                    "end": 562,
                    "matchedPaperCorpusId": "6203757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.262939453125
        },
        {
            "corpus_id": "221995570",
            "title": "Learn like a Pathologist: Curriculum Learning by Annotator Agreement for Histopathology Image Classification",
            "text": "Curriculum learning. One of the earliest works demonstrating the benefit of curriculum learning [1] posits that learning occurs better when examples are not randomly presented but instead organized in a meaningful order that gradually shows more concepts and complexity. Although the intuition behind this approach seems obvious in the context of human and animal learning, it is often unclear how to best apply this strategy for training neural networks. As such, a diverse set of approaches has been explored in this area of research. These approaches generally first score examples by difficulty and then train models using a schedule based on example difficulty, where easier examples are typically seen first and harder examples are seen later. For instance, Bengio et al.'s original work [1] explored a noising-based curriculum for shape detection and a vocabulary-size based task for language modeling. As popular recent examples, Weinshall et al. [10] use the confidence of a pre-trained classifier as an estimator for difficulty; Korbar et al. [11] use a schedule with self-defined easy and hard examples for learning of audio-visual temporal synchronization; Ganesh and Corso [12] propose to incrementally learn labels instead of learning difficult examples; and various teacher-student frameworks have been proposed in the context of curriculum learning [13,14]. \n\nChallenges of curriculum learning. Despite the appeal of teaching machines to learn like humans, curriculum learn-ing has been seen by some [10] as mostly remaining in the fringes of machine learning research. Based on the strategies of prior work, we broadly see two central challenges that arise when applying curriculum learning. \n\nFirst, curriculum learning assumes that a range of easy and hard examples exists. Although it could be argued that this is a true statement for any given dataset for at least some definition of easy and hard, the distribution of example difficulties likely varies based on the nature of the task and the dataset. Since the added value of curriculum learning comes from utilizing the varying degrees of difficulty in a task, tasks with a smaller range of example difficulty are less conducive to effective curriculum learning.",
            "score": 0.3952899170800264,
            "section_title": "Curriculum Learning Intuitions",
            "char_start_offset": 2918,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1708
                },
                {
                    "start": 1711,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2236
                }
            ],
            "ref_mentions": [
                {
                    "start": 96,
                    "end": 99,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 794,
                    "end": 797,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 955,
                    "end": 959,
                    "matchedPaperCorpusId": "5004002"
                },
                {
                    "start": 1053,
                    "end": 1057,
                    "matchedPaperCorpusId": "53280782"
                },
                {
                    "start": 1365,
                    "end": 1369,
                    "matchedPaperCorpusId": "213207497"
                },
                {
                    "start": 1369,
                    "end": 1372,
                    "matchedPaperCorpusId": "52284093"
                },
                {
                    "start": 1516,
                    "end": 1520,
                    "matchedPaperCorpusId": "5004002"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1876220703125
        },
        {
            "corpus_id": "267412105",
            "title": "Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases",
            "text": "The utilization of synthetic data produced by LLMs has demonstrated remarkable effectiveness across a wide range of tasks, including code generation [25,36], mathematical reasoning [38,19], text embedding [33] and text2image [3]. The advantage of using synthetic data is not only in cost savings; it can also be beneficial for low-resource tasks or imbalanced data distributions [21]. Following these works, our system generates high-quality evenly distributed synthetic boundary samples, that result in a more efficient optimization process and higher-quality results. \n\nSynthetic data was also proven to be an effective method to distil knowledge from black-box LLMs, by training on synthetic data that was generated by those models [31,22,12]. However, in these works the generated data was used to fully train the student model. In contrast, our work demonstrates the effectiveness of synthetic data to distil knowledge between two black-box models via automatic prompt engineering. \n\nCurriculum Learning. Arranging the data samples for training machine learning models in a meaningful way, starting from easier samples and progressing to more challenging ones, can yield performance enhancements compared to the conventional method of training based on random data shuffling. This approach is known as curriculum learning [30,2]. Curriculum Learning has been proven to be effective in various fields such as object localization [13,28], object detection [7,26] and NLP [14,23]. Inspired by these ideas, in [8] they propose to fine-tune LLMs by iteratively generating synthetic data and refining the policy to distinguish between the synthetic data and the human-annotated data. In our work, we use a similar approach, where the system iteratively generates more challenging cases that resolve the previous prompt ambiguity in order to more efficiently tune to the user intent.",
            "score": 0.39516560244557747,
            "section_title": "GT prompt",
            "char_start_offset": 20252,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1881
                }
            ],
            "ref_mentions": [
                {
                    "start": 1327,
                    "end": 1331,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 1331,
                    "end": 1333,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1433,
                    "end": 1437,
                    "matchedPaperCorpusId": "879067"
                },
                {
                    "start": 1437,
                    "end": 1440,
                    "matchedPaperCorpusId": "6954583"
                },
                {
                    "start": 1459,
                    "end": 1462,
                    "matchedPaperCorpusId": "5658192"
                },
                {
                    "start": 1462,
                    "end": 1465,
                    "matchedPaperCorpusId": "3405508"
                },
                {
                    "start": 1474,
                    "end": 1478,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 1478,
                    "end": 1481,
                    "matchedPaperCorpusId": "85498775"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1866455078125
        },
        {
            "corpus_id": "220045816",
            "title": "Curriculum Learning for Natural Language Understanding",
            "text": "In this section we describe our method to arrange the training examples D into a learning curriculum according to their difficulty scores C. We design our curriculum in a multi-stage setting {S i : i = 1, 2, . . . , N }. Within each stage S i , the examples are still shuffled to keep local stochastics, and examples from different stages do not overlap in order to prevent overfitting. \n\nThe sampling algorithm is built upon such principle: \n\nThe proportion of difficult examples in each stage should start with 0, and gradually increase until it reachs how much it accounts for in the original dataset distribution. \n\nWe first sort all examples by their difficulty score C, and divide them into N buckets: {C i : i = 1, 2, . . . , N }, so the examples are now collected into N different levels of difficulty, ranging from C 1 (the easiest) to C N (the hardest), with the proportion distribution as: \n\nFor tasks with discrete metrics, such distribution is naturally formed by the difficulty score hierarchy, and directly reflects the intrinsic difficulty distribution of the dataset. For other tasks, we manually divide C uniformly1 . Based on these buckets, we construct the learning curriculum one stage after another. For each learning stage S i , we sample examples from all antecedent buckets {C j : j = 1, 2, . . . , i} by the following proportion: \n\nand the final curriculum {S i : i = 1, 2, . . . , N } is formed as such. We refer to the arrangement algorithm as Annealing method for it provides a soft transition through multi learning stages. \n\nAt each stage, the model is trained for one epoch. When the training reached S N , the model should be ready for the original distribution in trainset D, so we finally add another stage S N +1 which covers the entire trainset, and the model is trained on it until converges.",
            "score": 0.3946961894953656,
            "section_title": "Curriculum Arrangement",
            "char_start_offset": 12090,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 386
                },
                {
                    "start": 389,
                    "end": 441
                },
                {
                    "start": 444,
                    "end": 617
                },
                {
                    "start": 620,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1355
                },
                {
                    "start": 1358,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1553
                },
                {
                    "start": 1556,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1830
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.325927734375
        },
        {
            "corpus_id": "267094971",
            "title": "DsDm: Model-Aware Dataset Selection with Datamodels",
            "text": "While we can calculate the performance of a specific training set by training a model on that set (and then evaluating), it is (generally) unclear how to calculate the best possible training subset without examining every possible subset one by one, a computationally infeasible procedure. \n\nWe instead approximate the optimal subset by (approximately) modeling how the learning algorithm actually uses training data to predict. Specifically, in Section 2, we model target task performance as a function of training subset using datamodels (which efficiently approximate the mapping between training subset and model performance (Ilyas et al., 2022)), and select the subset that maximizes our estimate. Then, in Section 3, we demonstrate that our resulting method, dataset selection with datamodels (DsDm), consistently improves language model performance on diverse target tasks (e.g., SQuAD (Rajpurkar et al., 2016) and LAMBADA (Paperno et al., 2016)), even when existing selection methods do not. \n\nDsDm-selected data can improve performance on pre-specified tasks. However, in practice we train large-scale models to generalize to yet unseen tasks. Our framework suggests a principled approach to selecting data in this scenario too: choose target tasks similar to those we expect at deployment time, then select the optimal dataset subset for these target tasks. Following this strategy, in Section 4, we choose target tasks that cover a range of natural language problem categories (SQuAD, Jeopardy (MosaicML, 2023), and LAMBADA), and select data from C4, a canonical web crawl (Raffel et al., 2020). Our selections deliver a 2\u00d7 compute multiplier on a diverse set of test benchmarks: DsDm-selected datasets yield LMs that perform as well as those trained with 2\u00d7 the compute budget on randomly selected data (we train up to 1.8B parameter models). In contrast, no baseline method outperforms randomly selecting data-even at the same compute budget.",
            "score": 0.3946961894953656,
            "section_title": "Introduction",
            "char_start_offset": 2008,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 289
                },
                {
                    "start": 292,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 999
                },
                {
                    "start": 1002,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1955
                }
            ],
            "ref_mentions": [
                {
                    "start": 1584,
                    "end": 1605,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91650390625
        },
        {
            "corpus_id": "257279830",
            "title": "Distillation from Heterogeneous Models for Top-K Recommendation",
            "text": "Inspired by the learning process of humans, easy-to-hard learning has been extensively studied in various fields of machine learning [2,6,13,21,27,29,43,46]. It has been widely used when direct optimization of a non-convex objective function may converge to poor local minima and has been proven to play an important role in achieving a better generalization [2]. Curriculum learning [2,43] trains a model by gradually including data samples in ascending order of difficulty defined by prior knowledge. On the other hand, self-paced learning [21] makes the curriculum dynamically adjusted during the training, usually based on training loss [21] or performance on the validation set [6,49]. The easy-tohard learning has been applied to KD to improve the distillation efficacy in computer vision [14,39] and natural language processing [52,55]. [3,14,39] exploit the teacher's optimization route to form a curriculum for the student, [52] gradually includes an increasing number of fine-grained document pairs during the training. \n\nRemarks. The existing KD methods for RS focus on distillation from a homogeneous teacher that has the same model type to the student model. Distillation from heterogeneous teachers, which have distinct architectures and learning objectives to the student model, has not been studied well. In this work, we show the necessity and difficulty of distilling the ensemble of heterogeneous teachers and apply the easy-to-hard learning to cope with the problem. Further, the prior KD works with the easy-to-hard learning focus on classification [14,31,39] or rely on domain-specific features [55], which makes them hard to apply to the ranking problem and recommender system. Our work provides a solution tailored to compress ranking models by distilling an easy-to-hard sequence of ranking knowledge considering the student's learning state.",
            "score": 0.39469476978561047,
            "section_title": "RELATED WORK",
            "char_start_offset": 8047,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1029
                },
                {
                    "start": 1032,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1867
                }
            ],
            "ref_mentions": [
                {
                    "start": 133,
                    "end": 136,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 136,
                    "end": 138,
                    "matchedPaperCorpusId": "236980096"
                },
                {
                    "start": 138,
                    "end": 141,
                    "matchedPaperCorpusId": "10891229"
                },
                {
                    "start": 144,
                    "end": 147,
                    "matchedPaperCorpusId": "13030414"
                },
                {
                    "start": 147,
                    "end": 150,
                    "matchedPaperCorpusId": "216641819"
                },
                {
                    "start": 150,
                    "end": 153,
                    "matchedPaperCorpusId": "232362223"
                },
                {
                    "start": 153,
                    "end": 156,
                    "matchedPaperCorpusId": "227343966"
                },
                {
                    "start": 359,
                    "end": 362,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 384,
                    "end": 387,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 387,
                    "end": 390,
                    "matchedPaperCorpusId": "232362223"
                },
                {
                    "start": 683,
                    "end": 686,
                    "matchedPaperCorpusId": "236980096"
                },
                {
                    "start": 686,
                    "end": 689,
                    "matchedPaperCorpusId": "209862398"
                },
                {
                    "start": 795,
                    "end": 799,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 799,
                    "end": 802,
                    "matchedPaperCorpusId": "236134146"
                },
                {
                    "start": 835,
                    "end": 839,
                    "matchedPaperCorpusId": "248426770"
                },
                {
                    "start": 839,
                    "end": 842,
                    "matchedPaperCorpusId": "244119160"
                },
                {
                    "start": 844,
                    "end": 847,
                    "matchedPaperCorpusId": "247597241"
                },
                {
                    "start": 847,
                    "end": 850,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 850,
                    "end": 853,
                    "matchedPaperCorpusId": "236134146"
                },
                {
                    "start": 933,
                    "end": 937,
                    "matchedPaperCorpusId": "248426770"
                },
                {
                    "start": 1570,
                    "end": 1574,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 1574,
                    "end": 1577,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1577,
                    "end": 1580,
                    "matchedPaperCorpusId": "236134146"
                },
                {
                    "start": 1617,
                    "end": 1621,
                    "matchedPaperCorpusId": "244119160"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1451416015625
        },
        {
            "corpus_id": "243865344",
            "title": "Progressive Self-Training with Discriminator for Aspect Term Extraction",
            "text": "The self-training proposed by Scudder (1965) is a semi-supervised approach that leverages unlabeled data to create better models. Self-training first trains a base model on a small amount of labeled data; then utilizes it to pseudolabel unlabeled data, and uses pseudo-labels data to augment the labeled data; finally iteratively retrains the model. Recently, it yields state-of-the-art performance on machine learning tasks like image classification (Zoph et al., 2020), few-shot text classification (Mukherjee and Awadallah, 2020), and neural machine translation (He et al., 2019). The error propagation (Wang et al., 2021) from noisy pseudo-labels is an obvious problem in selftraining. In this paper, we alleviate noisy in the pseudo-labels by using progressive subsets (i.e., curriculum learning) and a discriminator. \n\nCurriculum Learning Learning from easier samples first and harder samples later is a common strategy in curriculum learning (Bengio et al., 2009). Our progressive self-training method focuses on easier samples in the early stage, and uses hard samples in the later stage. Our aim is to reduce the noise in the pseudo-labels: the pseudolabels for easy examples are less prone to errors, and model that have been previously learned could yield more accurate pseudo-labels at later stage.",
            "score": 0.39460418737776587,
            "section_title": "Self-training",
            "char_start_offset": 6121,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 822
                },
                {
                    "start": 825,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1310
                }
            ],
            "ref_mentions": [
                {
                    "start": 30,
                    "end": 44,
                    "matchedPaperCorpusId": "30807376"
                },
                {
                    "start": 501,
                    "end": 532,
                    "matchedPaperCorpusId": "227276483"
                },
                {
                    "start": 565,
                    "end": 582,
                    "matchedPaperCorpusId": "1964279"
                },
                {
                    "start": 606,
                    "end": 625,
                    "matchedPaperCorpusId": "219531726"
                },
                {
                    "start": 949,
                    "end": 970,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2371826171875
        },
        {
            "corpus_id": "259376711",
            "title": "BERTastic at SemEval-2023 Task 3: Fine-Tuning Pretrained Multilingual Transformers Does Order Matter?",
            "text": "Deep learning models are state-of-the-art (SOTA) in many fields including natural language processing (NLP). In NLP, the current SOTA models (Wang et al., 2019) are based on transformers, which are deep-learning models with attention mechanism (Vaswani et al., 2017). While many transformers are monolingual, there has been increased research and public interest in multilingual transformers (Doddapaneni et al., 2021). Notably, pretrained transformers, require huge amounts of training data, no matter the domain (Devlin et al., 2019;Dosovitskiy et al., 2021). imposing an order on the training data in which PMTs are fine-tuned sequentially language by language matters. PMTs tend to perform relatively better on languages at either end of the training sequence. The performance improvement is measured against the na\u00efve approach of language-agnostic uniform sampling. \n\nConsequently, this results in a substantial carbon footprint (Strubell et al., 2019), which is against global sustainability objectives. \n\n1 There are many approaches to address the AI carbon footprint concerns ranging from using more carbon-efficient energy sources to applying more efficient AI models and training algorithms. Curriculum learning (CL) encompasses a specific class of efficient training strategies for deep learning models. On the one hand, the na\u00efve approach of fine-tuning pretrained deep learning models on downstream tasks involves feeding them mini-batches of randomly sampled subsets of the available training data. On the other hand, in curriculum learning, the idea is to finetune the model with a sequence of progressively more challenging examples. This is motivated by and mimics the way humans learn, where we start with simpler concepts and gradually build up more complex ones. Curriculum learning research (Soviany et al., 2022) shows that such a strategy helps the model achieve better performance and converge faster. \n\nContributions Following the success of CL approaches, and inspired by cognitive science, we propose an approach to fine-tuning PMTs on multilingual data. We investigate the significance of doing this in a sequential fashion language by language. Unlike CL where the model is presented with increasingly complex examples, we do not adopt a notion of \"easy\" and \"hard\" examples.",
            "score": 0.39454053209970985,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 870
                },
                {
                    "start": 873,
                    "end": 1009
                },
                {
                    "start": 1012,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1925
                },
                {
                    "start": 1928,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2173
                },
                {
                    "start": 2174,
                    "end": 2304
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 160,
                    "matchedPaperCorpusId": "143424870"
                },
                {
                    "start": 244,
                    "end": 266,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 392,
                    "end": 418,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 514,
                    "end": 535,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 535,
                    "end": 560,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 934,
                    "end": 957,
                    "matchedPaperCorpusId": "174802812"
                },
                {
                    "start": 1812,
                    "end": 1834,
                    "matchedPaperCorpusId": "231709290"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.400390625
        },
        {
            "corpus_id": "257525777",
            "title": "Domain Word Extension Using Curriculum Learning",
            "text": "We introduce curriculum learning to provide better initial values for new expansion vocabulary without transforming or adding the structure of the existing pre-trained language models. The curriculum learning [13] is a learning strategy formulated in the context of machine learning that humans and animals learn better when training in the order of from easy to complex examples. At first, you train the models with an \"easy sample\" to identify the outline of the whole and gradually learn complex examples to find the best results. Curriculum learning makes a faster convergence of the learning process and guides the learning machine so that the initial learning step can be better transferred to the final learning step. From the point of view of curriculum learning, the newly added expanded vocabulary corresponds to a \"difficult\" sample where the weight of the existing pre-trained language models have not been learned. On the contrary, the vocabulary of the existing pre-trained language models are an \"easy\" sample that has already learned enough meaning. We train both the embedding and weight of the existing pre-trained language models about the domain data and transfer the meaning of the vocabulary to the target domain. Then, using the subword's embedding, we initialize the extended vocabulary embedding and perform fine-tuning. \n\nSince subword embeddings and models weights are re-trained in Downstream Task Domain, the average of subword embedding is also not a big difference in models weight and distribution. In addition, since each subword embedding has been learned with domain data, the initial value of the new expansion vocabulary embedding can be set close to the value used in the corresponding domain. Figure 4 illustrates the structure of the models that perform the proposed method. Algorithm 1 outlines the process for setting up an embedding of a domain vocabulary V D , which consists of a base vocabulary V B with an extended vocabulary V A added. The model sM that predict output y for input x have a weight parameter \u03b8, which the embedding module E uses to map input tokens V to embedding vectors. The first step is to conduct a warm-up training phase based on a pre-trained language models.",
            "score": 0.3938614272939297,
            "section_title": "Creating the Extended Word Embedding and Application Method",
            "char_start_offset": 14822,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1345
                },
                {
                    "start": 1348,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2135
                },
                {
                    "start": 2136,
                    "end": 2229
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 213,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3359375
        },
        {
            "corpus_id": "267770178",
            "title": "An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling",
            "text": "Sequence labeling is a fundamental task in natural language processing (NLP) that involves assigning labels to individual elements in a sequence. Recent advancements in neural network approaches have demonstrated remarkable performance in sequence labeling tasks (Zhang et al., 2014;Chen et al., 2017;Zhang et al., 2018;Tian et al., 2020a;Nguyen et al., 2021;Hou et al., 2021;Liu et al., 2021). Some of these works have explored the integration of external knowledge, such as n-grams, lexicons, and syntax, to enhance sequence labeling models. However, this integration increases the heterogeneity and complexity of the input data. Furthermore, incorporating knowledge into sequence labeling models often requires additional encode modules, such as attention mechanisms (Liu et al., 2021;Tian et al., 2020a) or graph neural network (GNN) (Gui et al., 2019;Nie et al., 2022). These additional modules increase the model's parameters, making it more time-consuming and resourceintensive to develop a high-performing system. \n\nCurriculum Learning (CL) is an effective approach to tackle these challenges (Bengio et al., 2009), as it mimics the human learning process by gradually introducing training samples from easy to hard, thereby enabling efficient learning in heterogeneous data while improving both the speed and quality of the model (Wang et al., 2021). CL has demonstrated success in various NLP tasks, including machine translation (Wan et al., 2020), dialogue generation (Zhu et al., 2021), and text classification (Zhang et al., 2022). \n\nIn CL, data-selection curriculum strategies play a crucial role. Recently, Mohiuddin et al. (2022) proposed metrics that rely on the similarity of parallel sentence pairs to guide data selection in machine translation tasks. Similarly, Yuan et al. (2022) introduced difficulty metrics for entity typing based on training loss. However, these difficulty metrics primarily focus on the sentence level, and there is a lack of token-level and word-level metrics to measure the difficulty of sequence labeling tasks.",
            "score": 0.3937622725599065,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1021
                },
                {
                    "start": 1024,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 263,
                    "end": 283,
                    "matchedPaperCorpusId": "10585502"
                },
                {
                    "start": 283,
                    "end": 301,
                    "matchedPaperCorpusId": "30285737"
                },
                {
                    "start": 301,
                    "end": 320,
                    "matchedPaperCorpusId": "44135302"
                },
                {
                    "start": 320,
                    "end": 339,
                    "matchedPaperCorpusId": "220046453"
                },
                {
                    "start": 359,
                    "end": 376,
                    "matchedPaperCorpusId": "241583255"
                },
                {
                    "start": 788,
                    "end": 807,
                    "matchedPaperCorpusId": "220046453"
                },
                {
                    "start": 856,
                    "end": 873,
                    "matchedPaperCorpusId": "245844443"
                },
                {
                    "start": 1101,
                    "end": 1122,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1440,
                    "end": 1458,
                    "matchedPaperCorpusId": "222272176"
                },
                {
                    "start": 1480,
                    "end": 1498,
                    "matchedPaperCorpusId": "244119160"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.427001953125
        },
        {
            "corpus_id": "237491889",
            "title": "Efficient Contrastive Learning via Novel Data Augmentation and Curriculum Learning",
            "text": "Many state-of-the-art language models involve the paradigm of unsupervised pretraining followed by fine-tuning on downstream tasks (Devlin et al., 2019;Liu et al., 2019;Brown et al., 2020). However, pretraining a language model from scratch with a huge corpus has high computational costs. One way to cut down the cost is to use continual training of a pretrained model which could improve the language model with less computation (Giorgi et al., 2021). Contrastive learning is effective for selfsupervised learning for image classification (Chen et al., 2020b,c), and it works by allowing the model to put similar examples close and different examples far from one another. Often in contrastive learning, data augmentation is used to make the positive pairs. Recent papers describe how to apply contrastive learning to the language domain (Meng et al., 2021;Gunel et al., 2020;Qu et al., 2020;Wu et al., 2020), and even a combination of contrastive learning with continual pretraining (Giorgi et al., 2021). However, because of the sequential nature of language, it is difficult to apply data augmentation methods used in images directly to language modeling.\n\nAdditionally, curriculum learning is a powerful training technique for deep networks (Hacohen and Weinshall, 2019;Cai et al., 2018). By training easy to hard examples in order, it facilitates faster convergence, leading to better performance. Other studies show that it is also effective for language modeling (Xu et al., 2020;Wei et al., 2021;Press et al., 2021;Li et al., 2021).\n\nWe propose an efficient yet powerful continual pretraining method using contrastive learning and curriculum learning. The contribution of this paper is as follows:\n\n\u2022 We suggest a novel data augmentation method for contrastive learning: first cutoff, then PCA jittering. This leads to robustness to sentencelevel noise, resulting in a better sentence-level representation. \u2022 We apply curriculum learning by increasing the noise degree of augmentation for each level of difficulty. This leads to faster convergence at the pretraining stage. \u2022 In addition to outperforming baseline models on",
            "score": 0.3937438741210267,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 152,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 169,
                    "end": 188,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 431,
                    "end": 452,
                    "matchedPaperCorpusId": "219530980"
                },
                {
                    "start": 894,
                    "end": 910,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 986,
                    "end": 1007,
                    "matchedPaperCorpusId": "219530980"
                },
                {
                    "start": 1247,
                    "end": 1276,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 1506,
                    "end": 1525,
                    "matchedPaperCorpusId": "229924221"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.548828125
        },
        {
            "corpus_id": "272693917",
            "title": "Leveraging Distillation Techniques for Document Understanding: A Case Study with FLAN-T5",
            "text": "Figure 2 shows an overview of the curriculum learning approach, which is motivated by the temperature sampling applied by language models to diversify their outputs [Ch23]: where  is a similarity measure,  is the label for sample  and   \u2208 R is a temperature parameter that is decreased with each epoch. For  we use normalized Levenshtein distance [Yu07] after normalizing both  and , i.e. by removing whitespace in the JSON structure. We take 0.01 as minimum similarity as the domain of   = {(, ) \u2208 R 2 |  \u2260 0 or  \u2265 0}. This definition of   results in the training data being presented in order of increasing difficulty to the student model across the epochs.",
            "score": 0.3937364096259835,
            "section_title": "Curriculum Learning",
            "char_start_offset": 7206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 659
                }
            ],
            "ref_mentions": [
                {
                    "start": 347,
                    "end": 353,
                    "matchedPaperCorpusId": "14854749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.197265625
        },
        {
            "paperId": "e06cd6a7649e7c753523fa9977e36c62945d58eb",
            "corpusId": 248562812,
            "title": "From Easy to Hard: Learning Language-Guided Curriculum for Visual Question Answering on Remote Sensing Data",
            "venue": "IEEE Transactions on Geoscience and Remote Sensing",
            "year": 2022,
            "referenceCount": 55,
            "citationCount": 66,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/36/4358825/09771224.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.03147, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "34589688",
                    "name": "Zhenghang Yuan"
                },
                {
                    "authorId": "35041003",
                    "name": "Lichao Mou"
                },
                {
                    "authorId": "50621207",
                    "name": "Q. Wang"
                },
                {
                    "authorId": "2125159330",
                    "name": "Xiao Xiang Zhu"
                }
            ],
            "abstract": "Visual question answering (VQA) for remote sensing scene has great potential in intelligent human\u2013computer interaction system. Although VQA in computer vision has been widely researched, VQA for remote sensing data (RSVQA) is still in its infancy. There are two characteristics that need to be specially considered for the RSVQA task: 1) no object annotations are available in the RSVQA datasets, which makes it difficult for models to exploit informative region representation and 2) there are questions with clearly different difficulty levels for each image in the RSVQA task. Directly training a model with questions in a random order may confuse the model and limit the performance. To address these two problems, in this article, a multi-level visual feature learning method is proposed to jointly extract language-guided holistic and regional image features. Besides, a self-paced curriculum learning (SPCL)-based VQA model is developed to train networks with samples in an easy-to-hard way. To be more specific, a language-guided SPCL method with a soft weighting strategy is explored in this work. The proposed model is evaluated on three public datasets, and extensive experimental results show that the proposed RSVQA framework can achieve promising performance. Code will be available at https://gitlab.lrz.de/ai4eo/reasoning/VQA-easy2hard.",
            "corpus_id": "248562812",
            "text": "Visual question answering (VQA) for remote sensing scene has great potential in intelligent human\u2013computer interaction system. Although VQA in computer vision has been widely researched, VQA for remote sensing data (RSVQA) is still in its infancy. There are two characteristics that need to be specially considered for the RSVQA task: 1) no object annotations are available in the RSVQA datasets, which makes it difficult for models to exploit informative region representation and 2) there are questions with clearly different difficulty levels for each image in the RSVQA task. Directly training a model with questions in a random order may confuse the model and limit the performance. To address these two problems, in this article, a multi-level visual feature learning method is proposed to jointly extract language-guided holistic and regional image features. Besides, a self-paced curriculum learning (SPCL)-based VQA model is developed to train networks with samples in an easy-to-hard way. To be more specific, a language-guided SPCL method with a soft weighting strategy is explored in this work. The proposed model is evaluated on three public datasets, and extensive experimental results show that the proposed RSVQA framework can achieve promising performance. Code will be available at https://gitlab.lrz.de/ai4eo/reasoning/VQA-easy2hard.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.042083740234375
        },
        {
            "paperId": "9b39f3b9906cc595e2d4ff532093a0d84bfc48dd",
            "corpusId": 266257665,
            "title": "Active Learning-Based Sample Selection for Label-Efficient Blind Image Quality Assessment",
            "venue": "IEEE transactions on circuits and systems for video technology (Print)",
            "year": 2024,
            "referenceCount": 68,
            "citationCount": 4,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3341611?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3341611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2055313269",
                    "name": "Tianshu Song"
                },
                {
                    "authorId": "2258321929",
                    "name": "Leida Li"
                },
                {
                    "authorId": "2274352606",
                    "name": "Deqiang Cheng"
                },
                {
                    "authorId": "2158171518",
                    "name": "Pengfei Chen"
                },
                {
                    "authorId": "1758552",
                    "name": "Jinjian Wu"
                }
            ],
            "abstract": "Despite the considerable effort devoted to high-generalizable blind image quality assessment (BIQA), the generalization performance of the state-of-the-art metrics remains limited when facing new visual scenes. A straightforward way to address the dilemma is labeling a great number of images from the new scene and subsequently training a new model, which is quite labor-intensive and cost-expensive. Hence, there is an urgent need to mitigate the dependency on labeled samples by designing a data-efficient BIQA algorithm. Motivated by the above facts, this paper presents an Active Learning-based IQA (AL-IQA) framework, which reduces the requirement for training samples by selecting representative images from two perspectives, including distortion and content. Specifically, in terms of distortion, we design distortion prompts and adopt Contrastive Language-Image Pre-Training (CLIP) to predict image distortion in a zero-shot manner. Then, we employ curriculum learning-inspired strategy to select samples with gradually increasing difficulty (measured by prediction uncertainty of CLIP), in order to facilitate model training. Meantime, in terms of content, we adopt distribution matching-based dataset distillation to distill unlabeled images into several high-density informative synthetic images. Then, feature distances between unlabeled images and distilled images are compared to identify images with the most representative content. Finally, Borda count is adopted to capture a consensus of both distortion and content through weighted counting, and prompt tuning is utilized for adapting the model to the IQA task. Extensive experiments are conducted on five IQA datasets, and the results demonstrate that the proposed AL-IQA not only effectively reduces the number of training samples but also achieves state-of-the-art prediction accuracy and generalization performance. The source code is available at https://github.com/esnthere/AL-IQA.",
            "corpus_id": "266257665",
            "text": "Despite the considerable effort devoted to high-generalizable blind image quality assessment (BIQA), the generalization performance of the state-of-the-art metrics remains limited when facing new visual scenes. A straightforward way to address the dilemma is labeling a great number of images from the new scene and subsequently training a new model, which is quite labor-intensive and cost-expensive. Hence, there is an urgent need to mitigate the dependency on labeled samples by designing a data-efficient BIQA algorithm. Motivated by the above facts, this paper presents an Active Learning-based IQA (AL-IQA) framework, which reduces the requirement for training samples by selecting representative images from two perspectives, including distortion and content. Specifically, in terms of distortion, we design distortion prompts and adopt Contrastive Language-Image Pre-Training (CLIP) to predict image distortion in a zero-shot manner. Then, we employ curriculum learning-inspired strategy to select samples with gradually increasing difficulty (measured by prediction uncertainty of CLIP), in order to facilitate model training. Meantime, in terms of content, we adopt distribution matching-based dataset distillation to distill unlabeled images into several high-density informative synthetic images. Then, feature distances between unlabeled images and distilled images are compared to identify images with the most representative content. Finally, Borda count is adopted to capture a consensus of both distortion and content through weighted counting, and prompt tuning is utilized for adapting the model to the IQA task. Extensive experiments are conducted on five IQA datasets, and the results demonstrate that the proposed AL-IQA not only effectively reduces the number of training samples but also achieves state-of-the-art prediction accuracy and generalization performance. The source code is available at https://github.com/esnthere/AL-IQA.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.08648681640625
        },
        {
            "paperId": "18be8449f3b4a26dfe8728c04fde77192d4509f7",
            "corpusId": 278996248,
            "title": "Learning Compositional Functions with Transformers from Easy-to-Hard Data",
            "venue": "",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.23683, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2305661314",
                    "name": "Zixuan Wang"
                },
                {
                    "authorId": "19209293",
                    "name": "Eshaan Nichani"
                },
                {
                    "authorId": "2253484313",
                    "name": "Alberto Bietti"
                },
                {
                    "authorId": "2284986062",
                    "name": "Alex Damian"
                },
                {
                    "authorId": "2305684189",
                    "name": "Daniel Hsu"
                },
                {
                    "authorId": "2268337987",
                    "name": "Jason D. Lee"
                },
                {
                    "authorId": "32329596",
                    "name": "Denny Wu"
                }
            ],
            "abstract": "Transformer-based language models have demonstrated impressive capabilities across a range of complex reasoning tasks. Prior theoretical work exploring the expressive power of transformers has shown that they can efficiently perform multi-step reasoning tasks involving parallelizable computations. However, the learnability of such constructions, particularly the conditions on the data distribution that enable efficient learning via gradient-based optimization, remains an open question. Towards answering this question, in this work we study the learnability of the $k$-fold composition task, which requires computing an interleaved composition of $k$ input permutations and $k$ hidden permutations, and can be expressed by a transformer with $O(\\log k)$ layers. On the negative front, we prove a Statistical Query (SQ) lower bound showing that any SQ learner that makes only polynomially-many queries to an SQ oracle for the $k$-fold composition task distribution must have sample size exponential in $k$, thus establishing a statistical-computational gap. On the other hand, we show that this function class can be efficiently learned, with runtime and sample complexity polynomial in $k$, by gradient descent on an $O(\\log k)$-depth transformer via two different curriculum learning strategies: one in which data consists of $k'$-fold composition functions with $k' \\le k$ presented in increasing difficulty, and another in which all such data is presented simultaneously. Our work sheds light on the necessity and sufficiency of having both easy and hard examples in the data distribution for transformers to learn complex compositional tasks.",
            "corpus_id": "278996248",
            "text": "Transformer-based language models have demonstrated impressive capabilities across a range of complex reasoning tasks. Prior theoretical work exploring the expressive power of transformers has shown that they can efficiently perform multi-step reasoning tasks involving parallelizable computations. However, the learnability of such constructions, particularly the conditions on the data distribution that enable efficient learning via gradient-based optimization, remains an open question. Towards answering this question, in this work we study the learnability of the $k$-fold composition task, which requires computing an interleaved composition of $k$ input permutations and $k$ hidden permutations, and can be expressed by a transformer with $O(\\log k)$ layers. On the negative front, we prove a Statistical Query (SQ) lower bound showing that any SQ learner that makes only polynomially-many queries to an SQ oracle for the $k$-fold composition task distribution must have sample size exponential in $k$, thus establishing a statistical-computational gap. On the other hand, we show that this function class can be efficiently learned, with runtime and sample complexity polynomial in $k$, by gradient descent on an $O(\\log k)$-depth transformer via two different curriculum learning strategies: one in which data consists of $k'$-fold composition functions with $k' \\le k$ presented in increasing difficulty, and another in which all such data is presented simultaneously. Our work sheds light on the necessity and sufficiency of having both easy and hard examples in the data distribution for transformers to learn complex compositional tasks.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.451904296875
        },
        {
            "paperId": "a7381c3a8184d6c259eda7a2412edad50f2d50de",
            "corpusId": 277628042,
            "title": "Efficient Reinforcement Finetuning via Adaptive Curriculum Learning",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 40,
            "citationCount": 15,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.05520, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2352941279",
                    "name": "Taiwei Shi"
                },
                {
                    "authorId": "2354282524",
                    "name": "Yiyang Wu"
                },
                {
                    "authorId": "2355252250",
                    "name": "Linxin Song"
                },
                {
                    "authorId": "2239167231",
                    "name": "Tianyi Zhou"
                },
                {
                    "authorId": "2266812106",
                    "name": "Jieyu Zhao"
                }
            ],
            "abstract": "Reinforcement finetuning (RFT) has shown great potential for enhancing the mathematical reasoning capabilities of large language models (LLMs), but it is often sample- and compute-inefficient, requiring extensive training. In this work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a method that significantly improves both the efficiency and final accuracy of RFT through adaptive curriculum learning. AdaRFT dynamically adjusts the difficulty of training problems based on the model's recent reward signals, ensuring that the model consistently trains on tasks that are challenging but solvable. This adaptive sampling strategy accelerates learning by maintaining an optimal difficulty range, avoiding wasted computation on problems that are too easy or too hard. AdaRFT requires only a lightweight extension to standard RFT algorithms like Proximal Policy Optimization (PPO), without modifying the reward function or model architecture. Experiments on competition-level math datasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT significantly improves both training efficiency and reasoning performance. We evaluate AdaRFT across multiple data distributions and model sizes, showing that it reduces training time by up to 2x and improves accuracy by a considerable margin, offering a more scalable and effective RFT framework.",
            "corpus_id": "277628042",
            "text": "Reinforcement finetuning (RFT) has shown great potential for enhancing the mathematical reasoning capabilities of large language models (LLMs), but it is often sample- and compute-inefficient, requiring extensive training. In this work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a method that significantly improves both the efficiency and final accuracy of RFT through adaptive curriculum learning. AdaRFT dynamically adjusts the difficulty of training problems based on the model's recent reward signals, ensuring that the model consistently trains on tasks that are challenging but solvable. This adaptive sampling strategy accelerates learning by maintaining an optimal difficulty range, avoiding wasted computation on problems that are too easy or too hard. AdaRFT requires only a lightweight extension to standard RFT algorithms like Proximal Policy Optimization (PPO), without modifying the reward function or model architecture. Experiments on competition-level math datasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT significantly improves both training efficiency and reasoning performance. We evaluate AdaRFT across multiple data distributions and model sizes, showing that it reduces training time by up to 2x and improves accuracy by a considerable margin, offering a more scalable and effective RFT framework.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.57763671875
        },
        {
            "paperId": "d6f98e68f80376d55fc351c68c57314c0ee99fb2",
            "corpusId": 267627563,
            "title": "Data Distribution-Based Curriculum Learning",
            "venue": "IEEE Access",
            "year": 2024,
            "referenceCount": 47,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2024.3465793",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.07352, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2131842",
                    "name": "Shonal Chaudhry"
                },
                {
                    "authorId": "2283862842",
                    "name": "Anuraganand Sharma"
                }
            ],
            "abstract": "The order of training samples can have a significant impact on a model\u2019s performance. Curriculum learning is an approach for gradually training a model by ordering samples from \u2018easy\u2019 to \u2018hard\u2019. This paper proposes the novel idea of a curriculum learning strategy called Data Distribution-based Curriculum Learning (DDCL). DDCL uses the inherent data distribution of a dataset to build a curriculum based on the order of samples. Our proposed approach is innovative as it incorporates two distinct scoring methods known as DDCL-Density and DDCL-Point to determine the order of training samples. The DDCL-Density method assigns scores based on the density of samples favoring denser regions that can make initial learning easier. Conversely, DDCL-Point utilizes the Euclidean distance from the centroid of the dataset as a reference point to score samples providing an alternative perspective on sample difficulty. We evaluate the proposed DDCL approach by conducting experiments across various classifiers using a diverse set of small to medium-sized medical datasets. Results show that DDCL improves the classification accuracy, achieving increases ranging from 2% to 10% compared to baseline methods and other state-of-the-art techniques. Moreover, analysis of the error losses for a single training epoch reveals that DDCL not only improves accuracy but also increases the convergence rate, underlining its potential for more efficient training. The findings suggest that DDCL can specifically be of benefit to medical applications where data is often limited and indicate promising directions for future research in domains that involve limited datasets.",
            "corpus_id": "267627563",
            "text": "The order of training samples can have a significant impact on a model\u2019s performance. Curriculum learning is an approach for gradually training a model by ordering samples from \u2018easy\u2019 to \u2018hard\u2019. This paper proposes the novel idea of a curriculum learning strategy called Data Distribution-based Curriculum Learning (DDCL). DDCL uses the inherent data distribution of a dataset to build a curriculum based on the order of samples. Our proposed approach is innovative as it incorporates two distinct scoring methods known as DDCL-Density and DDCL-Point to determine the order of training samples. The DDCL-Density method assigns scores based on the density of samples favoring denser regions that can make initial learning easier. Conversely, DDCL-Point utilizes the Euclidean distance from the centroid of the dataset as a reference point to score samples providing an alternative perspective on sample difficulty. We evaluate the proposed DDCL approach by conducting experiments across various classifiers using a diverse set of small to medium-sized medical datasets. Results show that DDCL improves the classification accuracy, achieving increases ranging from 2% to 10% compared to baseline methods and other state-of-the-art techniques. Moreover, analysis of the error losses for a single training epoch reveals that DDCL not only improves accuracy but also increases the convergence rate, underlining its potential for more efficient training. The findings suggest that DDCL can specifically be of benefit to medical applications where data is often limited and indicate promising directions for future research in domains that involve limited datasets.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.175537109375
        },
        {
            "paperId": "bc362c6963a5b0a626a5d33a24a295a91340aaef",
            "corpusId": 277189103,
            "title": "SPCNet: Deep Self-Paced Curriculum Network Incorporated With Inductive Bias.",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TNNLS.2025.3544724?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TNNLS.2025.3544724, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2254062901",
                    "name": "Yue Zhao"
                },
                {
                    "authorId": "2284983858",
                    "name": "Maoguo Gong"
                },
                {
                    "authorId": "2108147319",
                    "name": "Mingyang Zhang"
                },
                {
                    "authorId": "2166500540",
                    "name": "A. K. Qin"
                },
                {
                    "authorId": "151499559",
                    "name": "Fenlong Jiang"
                },
                {
                    "authorId": "2130167832",
                    "name": "Jianzhao Li"
                }
            ],
            "abstract": "The vulnerability to poor local optimum and the memorization of noise data limit the generalizability and reliability of massively parameterized convolutional neural networks (CNNs) on complex real-world data. Self-paced curriculum learning (SPCL), which models the easy-to-hard learning progression from human beings, is considered as a potential savior. In spite of the fact that numerous SPCL solutions have been explored, it still confronts two main challenges exactly in solving deep networks. By virtue of various designed regularizers, existing weighting schemes independent of the learning objective heavily rely on the prior knowledge. In addition, alternative optimization strategy (AOS) enables the tedious iterative training procedure, thus there is still not an efficient framework that integrates the SPCL paradigm well with networks. This article delivers a novel insight that attention mechanism allows for adaptive enhancement in the contribution of diverse instance information to the gradient propagation. Accordingly, we propose a general-purpose deep SPCL paradigm that incorporates the preferences of implicit regularizer for different samples into the network structure with inductive bias, which in turn is formalized as the self-paced curriculum network (SPCNet). Our proposal allows simultaneous online difficulty estimation, adaptive sample selection, and model updating in an end-to-end manner, which significantly facilitates the collaboration of SPCL to deep networks. Experiments on image classification and scene classification tasks demonstrate that our approach surpasses the state-of-the-art schemes and obtains superior performance.",
            "corpus_id": "277189103",
            "text": "The vulnerability to poor local optimum and the memorization of noise data limit the generalizability and reliability of massively parameterized convolutional neural networks (CNNs) on complex real-world data. Self-paced curriculum learning (SPCL), which models the easy-to-hard learning progression from human beings, is considered as a potential savior. In spite of the fact that numerous SPCL solutions have been explored, it still confronts two main challenges exactly in solving deep networks. By virtue of various designed regularizers, existing weighting schemes independent of the learning objective heavily rely on the prior knowledge. In addition, alternative optimization strategy (AOS) enables the tedious iterative training procedure, thus there is still not an efficient framework that integrates the SPCL paradigm well with networks. This article delivers a novel insight that attention mechanism allows for adaptive enhancement in the contribution of diverse instance information to the gradient propagation. Accordingly, we propose a general-purpose deep SPCL paradigm that incorporates the preferences of implicit regularizer for different samples into the network structure with inductive bias, which in turn is formalized as the self-paced curriculum network (SPCNet). Our proposal allows simultaneous online difficulty estimation, adaptive sample selection, and model updating in an end-to-end manner, which significantly facilitates the collaboration of SPCL to deep networks. Experiments on image classification and scene classification tasks demonstrate that our approach surpasses the state-of-the-art schemes and obtains superior performance.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.041839599609375
        },
        {
            "paperId": "3e408a2c39567901a290642fcb98bf1f6e277e92",
            "corpusId": 268531971,
            "title": "Improving Generalization via Meta-Learning on Hard Samples",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2024,
            "referenceCount": 43,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2403.12236",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.12236, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292203771",
                    "name": "Nishant Jain"
                },
                {
                    "authorId": "2170091",
                    "name": "A. Suggala"
                },
                {
                    "authorId": "2267768632",
                    "name": "Pradeep Shenoy"
                }
            ],
            "abstract": "Learned reweighting (LRW) approaches to supervised learning use an optimization criterion to assign weights for training instances, in order to maximize performance on a representative validation dataset. We pose and formalize the problem of optimized selection of the validation set used in LRW training, to improve classifier generalization. In particular, we show that using hard-to-classify instances in the validation set has both a theoretical connection to, and strong empirical evidence of generalization. We provide an efficient algorithm for training this meta-optimized model, as well as a simple train-twice heuristic for careful comparative study. We demonstrate that LRW with easy validation data performs consistently worse than LRW with hard validation data, establishing the validity of our meta-optimization problem. Our proposed algorithm outperforms a wide range of baselines on a range of datasets and domain shift challenges (Imagenet-1K, CIFAR-100, Clothing-1M, CAMELYON, WILDS, etc.), with 1% gains using VIT-B on Imagenet. We also show that using naturally hard examples for validation (Imagenet-R/Imagenet-A) in LRW training for Imagenet improves performance on both clean and naturally hard test instances by 1-2%. Secondary analyses show that using hard validation data in an LRW framework improves margins on test data, hinting at the mechanism underlying our empirical gains. We believe this work opens up new research directions for the meta-optimization of meta-learning in a supervised learning context.",
            "corpus_id": "268531971",
            "text": "Learned reweighting (LRW) approaches to supervised learning use an optimization criterion to assign weights for training instances, in order to maximize performance on a representative validation dataset. We pose and formalize the problem of optimized selection of the validation set used in LRW training, to improve classifier generalization. In particular, we show that using hard-to-classify instances in the validation set has both a theoretical connection to, and strong empirical evidence of generalization. We provide an efficient algorithm for training this meta-optimized model, as well as a simple train-twice heuristic for careful comparative study. We demonstrate that LRW with easy validation data performs consistently worse than LRW with hard validation data, establishing the validity of our meta-optimization problem. Our proposed algorithm outperforms a wide range of baselines on a range of datasets and domain shift challenges (Imagenet-1K, CIFAR-100, Clothing-1M, CAMELYON, WILDS, etc.), with 1% gains using VIT-B on Imagenet. We also show that using naturally hard examples for validation (Imagenet-R/Imagenet-A) in LRW training for Imagenet improves performance on both clean and naturally hard test instances by 1-2%. Secondary analyses show that using hard validation data in an LRW framework improves margins on test data, hinting at the mechanism underlying our empirical gains. We believe this work opens up new research directions for the meta-optimization of meta-learning in a supervised learning context.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.226806640625
        },
        {
            "paperId": "bbe5ea6dc1470b33f3396a417bd546638948f535",
            "corpusId": 257293067,
            "title": "Self-Training With Double Selectors for Low-Resource Named Entity Recognition",
            "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
            "year": 2023,
            "referenceCount": 55,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TASLP.2023.3250828?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TASLP.2023.3250828, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2110659921",
                    "name": "Yingwen Fu"
                },
                {
                    "authorId": "67285699",
                    "name": "Nankai Lin"
                },
                {
                    "authorId": "2143475374",
                    "name": "Xiao-sheng Yu"
                },
                {
                    "authorId": "2130537542",
                    "name": "Shengyi Jiang"
                }
            ],
            "abstract": "Named Entity Recognition (NER) is fundamental to multiple downstream natural language processing (NLP) tasks, but most advanced NER methods heavily rely on massive labeled data with high cost. In this paper, we explore the effectiveness of self-training for low-resource NER. It is one of the semi-supervised approaches to reduce the reliance on manual annotation. However, random pseudo sample selection in standard self-training framework may cause serious error propagation, especially for token-level tasks. To that end, this paper focuses on pseudo sample selection and proposes a new self-training framework with double selectors, namely auxiliary judge task and entropy-based confidence measurement. Specifically, the auxiliary judge task is proposed to filter out the pseudo samples with wrong predictions. The entropy-based confidence measurement is introduced to select pseudo samples with high quality. In addition, to make full use of all pseudo samples, we propose a cumulative function based on the idea of curriculum learning to prompt the model to learn from easy samples to hard ones. Samples with low quality are filtered out through the double selectors, which is more conducive to the training of student models. Experimental results on five NER benchmark datasets from different languages indicate the effectiveness of the proposed framework over several advanced baselines.",
            "corpus_id": "257293067",
            "text": "Named Entity Recognition (NER) is fundamental to multiple downstream natural language processing (NLP) tasks, but most advanced NER methods heavily rely on massive labeled data with high cost. In this paper, we explore the effectiveness of self-training for low-resource NER. It is one of the semi-supervised approaches to reduce the reliance on manual annotation. However, random pseudo sample selection in standard self-training framework may cause serious error propagation, especially for token-level tasks. To that end, this paper focuses on pseudo sample selection and proposes a new self-training framework with double selectors, namely auxiliary judge task and entropy-based confidence measurement. Specifically, the auxiliary judge task is proposed to filter out the pseudo samples with wrong predictions. The entropy-based confidence measurement is introduced to select pseudo samples with high quality. In addition, to make full use of all pseudo samples, we propose a cumulative function based on the idea of curriculum learning to prompt the model to learn from easy samples to hard ones. Samples with low quality are filtered out through the double selectors, which is more conducive to the training of student models. Experimental results on five NER benchmark datasets from different languages indicate the effectiveness of the proposed framework over several advanced baselines.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.398193359375
        },
        {
            "paperId": "b62937e56a5a2979e39afb79821b804e63da4b31",
            "corpusId": 237786175,
            "title": "Deep-Sea Trenches of the Pacific Ocean: A Comparative Analysis of the Submarine Geomorphology Using Data Modeling by Gmt, Python and R",
            "venue": "Social Science Research Network",
            "year": 2021,
            "referenceCount": 0,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dipot.ulb.ac.be/dspace/bitstream/2013/364595/3/SSRN-id3858289.pdf",
                "status": "GREEN",
                "license": "other-oa",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.2139/ssrn.3858289?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.2139/ssrn.3858289, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "52194775",
                    "name": "Polina Lemenkova"
                }
            ],
            "abstract": "Geomorphological landforms of the oceanic trenches, their formation and variation of the geometric shapes is a question of special importance to the scientific community in marine geology. The actuality of this question has significantly increased since the beginning of the rapid development of the IT tools and methods of the advanced data analysis, yet its understanding remains patchy. Since the majority of the oceanic trenches are located along the margins of the Pacific Ocean, it plays a central role for their analysis and understanding their formation oceanic trenches. Specific geological conditions, presence of the tectonic subduction zones, vast territory of the Pacific Ocean with complex circulation system, extension of the \u2019Ring of Fire\u2019, a seismically active belt of the earthquakes and volcanic, make the trenches of the Pacific Ocean highly sensitive to the factors affecting their formation which cause variations in their geomorphic shape forms. In this context, the most representative indicators of the variations in the deep-sea trenches are geological and tectonic factors, such as dynamics of lithosphere crust affecting speed and intensity of plates subduction, magnitude and frequency of the submarine volcanoes causing active sedimentation. Nowadays, studying marine geological phenomena and complex processes by programming and scripting has been a powerful method. Rapid development of the advanced methods of data analysis presented such effective tools as GMT, Octave/MATLAB, R and Python. It is particularly efficient when applied to the massive amounts of marine geological data. Big data processing by advanced scripting is a crucial approach, as algorithms of libraries give access to the accurate and rapid data analysis [373]. Specific information about distant and hard-to-reach deep-sea trenches can be gained for precise visual- ization and analysis of their submarine geomorphology from local to regional and global scales. However, despite all the efforts, there is a lack of uniformity in studying deep-sea trenches, a shortage of systematic mapping of the Pacific trenches and a lack of understanding of the geomorphological variation between the trench profiles in different parts of the ocean: southern and northern, eastern and western, and their response to the geological and tectonic local settings in the places of formation. Therefore, this dissertation develops a systematic approach to monitoring and comparative analysis of the geomorphological shape forms of the deep-sea trenches formed under specific geological and tectonic conditions along the margins of the Pacific Ocean. The study area encompasses Pacific Ocean, and more specifically, includes 20 selected target trenches: Aleutian, Mariana, Philippine, Kuril-Kamchatka, Middle America, Peru-Chile, Palau, Japan, Kermadec, Tonga, Izu-Bonin, New Britain, San Cristobal, Manila, Yap, New Hebrides, Puysegur, Hikurangi, Vityaz and Ryukyu. These are the major trenches of the Pacific Ocean and, therefore, the most representative for the geomorphological modelling. This dissertation identifies tectonic plates formation, slab subduction, historical geological development, earthquakes and submarine volcanoes as the primary types of impact factors affecting trenches formation. Secondary factors include ocean currents, sedimentation and biota contributing to the sedimentation. Seafloor geomorphology in hadal trenches is strongly affected by a variety of factors that necessarily affect the shape of their landforms. Using data modelling, the shapes of the profiles transecting the trenches in an orthogonal direction were compared and analyzed in order to highlight the differences and variations in their geomorphology. The objective of this PhD study is to perform a geomorphological classification of the shape forms of the trenches through ordering them into groups base don the common characteristics of the trenches\u2019 landforms in plan and attaching labels to these groups. Following geomorphological profile shape types have been identifies and trenches are classified into seven types: U-formed (in plan), V-formed (in plan), asymmetric, crescent-formed, sinuous-formed, elongated, cascade-formed. For each type (U, V, asymmetric and so on) characteristic steepness sub-types are identified: strong, very strong, extreme, steep, very steep. Valley slopes are classified as follows: very high, high, moderate, low, based on the curvature degree. Size and valley slope classes are analyzed in the context of physical environment and tectonic and geological development of the area of trench formation. \nTechnical aim of this PhD study was to experiment with and extend current methods of geospatial modelling for geomorphological classification of the submarine landforms of the trenches. Using methods of the advanced data analysis is crucial for the precise and reliable data processing, since understanding seafloor landforms can only be based on the computer-based data modelling due to their unreachable location. The selection of the methodology, tools and algorithms is explained by research objectives and goals. The specifics of the marine geology consists in the high requirements towards data processing. Datasets were processed, computed and analyzed in semi-automatic regime by Machine Learning (ML) approaches, using advanced algorithms of data analysis and effective visualization through application of the advanced programming tools and Generic Mapping Tools (GMT) scripting toolsets. \nThis dissertation presents an automated workflow enabling large-scale profile cross-sectioning aimed at transect geomorphological mapping, quantitative comparative analysis and classification of the 20 trenches of the Pacific Ocean. The methodology of the GMT includes algorithms of sequential scripting for the cartographic visualization and mapping, automatic digitizing of the cross-section transect profiles, and geomorphic modelling of the trenches. In total 50 modules of GMT scripting toolset were trained on extensive datasets collected from 20 trenches across the whole region of the Pacific Ocean. Using high-resolution bathymetric datasets (General Bathymetric Chart of the Oceans (GEBCO), ETOPO1 and Shuttle Radar Topographic Mission (SRTM)), sample transects of the trenches were modeled, analyzed and compared. Variations in shape forms, steepness and curvature were analyzed by computed models for each trench. The tables were converted from QGIS plugins to Python libraries and R packages, and from GMT to Octave via AWK languages. The results revealed variation in the shape and steepness of the submarine geomorphology in 20 trenches of the Pacific Ocean. A strong correlation between the geomorphic profile shapes with geological factors and level of tectonic activities (earthquakes, volcanism, speed of tectonic plate subduction) and the scale of trench steepness, curvature and shape unevenness is confirmed and analyzed. Geomorphological structure of the trenches and dynamics of the tectonic plates subduction are analyzed and assessed at each trench regionally (north, south, west and east Pacific). The novelty of the study consists in presented systematic classification and comparative modelling of the geomorphic profiles of the deep-sea trenches by means of the sequential usage of the advanced scripting toolsets. Technical innovativeness consists in a combination of GIS, GMT, Python, AWK, R. \nThe actuality of this dissertation lies in its strongly multi-disciplinary nature demonstrating a com- bination of the following approaches: 1) systematic multi-source geospatial data analysis; 2) statistical data modelling and processing by libraries of the Python and R, AWK and Octave/Matlab; 3) geological literature analysis; 4) cartographic mapping and modelling by GMT shell scripts and visualization in QGIS. Technical scripts used for advanced statistical analysis are presented in full in the Appendix A for future replication and reproducible analysis in other trenches of the World Ocean.",
            "corpus_id": "237786175",
            "text": "Geomorphological landforms of the oceanic trenches, their formation and variation of the geometric shapes is a question of special importance to the scientific community in marine geology. The actuality of this question has significantly increased since the beginning of the rapid development of the IT tools and methods of the advanced data analysis, yet its understanding remains patchy. Since the majority of the oceanic trenches are located along the margins of the Pacific Ocean, it plays a central role for their analysis and understanding their formation oceanic trenches. Specific geological conditions, presence of the tectonic subduction zones, vast territory of the Pacific Ocean with complex circulation system, extension of the \u2019Ring of Fire\u2019, a seismically active belt of the earthquakes and volcanic, make the trenches of the Pacific Ocean highly sensitive to the factors affecting their formation which cause variations in their geomorphic shape forms. In this context, the most representative indicators of the variations in the deep-sea trenches are geological and tectonic factors, such as dynamics of lithosphere crust affecting speed and intensity of plates subduction, magnitude and frequency of the submarine volcanoes causing active sedimentation. Nowadays, studying marine geological phenomena and complex processes by programming and scripting has been a powerful method. Rapid development of the advanced methods of data analysis presented such effective tools as GMT, Octave/MATLAB, R and Python. It is particularly efficient when applied to the massive amounts of marine geological data. Big data processing by advanced scripting is a crucial approach, as algorithms of libraries give access to the accurate and rapid data analysis [373]. Specific information about distant and hard-to-reach deep-sea trenches can be gained for precise visual- ization and analysis of their submarine geomorphology from local to regional and global scales. However, despite all the efforts, there is a lack of uniformity in studying deep-sea trenches, a shortage of systematic mapping of the Pacific trenches and a lack of understanding of the geomorphological variation between the trench profiles in different parts of the ocean: southern and northern, eastern and western, and their response to the geological and tectonic local settings in the places of formation. Therefore, this dissertation develops a systematic approach to monitoring and comparative analysis of the geomorphological shape forms of the deep-sea trenches formed under specific geological and tectonic conditions along the margins of the Pacific Ocean. The study area encompasses Pacific Ocean, and more specifically, includes 20 selected target trenches: Aleutian, Mariana, Philippine, Kuril-Kamchatka, Middle America, Peru-Chile, Palau, Japan, Kermadec, Tonga, Izu-Bonin, New Britain, San Cristobal, Manila, Yap, New Hebrides, Puysegur, Hikurangi, Vityaz and Ryukyu. These are the major trenches of the Pacific Ocean and, therefore, the most representative for the geomorphological modelling. This dissertation identifies tectonic plates formation, slab subduction, historical geological development, earthquakes and submarine volcanoes as the primary types of impact factors affecting trenches formation. Secondary factors include ocean currents, sedimentation and biota contributing to the sedimentation. Seafloor geomorphology in hadal trenches is strongly affected by a variety of factors that necessarily affect the shape of their landforms. Using data modelling, the shapes of the profiles transecting the trenches in an orthogonal direction were compared and analyzed in order to highlight the differences and variations in their geomorphology. The objective of this PhD study is to perform a geomorphological classification of the shape forms of the trenches through ordering them into groups base don the common characteristics of the trenches\u2019 landforms in plan and attaching labels to these groups. Following geomorphological profile shape types have been identifies and trenches are classified into seven types: U-formed (in plan), V-formed (in plan), asymmetric, crescent-formed, sinuous-formed, elongated, cascade-formed. For each type (U, V, asymmetric and so on) characteristic steepness sub-types are identified: strong, very strong, extreme, steep, very steep. Valley slopes are classified as follows: very high, high, moderate, low, based on the curvature degree. Size and valley slope classes are analyzed in the context of physical environment and tectonic and geological development of the area of trench formation. \nTechnical aim of this PhD study was to experiment with and extend current methods of geospatial modelling for geomorphological classification of the submarine landforms of the trenches. Using methods of the advanced data analysis is crucial for the precise and reliable data processing, since understanding seafloor landforms can only be based on the computer-based data modelling due to their unreachable location. The selection of the methodology, tools and algorithms is explained by research objectives and goals. The specifics of the marine geology consists in the high requirements towards data processing. Datasets were processed, computed and analyzed in semi-automatic regime by Machine Learning (ML) approaches, using advanced algorithms of data analysis and effective visualization through application of the advanced programming tools and Generic Mapping Tools (GMT) scripting toolsets. \nThis dissertation presents an automated workflow enabling large-scale profile cross-sectioning aimed at transect geomorphological mapping, quantitative comparative analysis and classification of the 20 trenches of the Pacific Ocean. The methodology of the GMT includes algorithms of sequential scripting for the cartographic visualization and mapping, automatic digitizing of the cross-section transect profiles, and geomorphic modelling of the trenches. In total 50 modules of GMT scripting toolset were trained on extensive datasets collected from 20 trenches across the whole region of the Pacific Ocean. Using high-resolution bathymetric datasets (General Bathymetric Chart of the Oceans (GEBCO), ETOPO1 and Shuttle Radar Topographic Mission (SRTM)), sample transects of the trenches were modeled, analyzed and compared. Variations in shape forms, steepness and curvature were analyzed by computed models for each trench. The tables were converted from QGIS plugins to Python libraries and R packages, and from GMT to Octave via AWK languages. The results revealed variation in the shape and steepness of the submarine geomorphology in 20 trenches of the Pacific Ocean. A strong correlation between the geomorphic profile shapes with geological factors and level of tectonic activities (earthquakes, volcanism, speed of tectonic plate subduction) and the scale of trench steepness, curvature and shape unevenness is confirmed and analyzed. Geomorphological structure of the trenches and dynamics of the tectonic plates subduction are analyzed and assessed at each trench regionally (north, south, west and east Pacific). The novelty of the study consists in presented systematic classification and comparative modelling of the geomorphic profiles of the deep-sea trenches by means of the sequential usage of the advanced scripting toolsets. Technical innovativeness consists in a combination of GIS, GMT, Python, AWK, R. \nThe actuality of this dissertation lies in its strongly multi-disciplinary nature demonstrating a com- bination of the following approaches: 1) systematic multi-source geospatial data analysis; 2) statistical data modelling and processing by libraries of the Python and R, AWK and Octave/Matlab; 3) geological literature analysis; 4) cartographic mapping and modelling by GMT shell scripts and visualization in QGIS. Technical scripts used for advanced statistical analysis are presented in full in the Appendix A for future replication and reproducible analysis in other trenches of the World Ocean.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0013246536254882812
        }
    ],
    "quotes": {
        "cost": 0.203376,
        "quotes": [
            {
                "idx": 0,
                "key": "[155089817 | Zhang et al. | 2019 | Citations: 124]",
                "snippets": "We introduce a curriculum learning approach to adapt generic neural machine translation models to a specific domain. Samples are grouped by their similarities to the domain of interest and each group is fed to the training algorithm with a particular schedule.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We introduce a curriculum learning approach to adapt generic neural machine translation models to a specific domain. Samples are grouped by their similarities to the domain of interest and each group is fed to the training algorithm with a particular schedule.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[208031414 | Ma et al. | 2019 | Citations: 95]",
                "snippets": "Our method is inspired by the work on curriculum learning and recent work on data selection for transfer learning. \n\nCurriculum Learning: Curriculum Learning (Bengio et al., 2009) deals with the question of how to use prior knowledge about the difficulty of the training examples, to boost the rate of learning and the performance of the final model. The ranking or weighting of the training examples is used to guide the order of presentation of examples to the learner. The idea is to build a curriculum of progressively harder samples in order to significantly accelerate a neural network's train-ing.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 604,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 114
                            },
                            {
                                "start": 117,
                                "end": 350
                            },
                            {
                                "start": 351,
                                "end": 471
                            },
                            {
                                "start": 472,
                                "end": 604
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Our method is inspired by the work on curriculum learning and recent work on data selection for transfer learning. \n\nCurriculum Learning: Curriculum Learning (Bengio et al., 2009) deals with the question of how to use prior knowledge about the difficulty of the training examples, to boost the rate of learning and the performance of the final model. The ranking or weighting of the training examples is used to guide the order of presentation of examples to the learner. The idea is to build a curriculum of progressively harder samples in order to significantly accelerate a neural network's train-ing."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[226964591 | Pillai et al. | 2020 | Citations: 3]",
                "snippets": "Ordering the selection of training data using active learning can lead to improvements in learning efficiently from smaller corpora.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 132,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Ordering the selection of training data using active learning can lead to improvements in learning efficiently from smaller corpora."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[227905455 | Laverghetta et al. | 2020 | Citations: 2]",
                "snippets": "Curriculum learning (Elman, 1993), a strategy where the model is trained on easier examples before harder ones, has recently been shown to improve performance and reduce training time on a variety of NLP tasks, especially machine translation (Liu et al., 2020;(Wang et al., 2019)(Zhou et al., 2020)(Xu et al., 2020). \n\nThe success of this research shows that the order in which training data is presented to a model is important, but how to best apply curriculum learning broadly to NLP and how to design effective measures of difficulty to create curricula remain relatively unexplored.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220045412 | Wang et al. | 2019 | Citations: 39]": "Most data selection research in machine translation focuses on improving a single domain. We perform data selection for multiple domains at once. This is achieved by carefully introducing instance-level domain-relevance features and automatically constructing a training curriculum to gradually concentrate on multi-domain relevant and noise-reduced data batches. Both the choice of features and the use of curriculum are crucial for balancing and improving all domains, including out-of-domain. In large-scale experiments, the multi-domain curriculum simultaneously reaches or outperforms the individual performance and brings solid gains over no-curriculum training.",
                    "[220045816 | Xu et al. | 2020 | Citations: 206]": "With the great success of pre-trained language models, the pretrain-finetune paradigm now becomes the undoubtedly dominant solution for natural language understanding (NLU) tasks. At the fine-tune stage, target task data is usually introduced in a completely random order and treated equally. However, examples in NLU tasks can vary greatly in difficulty, and similar to human learning procedure, language models can benefit from an easy-to-difficult curriculum. Based on this idea, we propose our Curriculum Learning approach. By reviewing the trainset in a crossed way, we are able to distinguish easy examples from difficult ones, and arrange a curriculum for language models. Without any manual model architecture design or use of external data, our Curriculum Learning approach obtains significant and universal performance improvements on a wide range of NLU tasks.",
                    "[220047761 | Zhou et al. | 2020 | Citations: 96]": "Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 663,
                        "end": 1247,
                        "sentence_offsets": [
                            {
                                "start": 663,
                                "end": 976
                            },
                            {
                                "start": 979,
                                "end": 1247
                            }
                        ],
                        "ref_mentions": [
                            "2105042",
                            "220045412",
                            "220047761",
                            "220045816"
                        ],
                        "quote": "Curriculum learning (Elman, 1993), a strategy where the model is trained on easier examples before harder ones, has recently been shown to improve performance and reduce training time on a variety of NLP tasks, especially machine translation (Liu et al., 2020;(Wang et al., 2019)(Zhou et al., 2020)(Xu et al., 2020). \n\nThe success of this research shows that the order in which training data is presented to a model is important, but how to best apply curriculum learning broadly to NLP and how to design effective measures of difficulty to create curricula remain relatively unexplored."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[231709290 | Soviany et al. | 2021 | Citations: 359]",
                "snippets": "(Zhou et al., 2020) introduce an uncertainty-based curriculum batching approach for neural machine translation. They propose using uncertainty at the data level, for establishing the easy-to-hard ordering, and the model level, to decide the right moment to enhance the training set with more difficult samples. To measure the difficulty of the examples, they start from the intuition that samples with higher crossentropy and uncertainty are more difficult to translate.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220047761 | Zhou et al. | 2020 | Citations: 96]": "Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule."
                },
                "metadata": [
                    {
                        "section_title": "Natural Language Processing",
                        "pdf_hash": "",
                        "start": 1311,
                        "end": 1781,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "220047761"
                        ],
                        "quote": "(Zhou et al., 2020) introduce an uncertainty-based curriculum batching approach for neural machine translation. They propose using uncertainty at the data level, for establishing the easy-to-hard ordering, and the model level, to decide the right moment to enhance the training set with more difficult samples. To measure the difficulty of the examples, they start from the intuition that samples with higher crossentropy and uncertainty are more difficult to translate."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[237553761 | Agrawal et al. | 2021 | Citations: 7]",
                "snippets": "The intuition behind this approach is to mimic the manner in which humans learn. Training samples are organized by levels of difficulty and training proceeds in steps where the model is first trained on a subset of the corpus at a given difficulty level before being trained on another difficulty level, and so on.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 42,
                        "end": 356,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "The intuition behind this approach is to mimic the manner in which humans learn. Training samples are organized by levels of difficulty and training proceeds in steps where the model is first trained on a subset of the corpus at a given difficulty level before being trained on another difficulty level, and so on."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[237581256 | Iter et al. | 2021 | Citations: 22]",
                "snippets": "Dynamic selection (Wees et al., 2017) proposes to increase the selection threshold \u03c4 t as training progresses, gradually transitioning from generic to in-domain training. This gradual adaptation of neural network is related to curriculum learning (Bengio et al., 2009) which studies the ordering of examples and tasks during model training.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7921428 | Wees et al. | 2017 | Citations: 154]": "Intelligent selection of training data has proven a successful technique to simultaneously increase training efficiency and translation performance for phrase-based machine translation (PBMT). With the recent increase in popularity of neural machine translation (NMT), we explore in this paper to what extent and how NMT can also benefit from data selection. While state-of-the-art data selection (Axelrod et al., 2011) consistently performs well for PBMT, we show that gains are substantially lower for NMT. Next, we introduce \u2018dynamic data selection\u2019 for NMT, a method in which we vary the selected subset of training data between different training epochs. Our experiments show that the best results are achieved when applying a technique we call \u2018gradual fine-tuning\u2019, with improvements up to +2.6 BLEU over the original data selection approach and up to +3.1 BLEU over a general baseline."
                },
                "metadata": [
                    {
                        "section_title": "Intelligent Selection",
                        "pdf_hash": "",
                        "start": 1498,
                        "end": 1846,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "7921428",
                            "873046"
                        ],
                        "quote": "Dynamic selection (Wees et al., 2017) proposes to increase the selection threshold \u03c4 t as training progresses, gradually transitioning from generic to in-domain training. This gradual adaptation of neural network is related to curriculum learning (Bengio et al., 2009) which studies the ordering of examples and tasks during model training."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[247762191 | Mohiuddin et al. | 2022 | Citations: 13]",
                "snippets": "Curriculum Learning Inspired by human learners, (Elman, 1993) argues that optimization of neural network training can be accelerated by gradually increasing the difficulty of the concepts. (Bengio et al., 2009) were the first to use the term \"curricu-lum learning\" to refer to the easy-to-hard training strategies in the context of machine learning. Using an easy-to-hard curriculum based on increasing vocabulary size in language model training, they achieved performance improvement. Recent work (Jiang et al., 2015)(Hacohen et al., 2019)(Zhou et al., 2020) shows that manoeuvring the sequence of training data can improve both training efficiency and model accuracy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[102350936 | Hacohen et al. | 2019 | Citations: 449]": "Training neural networks is traditionally done by providing a sequence of random mini-batches sampled uniformly from the entire training data. In this work, we analyze the effect of curriculum learning, which involves the non-uniform sampling of mini-batches, on the training of deep networks, and specifically CNNs trained for image recognition. To employ curriculum learning, the training algorithm must resolve 2 problems: (i) sort the training examples by difficulty; (ii) compute a series of mini-batches that exhibit an increasing level of difficulty. We address challenge (i) using two methods: transfer learning from some competitive ``teacher\" network, and bootstrapping. In our empirical evaluation, both methods show similar benefits in terms of increased learning speed and improved final performance on test data. We address challenge (ii) by investigating different pacing functions to guide the sampling. The empirical investigation includes a variety of network architectures, using images from CIFAR-10, CIFAR-100 and subsets of ImageNet. We conclude with a novel theoretical analysis of curriculum learning, where we show how it effectively modifies the optimization landscape. We then define the concept of an ideal curriculum, and show that under mild conditions it does not change the corresponding global minimum of the optimization function.",
                    "[10891229 | Jiang et al. | 2015 | Citations: 526]": "\n \n Curriculum learning (CL) or self-paced learning (SPL) represents a recently proposed learning regime inspired by the learning process of humans and animals that gradually proceeds from easy to more complex samples in training. The two methods share a similar conceptual learning paradigm, but differ in specific learning schemes. In CL, the curriculum is predetermined by prior knowledge, and remain fixed thereafter. Therefore, this type of method heavily relies on the quality of prior knowledge while ignoring feedback about the learner. In SPL, the curriculum is dynamically determined to adjust to the learning pace of the leaner. However, SPL is unable to deal with prior knowledge, rendering it prone to overfitting. In this paper, we discover the missing link between CL and SPL, and propose a unified framework named self-paced curriculum leaning (SPCL). SPCL is formulated as a concise optimization problem that takes into account both prior knowledge known before training and the learning progress during training. In comparison to human education, SPCL is analogous to \"instructor-student-collaborative\" learning mode, as opposed to \"instructor-driven\" in CL or \"student-driven\" in SPL. Empirically, we show that the advantage of SPCL on two tasks.\n \n"
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 673,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 187
                            },
                            {
                                "start": 188,
                                "end": 347
                            },
                            {
                                "start": 348,
                                "end": 483
                            },
                            {
                                "start": 484,
                                "end": 673
                            }
                        ],
                        "ref_mentions": [
                            "2105042",
                            "873046",
                            "10891229",
                            "102350936",
                            "227275560"
                        ],
                        "quote": "Curriculum Learning Inspired by human learners, (Elman, 1993) argues that optimization of neural network training can be accelerated by gradually increasing the difficulty of the concepts. (Bengio et al., 2009) were the first to use the term \"curricu-lum learning\" to refer to the easy-to-hard training strategies in the context of machine learning. Using an easy-to-hard curriculum based on increasing vocabulary size in language model training, they achieved performance improvement. Recent work (Jiang et al., 2015)(Hacohen et al., 2019)(Zhou et al., 2020) shows that manoeuvring the sequence of training data can improve both training efficiency and model accuracy."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[252280753 | Yu et al. | 2023 | Citations: 23]",
                "snippets": "We study cold-start data selection for text classification with c classes formulated as follows: Given a pool of unlabeled samples D u = {x j } U j=1 and an empty training set D l = \u2205, we aim to fine-tune a pre-trained language model M denoted as f (\u2022; \u03b8) under limited labeling budget |B| interactively: In each round, we use an acquisition function F(\u2022) to query b samples denoted as Q from D u . Next, the acquired samples are labeled and moved from D u to D l . Then we fine-tune the pre-trained language model f (\u2022; \u03b8) with D l to maximize the performance on downstream classification tasks. The above steps can either be one-round (Chang et al., 2021)(Hacohen et al., 2022)) (b = |B| in this case) or repeated for multiple rounds (Yuan et al., 2020) (b = |B|/|Rounds|) until reaching the budget |B|.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[224724415 | Yuan et al. | 2020 | Citations: 184]": "Active learning strives to reduce annotation costs by choosing the most critical examples to label. Typically, the active learning strategy is contingent on the classification model. For instance, uncertainty sampling depends on poorly calibrated model confidence scores. In the cold-start setting, active learning is impractical because of model instability and data scarcity. Fortunately, modern NLP provides an additional source of information: pre-trained language models. The pre-training loss can find examples that surprise the model and should be labeled for efficient fine-tuning. Therefore, we treat the language modeling loss as a proxy for classification uncertainty. With BERT, we develop a simple strategy based on the masked language modeling loss that minimizes labeling costs for text classification. Compared to other baselines, our approach reaches higher accuracy within less sampling iterations and computation time.",
                    "[235755110 | Chang et al. | 2021 | Citations: 41]": "Large-scale pretrained language models have led to dramatic improvements in text generation. Impressive performance can be achieved by finetuning only on a small number of instances (few-shot setting). Nonetheless, almost all previous work simply applies random sampling to select the few-shot training instances. Little to no attention has been paid to the selection strategies and how they would affect model performance. In this work, we present a study on training instance selection in few-shot neural text generation. The selection decision is made based only on the unlabeled data so as to identify the most worthwhile data points that should be annotated under some budget of labeling cost. Based on the intuition that the few-shot training instances should be diverse and representative of the entire data distribution, we propose a simple selection strategy with K-means clustering. We show that even with the naive clustering-based approach, the generation models consistently outperform random sampling on three text generation tasks: data-to-text generation, document summarization and question generation. The code and training data are made available. We hope that this work will call for more attention on this largely unexplored area.",
                    "[246634642 | Hacohen et al. | 2022 | Citations: 123]": "Investigating active learning, we focus on the relation between the number of labeled examples (budget size), and suitable querying strategies. Our theoretical analysis shows a behavior reminiscent of phase transition: typical examples are best queried when the budget is low, while unrepresentative examples are best queried when the budget is large. Combined evidence shows that a similar phenomenon occurs in common classification models. Accordingly, we propose TypiClust -- a deep active learning strategy suited for low budgets. In a comparative empirical investigation of supervised learning, using a variety of architectures and image datasets, TypiClust outperforms all other active learning strategies in the low-budget regime. Using TypiClust in the semi-supervised framework, performance gets an even more significant boost. In particular, state-of-the-art semi-supervised methods trained on CIFAR-10 with 10 labeled examples selected by TypiClust, reach 93.2% accuracy -- an improvement of 39.4% over random selection. Code is available at https://github.com/avihu111/TypiClust."
                },
                "metadata": [
                    {
                        "section_title": "Problem Formulation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 803,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 398
                            },
                            {
                                "start": 399,
                                "end": 465
                            },
                            {
                                "start": 466,
                                "end": 596
                            },
                            {
                                "start": 597,
                                "end": 803
                            }
                        ],
                        "ref_mentions": [
                            "235755110",
                            "246634642",
                            "224724415"
                        ],
                        "quote": "We study cold-start data selection for text classification with c classes formulated as follows: Given a pool of unlabeled samples D u = {x j } U j=1 and an empty training set D l = \u2205, we aim to fine-tune a pre-trained language model M denoted as f (\u2022; \u03b8) under limited labeling budget |B| interactively: In each round, we use an acquisition function F(\u2022) to query b samples denoted as Q from D u . Next, the acquired samples are labeled and moved from D u to D l . Then we fine-tune the pre-trained language model f (\u2022; \u03b8) with D l to maximize the performance on downstream classification tasks. The above steps can either be one-round (Chang et al., 2021)(Hacohen et al., 2022)) (b = |B| in this case) or repeated for multiple rounds (Yuan et al., 2020) (b = |B|/|Rounds|) until reaching the budget |B|."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[257901096 | Liu et al. | 2023 | Citations: 3]",
                "snippets": "Motivated by the success of curriculum learning (Qian et al., 2020)(Guo et al., 2020)Liu et al. 2020), we further introduce a hard-to-easy learning strategy to improve the performance. (Ding et al., 2020) show that pretraining with raw data can improve the performance of NAT by rejuvenating low-frequency words. To keep the merits of low-mode, they further trained the pretrained model on distilled data. We combine this idea with our data selection method by decreasing the ratio of raw data in the training process.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220046693 | Guo et al. | 2020 | Citations: 79]": "The masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks. However, few works have adopted this technique in the sequence-to-sequence models. In this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation~(NAT). Specifically, we first empirically study the functionalities of the encoder and the decoder in NAT models, and find that the encoder takes a more important role than the decoder regarding the translation quality. Therefore, we propose to train the encoder more rigorously by masking the encoder input while training. As for the decoder, we propose to train it based on the consecutive masking of the decoder input with an n-gram loss function to alleviate the problem of translating duplicate words. The two types of masks are applied to the model jointly at the training stage. We conduct experiments on five benchmark machine translation tasks, and our model can achieve 27.69/32.24 BLEU scores on WMT14 English-German/German-English tasks with 5+ times speed up compared with an autoregressive model.",
                    "[221150562 | Qian et al. | 2020 | Citations: 158]": "Recent work on non-autoregressive neural machine translation (NAT) aims at improving the efficiency by parallel decoding without sacrificing the quality. However, existing NAT methods are either inferior to Transformer or require multiple decoding passes, leading to reduced speedup. We propose the Glancing Language Model (GLM) for single-pass parallel generation models. With GLM, we develop Glancing Transformer (GLAT) for machine translation. With only single-pass parallel decoding, GLAT is able to generate high-quality translation with 8\u00d7-15\u00d7 speedup. Note that GLAT does not modify the network architecture, which is a training method to learn word interdependency. Experiments on multiple WMT language directions show that GLAT outperforms all previous single pass non-autoregressive methods, and is nearly comparable to Transformer, reducing the gap to 0.25-0.9 BLEU points.",
                    "[229923128 | Ding et al. | 2020 | Citations: 77]": "Knowledge distillation (KD) is essential for training non-autoregressive translation (NAT) models by reducing the complexity of the raw data with an autoregressive teacher model. In this study, we empirically show that as a side effect of this training, the lexical choice errors on low-frequency words are propagated to the NAT model from the teacher model. To alleviate this problem, we propose to expose the raw data to NAT models to restore the useful information of low-frequency words, which are missed in the distilled data. To this end, we introduce an extra Kullback-Leibler divergence term derived by comparing the lexical choice of NAT model and that embedded in the raw data. Experimental results across language pairs and model architectures demonstrate the effectiveness and universality of the proposed approach. Extensive analyses confirm our claim that our approach improves performance by reducing the lexical choice errors on low-frequency words. Encouragingly, our approach pushes the SOTA NAT performance on the WMT14 English-German and WMT16 Romanian-English datasets up to 27.8 and 33.8 BLEU points, respectively. The source code will be released."
                },
                "metadata": [
                    {
                        "section_title": "Hard-to-Easy Data Selection",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 516,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 182
                            },
                            {
                                "start": 183,
                                "end": 310
                            },
                            {
                                "start": 311,
                                "end": 403
                            },
                            {
                                "start": 404,
                                "end": 516
                            }
                        ],
                        "ref_mentions": [
                            "221150562",
                            "220046693",
                            "229923128"
                        ],
                        "quote": "Motivated by the success of curriculum learning (Qian et al., 2020)(Guo et al., 2020)Liu et al. 2020), we further introduce a hard-to-easy learning strategy to improve the performance. (Ding et al., 2020) show that pretraining with raw data can improve the performance of NAT by rejuvenating low-frequency words. To keep the merits of low-mode, they further trained the pretrained model on distilled data. We combine this idea with our data selection method by decreasing the ratio of raw data in the training process."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[260203057 | Chen et al. | 2023 | Citations: 65]",
                "snippets": "The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 559,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[261048902 | Deng et al. | 2023 | Citations: 5]",
                "snippets": "Curriculum learning, as advocated by (Bengio et al., 2009), prioritizes easy points with low label noise before uniformly training on all data points. While this strategy enhances convergence, it fails to address the issue of skipping redundant points already learned.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Works",
                        "pdf_hash": "",
                        "start": 1029,
                        "end": 1279,
                        "sentence_offsets": [
                            {
                                "start": 1029,
                                "end": 1161
                            },
                            {
                                "start": 1162,
                                "end": 1279
                            }
                        ],
                        "ref_mentions": [
                            "873046"
                        ],
                        "quote": "Curriculum learning, as advocated by (Bengio et al., 2009), prioritizes easy points with low label noise before uniformly training on all data points. While this strategy enhances convergence, it fails to address the issue of skipping redundant points already learned."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[263909051 | Maharana et al. | 2023 | Citations: 34]",
                "snippets": "Analytical theories suggest that higher-quality data can lead to lower test errors in models trained on a fixed data budget. Moreover, a model can be trained on a lower compute budget without compromising performance if a dataset can be stripped of its redundancies. Coreset selection (or data pruning) seeks to select a subset of the training data so as to maximize the performance of models trained on this subset, also referred to as coreset. There are two dominant approaches: (1) geometry-based data selection for maximizing data diversity in the coreset, and (2) functions that assign difficulty scores to samples based on training dynamics. Optimizing for data diversity leads to a coreset that is biased towards easier samples, whereas, selection by difficulty ranking omits easy samples that are necessary for the training of deep learning models. This demonstrates that data diversity and importance scores are two complementary factors that need to be jointly considered during coreset selection.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Analytical theories suggest that higher-quality data can lead to lower test errors in models trained on a fixed data budget. Moreover, a model can be trained on a lower compute budget without compromising performance if a dataset can be stripped of its redundancies. Coreset selection (or data pruning) seeks to select a subset of the training data so as to maximize the performance of models trained on this subset, also referred to as coreset. There are two dominant approaches: (1) geometry-based data selection for maximizing data diversity in the coreset, and (2) functions that assign difficulty scores to samples based on training dynamics. Optimizing for data diversity leads to a coreset that is biased towards easier samples, whereas, selection by difficulty ranking omits easy samples that are necessary for the training of deep learning models. This demonstrates that data diversity and importance scores are two complementary factors that need to be jointly considered during coreset selection.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[264439064 | Fan et al. | 2023 | Citations: 11]",
                "snippets": "Nevertheless, few research have succeeded applying traditional sample-level selection schemes for large langauge model pretraining. Online batch selection methods [Loshchilov and Hutter, 2015(Katharopoulos et al., 2018), Jiang et al., 2019, Schaul et al., 2015] select hard samples with high loss or high gradient norm, which require a second forward/backward pass. That introduces large extra computation costs when the model size is large, which hurts the scalability. On the other side, Campos [2021] shows that the linguistic-based curriculum learning failed to improve on causal language model pretraining.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[3663876 | Katharopoulos et al. | 2018 | Citations: 522]": "Deep neural network training spends most of the computation on examples that are properly handled, and could be ignored. We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on \"informative\" examples, and reduces the variance of the stochastic gradients during training. Our contribution is twofold: first, we derive a tractable upper bound to the per-sample gradient norm, and second we derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup. The resulting scheme can be used by changing a few lines of code in a standard SGD procedure, and we demonstrate experimentally, on image classification, CNN fine-tuning, and RNN training, that for a fixed wall-clock time budget, it provides a reduction of the train losses of up to an order of magnitude and a relative improvement of test errors between 5% and 17%."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1168,
                        "end": 1784,
                        "sentence_offsets": [
                            {
                                "start": 1168,
                                "end": 1299
                            },
                            {
                                "start": 1300,
                                "end": 1538
                            },
                            {
                                "start": 1539,
                                "end": 1643
                            },
                            {
                                "start": 1644,
                                "end": 1784
                            }
                        ],
                        "ref_mentions": [
                            "3663876"
                        ],
                        "quote": "Nevertheless, few research have succeeded applying traditional sample-level selection schemes for large langauge model pretraining. Online batch selection methods [Loshchilov and Hutter, 2015(Katharopoulos et al., 2018), Jiang et al., 2019, Schaul et al., 2015] select hard samples with high loss or high gradient norm, which require a second forward/backward pass. That introduces large extra computation costs when the model size is large, which hurts the scalability. On the other side, Campos [2021] shows that the linguistic-based curriculum learning failed to improve on causal language model pretraining."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[265506572 | Chobey et al. | 2023 | Citations: 5]",
                "snippets": "Curriculum learning (Bengio et al., 2009) refers to training models through a difficulty-based ordering of training examples (i.e. a curriculum), most often \"starting small\" (Elman, 1993) from easy examples before progressing to increasingly difficult sentences. In NLP, curriculum learning has been widely used for Machine Translation (e.g., Platanios et al., 2019), but has also been applied more recently to other natural language understanding tasks (Xu et al., 2020). For a survey see (Soviany et al., 2021); (Wang et al., 2021). \n\nThere are two steps involved in designing a curriculum: assigning a difficulty score to every training example (\"difficulty measurer\") and using these difficulty scores to determine the order in which training examples are presented to the model (\"training scheduler\") (Wang et al., 2021).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[231709290 | Soviany et al. | 2021 | Citations: 359]": "Training machine learning models in a meaningful order, from the easy samples to the hard ones, using curriculum learning can provide performance improvements over the standard training approach based on random data shuffling, without any additional computational costs. Curriculum learning strategies have been successfully employed in all areas of machine learning, in a wide range of tasks. However, the necessity of finding a way to rank the samples from easy to hard, as well as the right pacing function for introducing more difficult data can limit the usage of the curriculum approaches. In this survey, we show how these limits have been tackled in the literature, and we present different curriculum learning instantiations for various tasks in machine learning. We construct a multi-perspective taxonomy of curriculum learning approaches by hand, considering various classification criteria. We further build a hierarchical tree of curriculum learning methods using an agglomerative clustering algorithm, linking the discovered clusters with our taxonomy. At the end, we provide some interesting directions for future work.",
                    "[220045816 | Xu et al. | 2020 | Citations: 206]": "With the great success of pre-trained language models, the pretrain-finetune paradigm now becomes the undoubtedly dominant solution for natural language understanding (NLU) tasks. At the fine-tune stage, target task data is usually introduced in a completely random order and treated equally. However, examples in NLU tasks can vary greatly in difficulty, and similar to human learning procedure, language models can benefit from an easy-to-difficult curriculum. Based on this idea, we propose our Curriculum Learning approach. By reviewing the trainset in a crossed way, we are able to distinguish easy examples from difficult ones, and arrange a curriculum for language models. Without any manual model architecture design or use of external data, our Curriculum Learning approach obtains significant and universal performance improvements on a wide range of NLU tasks.",
                    "[232362223 | Wang et al. | 2021 | Citations: 611]": "Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of <italic>Difficulty Measurer <inline-formula><tex-math notation=\"LaTeX\">$+$</tex-math><alternatives><mml:math><mml:mo>+</mml:mo></mml:math><inline-graphic xlink:href=\"wang-ieq1-3069908.gif\"/></alternatives></inline-formula> Training Scheduler</italic> and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. We also analyze principles to select different CL designs that may benefit practical applications. Finally, we present our insights on the relationships connecting CL and other machine learning concepts including transfer learning, meta-learning, continual learning and active learning, etc., then point out challenges in CL as well as potential future research directions deserving further investigations."
                },
                "metadata": [
                    {
                        "section_title": "Background",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 824,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 262
                            },
                            {
                                "start": 263,
                                "end": 472
                            },
                            {
                                "start": 473,
                                "end": 532
                            },
                            {
                                "start": 535,
                                "end": 824
                            }
                        ],
                        "ref_mentions": [
                            "873046",
                            "220045816",
                            "231709290",
                            "232362223",
                            "232362223"
                        ],
                        "quote": "Curriculum learning (Bengio et al., 2009) refers to training models through a difficulty-based ordering of training examples (i.e. a curriculum), most often \"starting small\" (Elman, 1993) from easy examples before progressing to increasingly difficult sentences. In NLP, curriculum learning has been widely used for Machine Translation (e.g., Platanios et al., 2019), but has also been applied more recently to other natural language understanding tasks (Xu et al., 2020). For a survey see (Soviany et al., 2021); (Wang et al., 2021). \n\nThere are two steps involved in designing a curriculum: assigning a difficulty score to every training example (\"difficulty measurer\") and using these difficulty scores to determine the order in which training examples are presented to the model (\"training scheduler\") (Wang et al., 2021)."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[267094971 | Engstrom et al. | 2024 | Citations: 61]",
                "snippets": "When selecting data for training large-scale models, standard practice is to filter for examples that match human notions of data quality. Such filtering yields qualitatively clean datapoints that intuitively should improve model behavior. However, in practice the opposite can often happen: we find that selecting according to similarity with\"high quality\"data sources may not increase (and can even hurt) performance compared to randomly selecting data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 455,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "When selecting data for training large-scale models, standard practice is to filter for examples that match human notions of data quality. Such filtering yields qualitatively clean datapoints that intuitively should improve model behavior. However, in practice the opposite can often happen: we find that selecting according to similarity with\"high quality\"data sources may not increase (and can even hurt) performance compared to randomly selecting data."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[267095066 | Chen et al. | 2024 | Citations: 4]",
                "snippets": "Considering that humans acquire knowledge in a deliberate order (Evanson et al., 2023), it is plausible that language models might also benefit from a structured learning order when processing training data. Curriculum learning (Bengio et al., 2009) has been suggested as a method to organize the learning process by progressively increasing the complexity of the training data. However, most prior studies have concentrated on sample-level data and smaller datasets. Chen et al. ( 2023) employed a skills-based framework for training data selection and continuous pretraining with a 3B-parameter language model. This approach achieved greater accuracy compared to the baseline method of uniform data source sampling, suggesting the potential efficacy of strategic data scheduling.\n\nIn training the Orion-14B model, we intentionally develop a data scheduling strategy that organizes training data to incrementally increase its complexity. We divide the training data into several stages based on the data sources and their complexity. These stages are differentiated by the mix ratios of data sources. Initial stages primarily include data with common knowledge, such as web pages and news articles. In the subsequent stages, we gradually augment the proportion of data containing more complex knowledge, including textbooks, academic papers, and source code.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Data Scheduling",
                        "pdf_hash": "",
                        "start": 254,
                        "end": 1613,
                        "sentence_offsets": [
                            {
                                "start": 85,
                                "end": 300
                            },
                            {
                                "start": 301,
                                "end": 508
                            },
                            {
                                "start": 509,
                                "end": 679
                            },
                            {
                                "start": 680,
                                "end": 768
                            },
                            {
                                "start": 769,
                                "end": 913
                            },
                            {
                                "start": 914,
                                "end": 1082
                            },
                            {
                                "start": 1085,
                                "end": 1240
                            },
                            {
                                "start": 1241,
                                "end": 1336
                            },
                            {
                                "start": 1337,
                                "end": 1403
                            },
                            {
                                "start": 1404,
                                "end": 1501
                            },
                            {
                                "start": 1502,
                                "end": 1661
                            }
                        ],
                        "ref_mentions": [
                            "873046"
                        ],
                        "quote": "Considering that humans acquire knowledge in a deliberate order (Evanson et al., 2023), it is plausible that language models might also benefit from a structured learning order when processing training data. Curriculum learning (Bengio et al., 2009) has been suggested as a method to organize the learning process by progressively increasing the complexity of the training data. However, most prior studies have concentrated on sample-level data and smaller datasets. Chen et al. ( 2023) employed a skills-based framework for training data selection and continuous pretraining with a 3B-parameter language model. This approach achieved greater accuracy compared to the baseline method of uniform data source sampling, suggesting the potential efficacy of strategic data scheduling.\n\nIn training the Orion-14B model, we intentionally develop a data scheduling strategy that organizes training data to incrementally increase its complexity. We divide the training data into several stages based on the data sources and their complexity. These stages are differentiated by the mix ratios of data sources. Initial stages primarily include data with common knowledge, such as web pages and news articles. In the subsequent stages, we gradually augment the proportion of data containing more complex knowledge, including textbooks, academic papers, and source code."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[267740312 | Mekala et al. | 2024 | Citations: 21]",
                "snippets": "In this paper, we delve into the measurement of sample difficulty from the model's perspective. Drawing inspiration from the learning order metric in (Mekala et al., 2022), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered. We then select the most difficult sample subsets based on this ranking and instruction-tune a language model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249060677 | Mekala et al. | 2022 | Citations: 20]": "Weakly supervised text classification methods typically train a deep neural classifier based on pseudo-labels. The quality of pseudo-labels is crucial to final performance but they are inevitably noisy due to their heuristic nature, so selecting the correct ones has a huge potential for performance boost. One straightforward solution is to select samples based on the softmax probability scores in the neural classifier corresponding to their pseudo-labels. However, we show through our experiments that such solutions are ineffective and unstable due to the erroneously high-confidence predictions from poorly calibrated models. Recent studies on the memorization effects of deep neural models suggest that these models first memorize training samples with clean labels and then those with noisy labels. Inspired by this observation, we propose a novel pseudo-label selection method LOPS that takes learning order of samples into consideration. We hypothesize that the learning order reflects the probability of wrong annotation in terms of ranking, and therefore, propose to select the samples that are learnt earlier. LOPS can be viewed as a strong performance-boost plug-in to most of existing weakly-supervised text classification methods, as confirmed in extensive experiments on four real-world datasets."
                },
                "metadata": [
                    {
                        "section_title": "arXiv:2402.10430v1 [cs.CL] 16 Feb 2024",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 539,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 95
                            },
                            {
                                "start": 96,
                                "end": 330
                            },
                            {
                                "start": 331,
                                "end": 429
                            },
                            {
                                "start": 430,
                                "end": 539
                            }
                        ],
                        "ref_mentions": [
                            "249060677"
                        ],
                        "quote": "In this paper, we delve into the measurement of sample difficulty from the model's perspective. Drawing inspiration from the learning order metric in (Mekala et al., 2022), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered. We then select the most difficult sample subsets based on this ranking and instruction-tune a language model."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[268363364 | Yang et al. | 2024 | Citations: 23]",
                "snippets": "Despite the effectiveness of data selection for large language models (LLMs) during pretraining and instruction fine-tuning phases, improving data efficiency in supervised fine-tuning (SFT) for specialized domains poses significant challenges due to the complexity of fine-tuning data. To bridge this gap, we introduce an effective and scalable data selection method for SFT, SmallToLarge (S2L), which leverages training trajectories from small models to guide the data selection for larger models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Despite the effectiveness of data selection for large language models (LLMs) during pretraining and instruction fine-tuning phases, improving data efficiency in supervised fine-tuning (SFT) for specialized domains poses significant challenges due to the complexity of fine-tuning data. To bridge this gap, we introduce an effective and scalable data selection method for SFT, SmallToLarge (S2L), which leverages training trajectories from small models to guide the data selection for larger models.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[269756933 | Kim et al. | 2024 | Citations: 10]",
                "snippets": "The rapid advancement of Large Language Models (LLMs) has improved text understanding and generation but poses challenges in computational resources. This study proposes a curriculum learning-inspired, data-centric training strategy that begins with simpler tasks and progresses to more complex ones, using criteria such as prompt length, attention scores, and loss values to structure the training data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 404,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "The rapid advancement of Large Language Models (LLMs) has improved text understanding and generation but poses challenges in computational resources. This study proposes a curriculum learning-inspired, data-centric training strategy that begins with simpler tasks and progresses to more complex ones, using criteria such as prompt length, attention scores, and loss values to structure the training data."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[270371045 | Yu et al. | 2024 | Citations: 37]",
                "snippets": "On the other hand, many researchers have proposed curriculum learning strategies that dynamically adjust the data distribution in the pretraining [11; 24; 47; 51; 56; 62]. ELECTRA-style models (208229926) incorporated curriculum learning in their pretraining process by synchronously training the model with an auxiliary generator, which provided more and more difficult training signals for the discriminator. This implicit curriculum significantly improved the pretraining efficiency on denoising language models [4; 11; 19; 39; 65]. Other methods have explicitly designed the curriculum for pretraining data selection, such as decreasing gradient norms (Thakkar et al., 2023), least certainty [24; 51], and increasing expertise (Wettig et al., 2024), demonstrating the benefits of adapting the pretraining data signal according to the model's ever-changing preferences.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[267681974 | Wettig et al. | 2024 | Citations: 79]": "Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that can capture human intuitions about data quality. In this paper, we investigate four qualities - writing style, required expertise, facts&trivia, and educational value - and find that LLMs are able to discern these qualities, especially when making pairwise judgments of texts. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity. When we sample using quality ratings as logits over documents, our models obtain lower perplexity and stronger in-context learning performance than baselines. Our best model is based on educational value and performs similarly to a model trained with uniform sampling for 50% more steps. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications.",
                    "[264935129 | Thakkar et al. | 2023 | Citations: 26]": "Language Models (LMs) pre-trained with self-supervision on large text corpora have become the default starting point for developing models for various NLP tasks. Once the pre-training corpus has been assembled, all data samples in the corpus are treated with equal importance during LM pre-training. However, due to varying levels of relevance and quality of data, equal importance to all the data samples may not be the optimal choice. While data reweighting has been explored in the context of task-specific supervised learning and LM fine-tuning, model-driven reweighting for pre-training data has not been explored. We fill this important gap and propose PRESENCE, a method for jointly reweighting samples by leveraging self-influence (SI) scores as an indicator of sample importance and pre-training. PRESENCE promotes novelty and stability for model pre-training. Through extensive analysis spanning multiple model sizes, datasets, and tasks, we present PRESENCE as an important first step in the research direction of sample reweighting for pre-training language models."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 179,
                        "end": 1009,
                        "sentence_offsets": [
                            {
                                "start": 179,
                                "end": 350
                            },
                            {
                                "start": 351,
                                "end": 582
                            },
                            {
                                "start": 583,
                                "end": 707
                            },
                            {
                                "start": 708,
                                "end": 1009
                            }
                        ],
                        "ref_mentions": [
                            "208229926",
                            "264935129",
                            "267681974"
                        ],
                        "quote": "On the other hand, many researchers have proposed curriculum learning strategies that dynamically adjust the data distribution in the pretraining [11; 24; 47; 51; 56; 62]. ELECTRA-style models (208229926) incorporated curriculum learning in their pretraining process by synchronously training the model with an auxiliary generator, which provided more and more difficult training signals for the discriminator. This implicit curriculum significantly improved the pretraining efficiency on denoising language models [4; 11; 19; 39; 65]. Other methods have explicitly designed the curriculum for pretraining data selection, such as decreasing gradient norms (Thakkar et al., 2023), least certainty [24; 51], and increasing expertise (Wettig et al., 2024), demonstrating the benefits of adapting the pretraining data signal according to the model's ever-changing preferences."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[270620248 | Liu et al. | 2024 | Citations: 2]",
                "snippets": "Recent research highlights that data quality is more critical than data quantity for effective fine-tuning (Nakkiran et al., 2019)(Shumailov et al., 2024)(Zhou et al., 2023)Jindal et al., 2024). As a result, several data curation techniques have been proposed, such as data selection (Chen et al., 2023)(Li et al., 2023), data evolution (Wang et al., 2022)Xu et al., 2023), and data reflection (Mukherjee et al., 2023)(Yin et al., 2023). Data selection, in particular, involves choosing a high-quality subset from a candidate dataset based on specific selection criteria, enhancing the model's performance while improving training efficiency by reducing the number of samples.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207808916 | Nakkiran et al. | 2019 | Citations: 945]": "We show that a variety of modern deep learning tasks exhibit a \u2018double-descent\u2019 phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.",
                    "[254877310 | Wang et al. | 2022 | Citations: 2251]": "Large \u201cinstruction-tuned\u201d language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.",
                    "[258822910 | Zhou et al. | 2023 | Citations: 850]": "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.",
                    "[258841263 | Yin et al. | 2023 | Citations: 39]": "Instruction tuning has emerged to enhance the capabilities of large language models (LLMs) to comprehend instructions and generate appropriate responses. Existing methods either manually annotate or employ LLM (e.g., GPT-series) to generate data for instruction tuning. However, they often overlook associating instructions with existing annotated datasets. In this paper, we propose Dynosaur, a dynamic growth paradigm for the automatic curation of instruction-tuning data. Based on the metadata of existing datasets, we use LLMs to automatically construct instruction-tuning data by identifying relevant data fields and generating appropriate instructions. By leveraging the existing annotated datasets, Dynosaur offers several advantages: 1) it reduces the API cost for generating instructions (e.g., it costs less than $12 USD by calling GPT-3.5-turbo for generating 800K instruction tuning samples; 2) it provides high-quality data for instruction tuning (e.g., it performs better than Alpaca and Flan on Super-NI and Longform with comparable data sizes); and 3) it supports the continuous improvement of models by generating instruction-tuning data when a new annotated dataset becomes available. We further investigate a continual learning scheme for learning with the ever-growing instruction-tuning dataset, and demonstrate that replaying tasks with diverse instruction embeddings not only helps mitigate forgetting issues but generalizes to unseen tasks better. Code and data are available at https://github.com/WadeYin9712/Dynosaur.",
                    "[259075316 | Mukherjee et al. | 2023 | Citations: 276]": "Recent research has focused on enhancing the capability of smaller models through imitation learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from limited imitation signals from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in overestimating the small model's capability as they tend to learn to imitate the style, but not the reasoning process of LFMs. To address these challenges, we develop Orca (We are working with our legal team to publicly release a diff of the model weights in accordance with LLaMA's release policy to be published at https://aka.ms/orca-lm), a 13-billion parameter model that learns to imitate the reasoning process of LFMs. Orca learns from rich signals from GPT-4 including explanation traces; step-by-step thought processes; and other complex instructions, guided by teacher assistance from ChatGPT. To promote this progressive learning, we tap into large-scale and diverse imitation data with judicious sampling and selection. Orca surpasses conventional state-of-the-art instruction-tuned models such as Vicuna-13B by more than 100% in complex zero-shot reasoning benchmarks like Big-Bench Hard (BBH) and 42% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH benchmark and shows competitive performance (4 pts gap with optimized system message) in professional and academic examinations like the SAT, LSAT, GRE, and GMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our research indicates that learning from step-by-step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills.",
                    "[261076515 | Li et al. | 2023 | Citations: 211]": "In the realm of Large Language Models (LLMs), the balance between instruction data quality and quantity is a focal point. Recognizing this, we introduce a self-guided methodology for LLMs to autonomously discern and select cherry samples from open-source datasets, effectively minimizing manual curation and potential cost for instruction tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD) metric, emerges as a pivotal metric to identify discrepancies between a model\u2019s expected responses and its intrinsic generation capability. Through the application of IFD, cherry samples can be pinpointed, leading to a marked uptick in model training efficiency. Empirical validations on datasets like Alpaca and WizardLM underpin our findings; with a mere 10% of original data input, our strategy showcases improved results. This synthesis of self-guided cherry-picking and the IFD metric signifies a transformative leap in the instruction tuning of LLMs, promising both efficiency and resource-conscious advancements. Codes, data, and models are available.",
                    "[271448069 | Shumailov et al. | 2024 | Citations: 277]": "Stable diffusion revolutionized image creation from descriptive text. GPT-2 (ref.\u20091), GPT-3(.5) (ref.\u20092) and GPT-4 (ref.\u20093) demonstrated high performance across a variety of language tasks. ChatGPT introduced such language models to the public. It is now clear that generative artificial intelligence (AI) such as large language models (LLMs) is here to stay and will substantially change the ecosystem of online text and images. Here we consider what may happen to GPT-{n} once LLMs contribute much of the text found online. We find that indiscriminate use of model-generated content in training causes irreversible defects in the resulting models, in which tails of the original content distribution disappear. We refer to this effect as \u2018model collapse\u2019 and show that it can occur in LLMs as well as in variational autoencoders (VAEs) and Gaussian mixture models (GMMs). We build theoretical intuition behind the phenomenon and portray its ubiquity among all learned generative models. We demonstrate that it must be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of LLM-generated content in data crawled from the Internet."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 294,
                        "end": 967,
                        "sentence_offsets": [
                            {
                                "start": 294,
                                "end": 486
                            },
                            {
                                "start": 487,
                                "end": 728
                            },
                            {
                                "start": 729,
                                "end": 967
                            }
                        ],
                        "ref_mentions": [
                            "207808916",
                            "271448069",
                            "258822910",
                            "259937133",
                            "261076515",
                            "254877310",
                            "259075316",
                            "258841263"
                        ],
                        "quote": "Recent research highlights that data quality is more critical than data quantity for effective fine-tuning (Nakkiran et al., 2019)(Shumailov et al., 2024)(Zhou et al., 2023)Jindal et al., 2024). As a result, several data curation techniques have been proposed, such as data selection (Chen et al., 2023)(Li et al., 2023), data evolution (Wang et al., 2022)Xu et al., 2023), and data reflection (Mukherjee et al., 2023)(Yin et al., 2023). Data selection, in particular, involves choosing a high-quality subset from a candidate dataset based on specific selection criteria, enhancing the model's performance while improving training efficiency by reducing the number of samples."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[271064746 | Yin et al. | 2024 | Citations: 27]",
                "snippets": "Data is the cornerstone of large language models (LLMs), but not all data is useful for model learning. Carefully selected data can better elicit the capabilities of LLMs with much less computational overhead. Most methods concentrate on evaluating the quality of individual samples in data selection, while the combinatorial effects among samples are neglected. Even if each sample is of perfect quality, their combinations may be suboptimal in teaching LLMs due to their intrinsic homogeneity or contradiction.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 512,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Data is the cornerstone of large language models (LLMs), but not all data is useful for model learning. Carefully selected data can better elicit the capabilities of LLMs with much less computational overhead. Most methods concentrate on evaluating the quality of individual samples in data selection, while the combinatorial effects among samples are neglected. Even if each sample is of perfect quality, their combinations may be suboptimal in teaching LLMs due to their intrinsic homogeneity or contradiction."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[271710435 | Qin et al. | 2024 | Citations: 13]",
                "snippets": "We aim at finding the most informative subset S b \u2282 S from the entire set S under the given budget |S b | \u2264 b. Mathematically, the selection of S b requires the quantitative evaluation q(\u2022) on each datapoint x i and an elaborated sampling mechanism \u03c0: \n\nwhere \u03c0(\u2022, b) denotes the sampling process with a maximum b datapoints. With respect to the detailed implementation of \u03c0, either an iterative, greedy algorithm or a batch-wise heuristic rule can be adopted for compatibility with q(\u2022). The expected benefits of such selection include: 1) the reduction of noise by ignoring those mislabeled, mismatched instruction-response pairs, 2) the re-balance of data distributions by down-sampling those easy, common, and similar examples while up-sampling hard, rare ones, and 3) the expedition of training in return for efficient iterations of LLMs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Data Assessment and Selection",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 843,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 110
                            },
                            {
                                "start": 111,
                                "end": 251
                            },
                            {
                                "start": 254,
                                "end": 325
                            },
                            {
                                "start": 326,
                                "end": 488
                            },
                            {
                                "start": 489,
                                "end": 843
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We aim at finding the most informative subset S b \u2282 S from the entire set S under the given budget |S b | \u2264 b. Mathematically, the selection of S b requires the quantitative evaluation q(\u2022) on each datapoint x i and an elaborated sampling mechanism \u03c0: \n\nwhere \u03c0(\u2022, b) denotes the sampling process with a maximum b datapoints. With respect to the detailed implementation of \u03c0, either an iterative, greedy algorithm or a batch-wise heuristic rule can be adopted for compatibility with q(\u2022). The expected benefits of such selection include: 1) the reduction of noise by ignoring those mislabeled, mismatched instruction-response pairs, 2) the re-balance of data distributions by down-sampling those easy, common, and similar examples while up-sampling hard, rare ones, and 3) the expedition of training in return for efficient iterations of LLMs."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[273233719 | Bai et al. | 2024 | Citations: 6]",
                "snippets": "Efficient data selection is crucial for the pretraining of large language models (LLMs), as the quality of training data significantly impacts the statistical efficiency of the training procedure and the model performance (Brown, 2020;(Du et al., 2021)(Chowdhery et al., 2022). Recently, we have witnessed numerous approaches, such as filtering high-quality data (Xie et al., 2023)(Wettig et al., 2024), mixing data from multiple domains (Xie et al., 2023)Liu et al., 2024), and selecting data that optimally boosts downstream task performance dynamically (Engstrom et al., 2024;Yu et al., 2024), which aim to improve data efficiency by prioritizing more informative training samples.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[267681974 | Wettig et al. | 2024 | Citations: 79]": "Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that can capture human intuitions about data quality. In this paper, we investigate four qualities - writing style, required expertise, facts&trivia, and educational value - and find that LLMs are able to discern these qualities, especially when making pairwise judgments of texts. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity. When we sample using quality ratings as logits over documents, our models obtain lower perplexity and stronger in-context learning performance than baselines. Our best model is based on educational value and performs similarly to a model trained with uniform sampling for 50% more steps. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications.",
                    "[245124124 | Du et al. | 2021 | Citations: 826]": "Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.",
                    "[247951931 | Chowdhery et al. | 2022 | Citations: 6293]": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",
                    "[256627727 | Xie et al. | 2023 | Citations: 195]": "Selecting a suitable pretraining dataset is crucial for both general-domain (e.g., GPT-3) and domain-specific (e.g., Codex) language models (LMs). We formalize this problem as selecting a subset of a large raw unlabeled dataset to match a desired target distribution given unlabeled target samples. Due to the scale and dimensionality of the raw text data, existing methods use simple heuristics or require human experts to manually curate data. Instead, we extend the classic importance resampling approach used in low-dimensions for LM data selection. We propose Data Selection with Importance Resampling (DSIR), an efficient and scalable framework that estimates importance weights in a reduced feature space for tractability and selects data with importance resampling according to these weights. We instantiate the DSIR framework with hashed n-gram features for efficiency, enabling the selection of 100M documents from the full Pile dataset in 4.5 hours. To measure whether hashed n-gram features preserve the aspects of the data that are relevant to the target, we define KL reduction, a data metric that measures the proximity between the selected pretraining data and the target on some feature space. Across 8 data selection methods (including expert selection), KL reduction on hashed n-gram features highly correlates with average downstream accuracy (r=0.82). When selecting data for continued pretraining on a specific domain, DSIR performs comparably to expert curation across 8 target distributions. When pretraining general-domain models (target is Wikipedia and books), DSIR improves over random selection and heuristic filtering baselines by 2-2.5% on the GLUE benchmark. Code is available at https://github.com/p-lambda/dsir.",
                    "[258741043 | Xie et al. | 2023 | Citations: 203]": "The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to set the domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% points over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no knowledge of downstream tasks, even matches the performance of using domain weights tuned on downstream tasks."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 683,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 275
                            },
                            {
                                "start": 276,
                                "end": 683
                            }
                        ],
                        "ref_mentions": [
                            "245124124",
                            "247951931",
                            "256627727",
                            "267681974",
                            "258741043"
                        ],
                        "quote": "Efficient data selection is crucial for the pretraining of large language models (LLMs), as the quality of training data significantly impacts the statistical efficiency of the training procedure and the model performance (Brown, 2020;(Du et al., 2021)(Chowdhery et al., 2022). Recently, we have witnessed numerous approaches, such as filtering high-quality data (Xie et al., 2023)(Wettig et al., 2024), mixing data from multiple domains (Xie et al., 2023)Liu et al., 2024), and selecting data that optimally boosts downstream task performance dynamically (Engstrom et al., 2024;Yu et al., 2024), which aim to improve data efficiency by prioritizing more informative training samples."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[273350576 | Jiang et al. | 2024 | Citations: 22]",
                "snippets": "Data curation and selection. For current large language models, compute often poses a greater constraint than data availability, making data selection crucial (Albalak et al., 2024). A widely used approach is data filtering (Soboleva et al., 2023;Penedo et al., 2023;2024), where undesirable data points are removed based on heuristics like perplexity, repetition (Lee et al., 2021), or semantic similarity (Abbas et al., 2023). This filtering process is foundational for constructing most largescale datasets today. After filtering, data is often categorized into subsets or domains (e.g., code, books), and deciding how much data to use from each domain becomes an important step. \n\nTwo primary strategies for data selection are prevalent: one focuses on deciding whether individual data points should be included based on various criteria (Mindermann et al., 2022)Engstrom et al., 2024), and the other uses all available data but samples from different domains with varying probabilities (Xie et al., 2024;Fan et al., 2023;(Albalak et al., 2023). While data selection aims to enhance training efficiency, these methods may introduce considerable computational overhead (Xie et al., 2024;(Chen et al., 2023)Fan et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[260203057 | Chen et al. | 2023 | Citations: 65]": "The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training. Using this intuition, our framework formalizes the notion of a skill and of an ordered set of skills in terms of the associated data. First, using both synthetic and real data, we demonstrate that these ordered skill sets exist, and that their existence enables more advanced skills to be learned with less data when we train on their prerequisite skills. Second, using our proposed framework, we introduce an online data sampling algorithm, Skill-It, over mixtures of skills for both continual pre-training and fine-tuning regimes, where the objective is to efficiently learn multiple skills in the former and an individual skill in the latter. On the LEGO synthetic in the continual pre-training setting, Skill-It obtains 36.5 points higher accuracy than random sampling. On the Natural Instructions dataset in the fine-tuning setting, Skill-It reduces the validation loss on the target skill by 13.6% versus training on data associated with the target skill itself. We apply our skills framework on the recent RedPajama dataset to continually pre-train a 3B-parameter LM, achieving higher accuracy on the LM Evaluation Harness with 1B tokens than the baseline approach of sampling uniformly over data sources with 3B tokens.",
                    "[235743024 | Mindermann et al. | 2022 | Citations: 164]": "Training on web-scale data can take months. But most computation and time is wasted on redundant and noisy points that are already learnt or not learnable. To accelerate training, we introduce Reducible Holdout Loss Selection (RHO-LOSS), a simple but principled technique which selects approximately those points for training that most reduce the model's generalization loss. As a result, RHO-LOSS mitigates the weaknesses of existing data selection methods: techniques from the optimization literature typically select 'hard' (e.g. high loss) points, but such points are often noisy (not learnable) or less task-relevant. Conversely, curriculum learning prioritizes 'easy' points, but such points need not be trained on once learned. In contrast, RHO-LOSS selects points that are learnable, worth learning, and not yet learnt. RHO-LOSS trains in far fewer steps than prior art, improves accuracy, and speeds up training on a wide range of datasets, hyperparameters, and architectures (MLPs, CNNs, and BERT). On the large web-scraped image dataset Clothing-1M, RHO-LOSS trains in 18x fewer steps and reaches 2% higher final accuracy than uniform data shuffling.",
                    "[265658930 | Albalak et al. | 2023 | Citations: 45]": "The data used to pretrain large language models has a decisive impact on a model's downstream performance, which has led to a large body of work on data selection methods that aim to automatically determine the most suitable data to use for pretraining. Existing data selection methods suffer from slow and computationally expensive processes, a problem amplified by the increasing size of models and of pretraining datasets. Data mixing, on the other hand, reduces the complexity of data selection by grouping data points together and determining sampling probabilities across entire groups. However, data mixing proportions are typically fixed before training and therefore cannot adapt to changing training dynamics. To address these limitations, we develop an efficient algorithm for Online Data Mixing (ODM) that combines elements from both data selection and data mixing. Based on multi-armed bandit algorithms, our online approach optimizes the data mixing proportions during training. Remarkably, our method trains a model that reaches the final perplexity of the next best method with 19\\% fewer training iterations, and improves performance on the 5-shot MMLU benchmark by 1.9% relative accuracy, while adding negligible wall-clock time during pretraining."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORKS",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1225,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 28
                            },
                            {
                                "start": 29,
                                "end": 182
                            },
                            {
                                "start": 183,
                                "end": 428
                            },
                            {
                                "start": 429,
                                "end": 516
                            },
                            {
                                "start": 517,
                                "end": 682
                            },
                            {
                                "start": 685,
                                "end": 1048
                            },
                            {
                                "start": 1049,
                                "end": 1225
                            }
                        ],
                        "ref_mentions": [
                            "235743024",
                            "265658930",
                            "260203057"
                        ],
                        "quote": "Data curation and selection. For current large language models, compute often poses a greater constraint than data availability, making data selection crucial (Albalak et al., 2024). A widely used approach is data filtering (Soboleva et al., 2023;Penedo et al., 2023;2024), where undesirable data points are removed based on heuristics like perplexity, repetition (Lee et al., 2021), or semantic similarity (Abbas et al., 2023). This filtering process is foundational for constructing most largescale datasets today. After filtering, data is often categorized into subsets or domains (e.g., code, books), and deciding how much data to use from each domain becomes an important step. \n\nTwo primary strategies for data selection are prevalent: one focuses on deciding whether individual data points should be included based on various criteria (Mindermann et al., 2022)Engstrom et al., 2024), and the other uses all available data but samples from different domains with varying probabilities (Xie et al., 2024;Fan et al., 2023;(Albalak et al., 2023). While data selection aims to enhance training efficiency, these methods may introduce considerable computational overhead (Xie et al., 2024;(Chen et al., 2023)Fan et al., 2023)."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[273502726 | Na et al. | 2024 | Citations: 5]",
                "snippets": "Efficient data selection Given that data ablations on large language models is expensive, one class of approaches relies on approximating them on smaller models. Relevant work studies scaling laws for model parameters vs training tokens (Hoffmann et al., 2022;(Biderman et al., 2023), empirical effects of including or excluding different sources of data (Longpre et al., 2023), and the effects of training over multiple epochs vs new training tokens (Muennighoff et al., 2023). Previous work has also explored improving domain-specific fit via continued pre-training (Gururangan et al., 2020), predicting domain fit using lexical features (Reid et al., 2022), or improving general test-time adaptation via dynamic data selection, either by distributionally robust optimization with a small proxy model (Oren et al., 2019)Xie et al., 2023) or online using a multi-armed bandit approach (Albalak et al., 2023). Additional previous works aim to adapt to known downstream tasks via data selection, including at the individual example level (Wang et al., 2020) or even by explicitly fine-tuning models on many tasks (Aghajanyan et al., 2021).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[202541009 | Oren et al. | 2019 | Citations: 175]": "Language models are generally trained on data spanning a wide range of topics (e.g., news, reviews, fiction), but they might be applied to an a priori unknown target distribution (e.g., restaurant reviews). In this paper, we first show that training on text outside the test distribution can degrade test performance when using standard maximum likelihood (MLE) training. To remedy this without the knowledge of the test distribution, we propose an approach which trains a model that performs well over a wide range of potential test distributions. In particular, we derive a new distributionally robust optimization (DRO) procedure which minimizes the loss of the model over the worst-case mixture of topics with sufficient overlap with the training distribution. Our approach, called topic conditional value at risk (topic CVaR), obtains a 5.5 point perplexity reduction over MLE when the language models are trained on a mixture of Yelp reviews and news and tested only on reviews.",
                    "[216080466 | Gururangan et al. | 2020 | Citations: 2436]": "Language models pretrained on text from a wide variety of sources form the foundation of today\u2019s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task\u2019s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.",
                    "[231718729 | Aghajanyan et al. | 2021 | Citations: 269]": "We propose pre-finetuning, an additional large-scale learning stage between language model pre-training and fine-tuning. Pre-finetuning is massively multi-task learning (around 50 datasets, over 4.8 million total labeled examples), and is designed to encourage learning of representations that generalize better to many different tasks. We show that pre-finetuning consistently improves performance for pretrained discriminators (e.g. RoBERTa) and generation models (e.g. BART) on a wide range of tasks (sentence prediction, commonsense reasoning, MRC, etc.), while also significantly improving sample efficiency during fine-tuning. We also show that large-scale multi-tasking is crucial; pre-finetuning can hurt performance when few tasks are used up until a critical point (usually above 15) after which performance improves linearly in the number of tasks.",
                    "[252907924 | Reid et al. | 2022 | Citations: 22]": "We present M2D2, a fine-grained, massively multi-domain corpus for studying domain adaptation in language models (LMs). M2D2 consists of 8.5B tokens and spans 145 domains extracted from Wikipedia and Semantic Scholar. Using ontologies derived from Wikipedia and ArXiv categories, we organize the domains in each data source into 22 groups. This two-level hierarchy enables the study of relationships between domains and their effects on in- and out-of-domain performance after adaptation. We also present a number of insights into the nature of effective domain adaptation in LMs, as examples of the new types of studies M2D2 enables. To improve in-domain performance, we show the benefits of adapting the LM along a domain hierarchy; adapting to smaller amounts of fine-grained domain-specific data can lead to larger in-domain performance gains than larger amounts of weakly relevant data. We further demonstrate a trade-off between in-domain specialization and out-of-domain generalization within and across ontologies, as well as a strong correlation between out-of-domain performance and lexical overlap between domains.",
                    "[257921893 | Biderman et al. | 2023 | Citations: 1306]": "How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \\textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \\url{https://github.com/EleutherAI/pythia}.",
                    "[258888192 | Muennighoff et al. | 2023 | Citations: 226]": "The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github.com/huggingface/datablations.",
                    "[265658930 | Albalak et al. | 2023 | Citations: 45]": "The data used to pretrain large language models has a decisive impact on a model's downstream performance, which has led to a large body of work on data selection methods that aim to automatically determine the most suitable data to use for pretraining. Existing data selection methods suffer from slow and computationally expensive processes, a problem amplified by the increasing size of models and of pretraining datasets. Data mixing, on the other hand, reduces the complexity of data selection by grouping data points together and determining sampling probabilities across entire groups. However, data mixing proportions are typically fixed before training and therefore cannot adapt to changing training dynamics. To address these limitations, we develop an efficient algorithm for Online Data Mixing (ODM) that combines elements from both data selection and data mixing. Based on multi-armed bandit algorithms, our online approach optimizes the data mixing proportions during training. Remarkably, our method trains a model that reaches the final perplexity of the next best method with 19\\% fewer training iterations, and improves performance on the 5-shot MMLU benchmark by 1.9% relative accuracy, while adding negligible wall-clock time during pretraining."
                },
                "metadata": [
                    {
                        "section_title": "Efficiency analysis",
                        "pdf_hash": "",
                        "start": 549,
                        "end": 1686,
                        "sentence_offsets": [
                            {
                                "start": 549,
                                "end": 710
                            },
                            {
                                "start": 711,
                                "end": 1026
                            },
                            {
                                "start": 1027,
                                "end": 1457
                            },
                            {
                                "start": 1458,
                                "end": 1686
                            }
                        ],
                        "ref_mentions": [
                            "257921893",
                            "258888192",
                            "216080466",
                            "252907924",
                            "202541009",
                            "265658930",
                            "231718729"
                        ],
                        "quote": "Efficient data selection Given that data ablations on large language models is expensive, one class of approaches relies on approximating them on smaller models. Relevant work studies scaling laws for model parameters vs training tokens (Hoffmann et al., 2022;(Biderman et al., 2023), empirical effects of including or excluding different sources of data (Longpre et al., 2023), and the effects of training over multiple epochs vs new training tokens (Muennighoff et al., 2023). Previous work has also explored improving domain-specific fit via continued pre-training (Gururangan et al., 2020), predicting domain fit using lexical features (Reid et al., 2022), or improving general test-time adaptation via dynamic data selection, either by distributionally robust optimization with a small proxy model (Oren et al., 2019)Xie et al., 2023) or online using a multi-armed bandit approach (Albalak et al., 2023). Additional previous works aim to adapt to known downstream tasks via data selection, including at the individual example level (Wang et al., 2020) or even by explicitly fine-tuning models on many tasks (Aghajanyan et al., 2021)."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[274130626 | Khera et al. | 2024 | Citations: 1]",
                "snippets": "Data selection is a well known problem in literature with many algorithms such as filtering, coresets, importance sampling and more working towards the same goal [22]. Here, we present data selection in the domain of LLMs and natural language and how they can enable data efficient training. \n\nLIMA, proposed by [30] employs a small high quality dataset for fine-tuning. They show that by carefully curating only a 1000 data points, they are able to achieve remarkable performance which is generalizable to unseen data as well, thereby suggesting limited high quality tuning data is sufficient. \n\nAnother prominent study by [16] demonstrates that sample quality can reduce the data requirement without compromising the downstream performance. They investigate data engineering strategies in the fine-tuning paradigm from multiple facets to identify the characterises of good instruction tuning data. DEITA, the model family tuned by their proposed strategy to automatically select a complex and high quality dataset achieves comparable performance to open source models while using only a tenth of the data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "A.2 Related Works",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1107,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 167
                            },
                            {
                                "start": 168,
                                "end": 291
                            },
                            {
                                "start": 294,
                                "end": 370
                            },
                            {
                                "start": 371,
                                "end": 594
                            },
                            {
                                "start": 597,
                                "end": 742
                            },
                            {
                                "start": 743,
                                "end": 899
                            },
                            {
                                "start": 900,
                                "end": 1107
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Data selection is a well known problem in literature with many algorithms such as filtering, coresets, importance sampling and more working towards the same goal [22]. Here, we present data selection in the domain of LLMs and natural language and how they can enable data efficient training. \n\nLIMA, proposed by [30] employs a small high quality dataset for fine-tuning. They show that by carefully curating only a 1000 data points, they are able to achieve remarkable performance which is generalizable to unseen data as well, thereby suggesting limited high quality tuning data is sufficient. \n\nAnother prominent study by [16] demonstrates that sample quality can reduce the data requirement without compromising the downstream performance. They investigate data engineering strategies in the fine-tuning paradigm from multiple facets to identify the characterises of good instruction tuning data. DEITA, the model family tuned by their proposed strategy to automatically select a complex and high quality dataset achieves comparable performance to open source models while using only a tenth of the data."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[276250237 | Sow et al. | 2025 | Citations: 6]",
                "snippets": "Training Data Re-weighting/Selection for LLMs. Several recent studies (Xie et al., 2023;(Chen et al., 2023)Fan et al., 2023;Thakkar et al., 2023) have explored various reweighting techniques to enhance the generalization and efficiency of language models pretraining. For instance, Xie et al. ( 2023) and Fan et al. (2023) optimize the composition of pretraining corpora to achieve better performance across pretraining domains or for out-of-domain generalization. (Chen et al., 2023) introduce a framework for ordered skill learning, optimizing data selection based on how effectively it teaches interdependent skills for continual pretraining and fine-tuning regimes. Although effective, these techniques operate at the group level, whereas our work explores reweighting at the instance level, offering finer control over how individual samples are treated based on their loss values....The approach in Fan & Jaggi (2023) offers a curriculum learning framework that prioritizes samples with a higher learnability score, which is precomputed using another auxiliary model similar to DoReMi and DoGE.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[260203057 | Chen et al. | 2023 | Citations: 65]": "The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training. Using this intuition, our framework formalizes the notion of a skill and of an ordered set of skills in terms of the associated data. First, using both synthetic and real data, we demonstrate that these ordered skill sets exist, and that their existence enables more advanced skills to be learned with less data when we train on their prerequisite skills. Second, using our proposed framework, we introduce an online data sampling algorithm, Skill-It, over mixtures of skills for both continual pre-training and fine-tuning regimes, where the objective is to efficiently learn multiple skills in the former and an individual skill in the latter. On the LEGO synthetic in the continual pre-training setting, Skill-It obtains 36.5 points higher accuracy than random sampling. On the Natural Instructions dataset in the fine-tuning setting, Skill-It reduces the validation loss on the target skill by 13.6% versus training on data associated with the target skill itself. We apply our skills framework on the recent RedPajama dataset to continually pre-train a 3B-parameter LM, achieving higher accuracy on the LM Evaluation Harness with 1B tokens than the baseline approach of sampling uniformly over data sources with 3B tokens."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 885,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 46
                            },
                            {
                                "start": 47,
                                "end": 266
                            },
                            {
                                "start": 267,
                                "end": 463
                            },
                            {
                                "start": 464,
                                "end": 667
                            },
                            {
                                "start": 668,
                                "end": 884
                            }
                        ],
                        "ref_mentions": [
                            "260203057",
                            "260203057"
                        ],
                        "quote": "Training Data Re-weighting/Selection for LLMs. Several recent studies (Xie et al., 2023;(Chen et al., 2023)Fan et al., 2023;Thakkar et al., 2023) have explored various reweighting techniques to enhance the generalization and efficiency of language models pretraining. For instance, Xie et al. ( 2023) and Fan et al. (2023) optimize the composition of pretraining corpora to achieve better performance across pretraining domains or for out-of-domain generalization. (Chen et al., 2023) introduce a framework for ordered skill learning, optimizing data selection based on how effectively it teaches interdependent skills for continual pretraining and fine-tuning regimes. Although effective, these techniques operate at the group level, whereas our work explores reweighting at the instance level, offering finer control over how individual samples are treated based on their loss values"
                    },
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 1719,
                        "end": 1931,
                        "sentence_offsets": [
                            {
                                "start": 1719,
                                "end": 1930
                            }
                        ],
                        "ref_mentions": [],
                        "quote": ".The approach in Fan & Jaggi (2023) offers a curriculum learning framework that prioritizes samples with a higher learnability score, which is precomputed using another auxiliary model similar to DoReMi and DoGE."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[276575595 | Belenki et al. | 2025 | Citations: 0]",
                "snippets": "There is an extensive body of work on data selection and mixture optimization for pretraining language models. Albalak et al. (2024) offer a comprehensive recent survey. Approaches for data selection and cleaning consider different granularities of data, such as token-level, sample-level (individual documents or sentences can be selected or weighted), and group-level (where we consider samples in large groups assumed to have common characteristics, often derived from meta-data such as the web domain (like Wikipedia) or source collection name (such as C4).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 561,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 110
                            },
                            {
                                "start": 111,
                                "end": 169
                            },
                            {
                                "start": 170,
                                "end": 561
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "There is an extensive body of work on data selection and mixture optimization for pretraining language models. Albalak et al. (2024) offer a comprehensive recent survey. Approaches for data selection and cleaning consider different granularities of data, such as token-level, sample-level (individual documents or sentences can be selected or weighted), and group-level (where we consider samples in large groups assumed to have common characteristics, often derived from meta-data such as the web domain (like Wikipedia) or source collection name (such as C4)."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[388785 | Fan et al. | 2017 | Citations: 79]",
                "snippets": "Training data plays a critical role in machine learning. The data selection strategy along the training process could significantly impact the performance of the learned model. For example, an appropriate strategy of removing redundant data can lead to a better model by using less computational efforts. For another example, previous studies on curriculum learning (Bengio et al., 2009) and self-paced learning (Kumar et al., 2010) reveal some principles of how tailoring data based on its 'hardness' can favor the model training; that is, easy data instances are important at the early age of model training, while at later age, harder training examples tend to be more effective to improve the model, since easy ones bring minor changes.\n\nThese facts reveal that how to feed training samples into a machine learning system is nontrivial and feeding data in a totally random order is not always a good choice. To explore a better data selection strategy for training, previous works including curriculum learning (CL) and self-paced learning (SPL) adopt simple heuristic rules, such as shuffling the sequence length to train language model (Bengio et al., 2009), or abandoning training instances whose loss values are larger than a human-defined threshold (Kumar et al., 2010;Jiang et al., 2014a).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1299,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 56
                            },
                            {
                                "start": 57,
                                "end": 176
                            },
                            {
                                "start": 177,
                                "end": 304
                            },
                            {
                                "start": 305,
                                "end": 740
                            },
                            {
                                "start": 743,
                                "end": 912
                            },
                            {
                                "start": 913,
                                "end": 1300
                            }
                        ],
                        "ref_mentions": [
                            "873046",
                            "1977996",
                            "873046",
                            "1977996",
                            "207216972"
                        ],
                        "quote": "Training data plays a critical role in machine learning. The data selection strategy along the training process could significantly impact the performance of the learned model. For example, an appropriate strategy of removing redundant data can lead to a better model by using less computational efforts. For another example, previous studies on curriculum learning (Bengio et al., 2009) and self-paced learning (Kumar et al., 2010) reveal some principles of how tailoring data based on its 'hardness' can favor the model training; that is, easy data instances are important at the early age of model training, while at later age, harder training examples tend to be more effective to improve the model, since easy ones bring minor changes.\n\nThese facts reveal that how to feed training samples into a machine learning system is nontrivial and feeding data in a totally random order is not always a good choice. To explore a better data selection strategy for training, previous works including curriculum learning (CL) and self-paced learning (SPL) adopt simple heuristic rules, such as shuffling the sequence length to train language model (Bengio et al., 2009), or abandoning training instances whose loss values are larger than a human-defined threshold (Kumar et al., 2010;Jiang et al., 2014a)."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[53295888 | Zhang et al. | 2018 | Citations: 112]",
                "snippets": "Curriculum learning (Bengio et al., 2009) hypothesizes that choosing the order in which training samples are presented to a learning system can help train better models faster. In particular, presenting samples that are easier to learn from before presenting difficult samples is an intuitively attractive idea, which has been applied in various ways in Machine Learning and Natural Language Processing tasks (Bengio et al., 2009)Tsvetkov et al., 2016;Cirik et al., 2016;Graves et al., 2017, inter alia).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 504,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 176
                            },
                            {
                                "start": 177,
                                "end": 504
                            }
                        ],
                        "ref_mentions": [
                            "873046",
                            "873046"
                        ],
                        "quote": "Curriculum learning (Bengio et al., 2009) hypothesizes that choosing the order in which training samples are presented to a learning system can help train better models faster. In particular, presenting samples that are easier to learn from before presenting difficult samples is an intuitively attractive idea, which has been applied in various ways in Machine Learning and Natural Language Processing tasks (Bengio et al., 2009)Tsvetkov et al., 2016;Cirik et al., 2016;Graves et al., 2017, inter alia)."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[7403346 | Ruder et al. | 2017 | Citations: 186]",
                "snippets": "Inspired by work on curriculum learning, we propose to learn data selection measures using Bayesian Optimization and evaluate them across models, domains and tasks. Our learned measures outperform existing domain similarity measures significantly on three tasks: sentiment analysis, part-of-speech tagging, and parsing. We show the importance of complementing similarity with diversity, and that learned measures are\u2013to some degree\u2013transferable across models, domains, and even tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Inspired by work on curriculum learning, we propose to learn data selection measures using Bayesian Optimization and evaluate them across models, domains and tasks. Our learned measures outperform existing domain similarity measures significantly on three tasks: sentiment analysis, part-of-speech tagging, and parsing. We show the importance of complementing similarity with diversity, and that learned measures are\u2013to some degree\u2013transferable across models, domains, and even tasks.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.030879,
        "cot": "To effectively answer the user's query about data selection as an important problem for training good language models, with particular focus on ordering data points from easy to hard (curriculum learning) for sample efficient training, I'll organize the quotes into meaningful dimensions.\n\nFirst, I'll start with an \"Introduction to Curriculum Learning\" section to provide background and foundational concepts. This should be a synthesis format to clearly explain the concept, its origins, and its basic principles.\n\nNext, I'll focus on \"Core Principles of Curriculum Learning\" to explain how curriculum learning works in language model training. This will be a synthesis to cohesively explain the underlying mechanisms.\n\nThen, I'll create \"Implementation Approaches\" to outline different methods researchers have used to implement curriculum learning. This section works best as a synthesis to show how these approaches connect.\n\nI'll then address \"Difficulty Measurement Techniques\" to explain how researchers determine which samples are \"easy\" or \"hard.\" This is a critical aspect of curriculum learning and deserves its own section in synthesis format to explain the relationships between different measurement techniques.\n\nI should include \"Benefits of Curriculum Learning\" to highlight the advantages reported across various studies. This works well as a list to clearly enumerate distinct benefits.\n\n\"Applications in Language Models\" will focus specifically on how curriculum learning has been applied to language model training. This should be a synthesis to show the progression and evolution of these applications.\n\n\"Challenges and Limitations\" will address problems or constraints encountered when implementing curriculum learning approaches. This works well as a list to clearly separate distinct issues.\n\nFinally, \"Recent Advances\" will cover the newest developments in curriculum learning for language models. This should be a synthesis to show how the field is evolving.\n\nI've carefully analyzed the quotes and will assign each to the most relevant dimension(s).",
        "plan": {
            "Introduction to Curriculum Learning (synthesis)": [
                1,
                3,
                5,
                7,
                14,
                31
            ],
            "Core Principles of Curriculum Learning (synthesis)": [
                0,
                2,
                6,
                11,
                16,
                17,
                30
            ],
            "Implementation Approaches (synthesis)": [
                4,
                8,
                9,
                10,
                19,
                20,
                23,
                28
            ],
            "Difficulty Measurement Techniques (synthesis)": [
                4,
                14,
                17,
                19,
                30
            ],
            "Benefits of Curriculum Learning (list)": [
                2,
                3,
                7,
                18,
                19,
                22,
                30
            ],
            "Applications in Language Models (synthesis)": [
                9,
                13,
                15,
                20,
                21,
                24,
                27,
                28,
                29
            ],
            "Challenges and Limitations (list)": [
                11,
                12,
                13,
                15,
                22
            ],
            "Recent Advances (synthesis)": [
                10,
                16,
                18,
                21,
                24,
                25,
                26,
                27,
                28,
                29,
                32
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Curriculum Learning",
                "tldr": "Curriculum learning is an approach inspired by human learning that involves training models on progressively more difficult examples. It consists of determining data difficulty and designing appropriate training schedules to improve model performance and efficiency. (12 sources)",
                "text": "\nCurriculum learning (CL) is a training strategy inspired by how humans learn, where training samples are organized by difficulty levels and presented to models in an easy-to-hard progression <Paper corpusId=\"237553761\" paperTitle=\"(Agrawal et al., 2021)\" isShortName></Paper>. This approach was formally introduced to machine learning by Bengio et al. in 2009, though the concept traces back to Elman's 1993 \"starting small\" hypothesis <Paper corpusId=\"265506572\" paperTitle=\"(Chobey et al., 2023)\" isShortName></Paper> <Paper corpusId=\"247762191\" paperTitle=\"(Mohiuddin et al., 2022)\" isShortName></Paper>. The core intuition is that by mimicking human education, where learning begins with simpler concepts before tackling more complex ones, neural networks can achieve better performance and faster convergence <Paper corpusId=\"208031414\" paperTitle=\"(Ma et al., 2019)\" isShortName></Paper>.\n\nIn recent years, curriculum learning has demonstrated significant benefits across various NLP tasks, particularly in machine translation, and has expanded to other natural language understanding applications <Paper corpusId=\"227905455\" paperTitle=\"(Laverghetta et al., 2020)\" isShortName></Paper> <Paper corpusId=\"220045816\" paperTitle=\"(Xu et al., 2020)\" isShortName></Paper>. The strategy has proven valuable as a plug-in approach for improving model generalization capacity and convergence rate in multiple domains beyond NLP, including computer vision <Paper corpusId=\"232362223\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper> <Paper corpusId=\"53295888\" paperTitle=\"(Zhang et al., 2018)\" isShortName></Paper>.\n\nImplementing curriculum learning involves two fundamental components: (1) a difficulty measurer that assigns scores to training examples based on their complexity, and (2) a training scheduler that determines the order of presentation based on these difficulty scores <Paper corpusId=\"265506572\" paperTitle=\"(Chobey et al., 2023)\" isShortName></Paper>. These components can be designed manually using prior knowledge or automatically derived during the training process <Paper corpusId=\"232362223\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>. Recent research has explored various methodologies for both components, from transfer learning approaches to self-paced learning techniques that adapt to the learner's progress <Paper corpusId=\"102350936\" paperTitle=\"(Hacohen et al., 2019)\" isShortName></Paper> <Paper corpusId=\"10891229\" paperTitle=\"(Jiang et al., 2015)\" isShortName></Paper>.\n\nThe growing body of research consistently shows that the sequencing of training data significantly impacts both the efficiency of the learning process and the final performance of language models <Paper corpusId=\"220047761\" paperTitle=\"(Zhou et al., 2020)\" isShortName></Paper> <Paper corpusId=\"231709290\" paperTitle=\"(Soviany et al., 2021)\" isShortName></Paper>. This realization has sparked diverse approaches to curriculum design, with researchers continually exploring new ways to determine example difficulty and optimal training schedules.",
                "citations": [
                    {
                        "id": "(Agrawal et al., 2021)",
                        "snippets": [
                            "The intuition behind this approach is to mimic the manner in which humans learn. Training samples are organized by levels of difficulty and training proceeds in steps where the model is first trained on a subset of the corpus at a given difficulty level before being trained on another difficulty level, and so on."
                        ],
                        "paper": {
                            "corpus_id": 237553761,
                            "title": "On the Role of Corpus Ordering in Language Modeling",
                            "authors": [
                                {
                                    "authorId": "2628916",
                                    "name": "Ameeta Agrawal"
                                },
                                {
                                    "authorId": "2145771623",
                                    "name": "Suresh Singh"
                                },
                                {
                                    "authorId": "144958306",
                                    "name": "Lauren Schneider"
                                },
                                {
                                    "authorId": "151134500",
                                    "name": "Michael Samuels"
                                }
                            ],
                            "year": 2021,
                            "venue": "SUSTAINLP",
                            "n_citations": 7
                        },
                        "score": 0.79052734375
                    },
                    {
                        "id": "(Chobey et al., 2023)",
                        "snippets": [
                            "Curriculum learning (Bengio et al., 2009) refers to training models through a difficulty-based ordering of training examples (i.e. a curriculum), most often \"starting small\" (Elman, 1993) from easy examples before progressing to increasingly difficult sentences. In NLP, curriculum learning has been widely used for Machine Translation (e.g., Platanios et al., 2019), but has also been applied more recently to other natural language understanding tasks (Xu et al., 2020). For a survey see (Soviany et al., 2021); (Wang et al., 2021). \n\nThere are two steps involved in designing a curriculum: assigning a difficulty score to every training example (\"difficulty measurer\") and using these difficulty scores to determine the order in which training examples are presented to the model (\"training scheduler\") (Wang et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 265506572,
                            "title": "Can training neural language models on a curriculum with developmentally plausible data improve alignment with human reading behavior?",
                            "authors": [
                                {
                                    "authorId": "2268760204",
                                    "name": "Aryaman Chobey"
                                },
                                {
                                    "authorId": "2268760018",
                                    "name": "Oliver Smith"
                                },
                                {
                                    "authorId": "2268796061",
                                    "name": "Anzi Wang"
                                },
                                {
                                    "authorId": "2268760229",
                                    "name": "Grusha Prasad"
                                }
                            ],
                            "year": 2023,
                            "venue": "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning",
                            "n_citations": 5
                        },
                        "score": 0.69384765625
                    },
                    {
                        "id": "(Mohiuddin et al., 2022)",
                        "snippets": [
                            "Curriculum Learning Inspired by human learners, (Elman, 1993) argues that optimization of neural network training can be accelerated by gradually increasing the difficulty of the concepts. (Bengio et al., 2009) were the first to use the term \"curricu-lum learning\" to refer to the easy-to-hard training strategies in the context of machine learning. Using an easy-to-hard curriculum based on increasing vocabulary size in language model training, they achieved performance improvement. Recent work (Jiang et al., 2015)(Hacohen et al., 2019)(Zhou et al., 2020) shows that manoeuvring the sequence of training data can improve both training efficiency and model accuracy."
                        ],
                        "paper": {
                            "corpus_id": 247762191,
                            "title": "Data Selection Curriculum for Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "6838342",
                                    "name": "Tasnim Mohiuddin"
                                },
                                {
                                    "authorId": "1755162",
                                    "name": "Philipp Koehn"
                                },
                                {
                                    "authorId": "113810201",
                                    "name": "Vishrav Chaudhary"
                                },
                                {
                                    "authorId": "2059363961",
                                    "name": "James Cross"
                                },
                                {
                                    "authorId": "2116473",
                                    "name": "Shruti Bhosale"
                                },
                                {
                                    "authorId": "2708940",
                                    "name": "Shafiq R. Joty"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 13
                        },
                        "score": 0.71240234375
                    },
                    {
                        "id": "(Ma et al., 2019)",
                        "snippets": [
                            "Our method is inspired by the work on curriculum learning and recent work on data selection for transfer learning. \n\nCurriculum Learning: Curriculum Learning (Bengio et al., 2009) deals with the question of how to use prior knowledge about the difficulty of the training examples, to boost the rate of learning and the performance of the final model. The ranking or weighting of the training examples is used to guide the order of presentation of examples to the learner. The idea is to build a curriculum of progressively harder samples in order to significantly accelerate a neural network's train-ing."
                        ],
                        "paper": {
                            "corpus_id": 208031414,
                            "title": "Domain Adaptation with BERT-based Domain Classification and Data Selection",
                            "authors": [
                                {
                                    "authorId": "47646605",
                                    "name": "Xiaofei Ma"
                                },
                                {
                                    "authorId": "2091437540",
                                    "name": "Peng Xu"
                                },
                                {
                                    "authorId": "40296541",
                                    "name": "Zhiguo Wang"
                                },
                                {
                                    "authorId": "1701451",
                                    "name": "Ramesh Nallapati"
                                }
                            ],
                            "year": 2019,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 95
                        },
                        "score": 0.80029296875
                    },
                    {
                        "id": "(Laverghetta et al., 2020)",
                        "snippets": [
                            "Curriculum learning (Elman, 1993), a strategy where the model is trained on easier examples before harder ones, has recently been shown to improve performance and reduce training time on a variety of NLP tasks, especially machine translation (Liu et al., 2020;(Wang et al., 2019)(Zhou et al., 2020)(Xu et al., 2020). \n\nThe success of this research shows that the order in which training data is presented to a model is important, but how to best apply curriculum learning broadly to NLP and how to design effective measures of difficulty to create curricula remain relatively unexplored."
                        ],
                        "paper": {
                            "corpus_id": 227905455,
                            "title": "Towards a Task-Agnostic Model of Difficulty Estimation for Supervised Learning Tasks",
                            "authors": [
                                {
                                    "authorId": "2033736232",
                                    "name": "Antonio Laverghetta"
                                },
                                {
                                    "authorId": "1716200134",
                                    "name": "Jamshidbek Mirzakhalov"
                                },
                                {
                                    "authorId": "2143879",
                                    "name": "John Licato"
                                }
                            ],
                            "year": 2020,
                            "venue": "AACL",
                            "n_citations": 2
                        },
                        "score": 0.76953125
                    },
                    {
                        "id": "(Xu et al., 2020)",
                        "snippets": [
                            "With the great success of pre-trained language models, the pretrain-finetune paradigm now becomes the undoubtedly dominant solution for natural language understanding (NLU) tasks. At the fine-tune stage, target task data is usually introduced in a completely random order and treated equally. However, examples in NLU tasks can vary greatly in difficulty, and similar to human learning procedure, language models can benefit from an easy-to-difficult curriculum. Based on this idea, we propose our Curriculum Learning approach. By reviewing the trainset in a crossed way, we are able to distinguish easy examples from difficult ones, and arrange a curriculum for language models. Without any manual model architecture design or use of external data, our Curriculum Learning approach obtains significant and universal performance improvements on a wide range of NLU tasks."
                        ],
                        "paper": {
                            "corpus_id": 220045816,
                            "title": "Curriculum Learning for Natural Language Understanding",
                            "authors": [
                                {
                                    "authorId": "1754285124",
                                    "name": "Benfeng Xu"
                                },
                                {
                                    "authorId": "48378753",
                                    "name": "L. Zhang"
                                },
                                {
                                    "authorId": "1855978",
                                    "name": "Zhendong Mao"
                                },
                                {
                                    "authorId": "143906199",
                                    "name": "Quan Wang"
                                },
                                {
                                    "authorId": "143994657",
                                    "name": "Hongtao Xie"
                                },
                                {
                                    "authorId": "1699819",
                                    "name": "Yongdong Zhang"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 206
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2021)",
                        "snippets": [
                            "Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of <italic>Difficulty Measurer <inline-formula><tex-math notation=\"LaTeX\">$+$</tex-math><alternatives><mml:math><mml:mo>+</mml:mo></mml:math><inline-graphic xlink:href=\"wang-ieq1-3069908.gif\"/></alternatives></inline-formula> Training Scheduler</italic> and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. We also analyze principles to select different CL designs that may benefit practical applications. Finally, we present our insights on the relationships connecting CL and other machine learning concepts including transfer learning, meta-learning, continual learning and active learning, etc., then point out challenges in CL as well as potential future research directions deserving further investigations."
                        ],
                        "paper": {
                            "corpus_id": 232362223,
                            "title": "A Survey on Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "2153687490",
                                    "name": "Xin Wang"
                                },
                                {
                                    "authorId": "51310474",
                                    "name": "Yudong Chen"
                                },
                                {
                                    "authorId": "145583986",
                                    "name": "Wenwu Zhu"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                            "n_citations": 611
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2018)",
                        "snippets": [
                            "Curriculum learning (Bengio et al., 2009) hypothesizes that choosing the order in which training samples are presented to a learning system can help train better models faster. In particular, presenting samples that are easier to learn from before presenting difficult samples is an intuitively attractive idea, which has been applied in various ways in Machine Learning and Natural Language Processing tasks (Bengio et al., 2009)Tsvetkov et al., 2016;Cirik et al., 2016;Graves et al., 2017, inter alia)."
                        ],
                        "paper": {
                            "corpus_id": 53295888,
                            "title": "An Empirical Exploration of Curriculum Learning for Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "49469742",
                                    "name": "Xuan Zhang"
                                },
                                {
                                    "authorId": "48387892",
                                    "name": "Manish Kumar"
                                },
                                {
                                    "authorId": "3115181",
                                    "name": "Huda Khayrallah"
                                },
                                {
                                    "authorId": "38730896",
                                    "name": "Kenton Murray"
                                },
                                {
                                    "authorId": "3456371",
                                    "name": "Jeremy Gwinnup"
                                },
                                {
                                    "authorId": "3219152",
                                    "name": "Marianna J. Martindale"
                                },
                                {
                                    "authorId": "145324163",
                                    "name": "Paul McNamee"
                                },
                                {
                                    "authorId": "1800354",
                                    "name": "Kevin Duh"
                                },
                                {
                                    "authorId": "2954727",
                                    "name": "Marine Carpuat"
                                }
                            ],
                            "year": 2018,
                            "venue": "arXiv.org",
                            "n_citations": 112
                        },
                        "score": 0.708984375
                    },
                    {
                        "id": "(Hacohen et al., 2019)",
                        "snippets": [
                            "Training neural networks is traditionally done by providing a sequence of random mini-batches sampled uniformly from the entire training data. In this work, we analyze the effect of curriculum learning, which involves the non-uniform sampling of mini-batches, on the training of deep networks, and specifically CNNs trained for image recognition. To employ curriculum learning, the training algorithm must resolve 2 problems: (i) sort the training examples by difficulty; (ii) compute a series of mini-batches that exhibit an increasing level of difficulty. We address challenge (i) using two methods: transfer learning from some competitive ``teacher\" network, and bootstrapping. In our empirical evaluation, both methods show similar benefits in terms of increased learning speed and improved final performance on test data. We address challenge (ii) by investigating different pacing functions to guide the sampling. The empirical investigation includes a variety of network architectures, using images from CIFAR-10, CIFAR-100 and subsets of ImageNet. We conclude with a novel theoretical analysis of curriculum learning, where we show how it effectively modifies the optimization landscape. We then define the concept of an ideal curriculum, and show that under mild conditions it does not change the corresponding global minimum of the optimization function."
                        ],
                        "paper": {
                            "corpus_id": 102350936,
                            "title": "On The Power of Curriculum Learning in Training Deep Networks",
                            "authors": [
                                {
                                    "authorId": "94064232",
                                    "name": "Guy Hacohen"
                                },
                                {
                                    "authorId": "1789171",
                                    "name": "D. Weinshall"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 449
                        },
                        "score": 0
                    },
                    {
                        "id": "(Jiang et al., 2015)",
                        "snippets": [
                            "Curriculum learning (CL) or self-paced learning (SPL) represents a recently proposed learning regime inspired by the learning process of humans and animals that gradually proceeds from easy to more complex samples in training. The two methods share a similar conceptual learning paradigm, but differ in specific learning schemes. In CL, the curriculum is predetermined by prior knowledge, and remain fixed thereafter. Therefore, this type of method heavily relies on the quality of prior knowledge while ignoring feedback about the learner. In SPL, the curriculum is dynamically determined to adjust to the learning pace of the leaner. However, SPL is unable to deal with prior knowledge, rendering it prone to overfitting. In this paper, we discover the missing link between CL and SPL, and propose a unified framework named self-paced curriculum leaning (SPCL). SPCL is formulated as a concise optimization problem that takes into account both prior knowledge known before training and the learning progress during training. In comparison to human education, SPCL is analogous to \"instructor-student-collaborative\" learning mode, as opposed to \"instructor-driven\" in CL or \"student-driven\" in SPL. Empirically, we show that the advantage of SPCL on two tasks."
                        ],
                        "paper": {
                            "corpus_id": 10891229,
                            "title": "Self-Paced Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "39978626",
                                    "name": "Lu Jiang"
                                },
                                {
                                    "authorId": "1803714",
                                    "name": "Deyu Meng"
                                },
                                {
                                    "authorId": "46317290",
                                    "name": "Qian Zhao"
                                },
                                {
                                    "authorId": "145455919",
                                    "name": "S. Shan"
                                },
                                {
                                    "authorId": "7661726",
                                    "name": "Alexander Hauptmann"
                                }
                            ],
                            "year": 2015,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 526
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhou et al., 2020)",
                        "snippets": [
                            "Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule."
                        ],
                        "paper": {
                            "corpus_id": 220047761,
                            "title": "Uncertainty-Aware Curriculum Learning for Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "2110349143",
                                    "name": "Yikai Zhou"
                                },
                                {
                                    "authorId": "21299583",
                                    "name": "Baosong Yang"
                                },
                                {
                                    "authorId": "1758353",
                                    "name": "Derek F. Wong"
                                },
                                {
                                    "authorId": "153379180",
                                    "name": "Yu Wan"
                                },
                                {
                                    "authorId": "1774304",
                                    "name": "Lidia S. Chao"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 96
                        },
                        "score": 0
                    },
                    {
                        "id": "(Soviany et al., 2021)",
                        "snippets": [
                            "(Zhou et al., 2020) introduce an uncertainty-based curriculum batching approach for neural machine translation. They propose using uncertainty at the data level, for establishing the easy-to-hard ordering, and the model level, to decide the right moment to enhance the training set with more difficult samples. To measure the difficulty of the examples, they start from the intuition that samples with higher crossentropy and uncertainty are more difficult to translate."
                        ],
                        "paper": {
                            "corpus_id": 231709290,
                            "title": "Curriculum Learning: A Survey",
                            "authors": [
                                {
                                    "authorId": "40901288",
                                    "name": "Petru Soviany"
                                },
                                {
                                    "authorId": "1817759",
                                    "name": "Radu Tudor Ionescu"
                                },
                                {
                                    "authorId": "39337007",
                                    "name": "Paolo Rota"
                                },
                                {
                                    "authorId": "1703601",
                                    "name": "N. Sebe"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Journal of Computer Vision",
                            "n_citations": 359
                        },
                        "score": 0.8095703125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Core Principles of Curriculum Learning",
                "tldr": "Curriculum learning is based on two key principles: organizing training data by difficulty and presenting it in a strategic sequence. The approach includes measuring sample difficulty, scheduling training progressively, and adapting the curriculum based on the model's learning state. (9 sources)",
                "text": "\nCurriculum learning is fundamentally built on the principle of organizing training samples based on their difficulty and presenting them to the model in a strategic sequence, typically from easy to hard <Paper corpusId=\"388785\" paperTitle=\"(Fan et al., 2017)\" isShortName></Paper>. This approach draws inspiration from human education systems, where learning progresses from basic concepts to more complex ones. The core components of curriculum learning include difficulty measurement and training scheduling, which together form the framework for implementing this learning strategy.\n\nA central principle of curriculum learning is difficulty assessment, which can be approached through various methods. Some researchers have grouped samples by their similarity to a specific domain of interest, allowing for targeted training schedules <Paper corpusId=\"155089817\" paperTitle=\"(Zhang et al., 2019)\" isShortName></Paper>. Others have proposed measuring sample difficulty from the model's perspective, using metrics like \"learning percentage\" to self-rank training data based on how quickly the model learns from different examples <Paper corpusId=\"267740312\" paperTitle=\"(Mekala et al., 2024)\" isShortName></Paper> <Paper corpusId=\"249060677\" paperTitle=\"(Mekala et al., 2022)\" isShortName></Paper>.\n\nThe second key principle involves the training scheduler, which determines how and when different training examples are presented to the model. Traditional curriculum learning prioritizes easy points with low label noise before moving to all data points, enhancing convergence but potentially failing to address redundancy in the training data <Paper corpusId=\"261048902\" paperTitle=\"(Deng et al., 2023)\" isShortName></Paper>. More sophisticated approaches include dynamic data selection, which gradually transitions from generic to domain-specific training by increasing selection thresholds as training progresses <Paper corpusId=\"237581256\" paperTitle=\"(Iter et al., 2021)\" isShortName></Paper> <Paper corpusId=\"7921428\" paperTitle=\"(Wees et al., 2017)\" isShortName></Paper>.\n\nRecent implementations have expanded these principles to larger-scale language models, creating multi-stage training processes where data complexity increases incrementally. For instance, the Orion-14B model employed a data scheduling strategy that initially focused on common knowledge sources like web pages and news articles before gradually incorporating more complex data sources such as textbooks, academic papers, and source code <Paper corpusId=\"267095066\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>.\n\nAn important principle that complements curriculum learning is active learning, where the strategic ordering of training data selection can lead to more efficient learning from smaller datasets <Paper corpusId=\"226964591\" paperTitle=\"(Pillai et al., 2020)\" isShortName></Paper>. This approach recognizes that not all data points contribute equally to model learning, and selective sampling based on informativeness can enhance training efficiency.\n\nTogether, these principles form a framework for curriculum learning that moves beyond random data presentation, acknowledging that \"feeding data in a totally random order is not always a good choice\" <Paper corpusId=\"388785\" paperTitle=\"(Fan et al., 2017)\" isShortName></Paper>. By carefully structuring the learning process, curriculum approaches aim to improve both the efficiency of training and the final performance of language models.",
                "citations": [
                    {
                        "id": "(Fan et al., 2017)",
                        "snippets": [
                            "Training data plays a critical role in machine learning. The data selection strategy along the training process could significantly impact the performance of the learned model. For example, an appropriate strategy of removing redundant data can lead to a better model by using less computational efforts. For another example, previous studies on curriculum learning (Bengio et al., 2009) and self-paced learning (Kumar et al., 2010) reveal some principles of how tailoring data based on its 'hardness' can favor the model training; that is, easy data instances are important at the early age of model training, while at later age, harder training examples tend to be more effective to improve the model, since easy ones bring minor changes.\n\nThese facts reveal that how to feed training samples into a machine learning system is nontrivial and feeding data in a totally random order is not always a good choice. To explore a better data selection strategy for training, previous works including curriculum learning (CL) and self-paced learning (SPL) adopt simple heuristic rules, such as shuffling the sequence length to train language model (Bengio et al., 2009), or abandoning training instances whose loss values are larger than a human-defined threshold (Kumar et al., 2010;Jiang et al., 2014a)."
                        ],
                        "paper": {
                            "corpus_id": 388785,
                            "title": "Learning What Data to Learn",
                            "authors": [
                                {
                                    "authorId": "144566102",
                                    "name": "Yang Fan"
                                },
                                {
                                    "authorId": "143853336",
                                    "name": "Fei Tian"
                                },
                                {
                                    "authorId": "143826491",
                                    "name": "Tao Qin"
                                },
                                {
                                    "authorId": "152441498",
                                    "name": "Jiang Bian"
                                },
                                {
                                    "authorId": "2110264337",
                                    "name": "Tie-Yan Liu"
                                }
                            ],
                            "year": 2017,
                            "venue": "arXiv.org",
                            "n_citations": 79
                        },
                        "score": 0.88134765625
                    },
                    {
                        "id": "(Zhang et al., 2019)",
                        "snippets": [
                            "We introduce a curriculum learning approach to adapt generic neural machine translation models to a specific domain. Samples are grouped by their similarities to the domain of interest and each group is fed to the training algorithm with a particular schedule."
                        ],
                        "paper": {
                            "corpus_id": 155089817,
                            "title": "Curriculum Learning for Domain Adaptation in Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "49469742",
                                    "name": "Xuan Zhang"
                                },
                                {
                                    "authorId": "38581144",
                                    "name": "Pamela Shapiro"
                                },
                                {
                                    "authorId": "48387892",
                                    "name": "Manish Kumar"
                                },
                                {
                                    "authorId": "145324163",
                                    "name": "Paul McNamee"
                                },
                                {
                                    "authorId": "2954727",
                                    "name": "Marine Carpuat"
                                },
                                {
                                    "authorId": "1800354",
                                    "name": "Kevin Duh"
                                }
                            ],
                            "year": 2019,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 124
                        },
                        "score": 0.912109375
                    },
                    {
                        "id": "(Mekala et al., 2024)",
                        "snippets": [
                            "In this paper, we delve into the measurement of sample difficulty from the model's perspective. Drawing inspiration from the learning order metric in (Mekala et al., 2022), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered. We then select the most difficult sample subsets based on this ranking and instruction-tune a language model."
                        ],
                        "paper": {
                            "corpus_id": 267740312,
                            "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
                            "authors": [
                                {
                                    "authorId": "7565696",
                                    "name": "Dheeraj Mekala"
                                },
                                {
                                    "authorId": "2284673632",
                                    "name": "Alex Nguyen"
                                },
                                {
                                    "authorId": "2284595153",
                                    "name": "Jingbo Shang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 21
                        },
                        "score": 0.9140625
                    },
                    {
                        "id": "(Mekala et al., 2022)",
                        "snippets": [
                            "Weakly supervised text classification methods typically train a deep neural classifier based on pseudo-labels. The quality of pseudo-labels is crucial to final performance but they are inevitably noisy due to their heuristic nature, so selecting the correct ones has a huge potential for performance boost. One straightforward solution is to select samples based on the softmax probability scores in the neural classifier corresponding to their pseudo-labels. However, we show through our experiments that such solutions are ineffective and unstable due to the erroneously high-confidence predictions from poorly calibrated models. Recent studies on the memorization effects of deep neural models suggest that these models first memorize training samples with clean labels and then those with noisy labels. Inspired by this observation, we propose a novel pseudo-label selection method LOPS that takes learning order of samples into consideration. We hypothesize that the learning order reflects the probability of wrong annotation in terms of ranking, and therefore, propose to select the samples that are learnt earlier. LOPS can be viewed as a strong performance-boost plug-in to most of existing weakly-supervised text classification methods, as confirmed in extensive experiments on four real-world datasets."
                        ],
                        "paper": {
                            "corpus_id": 249060677,
                            "title": "LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification",
                            "authors": [
                                {
                                    "authorId": "7565696",
                                    "name": "Dheeraj Mekala"
                                },
                                {
                                    "authorId": "2113540861",
                                    "name": "Chengyu Dong"
                                },
                                {
                                    "authorId": "2884976",
                                    "name": "Jingbo Shang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 20
                        },
                        "score": 0
                    },
                    {
                        "id": "(Deng et al., 2023)",
                        "snippets": [
                            "Curriculum learning, as advocated by (Bengio et al., 2009), prioritizes easy points with low label noise before uniformly training on all data points. While this strategy enhances convergence, it fails to address the issue of skipping redundant points already learned."
                        ],
                        "paper": {
                            "corpus_id": 261048902,
                            "title": "Towards Accelerated Model Training via Bayesian Data Selection",
                            "authors": [
                                {
                                    "authorId": "145114723",
                                    "name": "Zhijie Deng"
                                },
                                {
                                    "authorId": "2153522384",
                                    "name": "Peng Cui"
                                },
                                {
                                    "authorId": "145254043",
                                    "name": "Jun Zhu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 5
                        },
                        "score": 0.7568359375
                    },
                    {
                        "id": "(Iter et al., 2021)",
                        "snippets": [
                            "Dynamic selection (Wees et al., 2017) proposes to increase the selection threshold \u03c4 t as training progresses, gradually transitioning from generic to in-domain training. This gradual adaptation of neural network is related to curriculum learning (Bengio et al., 2009) which studies the ordering of examples and tasks during model training."
                        ],
                        "paper": {
                            "corpus_id": 237581256,
                            "title": "The Trade-offs of Domain Adaptation for Neural Language Models",
                            "authors": [
                                {
                                    "authorId": "3310951",
                                    "name": "Dan Iter"
                                },
                                {
                                    "authorId": "2529182",
                                    "name": "David Grangier"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 22
                        },
                        "score": 0.9013671875
                    },
                    {
                        "id": "(Wees et al., 2017)",
                        "snippets": [
                            "Intelligent selection of training data has proven a successful technique to simultaneously increase training efficiency and translation performance for phrase-based machine translation (PBMT). With the recent increase in popularity of neural machine translation (NMT), we explore in this paper to what extent and how NMT can also benefit from data selection. While state-of-the-art data selection (Axelrod et al., 2011) consistently performs well for PBMT, we show that gains are substantially lower for NMT. Next, we introduce \u2018dynamic data selection\u2019 for NMT, a method in which we vary the selected subset of training data between different training epochs. Our experiments show that the best results are achieved when applying a technique we call \u2018gradual fine-tuning\u2019, with improvements up to +2.6 BLEU over the original data selection approach and up to +3.1 BLEU over a general baseline."
                        ],
                        "paper": {
                            "corpus_id": 7921428,
                            "title": "Dynamic Data Selection for Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "3110566",
                                    "name": "M. V. D. Wees"
                                },
                                {
                                    "authorId": "3242253",
                                    "name": "Arianna Bisazza"
                                },
                                {
                                    "authorId": "1696402",
                                    "name": "Christof Monz"
                                }
                            ],
                            "year": 2017,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 154
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2024)",
                        "snippets": [
                            "Considering that humans acquire knowledge in a deliberate order (Evanson et al., 2023), it is plausible that language models might also benefit from a structured learning order when processing training data. Curriculum learning (Bengio et al., 2009) has been suggested as a method to organize the learning process by progressively increasing the complexity of the training data. However, most prior studies have concentrated on sample-level data and smaller datasets. Chen et al. ( 2023) employed a skills-based framework for training data selection and continuous pretraining with a 3B-parameter language model. This approach achieved greater accuracy compared to the baseline method of uniform data source sampling, suggesting the potential efficacy of strategic data scheduling.\n\nIn training the Orion-14B model, we intentionally develop a data scheduling strategy that organizes training data to incrementally increase its complexity. We divide the training data into several stages based on the data sources and their complexity. These stages are differentiated by the mix ratios of data sources. Initial stages primarily include data with common knowledge, such as web pages and news articles. In the subsequent stages, we gradually augment the proportion of data containing more complex knowledge, including textbooks, academic papers, and source code."
                        ],
                        "paper": {
                            "corpus_id": 267095066,
                            "title": "Orion-14B: Open-source Multilingual Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2280380985",
                                    "name": "Du Chen"
                                },
                                {
                                    "authorId": "2280373513",
                                    "name": "Yi Huang"
                                },
                                {
                                    "authorId": "2280377400",
                                    "name": "Xiaopu Li"
                                },
                                {
                                    "authorId": "2280380419",
                                    "name": "Yongqiang Li"
                                },
                                {
                                    "authorId": "2280377865",
                                    "name": "Yongqiang Liu"
                                },
                                {
                                    "authorId": "1557313084",
                                    "name": "Haihui Pan"
                                },
                                {
                                    "authorId": "2280411885",
                                    "name": "Leichao Xu"
                                },
                                {
                                    "authorId": "2109546143",
                                    "name": "Dacheng Zhang"
                                },
                                {
                                    "authorId": "2280368716",
                                    "name": "Zhipeng Zhang"
                                },
                                {
                                    "authorId": "2281415463",
                                    "name": "Kun Han"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.83740234375
                    },
                    {
                        "id": "(Pillai et al., 2020)",
                        "snippets": [
                            "Ordering the selection of training data using active learning can lead to improvements in learning efficiently from smaller corpora."
                        ],
                        "paper": {
                            "corpus_id": 226964591,
                            "title": "Sampling Approach Matters: Active Learning for Robotic Language Acquisition",
                            "authors": [
                                {
                                    "authorId": "144339038",
                                    "name": "Nisha Pillai"
                                },
                                {
                                    "authorId": "34885007",
                                    "name": "Edward Raff"
                                },
                                {
                                    "authorId": "2064957151",
                                    "name": "Francis Ferraro"
                                },
                                {
                                    "authorId": "2674440",
                                    "name": "Cynthia Matuszek"
                                }
                            ],
                            "year": 2020,
                            "venue": "2020 IEEE International Conference on Big Data (Big Data)",
                            "n_citations": 3
                        },
                        "score": 0.72509765625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Implementation Approaches",
                "tldr": "Curriculum learning implementations range from uncertainty-based methods in machine translation to sophisticated data selection strategies for large language models. Approaches include clustering-based selection, hard-to-easy strategies, ordered skill learning, and dynamic reweighting based on model competence and data difficulty. (16 sources)",
                "text": "\nImplementing curriculum learning in language models requires designing effective mechanisms for data selection and training scheduling. One prominent approach is uncertainty-based curriculum learning for neural machine translation, which uses uncertainty at both the data and model levels to establish easy-to-hard ordering and determine when to introduce more difficult samples <Paper corpusId=\"231709290\" paperTitle=\"(Soviany et al., 2021)\" isShortName></Paper> <Paper corpusId=\"220047761\" paperTitle=\"(Zhou et al., 2020)\" isShortName></Paper>. This method leverages cross-entropy as a measure of data difficulty and exploits the variance of weight distributions to represent model uncertainty, resulting in improved translation quality and convergence speed <Paper corpusId=\"220047761\" paperTitle=\"(Zhou et al., 2020)\" isShortName></Paper>.\n\nFor text classification tasks, cold-start data selection strategies have been developed to optimize the fine-tuning of pre-trained language models under limited labeling budgets <Paper corpusId=\"252280753\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper>. These strategies include using pre-training loss as a proxy for classification uncertainty <Paper corpusId=\"224724415\" paperTitle=\"(Yuan et al., 2020)\" isShortName></Paper> and K-means clustering to select diverse and representative training instances <Paper corpusId=\"235755110\" paperTitle=\"(Chang et al., 2021)\" isShortName></Paper>. The TypiClust approach, for example, is specifically designed for low-budget scenarios, selecting typical examples when few labeled examples are available <Paper corpusId=\"246634642\" paperTitle=\"(Hacohen et al., 2022)\" isShortName></Paper>.\n\nIn non-autoregressive translation, researchers have introduced a hard-to-easy learning strategy that combines pretrained models with distilled data <Paper corpusId=\"257901096\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This approach gradually decreases the ratio of raw data in the training process, building on insights that pretraining with raw data can improve performance by rejuvenating low-frequency words <Paper corpusId=\"229923128\" paperTitle=\"(Ding et al., 2020)\" isShortName></Paper>. Similar masked sequence-to-sequence models have been developed with n-gram loss functions to address specific translation challenges <Paper corpusId=\"220046693\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>.\n\nFor large language models, more sophisticated frameworks have emerged based on the hypothesis that models, like humans, acquire interdependent skills in a natural order <Paper corpusId=\"260203057\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. The Skill-It algorithm implements this insight by sampling data based on prerequisite skills, showing significant improvements in both continual pre-training and fine-tuning scenarios <Paper corpusId=\"260203057\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper> <Paper corpusId=\"276250237\" paperTitle=\"(Sow et al., 2025)\" isShortName></Paper>. Other curriculum learning approaches for LLMs structure training data using criteria such as prompt length, attention scores, and loss values to progress from simpler to more complex tasks <Paper corpusId=\"269756933\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>.\n\nRecent research has also explored explicit curriculum design for pretraining data selection, including methods based on decreasing gradient norms, least certainty, and increasing expertise <Paper corpusId=\"270371045\" paperTitle=\"(Yu et al., 2024)\" isShortName></Paper>. The QuRating method, for example, uses language models to rate pretraining data based on writing style, required expertise, factual content, and educational value, creating a training curriculum that improves performance without changing the training dataset <Paper corpusId=\"267681974\" paperTitle=\"(Wettig et al., 2024)\" isShortName></Paper>. Similarly, PRESENCE jointly reweights samples using self-influence scores as indicators of sample importance <Paper corpusId=\"264935129\" paperTitle=\"(Thakkar et al., 2023)\" isShortName></Paper>.\n\nAt a more fundamental level, implementing curriculum learning requires quantitative evaluation of each datapoint and an elaborated sampling mechanism to select the most informative subset of data within a given budget <Paper corpusId=\"271710435\" paperTitle=\"(Qin et al., 2024)\" isShortName></Paper>. The expected benefits include noise reduction by ignoring mislabeled data, rebalancing data distributions by adjusting the sampling of easy versus hard examples, and expediting training for more efficient model iterations <Paper corpusId=\"271710435\" paperTitle=\"(Qin et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Soviany et al., 2021)",
                        "snippets": [
                            "(Zhou et al., 2020) introduce an uncertainty-based curriculum batching approach for neural machine translation. They propose using uncertainty at the data level, for establishing the easy-to-hard ordering, and the model level, to decide the right moment to enhance the training set with more difficult samples. To measure the difficulty of the examples, they start from the intuition that samples with higher crossentropy and uncertainty are more difficult to translate."
                        ],
                        "paper": {
                            "corpus_id": 231709290,
                            "title": "Curriculum Learning: A Survey",
                            "authors": [
                                {
                                    "authorId": "40901288",
                                    "name": "Petru Soviany"
                                },
                                {
                                    "authorId": "1817759",
                                    "name": "Radu Tudor Ionescu"
                                },
                                {
                                    "authorId": "39337007",
                                    "name": "Paolo Rota"
                                },
                                {
                                    "authorId": "1703601",
                                    "name": "N. Sebe"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Journal of Computer Vision",
                            "n_citations": 359
                        },
                        "score": 0.8095703125
                    },
                    {
                        "id": "(Zhou et al., 2020)",
                        "snippets": [
                            "Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule."
                        ],
                        "paper": {
                            "corpus_id": 220047761,
                            "title": "Uncertainty-Aware Curriculum Learning for Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "2110349143",
                                    "name": "Yikai Zhou"
                                },
                                {
                                    "authorId": "21299583",
                                    "name": "Baosong Yang"
                                },
                                {
                                    "authorId": "1758353",
                                    "name": "Derek F. Wong"
                                },
                                {
                                    "authorId": "153379180",
                                    "name": "Yu Wan"
                                },
                                {
                                    "authorId": "1774304",
                                    "name": "Lidia S. Chao"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 96
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yu et al., 2023)",
                        "snippets": [
                            "We study cold-start data selection for text classification with c classes formulated as follows: Given a pool of unlabeled samples D u = {x j } U j=1 and an empty training set D l = \u2205, we aim to fine-tune a pre-trained language model M denoted as f (\u2022; \u03b8) under limited labeling budget |B| interactively: In each round, we use an acquisition function F(\u2022) to query b samples denoted as Q from D u . Next, the acquired samples are labeled and moved from D u to D l . Then we fine-tune the pre-trained language model f (\u2022; \u03b8) with D l to maximize the performance on downstream classification tasks. The above steps can either be one-round (Chang et al., 2021)(Hacohen et al., 2022)) (b = |B| in this case) or repeated for multiple rounds (Yuan et al., 2020) (b = |B|/|Rounds|) until reaching the budget |B|."
                        ],
                        "paper": {
                            "corpus_id": 252280753,
                            "title": "Cold-Start Data Selection for Better Few-shot Language Model Fine-tuning: A Prompt-based Uncertainty Propagation Approach",
                            "authors": [
                                {
                                    "authorId": "1633124736",
                                    "name": "Yue Yu"
                                },
                                {
                                    "authorId": "46752897",
                                    "name": "Rongzhi Zhang"
                                },
                                {
                                    "authorId": "2115801998",
                                    "name": "Ran Xu"
                                },
                                {
                                    "authorId": "47540245",
                                    "name": "Jieyu Zhang"
                                },
                                {
                                    "authorId": "3363642",
                                    "name": "Jiaming Shen"
                                },
                                {
                                    "authorId": "145657504",
                                    "name": "Chao Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 23
                        },
                        "score": 0.8447265625
                    },
                    {
                        "id": "(Yuan et al., 2020)",
                        "snippets": [
                            "Active learning strives to reduce annotation costs by choosing the most critical examples to label. Typically, the active learning strategy is contingent on the classification model. For instance, uncertainty sampling depends on poorly calibrated model confidence scores. In the cold-start setting, active learning is impractical because of model instability and data scarcity. Fortunately, modern NLP provides an additional source of information: pre-trained language models. The pre-training loss can find examples that surprise the model and should be labeled for efficient fine-tuning. Therefore, we treat the language modeling loss as a proxy for classification uncertainty. With BERT, we develop a simple strategy based on the masked language modeling loss that minimizes labeling costs for text classification. Compared to other baselines, our approach reaches higher accuracy within less sampling iterations and computation time."
                        ],
                        "paper": {
                            "corpus_id": 224724415,
                            "title": "Cold-start Active Learning through Self-Supervised Language Modeling",
                            "authors": [
                                {
                                    "authorId": "39879969",
                                    "name": "Michelle Yuan"
                                },
                                {
                                    "authorId": "1798966",
                                    "name": "Hsuan-Tien Lin"
                                },
                                {
                                    "authorId": "1389036863",
                                    "name": "Jordan L. Boyd-Graber"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 184
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chang et al., 2021)",
                        "snippets": [
                            "Large-scale pretrained language models have led to dramatic improvements in text generation. Impressive performance can be achieved by finetuning only on a small number of instances (few-shot setting). Nonetheless, almost all previous work simply applies random sampling to select the few-shot training instances. Little to no attention has been paid to the selection strategies and how they would affect model performance. In this work, we present a study on training instance selection in few-shot neural text generation. The selection decision is made based only on the unlabeled data so as to identify the most worthwhile data points that should be annotated under some budget of labeling cost. Based on the intuition that the few-shot training instances should be diverse and representative of the entire data distribution, we propose a simple selection strategy with K-means clustering. We show that even with the naive clustering-based approach, the generation models consistently outperform random sampling on three text generation tasks: data-to-text generation, document summarization and question generation. The code and training data are made available. We hope that this work will call for more attention on this largely unexplored area."
                        ],
                        "paper": {
                            "corpus_id": 235755110,
                            "title": "On Training Instance Selection for Few-Shot Neural Text Generation",
                            "authors": [
                                {
                                    "authorId": "48025720",
                                    "name": "Ernie Chang"
                                },
                                {
                                    "authorId": "2562211",
                                    "name": "Xiaoyu Shen"
                                },
                                {
                                    "authorId": "2047999043",
                                    "name": "Hui-Syuan Yeh"
                                },
                                {
                                    "authorId": "2869436",
                                    "name": "Vera Demberg"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 41
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hacohen et al., 2022)",
                        "snippets": [
                            "Investigating active learning, we focus on the relation between the number of labeled examples (budget size), and suitable querying strategies. Our theoretical analysis shows a behavior reminiscent of phase transition: typical examples are best queried when the budget is low, while unrepresentative examples are best queried when the budget is large. Combined evidence shows that a similar phenomenon occurs in common classification models. Accordingly, we propose TypiClust -- a deep active learning strategy suited for low budgets. In a comparative empirical investigation of supervised learning, using a variety of architectures and image datasets, TypiClust outperforms all other active learning strategies in the low-budget regime. Using TypiClust in the semi-supervised framework, performance gets an even more significant boost. In particular, state-of-the-art semi-supervised methods trained on CIFAR-10 with 10 labeled examples selected by TypiClust, reach 93.2% accuracy -- an improvement of 39.4% over random selection. Code is available at https://github.com/avihu111/TypiClust."
                        ],
                        "paper": {
                            "corpus_id": 246634642,
                            "title": "Active Learning on a Budget: Opposite Strategies Suit High and Low Budgets",
                            "authors": [
                                {
                                    "authorId": "94064232",
                                    "name": "Guy Hacohen"
                                },
                                {
                                    "authorId": "2153469082",
                                    "name": "Avihu Dekel"
                                },
                                {
                                    "authorId": "1789171",
                                    "name": "D. Weinshall"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 123
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "Motivated by the success of curriculum learning (Qian et al., 2020)(Guo et al., 2020)Liu et al. 2020), we further introduce a hard-to-easy learning strategy to improve the performance. (Ding et al., 2020) show that pretraining with raw data can improve the performance of NAT by rejuvenating low-frequency words. To keep the merits of low-mode, they further trained the pretrained model on distilled data. We combine this idea with our data selection method by decreasing the ratio of raw data in the training process."
                        ],
                        "paper": {
                            "corpus_id": 257901096,
                            "title": "Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "2226017688",
                                    "name": "Min Liu"
                                },
                                {
                                    "authorId": "145854784",
                                    "name": "Yu Bao"
                                },
                                {
                                    "authorId": "144562857",
                                    "name": "Chengqi Zhao"
                                },
                                {
                                    "authorId": "2124946880",
                                    "name": "Shujian Huang"
                                }
                            ],
                            "year": 2023,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 3
                        },
                        "score": 0.71142578125
                    },
                    {
                        "id": "(Ding et al., 2020)",
                        "snippets": [
                            "Knowledge distillation (KD) is essential for training non-autoregressive translation (NAT) models by reducing the complexity of the raw data with an autoregressive teacher model. In this study, we empirically show that as a side effect of this training, the lexical choice errors on low-frequency words are propagated to the NAT model from the teacher model. To alleviate this problem, we propose to expose the raw data to NAT models to restore the useful information of low-frequency words, which are missed in the distilled data. To this end, we introduce an extra Kullback-Leibler divergence term derived by comparing the lexical choice of NAT model and that embedded in the raw data. Experimental results across language pairs and model architectures demonstrate the effectiveness and universality of the proposed approach. Extensive analyses confirm our claim that our approach improves performance by reducing the lexical choice errors on low-frequency words. Encouragingly, our approach pushes the SOTA NAT performance on the WMT14 English-German and WMT16 Romanian-English datasets up to 27.8 and 33.8 BLEU points, respectively. The source code will be released."
                        ],
                        "paper": {
                            "corpus_id": 229923128,
                            "title": "Understanding and Improving Lexical Choice in Non-Autoregressive Translation",
                            "authors": [
                                {
                                    "authorId": "46573238",
                                    "name": "Liang Ding"
                                },
                                {
                                    "authorId": "1800190",
                                    "name": "Longyue Wang"
                                },
                                {
                                    "authorId": "1390611971",
                                    "name": "Xuebo Liu"
                                },
                                {
                                    "authorId": "1758353",
                                    "name": "Derek F. Wong"
                                },
                                {
                                    "authorId": "143719920",
                                    "name": "D. Tao"
                                },
                                {
                                    "authorId": "2909321",
                                    "name": "Zhaopeng Tu"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 77
                        },
                        "score": 0
                    },
                    {
                        "id": "(Guo et al., 2020)",
                        "snippets": [
                            "The masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks. However, few works have adopted this technique in the sequence-to-sequence models. In this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation~(NAT). Specifically, we first empirically study the functionalities of the encoder and the decoder in NAT models, and find that the encoder takes a more important role than the decoder regarding the translation quality. Therefore, we propose to train the encoder more rigorously by masking the encoder input while training. As for the decoder, we propose to train it based on the consecutive masking of the decoder input with an n-gram loss function to alleviate the problem of translating duplicate words. The two types of masks are applied to the model jointly at the training stage. We conduct experiments on five benchmark machine translation tasks, and our model can achieve 27.69/32.24 BLEU scores on WMT14 English-German/German-English tasks with 5+ times speed up compared with an autoregressive model."
                        ],
                        "paper": {
                            "corpus_id": 220046693,
                            "title": "Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "13838086",
                                    "name": "Junliang Guo"
                                },
                                {
                                    "authorId": "2230211",
                                    "name": "Linli Xu"
                                },
                                {
                                    "authorId": "2227868312",
                                    "name": "Enhong Chen"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 79
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training."
                        ],
                        "paper": {
                            "corpus_id": 260203057,
                            "title": "Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models",
                            "authors": [
                                {
                                    "authorId": "48622329",
                                    "name": "Mayee F. Chen"
                                },
                                {
                                    "authorId": "2069037355",
                                    "name": "Nicholas Roberts"
                                },
                                {
                                    "authorId": "144383716",
                                    "name": "K. Bhatia"
                                },
                                {
                                    "authorId": "39597242",
                                    "name": "Jue Wang"
                                },
                                {
                                    "authorId": "1776014",
                                    "name": "Ce Zhang"
                                },
                                {
                                    "authorId": "2186982588",
                                    "name": "Frederic Sala"
                                },
                                {
                                    "authorId": "1803218",
                                    "name": "Christopher R\u00e9"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 65
                        },
                        "score": 0.916015625
                    },
                    {
                        "id": "(Sow et al., 2025)",
                        "snippets": [
                            "Training Data Re-weighting/Selection for LLMs. Several recent studies (Xie et al., 2023;(Chen et al., 2023)Fan et al., 2023;Thakkar et al., 2023) have explored various reweighting techniques to enhance the generalization and efficiency of language models pretraining. For instance, Xie et al. ( 2023) and Fan et al. (2023) optimize the composition of pretraining corpora to achieve better performance across pretraining domains or for out-of-domain generalization. (Chen et al., 2023) introduce a framework for ordered skill learning, optimizing data selection based on how effectively it teaches interdependent skills for continual pretraining and fine-tuning regimes. Although effective, these techniques operate at the group level, whereas our work explores reweighting at the instance level, offering finer control over how individual samples are treated based on their loss values",
                            ".The approach in Fan & Jaggi (2023) offers a curriculum learning framework that prioritizes samples with a higher learnability score, which is precomputed using another auxiliary model similar to DoReMi and DoGE."
                        ],
                        "paper": {
                            "corpus_id": 276250237,
                            "title": "Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining",
                            "authors": [
                                {
                                    "authorId": "143827145",
                                    "name": "Daouda Sow"
                                },
                                {
                                    "authorId": "2220057323",
                                    "name": "Herbert Woisetschl\u00e4ger"
                                },
                                {
                                    "authorId": "35693339",
                                    "name": "Saikiran Bulusu"
                                },
                                {
                                    "authorId": "2255363698",
                                    "name": "Shiqiang Wang"
                                },
                                {
                                    "authorId": "2254179381",
                                    "name": "Hans-Arno Jacobsen"
                                },
                                {
                                    "authorId": "2344954540",
                                    "name": "Yingbin Liang"
                                }
                            ],
                            "year": 2025,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 6
                        },
                        "score": 0.81494140625
                    },
                    {
                        "id": "(Kim et al., 2024)",
                        "snippets": [
                            "The rapid advancement of Large Language Models (LLMs) has improved text understanding and generation but poses challenges in computational resources. This study proposes a curriculum learning-inspired, data-centric training strategy that begins with simpler tasks and progresses to more complex ones, using criteria such as prompt length, attention scores, and loss values to structure the training data."
                        ],
                        "paper": {
                            "corpus_id": 269756933,
                            "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "2301165169",
                                    "name": "Jisu Kim"
                                },
                                {
                                    "authorId": "2301167177",
                                    "name": "Juhwan Lee"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.67236328125
                    },
                    {
                        "id": "(Yu et al., 2024)",
                        "snippets": [
                            "On the other hand, many researchers have proposed curriculum learning strategies that dynamically adjust the data distribution in the pretraining [11; 24; 47; 51; 56; 62]. ELECTRA-style models (208229926) incorporated curriculum learning in their pretraining process by synchronously training the model with an auxiliary generator, which provided more and more difficult training signals for the discriminator. This implicit curriculum significantly improved the pretraining efficiency on denoising language models [4; 11; 19; 39; 65]. Other methods have explicitly designed the curriculum for pretraining data selection, such as decreasing gradient norms (Thakkar et al., 2023), least certainty [24; 51], and increasing expertise (Wettig et al., 2024), demonstrating the benefits of adapting the pretraining data signal according to the model's ever-changing preferences."
                        ],
                        "paper": {
                            "corpus_id": 270371045,
                            "title": "MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models",
                            "authors": [
                                {
                                    "authorId": "2275526493",
                                    "name": "Zichun Yu"
                                },
                                {
                                    "authorId": "2305649905",
                                    "name": "Spandan Das"
                                },
                                {
                                    "authorId": "2275170116",
                                    "name": "Chenyan Xiong"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 37
                        },
                        "score": 0.8916015625
                    },
                    {
                        "id": "(Wettig et al., 2024)",
                        "snippets": [
                            "Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that can capture human intuitions about data quality. In this paper, we investigate four qualities - writing style, required expertise, facts&trivia, and educational value - and find that LLMs are able to discern these qualities, especially when making pairwise judgments of texts. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity. When we sample using quality ratings as logits over documents, our models obtain lower perplexity and stronger in-context learning performance than baselines. Our best model is based on educational value and performs similarly to a model trained with uniform sampling for 50% more steps. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications."
                        ],
                        "paper": {
                            "corpus_id": 267681974,
                            "title": "QuRating: Selecting High-Quality Data for Training Language Models",
                            "authors": [
                                {
                                    "authorId": "2127066887",
                                    "name": "Alexander Wettig"
                                },
                                {
                                    "authorId": "2284268826",
                                    "name": "Aatmik Gupta"
                                },
                                {
                                    "authorId": "2323513320",
                                    "name": "Saumya Malik"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 79
                        },
                        "score": 0.94970703125
                    },
                    {
                        "id": "(Thakkar et al., 2023)",
                        "snippets": [
                            "Language Models (LMs) pre-trained with self-supervision on large text corpora have become the default starting point for developing models for various NLP tasks. Once the pre-training corpus has been assembled, all data samples in the corpus are treated with equal importance during LM pre-training. However, due to varying levels of relevance and quality of data, equal importance to all the data samples may not be the optimal choice. While data reweighting has been explored in the context of task-specific supervised learning and LM fine-tuning, model-driven reweighting for pre-training data has not been explored. We fill this important gap and propose PRESENCE, a method for jointly reweighting samples by leveraging self-influence (SI) scores as an indicator of sample importance and pre-training. PRESENCE promotes novelty and stability for model pre-training. Through extensive analysis spanning multiple model sizes, datasets, and tasks, we present PRESENCE as an important first step in the research direction of sample reweighting for pre-training language models."
                        ],
                        "paper": {
                            "corpus_id": 264935129,
                            "title": "Self-Influence Guided Data Reweighting for Language Model Pre-training",
                            "authors": [
                                {
                                    "authorId": "2264977662",
                                    "name": "Megh Thakkar"
                                },
                                {
                                    "authorId": "2843215",
                                    "name": "Tolga Bolukbasi"
                                },
                                {
                                    "authorId": "1726355",
                                    "name": "Sriram Ganapathy"
                                },
                                {
                                    "authorId": "3404827",
                                    "name": "Shikhar Vashishth"
                                },
                                {
                                    "authorId": "123607932",
                                    "name": "Sarath Chandar"
                                },
                                {
                                    "authorId": "2408872",
                                    "name": "P. Talukdar"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 26
                        },
                        "score": 0
                    },
                    {
                        "id": "(Qin et al., 2024)",
                        "snippets": [
                            "We aim at finding the most informative subset S b \u2282 S from the entire set S under the given budget |S b | \u2264 b. Mathematically, the selection of S b requires the quantitative evaluation q(\u2022) on each datapoint x i and an elaborated sampling mechanism \u03c0: \n\nwhere \u03c0(\u2022, b) denotes the sampling process with a maximum b datapoints. With respect to the detailed implementation of \u03c0, either an iterative, greedy algorithm or a batch-wise heuristic rule can be adopted for compatibility with q(\u2022). The expected benefits of such selection include: 1) the reduction of noise by ignoring those mislabeled, mismatched instruction-response pairs, 2) the re-balance of data distributions by down-sampling those easy, common, and similar examples while up-sampling hard, rare ones, and 3) the expedition of training in return for efficient iterations of LLMs."
                        ],
                        "paper": {
                            "corpus_id": 271710435,
                            "title": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models",
                            "authors": [
                                {
                                    "authorId": "2267903811",
                                    "name": "Yulei Qin"
                                },
                                {
                                    "authorId": "2184288111",
                                    "name": "Yuncheng Yang"
                                },
                                {
                                    "authorId": "2314919460",
                                    "name": "Pengcheng Guo"
                                },
                                {
                                    "authorId": "2315086401",
                                    "name": "Gang Li"
                                },
                                {
                                    "authorId": "2314834176",
                                    "name": "Hang Shao"
                                },
                                {
                                    "authorId": "2315154473",
                                    "name": "Yuchen Shi"
                                },
                                {
                                    "authorId": "2267376632",
                                    "name": "Zihan Xu"
                                },
                                {
                                    "authorId": "2258793023",
                                    "name": "Yun Gu"
                                },
                                {
                                    "authorId": "2257346950",
                                    "name": "Ke Li"
                                },
                                {
                                    "authorId": "2314886050",
                                    "name": "Xing Sun"
                                }
                            ],
                            "year": 2024,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 13
                        },
                        "score": 0.943359375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Difficulty Measurement Techniques",
                "tldr": "Difficulty measurement in curriculum learning employs various metrics to determine the complexity of training examples, from cross-entropy and uncertainty scores to learning percentages and model competence indicators. These techniques enable effective implementation of easy-to-hard training progressions by quantifying sample difficulty from both data-centric and model-centric perspectives. (8 sources)",
                "text": "\nMeasuring the difficulty of training examples is a fundamental component of curriculum learning, providing the basis for organizing training data in meaningful sequences. One prominent approach uses uncertainty at the data level to establish easy-to-hard ordering, with higher cross-entropy and uncertainty values indicating more difficult samples to translate <Paper corpusId=\"220047761\" paperTitle=\"(Zhou et al., 2020)\" isShortName></Paper> <Paper corpusId=\"231709290\" paperTitle=\"(Soviany et al., 2021)\" isShortName></Paper>. This uncertainty-based difficulty assessment has proven particularly effective in neural machine translation, where it helps determine when to introduce more challenging examples based on the model's current capabilities.\n\nFrom a model-centric perspective, researchers have developed metrics that measure sample difficulty based on how the model itself interacts with the data. The learning percentage approach evaluates how quickly a model learns from different examples, with samples that yield more learning in earlier epochs being classified as easier <Paper corpusId=\"267740312\" paperTitle=\"(Mekala et al., 2024)\" isShortName></Paper>. This self-ranking methodology draws inspiration from the learning order metric that observes the memorization patterns of neural networks, which typically learn clean examples before noisy ones <Paper corpusId=\"249060677\" paperTitle=\"(Mekala et al., 2022)\" isShortName></Paper>.\n\nMore sophisticated difficulty measurement techniques incorporate multiple criteria to create structured training progressions. Some approaches use prompt length, attention scores, and loss values to determine the complexity of training data, enabling curriculum-based training that progresses from simpler to more complex tasks <Paper corpusId=\"269756933\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>. These multi-dimensional difficulty metrics provide a more nuanced assessment of sample complexity than single-criterion approaches.\n\nThe difficulty measurement process is critical because \"feeding data in a totally random order is not always a good choice\" <Paper corpusId=\"388785\" paperTitle=\"(Fan et al., 2017)\" isShortName></Paper>. Effective difficulty metrics enable curriculum learning to distinguish between \"easy\" examples that benefit early training stages and \"harder\" examples that become more valuable as training progresses. This principle aligns with observations from curriculum learning and self-paced learning, where the appropriate \"hardness\" of training data changes throughout the model's development <Paper corpusId=\"388785\" paperTitle=\"(Fan et al., 2017)\" isShortName></Paper>.\n\nAs the field advances, difficulty measurement continues to evolve beyond simple heuristic rules, with researchers developing increasingly sophisticated techniques to quantify training example complexity. These measures form the foundation for the second key component of curriculum learning\u2014training schedulers that determine when and how different examples are presented to the model <Paper corpusId=\"232362223\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper> <Paper corpusId=\"265506572\" paperTitle=\"(Chobey et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Zhou et al., 2020)",
                        "snippets": [
                            "Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule."
                        ],
                        "paper": {
                            "corpus_id": 220047761,
                            "title": "Uncertainty-Aware Curriculum Learning for Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "2110349143",
                                    "name": "Yikai Zhou"
                                },
                                {
                                    "authorId": "21299583",
                                    "name": "Baosong Yang"
                                },
                                {
                                    "authorId": "1758353",
                                    "name": "Derek F. Wong"
                                },
                                {
                                    "authorId": "153379180",
                                    "name": "Yu Wan"
                                },
                                {
                                    "authorId": "1774304",
                                    "name": "Lidia S. Chao"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 96
                        },
                        "score": 0
                    },
                    {
                        "id": "(Soviany et al., 2021)",
                        "snippets": [
                            "(Zhou et al., 2020) introduce an uncertainty-based curriculum batching approach for neural machine translation. They propose using uncertainty at the data level, for establishing the easy-to-hard ordering, and the model level, to decide the right moment to enhance the training set with more difficult samples. To measure the difficulty of the examples, they start from the intuition that samples with higher crossentropy and uncertainty are more difficult to translate."
                        ],
                        "paper": {
                            "corpus_id": 231709290,
                            "title": "Curriculum Learning: A Survey",
                            "authors": [
                                {
                                    "authorId": "40901288",
                                    "name": "Petru Soviany"
                                },
                                {
                                    "authorId": "1817759",
                                    "name": "Radu Tudor Ionescu"
                                },
                                {
                                    "authorId": "39337007",
                                    "name": "Paolo Rota"
                                },
                                {
                                    "authorId": "1703601",
                                    "name": "N. Sebe"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Journal of Computer Vision",
                            "n_citations": 359
                        },
                        "score": 0.8095703125
                    },
                    {
                        "id": "(Mekala et al., 2024)",
                        "snippets": [
                            "In this paper, we delve into the measurement of sample difficulty from the model's perspective. Drawing inspiration from the learning order metric in (Mekala et al., 2022), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered. We then select the most difficult sample subsets based on this ranking and instruction-tune a language model."
                        ],
                        "paper": {
                            "corpus_id": 267740312,
                            "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
                            "authors": [
                                {
                                    "authorId": "7565696",
                                    "name": "Dheeraj Mekala"
                                },
                                {
                                    "authorId": "2284673632",
                                    "name": "Alex Nguyen"
                                },
                                {
                                    "authorId": "2284595153",
                                    "name": "Jingbo Shang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 21
                        },
                        "score": 0.9140625
                    },
                    {
                        "id": "(Mekala et al., 2022)",
                        "snippets": [
                            "Weakly supervised text classification methods typically train a deep neural classifier based on pseudo-labels. The quality of pseudo-labels is crucial to final performance but they are inevitably noisy due to their heuristic nature, so selecting the correct ones has a huge potential for performance boost. One straightforward solution is to select samples based on the softmax probability scores in the neural classifier corresponding to their pseudo-labels. However, we show through our experiments that such solutions are ineffective and unstable due to the erroneously high-confidence predictions from poorly calibrated models. Recent studies on the memorization effects of deep neural models suggest that these models first memorize training samples with clean labels and then those with noisy labels. Inspired by this observation, we propose a novel pseudo-label selection method LOPS that takes learning order of samples into consideration. We hypothesize that the learning order reflects the probability of wrong annotation in terms of ranking, and therefore, propose to select the samples that are learnt earlier. LOPS can be viewed as a strong performance-boost plug-in to most of existing weakly-supervised text classification methods, as confirmed in extensive experiments on four real-world datasets."
                        ],
                        "paper": {
                            "corpus_id": 249060677,
                            "title": "LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification",
                            "authors": [
                                {
                                    "authorId": "7565696",
                                    "name": "Dheeraj Mekala"
                                },
                                {
                                    "authorId": "2113540861",
                                    "name": "Chengyu Dong"
                                },
                                {
                                    "authorId": "2884976",
                                    "name": "Jingbo Shang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 20
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kim et al., 2024)",
                        "snippets": [
                            "The rapid advancement of Large Language Models (LLMs) has improved text understanding and generation but poses challenges in computational resources. This study proposes a curriculum learning-inspired, data-centric training strategy that begins with simpler tasks and progresses to more complex ones, using criteria such as prompt length, attention scores, and loss values to structure the training data."
                        ],
                        "paper": {
                            "corpus_id": 269756933,
                            "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "2301165169",
                                    "name": "Jisu Kim"
                                },
                                {
                                    "authorId": "2301167177",
                                    "name": "Juhwan Lee"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.67236328125
                    },
                    {
                        "id": "(Fan et al., 2017)",
                        "snippets": [
                            "Training data plays a critical role in machine learning. The data selection strategy along the training process could significantly impact the performance of the learned model. For example, an appropriate strategy of removing redundant data can lead to a better model by using less computational efforts. For another example, previous studies on curriculum learning (Bengio et al., 2009) and self-paced learning (Kumar et al., 2010) reveal some principles of how tailoring data based on its 'hardness' can favor the model training; that is, easy data instances are important at the early age of model training, while at later age, harder training examples tend to be more effective to improve the model, since easy ones bring minor changes.\n\nThese facts reveal that how to feed training samples into a machine learning system is nontrivial and feeding data in a totally random order is not always a good choice. To explore a better data selection strategy for training, previous works including curriculum learning (CL) and self-paced learning (SPL) adopt simple heuristic rules, such as shuffling the sequence length to train language model (Bengio et al., 2009), or abandoning training instances whose loss values are larger than a human-defined threshold (Kumar et al., 2010;Jiang et al., 2014a)."
                        ],
                        "paper": {
                            "corpus_id": 388785,
                            "title": "Learning What Data to Learn",
                            "authors": [
                                {
                                    "authorId": "144566102",
                                    "name": "Yang Fan"
                                },
                                {
                                    "authorId": "143853336",
                                    "name": "Fei Tian"
                                },
                                {
                                    "authorId": "143826491",
                                    "name": "Tao Qin"
                                },
                                {
                                    "authorId": "152441498",
                                    "name": "Jiang Bian"
                                },
                                {
                                    "authorId": "2110264337",
                                    "name": "Tie-Yan Liu"
                                }
                            ],
                            "year": 2017,
                            "venue": "arXiv.org",
                            "n_citations": 79
                        },
                        "score": 0.88134765625
                    },
                    {
                        "id": "(Wang et al., 2021)",
                        "snippets": [
                            "Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of <italic>Difficulty Measurer <inline-formula><tex-math notation=\"LaTeX\">$+$</tex-math><alternatives><mml:math><mml:mo>+</mml:mo></mml:math><inline-graphic xlink:href=\"wang-ieq1-3069908.gif\"/></alternatives></inline-formula> Training Scheduler</italic> and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. We also analyze principles to select different CL designs that may benefit practical applications. Finally, we present our insights on the relationships connecting CL and other machine learning concepts including transfer learning, meta-learning, continual learning and active learning, etc., then point out challenges in CL as well as potential future research directions deserving further investigations."
                        ],
                        "paper": {
                            "corpus_id": 232362223,
                            "title": "A Survey on Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "2153687490",
                                    "name": "Xin Wang"
                                },
                                {
                                    "authorId": "51310474",
                                    "name": "Yudong Chen"
                                },
                                {
                                    "authorId": "145583986",
                                    "name": "Wenwu Zhu"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                            "n_citations": 611
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chobey et al., 2023)",
                        "snippets": [
                            "Curriculum learning (Bengio et al., 2009) refers to training models through a difficulty-based ordering of training examples (i.e. a curriculum), most often \"starting small\" (Elman, 1993) from easy examples before progressing to increasingly difficult sentences. In NLP, curriculum learning has been widely used for Machine Translation (e.g., Platanios et al., 2019), but has also been applied more recently to other natural language understanding tasks (Xu et al., 2020). For a survey see (Soviany et al., 2021); (Wang et al., 2021). \n\nThere are two steps involved in designing a curriculum: assigning a difficulty score to every training example (\"difficulty measurer\") and using these difficulty scores to determine the order in which training examples are presented to the model (\"training scheduler\") (Wang et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 265506572,
                            "title": "Can training neural language models on a curriculum with developmentally plausible data improve alignment with human reading behavior?",
                            "authors": [
                                {
                                    "authorId": "2268760204",
                                    "name": "Aryaman Chobey"
                                },
                                {
                                    "authorId": "2268760018",
                                    "name": "Oliver Smith"
                                },
                                {
                                    "authorId": "2268796061",
                                    "name": "Anzi Wang"
                                },
                                {
                                    "authorId": "2268760229",
                                    "name": "Grusha Prasad"
                                }
                            ],
                            "year": 2023,
                            "venue": "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning",
                            "n_citations": 5
                        },
                        "score": 0.69384765625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Benefits of Curriculum Learning",
                "tldr": "Curriculum learning offers multiple benefits for language model training, including improved model performance, faster convergence, and greater data efficiency. These advantages stem from the strategic sequencing of training examples that aligns with how models naturally learn. (10 sources)",
                "text": "\nCurriculum learning provides several key benefits for training language models:\n\n* **Improved Model Performance**: By presenting training data in an easy-to-hard sequence, curriculum learning has been shown to improve performance across a variety of NLP tasks, particularly in machine translation <Paper corpusId=\"227905455\" paperTitle=\"(Laverghetta et al., 2020)\" isShortName></Paper>. This structured approach can lead to better final model accuracy compared to random data presentation <Paper corpusId=\"247762191\" paperTitle=\"(Mohiuddin et al., 2022)\" isShortName></Paper> <Paper corpusId=\"102350936\" paperTitle=\"(Hacohen et al., 2019)\" isShortName></Paper>.\n\n* **Faster Convergence**: Curriculum learning accelerates the training process by enabling models to learn more efficiently. Studies have demonstrated that carefully maneuvering the sequence of training data can improve both training efficiency and model accuracy <Paper corpusId=\"247762191\" paperTitle=\"(Mohiuddin et al., 2022)\" isShortName></Paper> <Paper corpusId=\"220047761\" paperTitle=\"(Zhou et al., 2020)\" isShortName></Paper>.\n\n* **Enhanced Data Efficiency**: Curriculum approaches allow models to learn effectively from smaller datasets, making training more computationally efficient. Ordering the selection of training data using active learning principles can lead to improvements in learning efficiently from smaller corpora <Paper corpusId=\"226964591\" paperTitle=\"(Pillai et al., 2020)\" isShortName></Paper>.\n\n* **Reduced Computational Resources**: For large language models that require significant computational power, curriculum learning offers a way to achieve comparable or better results with less training data. This is particularly valuable given the challenges posed by LLMs in terms of computational resources <Paper corpusId=\"269756933\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>.\n\n* **Elimination of Redundant Data**: Curriculum approaches can help identify and remove redundant training examples, leading to a better model with less computational effort <Paper corpusId=\"388785\" paperTitle=\"(Fan et al., 2017)\" isShortName></Paper>.\n\n* **Knowledge Transfer Between Models**: Curriculum learning can facilitate knowledge transfer between models of different sizes. The SmallToLarge (S2L) method leverages training trajectories from small models to guide data selection for larger models, making supervised fine-tuning more efficient <Paper corpusId=\"268363364\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>.\n\n* **Optimized Sample Combinations**: Beyond evaluating individual sample quality, curriculum approaches can address the combinatorial effects among samples, preventing issues that arise from intrinsic homogeneity or contradictions in the training data <Paper corpusId=\"271064746\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>.\n\n* **Adaptive Learning**: Unlike fixed training regimes, curriculum learning recognizes that the appropriate \"hardness\" of training data changes throughout a model's development. Easy examples benefit early training stages, while harder examples become more valuable as training progresses <Paper corpusId=\"388785\" paperTitle=\"(Fan et al., 2017)\" isShortName></Paper> <Paper corpusId=\"10891229\" paperTitle=\"(Jiang et al., 2015)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Laverghetta et al., 2020)",
                        "snippets": [
                            "Curriculum learning (Elman, 1993), a strategy where the model is trained on easier examples before harder ones, has recently been shown to improve performance and reduce training time on a variety of NLP tasks, especially machine translation (Liu et al., 2020;(Wang et al., 2019)(Zhou et al., 2020)(Xu et al., 2020). \n\nThe success of this research shows that the order in which training data is presented to a model is important, but how to best apply curriculum learning broadly to NLP and how to design effective measures of difficulty to create curricula remain relatively unexplored."
                        ],
                        "paper": {
                            "corpus_id": 227905455,
                            "title": "Towards a Task-Agnostic Model of Difficulty Estimation for Supervised Learning Tasks",
                            "authors": [
                                {
                                    "authorId": "2033736232",
                                    "name": "Antonio Laverghetta"
                                },
                                {
                                    "authorId": "1716200134",
                                    "name": "Jamshidbek Mirzakhalov"
                                },
                                {
                                    "authorId": "2143879",
                                    "name": "John Licato"
                                }
                            ],
                            "year": 2020,
                            "venue": "AACL",
                            "n_citations": 2
                        },
                        "score": 0.76953125
                    },
                    {
                        "id": "(Mohiuddin et al., 2022)",
                        "snippets": [
                            "Curriculum Learning Inspired by human learners, (Elman, 1993) argues that optimization of neural network training can be accelerated by gradually increasing the difficulty of the concepts. (Bengio et al., 2009) were the first to use the term \"curricu-lum learning\" to refer to the easy-to-hard training strategies in the context of machine learning. Using an easy-to-hard curriculum based on increasing vocabulary size in language model training, they achieved performance improvement. Recent work (Jiang et al., 2015)(Hacohen et al., 2019)(Zhou et al., 2020) shows that manoeuvring the sequence of training data can improve both training efficiency and model accuracy."
                        ],
                        "paper": {
                            "corpus_id": 247762191,
                            "title": "Data Selection Curriculum for Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "6838342",
                                    "name": "Tasnim Mohiuddin"
                                },
                                {
                                    "authorId": "1755162",
                                    "name": "Philipp Koehn"
                                },
                                {
                                    "authorId": "113810201",
                                    "name": "Vishrav Chaudhary"
                                },
                                {
                                    "authorId": "2059363961",
                                    "name": "James Cross"
                                },
                                {
                                    "authorId": "2116473",
                                    "name": "Shruti Bhosale"
                                },
                                {
                                    "authorId": "2708940",
                                    "name": "Shafiq R. Joty"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 13
                        },
                        "score": 0.71240234375
                    },
                    {
                        "id": "(Hacohen et al., 2019)",
                        "snippets": [
                            "Training neural networks is traditionally done by providing a sequence of random mini-batches sampled uniformly from the entire training data. In this work, we analyze the effect of curriculum learning, which involves the non-uniform sampling of mini-batches, on the training of deep networks, and specifically CNNs trained for image recognition. To employ curriculum learning, the training algorithm must resolve 2 problems: (i) sort the training examples by difficulty; (ii) compute a series of mini-batches that exhibit an increasing level of difficulty. We address challenge (i) using two methods: transfer learning from some competitive ``teacher\" network, and bootstrapping. In our empirical evaluation, both methods show similar benefits in terms of increased learning speed and improved final performance on test data. We address challenge (ii) by investigating different pacing functions to guide the sampling. The empirical investigation includes a variety of network architectures, using images from CIFAR-10, CIFAR-100 and subsets of ImageNet. We conclude with a novel theoretical analysis of curriculum learning, where we show how it effectively modifies the optimization landscape. We then define the concept of an ideal curriculum, and show that under mild conditions it does not change the corresponding global minimum of the optimization function."
                        ],
                        "paper": {
                            "corpus_id": 102350936,
                            "title": "On The Power of Curriculum Learning in Training Deep Networks",
                            "authors": [
                                {
                                    "authorId": "94064232",
                                    "name": "Guy Hacohen"
                                },
                                {
                                    "authorId": "1789171",
                                    "name": "D. Weinshall"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 449
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhou et al., 2020)",
                        "snippets": [
                            "Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule."
                        ],
                        "paper": {
                            "corpus_id": 220047761,
                            "title": "Uncertainty-Aware Curriculum Learning for Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "2110349143",
                                    "name": "Yikai Zhou"
                                },
                                {
                                    "authorId": "21299583",
                                    "name": "Baosong Yang"
                                },
                                {
                                    "authorId": "1758353",
                                    "name": "Derek F. Wong"
                                },
                                {
                                    "authorId": "153379180",
                                    "name": "Yu Wan"
                                },
                                {
                                    "authorId": "1774304",
                                    "name": "Lidia S. Chao"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 96
                        },
                        "score": 0
                    },
                    {
                        "id": "(Pillai et al., 2020)",
                        "snippets": [
                            "Ordering the selection of training data using active learning can lead to improvements in learning efficiently from smaller corpora."
                        ],
                        "paper": {
                            "corpus_id": 226964591,
                            "title": "Sampling Approach Matters: Active Learning for Robotic Language Acquisition",
                            "authors": [
                                {
                                    "authorId": "144339038",
                                    "name": "Nisha Pillai"
                                },
                                {
                                    "authorId": "34885007",
                                    "name": "Edward Raff"
                                },
                                {
                                    "authorId": "2064957151",
                                    "name": "Francis Ferraro"
                                },
                                {
                                    "authorId": "2674440",
                                    "name": "Cynthia Matuszek"
                                }
                            ],
                            "year": 2020,
                            "venue": "2020 IEEE International Conference on Big Data (Big Data)",
                            "n_citations": 3
                        },
                        "score": 0.72509765625
                    },
                    {
                        "id": "(Kim et al., 2024)",
                        "snippets": [
                            "The rapid advancement of Large Language Models (LLMs) has improved text understanding and generation but poses challenges in computational resources. This study proposes a curriculum learning-inspired, data-centric training strategy that begins with simpler tasks and progresses to more complex ones, using criteria such as prompt length, attention scores, and loss values to structure the training data."
                        ],
                        "paper": {
                            "corpus_id": 269756933,
                            "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "2301165169",
                                    "name": "Jisu Kim"
                                },
                                {
                                    "authorId": "2301167177",
                                    "name": "Juhwan Lee"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.67236328125
                    },
                    {
                        "id": "(Fan et al., 2017)",
                        "snippets": [
                            "Training data plays a critical role in machine learning. The data selection strategy along the training process could significantly impact the performance of the learned model. For example, an appropriate strategy of removing redundant data can lead to a better model by using less computational efforts. For another example, previous studies on curriculum learning (Bengio et al., 2009) and self-paced learning (Kumar et al., 2010) reveal some principles of how tailoring data based on its 'hardness' can favor the model training; that is, easy data instances are important at the early age of model training, while at later age, harder training examples tend to be more effective to improve the model, since easy ones bring minor changes.\n\nThese facts reveal that how to feed training samples into a machine learning system is nontrivial and feeding data in a totally random order is not always a good choice. To explore a better data selection strategy for training, previous works including curriculum learning (CL) and self-paced learning (SPL) adopt simple heuristic rules, such as shuffling the sequence length to train language model (Bengio et al., 2009), or abandoning training instances whose loss values are larger than a human-defined threshold (Kumar et al., 2010;Jiang et al., 2014a)."
                        ],
                        "paper": {
                            "corpus_id": 388785,
                            "title": "Learning What Data to Learn",
                            "authors": [
                                {
                                    "authorId": "144566102",
                                    "name": "Yang Fan"
                                },
                                {
                                    "authorId": "143853336",
                                    "name": "Fei Tian"
                                },
                                {
                                    "authorId": "143826491",
                                    "name": "Tao Qin"
                                },
                                {
                                    "authorId": "152441498",
                                    "name": "Jiang Bian"
                                },
                                {
                                    "authorId": "2110264337",
                                    "name": "Tie-Yan Liu"
                                }
                            ],
                            "year": 2017,
                            "venue": "arXiv.org",
                            "n_citations": 79
                        },
                        "score": 0.88134765625
                    },
                    {
                        "id": "(Yang et al., 2024)",
                        "snippets": [
                            "Despite the effectiveness of data selection for large language models (LLMs) during pretraining and instruction fine-tuning phases, improving data efficiency in supervised fine-tuning (SFT) for specialized domains poses significant challenges due to the complexity of fine-tuning data. To bridge this gap, we introduce an effective and scalable data selection method for SFT, SmallToLarge (S2L), which leverages training trajectories from small models to guide the data selection for larger models."
                        ],
                        "paper": {
                            "corpus_id": 268363364,
                            "title": "SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models",
                            "authors": [
                                {
                                    "authorId": "2291083514",
                                    "name": "Yu Yang"
                                },
                                {
                                    "authorId": "2290975932",
                                    "name": "Siddhartha Mishra"
                                },
                                {
                                    "authorId": "2290912016",
                                    "name": "Jeffrey N Chiang"
                                },
                                {
                                    "authorId": "2389094",
                                    "name": "Baharan Mirzasoleiman"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 23
                        },
                        "score": 0.86669921875
                    },
                    {
                        "id": "(Yin et al., 2024)",
                        "snippets": [
                            "Data is the cornerstone of large language models (LLMs), but not all data is useful for model learning. Carefully selected data can better elicit the capabilities of LLMs with much less computational overhead. Most methods concentrate on evaluating the quality of individual samples in data selection, while the combinatorial effects among samples are neglected. Even if each sample is of perfect quality, their combinations may be suboptimal in teaching LLMs due to their intrinsic homogeneity or contradiction."
                        ],
                        "paper": {
                            "corpus_id": 271064746,
                            "title": "Entropy Law: The Story Behind Data Compression and LLM Performance",
                            "authors": [
                                {
                                    "authorId": "2260649859",
                                    "name": "Mingjia Yin"
                                },
                                {
                                    "authorId": "2276003321",
                                    "name": "Chuhan Wu"
                                },
                                {
                                    "authorId": "2283529247",
                                    "name": "Yufei Wang"
                                },
                                {
                                    "authorId": "2256768674",
                                    "name": "Hao Wang"
                                },
                                {
                                    "authorId": "2260810851",
                                    "name": "Wei Guo"
                                },
                                {
                                    "authorId": "2282544603",
                                    "name": "Yasheng Wang"
                                },
                                {
                                    "authorId": "2293683997",
                                    "name": "Yong Liu"
                                },
                                {
                                    "authorId": "2284295184",
                                    "name": "Ruiming Tang"
                                },
                                {
                                    "authorId": "1862782",
                                    "name": "Defu Lian"
                                },
                                {
                                    "authorId": "2113754294",
                                    "name": "Enhong Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 27
                        },
                        "score": 0.85693359375
                    },
                    {
                        "id": "(Jiang et al., 2015)",
                        "snippets": [
                            "Curriculum learning (CL) or self-paced learning (SPL) represents a recently proposed learning regime inspired by the learning process of humans and animals that gradually proceeds from easy to more complex samples in training. The two methods share a similar conceptual learning paradigm, but differ in specific learning schemes. In CL, the curriculum is predetermined by prior knowledge, and remain fixed thereafter. Therefore, this type of method heavily relies on the quality of prior knowledge while ignoring feedback about the learner. In SPL, the curriculum is dynamically determined to adjust to the learning pace of the leaner. However, SPL is unable to deal with prior knowledge, rendering it prone to overfitting. In this paper, we discover the missing link between CL and SPL, and propose a unified framework named self-paced curriculum leaning (SPCL). SPCL is formulated as a concise optimization problem that takes into account both prior knowledge known before training and the learning progress during training. In comparison to human education, SPCL is analogous to \"instructor-student-collaborative\" learning mode, as opposed to \"instructor-driven\" in CL or \"student-driven\" in SPL. Empirically, we show that the advantage of SPCL on two tasks."
                        ],
                        "paper": {
                            "corpus_id": 10891229,
                            "title": "Self-Paced Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "39978626",
                                    "name": "Lu Jiang"
                                },
                                {
                                    "authorId": "1803714",
                                    "name": "Deyu Meng"
                                },
                                {
                                    "authorId": "46317290",
                                    "name": "Qian Zhao"
                                },
                                {
                                    "authorId": "145455919",
                                    "name": "S. Shan"
                                },
                                {
                                    "authorId": "7661726",
                                    "name": "Alexander Hauptmann"
                                }
                            ],
                            "year": 2015,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 526
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Applications in Language Models",
                "tldr": "Curriculum learning is applied across various language model tasks, from machine translation to large-scale pretraining. These applications demonstrate how strategic data sequencing enhances model performance through improved translation quality, more efficient fine-tuning, and better resource utilization. (19 sources)",
                "text": "\nCurriculum learning has found widespread applications across various language model tasks, most notably in machine translation. For non-autoregressive translation (NAT), researchers have implemented a hard-to-easy learning strategy that gradually decreases the ratio of raw data in the training process, building on evidence that pretraining with raw data can improve performance by rejuvenating low-frequency words <Paper corpusId=\"257901096\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"229923128\" paperTitle=\"(Ding et al., 2020)\" isShortName></Paper>. This approach has proven particularly effective for masked sequence-to-sequence models designed with n-gram loss functions to address specific translation challenges <Paper corpusId=\"220046693\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper> <Paper corpusId=\"221150562\" paperTitle=\"(Qian et al., 2020)\" isShortName></Paper>.\n\nFor large language model (LLM) pretraining, curriculum learning approaches have evolved beyond simple heuristics. Recent research has explored explicit curriculum design for pretraining data selection, including methods based on decreasing gradient norms, least certainty, and increasing expertise <Paper corpusId=\"270371045\" paperTitle=\"(Yu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"264935129\" paperTitle=\"(Thakkar et al., 2023)\" isShortName></Paper>. The QuRating method, for example, uses language models to rate pretraining data based on writing style, required expertise, factual content, and educational value, creating a training curriculum that improves performance without changing the training dataset <Paper corpusId=\"267681974\" paperTitle=\"(Wettig et al., 2024)\" isShortName></Paper>. Similarly, PRESENCE jointly reweights samples using self-influence scores as indicators of sample importance <Paper corpusId=\"264935129\" paperTitle=\"(Thakkar et al., 2023)\" isShortName></Paper>.\n\nIn the context of fine-tuning LLMs, curriculum learning has shown that high-quality data selection can dramatically reduce data requirements. LIMA demonstrated that by carefully curating just 1,000 high-quality data points, models can achieve remarkable performance that generalizes to unseen data <Paper corpusId=\"274130626\" paperTitle=\"(Khera et al., 2024)\" isShortName></Paper>. This aligns with findings from Zhou et al., who showed that almost all knowledge in large language models is learned during pretraining, with only limited instruction tuning data necessary to teach models to produce high-quality output <Paper corpusId=\"258822910\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper>.\n\nThe Skill-It algorithm implements curriculum learning by sampling data based on prerequisite skills, showing significant improvements in both continual pre-training and fine-tuning scenarios <Paper corpusId=\"260203057\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. This approach is based on the hypothesis that models, like humans, acquire interdependent skills in a natural order <Paper corpusId=\"276250237\" paperTitle=\"(Sow et al., 2025)\" isShortName></Paper>. On the LEGO synthetic dataset in continual pre-training, Skill-It achieved 36.5 points higher accuracy than random sampling, while on the Natural Instructions dataset in fine-tuning, it reduced validation loss by 13.6% compared to training solely on target skill data <Paper corpusId=\"260203057\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>.\n\nTraditional online batch selection methods that select hard samples with high loss or high gradient norm often require a second forward/backward pass, introducing large computational costs that hurt scalability for large models <Paper corpusId=\"264439064\" paperTitle=\"(Fan et al., 2023)\" isShortName></Paper> <Paper corpusId=\"3663876\" paperTitle=\"(Katharopoulos et al., 2018)\" isShortName></Paper>. To address this challenge, researchers have explored group-level data selection approaches that consider samples in large groups with common characteristics, often derived from metadata such as web domains or source collection names <Paper corpusId=\"276575595\" paperTitle=\"(Belenki et al., 2025)\" isShortName></Paper>.\n\nIt's important to note that while data selection is crucial for effective training, simply filtering for examples that match human notions of data quality doesn't always improve model behavior. Research has found that selecting according to similarity with \"high-quality\" data sources may not increase (and can even hurt) performance compared to randomly selecting data <Paper corpusId=\"267094971\" paperTitle=\"(Engstrom et al., 2024)\" isShortName></Paper>. This underscores the complexity of curriculum design and the importance of empirical validation for any data selection strategy.\n\nThe efficiency gains from curriculum learning are particularly valuable for large language models, where the quality of training data significantly impacts the statistical efficiency of the training procedure and model performance <Paper corpusId=\"273233719\" paperTitle=\"(Bai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"245124124\" paperTitle=\"(Du et al., 2021)\" isShortName></Paper> <Paper corpusId=\"247951931\" paperTitle=\"(Chowdhery et al., 2022)\" isShortName></Paper>. Recent approaches like DoReMi, which optimizes domain weights for training without knowledge of downstream tasks, have shown remarkable efficiency improvements, reaching baseline accuracy with 2.6x fewer training steps <Paper corpusId=\"258741043\" paperTitle=\"(Xie et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "Motivated by the success of curriculum learning (Qian et al., 2020)(Guo et al., 2020)Liu et al. 2020), we further introduce a hard-to-easy learning strategy to improve the performance. (Ding et al., 2020) show that pretraining with raw data can improve the performance of NAT by rejuvenating low-frequency words. To keep the merits of low-mode, they further trained the pretrained model on distilled data. We combine this idea with our data selection method by decreasing the ratio of raw data in the training process."
                        ],
                        "paper": {
                            "corpus_id": 257901096,
                            "title": "Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "2226017688",
                                    "name": "Min Liu"
                                },
                                {
                                    "authorId": "145854784",
                                    "name": "Yu Bao"
                                },
                                {
                                    "authorId": "144562857",
                                    "name": "Chengqi Zhao"
                                },
                                {
                                    "authorId": "2124946880",
                                    "name": "Shujian Huang"
                                }
                            ],
                            "year": 2023,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 3
                        },
                        "score": 0.71142578125
                    },
                    {
                        "id": "(Ding et al., 2020)",
                        "snippets": [
                            "Knowledge distillation (KD) is essential for training non-autoregressive translation (NAT) models by reducing the complexity of the raw data with an autoregressive teacher model. In this study, we empirically show that as a side effect of this training, the lexical choice errors on low-frequency words are propagated to the NAT model from the teacher model. To alleviate this problem, we propose to expose the raw data to NAT models to restore the useful information of low-frequency words, which are missed in the distilled data. To this end, we introduce an extra Kullback-Leibler divergence term derived by comparing the lexical choice of NAT model and that embedded in the raw data. Experimental results across language pairs and model architectures demonstrate the effectiveness and universality of the proposed approach. Extensive analyses confirm our claim that our approach improves performance by reducing the lexical choice errors on low-frequency words. Encouragingly, our approach pushes the SOTA NAT performance on the WMT14 English-German and WMT16 Romanian-English datasets up to 27.8 and 33.8 BLEU points, respectively. The source code will be released."
                        ],
                        "paper": {
                            "corpus_id": 229923128,
                            "title": "Understanding and Improving Lexical Choice in Non-Autoregressive Translation",
                            "authors": [
                                {
                                    "authorId": "46573238",
                                    "name": "Liang Ding"
                                },
                                {
                                    "authorId": "1800190",
                                    "name": "Longyue Wang"
                                },
                                {
                                    "authorId": "1390611971",
                                    "name": "Xuebo Liu"
                                },
                                {
                                    "authorId": "1758353",
                                    "name": "Derek F. Wong"
                                },
                                {
                                    "authorId": "143719920",
                                    "name": "D. Tao"
                                },
                                {
                                    "authorId": "2909321",
                                    "name": "Zhaopeng Tu"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 77
                        },
                        "score": 0
                    },
                    {
                        "id": "(Guo et al., 2020)",
                        "snippets": [
                            "The masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks. However, few works have adopted this technique in the sequence-to-sequence models. In this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation~(NAT). Specifically, we first empirically study the functionalities of the encoder and the decoder in NAT models, and find that the encoder takes a more important role than the decoder regarding the translation quality. Therefore, we propose to train the encoder more rigorously by masking the encoder input while training. As for the decoder, we propose to train it based on the consecutive masking of the decoder input with an n-gram loss function to alleviate the problem of translating duplicate words. The two types of masks are applied to the model jointly at the training stage. We conduct experiments on five benchmark machine translation tasks, and our model can achieve 27.69/32.24 BLEU scores on WMT14 English-German/German-English tasks with 5+ times speed up compared with an autoregressive model."
                        ],
                        "paper": {
                            "corpus_id": 220046693,
                            "title": "Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "13838086",
                                    "name": "Junliang Guo"
                                },
                                {
                                    "authorId": "2230211",
                                    "name": "Linli Xu"
                                },
                                {
                                    "authorId": "2227868312",
                                    "name": "Enhong Chen"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 79
                        },
                        "score": 0
                    },
                    {
                        "id": "(Qian et al., 2020)",
                        "snippets": [
                            "Recent work on non-autoregressive neural machine translation (NAT) aims at improving the efficiency by parallel decoding without sacrificing the quality. However, existing NAT methods are either inferior to Transformer or require multiple decoding passes, leading to reduced speedup. We propose the Glancing Language Model (GLM) for single-pass parallel generation models. With GLM, we develop Glancing Transformer (GLAT) for machine translation. With only single-pass parallel decoding, GLAT is able to generate high-quality translation with 8\u00d7-15\u00d7 speedup. Note that GLAT does not modify the network architecture, which is a training method to learn word interdependency. Experiments on multiple WMT language directions show that GLAT outperforms all previous single pass non-autoregressive methods, and is nearly comparable to Transformer, reducing the gap to 0.25-0.9 BLEU points."
                        ],
                        "paper": {
                            "corpus_id": 221150562,
                            "title": "Glancing Transformer for Non-Autoregressive Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "2072789464",
                                    "name": "Lihua Qian"
                                },
                                {
                                    "authorId": "2111824520",
                                    "name": "Hao Zhou"
                                },
                                {
                                    "authorId": "145854784",
                                    "name": "Yu Bao"
                                },
                                {
                                    "authorId": "50468534",
                                    "name": "Mingxuan Wang"
                                },
                                {
                                    "authorId": "2068178455",
                                    "name": "Lin Qiu"
                                },
                                {
                                    "authorId": "2108309275",
                                    "name": "Weinan Zhang"
                                },
                                {
                                    "authorId": "1811427",
                                    "name": "Yong Yu"
                                },
                                {
                                    "authorId": "143900005",
                                    "name": "Lei Li"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 158
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yu et al., 2024)",
                        "snippets": [
                            "On the other hand, many researchers have proposed curriculum learning strategies that dynamically adjust the data distribution in the pretraining [11; 24; 47; 51; 56; 62]. ELECTRA-style models (208229926) incorporated curriculum learning in their pretraining process by synchronously training the model with an auxiliary generator, which provided more and more difficult training signals for the discriminator. This implicit curriculum significantly improved the pretraining efficiency on denoising language models [4; 11; 19; 39; 65]. Other methods have explicitly designed the curriculum for pretraining data selection, such as decreasing gradient norms (Thakkar et al., 2023), least certainty [24; 51], and increasing expertise (Wettig et al., 2024), demonstrating the benefits of adapting the pretraining data signal according to the model's ever-changing preferences."
                        ],
                        "paper": {
                            "corpus_id": 270371045,
                            "title": "MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models",
                            "authors": [
                                {
                                    "authorId": "2275526493",
                                    "name": "Zichun Yu"
                                },
                                {
                                    "authorId": "2305649905",
                                    "name": "Spandan Das"
                                },
                                {
                                    "authorId": "2275170116",
                                    "name": "Chenyan Xiong"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 37
                        },
                        "score": 0.8916015625
                    },
                    {
                        "id": "(Thakkar et al., 2023)",
                        "snippets": [
                            "Language Models (LMs) pre-trained with self-supervision on large text corpora have become the default starting point for developing models for various NLP tasks. Once the pre-training corpus has been assembled, all data samples in the corpus are treated with equal importance during LM pre-training. However, due to varying levels of relevance and quality of data, equal importance to all the data samples may not be the optimal choice. While data reweighting has been explored in the context of task-specific supervised learning and LM fine-tuning, model-driven reweighting for pre-training data has not been explored. We fill this important gap and propose PRESENCE, a method for jointly reweighting samples by leveraging self-influence (SI) scores as an indicator of sample importance and pre-training. PRESENCE promotes novelty and stability for model pre-training. Through extensive analysis spanning multiple model sizes, datasets, and tasks, we present PRESENCE as an important first step in the research direction of sample reweighting for pre-training language models."
                        ],
                        "paper": {
                            "corpus_id": 264935129,
                            "title": "Self-Influence Guided Data Reweighting for Language Model Pre-training",
                            "authors": [
                                {
                                    "authorId": "2264977662",
                                    "name": "Megh Thakkar"
                                },
                                {
                                    "authorId": "2843215",
                                    "name": "Tolga Bolukbasi"
                                },
                                {
                                    "authorId": "1726355",
                                    "name": "Sriram Ganapathy"
                                },
                                {
                                    "authorId": "3404827",
                                    "name": "Shikhar Vashishth"
                                },
                                {
                                    "authorId": "123607932",
                                    "name": "Sarath Chandar"
                                },
                                {
                                    "authorId": "2408872",
                                    "name": "P. Talukdar"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 26
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wettig et al., 2024)",
                        "snippets": [
                            "Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that can capture human intuitions about data quality. In this paper, we investigate four qualities - writing style, required expertise, facts&trivia, and educational value - and find that LLMs are able to discern these qualities, especially when making pairwise judgments of texts. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity. When we sample using quality ratings as logits over documents, our models obtain lower perplexity and stronger in-context learning performance than baselines. Our best model is based on educational value and performs similarly to a model trained with uniform sampling for 50% more steps. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications."
                        ],
                        "paper": {
                            "corpus_id": 267681974,
                            "title": "QuRating: Selecting High-Quality Data for Training Language Models",
                            "authors": [
                                {
                                    "authorId": "2127066887",
                                    "name": "Alexander Wettig"
                                },
                                {
                                    "authorId": "2284268826",
                                    "name": "Aatmik Gupta"
                                },
                                {
                                    "authorId": "2323513320",
                                    "name": "Saumya Malik"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 79
                        },
                        "score": 0.94970703125
                    },
                    {
                        "id": "(Khera et al., 2024)",
                        "snippets": [
                            "Data selection is a well known problem in literature with many algorithms such as filtering, coresets, importance sampling and more working towards the same goal [22]. Here, we present data selection in the domain of LLMs and natural language and how they can enable data efficient training. \n\nLIMA, proposed by [30] employs a small high quality dataset for fine-tuning. They show that by carefully curating only a 1000 data points, they are able to achieve remarkable performance which is generalizable to unseen data as well, thereby suggesting limited high quality tuning data is sufficient. \n\nAnother prominent study by [16] demonstrates that sample quality can reduce the data requirement without compromising the downstream performance. They investigate data engineering strategies in the fine-tuning paradigm from multiple facets to identify the characterises of good instruction tuning data. DEITA, the model family tuned by their proposed strategy to automatically select a complex and high quality dataset achieves comparable performance to open source models while using only a tenth of the data."
                        ],
                        "paper": {
                            "corpus_id": 274130626,
                            "title": "Efficient Alignment of Large Language Models via Data Sampling",
                            "authors": [
                                {
                                    "authorId": "21302492",
                                    "name": "Amrit Khera"
                                },
                                {
                                    "authorId": "2213553962",
                                    "name": "Rajat Ghosh"
                                },
                                {
                                    "authorId": "2267726934",
                                    "name": "Debojyoti Dutta"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.96337890625
                    },
                    {
                        "id": "(Zhou et al., 2023)",
                        "snippets": [
                            "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output."
                        ],
                        "paper": {
                            "corpus_id": 258822910,
                            "title": "LIMA: Less Is More for Alignment",
                            "authors": [
                                {
                                    "authorId": "2384711",
                                    "name": "Chunting Zhou"
                                },
                                {
                                    "authorId": "144118452",
                                    "name": "Pengfei Liu"
                                },
                                {
                                    "authorId": "2214843767",
                                    "name": "Puxin Xu"
                                },
                                {
                                    "authorId": "1900163",
                                    "name": "Srini Iyer"
                                },
                                {
                                    "authorId": "145478138",
                                    "name": "Jiao Sun"
                                },
                                {
                                    "authorId": "3375249",
                                    "name": "Yuning Mao"
                                },
                                {
                                    "authorId": "2378954",
                                    "name": "Xuezhe Ma"
                                },
                                {
                                    "authorId": "1388010852",
                                    "name": "Avia Efrat"
                                },
                                {
                                    "authorId": "2114104308",
                                    "name": "Ping Yu"
                                },
                                {
                                    "authorId": "49297123",
                                    "name": "L. Yu"
                                },
                                {
                                    "authorId": "2108244542",
                                    "name": "Susan Zhang"
                                },
                                {
                                    "authorId": "134007132",
                                    "name": "Gargi Ghosh"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "39455775",
                                    "name": "Omer Levy"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 850
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training."
                        ],
                        "paper": {
                            "corpus_id": 260203057,
                            "title": "Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models",
                            "authors": [
                                {
                                    "authorId": "48622329",
                                    "name": "Mayee F. Chen"
                                },
                                {
                                    "authorId": "2069037355",
                                    "name": "Nicholas Roberts"
                                },
                                {
                                    "authorId": "144383716",
                                    "name": "K. Bhatia"
                                },
                                {
                                    "authorId": "39597242",
                                    "name": "Jue Wang"
                                },
                                {
                                    "authorId": "1776014",
                                    "name": "Ce Zhang"
                                },
                                {
                                    "authorId": "2186982588",
                                    "name": "Frederic Sala"
                                },
                                {
                                    "authorId": "1803218",
                                    "name": "Christopher R\u00e9"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 65
                        },
                        "score": 0.916015625
                    },
                    {
                        "id": "(Sow et al., 2025)",
                        "snippets": [
                            "Training Data Re-weighting/Selection for LLMs. Several recent studies (Xie et al., 2023;(Chen et al., 2023)Fan et al., 2023;Thakkar et al., 2023) have explored various reweighting techniques to enhance the generalization and efficiency of language models pretraining. For instance, Xie et al. ( 2023) and Fan et al. (2023) optimize the composition of pretraining corpora to achieve better performance across pretraining domains or for out-of-domain generalization. (Chen et al., 2023) introduce a framework for ordered skill learning, optimizing data selection based on how effectively it teaches interdependent skills for continual pretraining and fine-tuning regimes. Although effective, these techniques operate at the group level, whereas our work explores reweighting at the instance level, offering finer control over how individual samples are treated based on their loss values",
                            ".The approach in Fan & Jaggi (2023) offers a curriculum learning framework that prioritizes samples with a higher learnability score, which is precomputed using another auxiliary model similar to DoReMi and DoGE."
                        ],
                        "paper": {
                            "corpus_id": 276250237,
                            "title": "Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining",
                            "authors": [
                                {
                                    "authorId": "143827145",
                                    "name": "Daouda Sow"
                                },
                                {
                                    "authorId": "2220057323",
                                    "name": "Herbert Woisetschl\u00e4ger"
                                },
                                {
                                    "authorId": "35693339",
                                    "name": "Saikiran Bulusu"
                                },
                                {
                                    "authorId": "2255363698",
                                    "name": "Shiqiang Wang"
                                },
                                {
                                    "authorId": "2254179381",
                                    "name": "Hans-Arno Jacobsen"
                                },
                                {
                                    "authorId": "2344954540",
                                    "name": "Yingbin Liang"
                                }
                            ],
                            "year": 2025,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 6
                        },
                        "score": 0.81494140625
                    },
                    {
                        "id": "(Fan et al., 2023)",
                        "snippets": [
                            "Nevertheless, few research have succeeded applying traditional sample-level selection schemes for large langauge model pretraining. Online batch selection methods [Loshchilov and Hutter, 2015(Katharopoulos et al., 2018), Jiang et al., 2019, Schaul et al., 2015] select hard samples with high loss or high gradient norm, which require a second forward/backward pass. That introduces large extra computation costs when the model size is large, which hurts the scalability. On the other side, Campos [2021] shows that the linguistic-based curriculum learning failed to improve on causal language model pretraining."
                        ],
                        "paper": {
                            "corpus_id": 264439064,
                            "title": "Irreducible Curriculum for Language Model Pretraining",
                            "authors": [
                                {
                                    "authorId": "2261456109",
                                    "name": "Simin Fan"
                                },
                                {
                                    "authorId": "2256984280",
                                    "name": "Martin Jaggi"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 11
                        },
                        "score": 0.88525390625
                    },
                    {
                        "id": "(Katharopoulos et al., 2018)",
                        "snippets": [
                            "Deep neural network training spends most of the computation on examples that are properly handled, and could be ignored. We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on \"informative\" examples, and reduces the variance of the stochastic gradients during training. Our contribution is twofold: first, we derive a tractable upper bound to the per-sample gradient norm, and second we derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup. The resulting scheme can be used by changing a few lines of code in a standard SGD procedure, and we demonstrate experimentally, on image classification, CNN fine-tuning, and RNN training, that for a fixed wall-clock time budget, it provides a reduction of the train losses of up to an order of magnitude and a relative improvement of test errors between 5% and 17%."
                        ],
                        "paper": {
                            "corpus_id": 3663876,
                            "title": "Not All Samples Are Created Equal: Deep Learning with Importance Sampling",
                            "authors": [
                                {
                                    "authorId": "3493855",
                                    "name": "Angelos Katharopoulos"
                                },
                                {
                                    "authorId": "2721983",
                                    "name": "F. Fleuret"
                                }
                            ],
                            "year": 2018,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 522
                        },
                        "score": 0
                    },
                    {
                        "id": "(Belenki et al., 2025)",
                        "snippets": [
                            "There is an extensive body of work on data selection and mixture optimization for pretraining language models. Albalak et al. (2024) offer a comprehensive recent survey. Approaches for data selection and cleaning consider different granularities of data, such as token-level, sample-level (individual documents or sentences can be selected or weighted), and group-level (where we consider samples in large groups assumed to have common characteristics, often derived from meta-data such as the web domain (like Wikipedia) or source collection name (such as C4)."
                        ],
                        "paper": {
                            "corpus_id": 276575595,
                            "title": "Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert Models",
                            "authors": [
                                {
                                    "authorId": "2316558829",
                                    "name": "Lior Belenki"
                                },
                                {
                                    "authorId": "2274120058",
                                    "name": "Alekh Agarwal"
                                },
                                {
                                    "authorId": "2346974785",
                                    "name": "Tianze Shi"
                                },
                                {
                                    "authorId": "2288931206",
                                    "name": "Kristina Toutanova"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.90380859375
                    },
                    {
                        "id": "(Engstrom et al., 2024)",
                        "snippets": [
                            "When selecting data for training large-scale models, standard practice is to filter for examples that match human notions of data quality. Such filtering yields qualitatively clean datapoints that intuitively should improve model behavior. However, in practice the opposite can often happen: we find that selecting according to similarity with\"high quality\"data sources may not increase (and can even hurt) performance compared to randomly selecting data."
                        ],
                        "paper": {
                            "corpus_id": 267094971,
                            "title": "DsDm: Model-Aware Dataset Selection with Datamodels",
                            "authors": [
                                {
                                    "authorId": "39468283",
                                    "name": "Logan Engstrom"
                                },
                                {
                                    "authorId": "2280334462",
                                    "name": "Axel Feldmann"
                                },
                                {
                                    "authorId": "143826246",
                                    "name": "A. Ma\u0327dry"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 61
                        },
                        "score": 0.9541015625
                    },
                    {
                        "id": "(Bai et al., 2024)",
                        "snippets": [
                            "Efficient data selection is crucial for the pretraining of large language models (LLMs), as the quality of training data significantly impacts the statistical efficiency of the training procedure and the model performance (Brown, 2020;(Du et al., 2021)(Chowdhery et al., 2022). Recently, we have witnessed numerous approaches, such as filtering high-quality data (Xie et al., 2023)(Wettig et al., 2024), mixing data from multiple domains (Xie et al., 2023)Liu et al., 2024), and selecting data that optimally boosts downstream task performance dynamically (Engstrom et al., 2024;Yu et al., 2024), which aim to improve data efficiency by prioritizing more informative training samples."
                        ],
                        "paper": {
                            "corpus_id": 273233719,
                            "title": "Efficient Pretraining Data Selection for Language Models via Multi-Actor Collaboration",
                            "authors": [
                                {
                                    "authorId": "2318978696",
                                    "name": "Tianyi Bai"
                                },
                                {
                                    "authorId": "2302788310",
                                    "name": "Ling Yang"
                                },
                                {
                                    "authorId": "2325175258",
                                    "name": "Zhen Hao Wong"
                                },
                                {
                                    "authorId": "2367277175",
                                    "name": "Fupeng Sun"
                                },
                                {
                                    "authorId": "2233445161",
                                    "name": "Jiahui Peng"
                                },
                                {
                                    "authorId": "2366068443",
                                    "name": "Xinlin Zhuang"
                                },
                                {
                                    "authorId": "2325489246",
                                    "name": "Chi Zhang"
                                },
                                {
                                    "authorId": "2325195689",
                                    "name": "Lijun Wu"
                                },
                                {
                                    "authorId": "2289911484",
                                    "name": "Jiantao Qiu"
                                },
                                {
                                    "authorId": "2302813081",
                                    "name": "Wentao Zhang"
                                },
                                {
                                    "authorId": "2303407552",
                                    "name": "Binhang Yuan"
                                },
                                {
                                    "authorId": "2291040348",
                                    "name": "Conghui He"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 6
                        },
                        "score": 0.96337890625
                    },
                    {
                        "id": "(Du et al., 2021)",
                        "snippets": [
                            "Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks."
                        ],
                        "paper": {
                            "corpus_id": 245124124,
                            "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts",
                            "authors": [
                                {
                                    "authorId": "2140321952",
                                    "name": "Nan Du"
                                },
                                {
                                    "authorId": "2145438541",
                                    "name": "Yanping Huang"
                                },
                                {
                                    "authorId": "2555924",
                                    "name": "Andrew M. Dai"
                                },
                                {
                                    "authorId": "2058177533",
                                    "name": "Simon Tong"
                                },
                                {
                                    "authorId": "150077954",
                                    "name": "Dmitry Lepikhin"
                                },
                                {
                                    "authorId": "2145139570",
                                    "name": "Yuanzhong Xu"
                                },
                                {
                                    "authorId": "2048712",
                                    "name": "M. Krikun"
                                },
                                {
                                    "authorId": "2389316",
                                    "name": "Yanqi Zhou"
                                },
                                {
                                    "authorId": "40625240",
                                    "name": "Adams Wei Yu"
                                },
                                {
                                    "authorId": "2345617",
                                    "name": "Orhan Firat"
                                },
                                {
                                    "authorId": "2368067",
                                    "name": "Barret Zoph"
                                },
                                {
                                    "authorId": "2096916416",
                                    "name": "L. Fedus"
                                },
                                {
                                    "authorId": "40377863",
                                    "name": "Maarten Bosma"
                                },
                                {
                                    "authorId": "1389392654",
                                    "name": "Zongwei Zhou"
                                },
                                {
                                    "authorId": null,
                                    "name": "Tao Wang"
                                },
                                {
                                    "authorId": "2153608756",
                                    "name": "Yu Emma Wang"
                                },
                                {
                                    "authorId": "20825661",
                                    "name": "Kellie Webster"
                                },
                                {
                                    "authorId": "97905921",
                                    "name": "Marie Pellat"
                                },
                                {
                                    "authorId": "2148473059",
                                    "name": "Kevin Robinson"
                                },
                                {
                                    "authorId": "1398655031",
                                    "name": "K. Meier-Hellstern"
                                },
                                {
                                    "authorId": "2145151992",
                                    "name": "Toju Duke"
                                },
                                {
                                    "authorId": "2065639113",
                                    "name": "Lucas Dixon"
                                },
                                {
                                    "authorId": "1556095165",
                                    "name": "Kun Zhang"
                                },
                                {
                                    "authorId": "2827616",
                                    "name": "Quoc V. Le"
                                },
                                {
                                    "authorId": "48607963",
                                    "name": "Yonghui Wu"
                                },
                                {
                                    "authorId": "2545358",
                                    "name": "Z. Chen"
                                },
                                {
                                    "authorId": "2052275005",
                                    "name": "Claire Cui"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 826
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chowdhery et al., 2022)",
                        "snippets": [
                            "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies."
                        ],
                        "paper": {
                            "corpus_id": 247951931,
                            "title": "PaLM: Scaling Language Modeling with Pathways",
                            "authors": [
                                {
                                    "authorId": "2841893",
                                    "name": "Aakanksha Chowdhery"
                                },
                                {
                                    "authorId": "46617804",
                                    "name": "Sharan Narang"
                                },
                                {
                                    "authorId": "39172707",
                                    "name": "Jacob Devlin"
                                },
                                {
                                    "authorId": "40377863",
                                    "name": "Maarten Bosma"
                                },
                                {
                                    "authorId": "2159632445",
                                    "name": "Gaurav Mishra"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "152399055",
                                    "name": "P. Barham"
                                },
                                {
                                    "authorId": "3351938",
                                    "name": "Hyung Won Chung"
                                },
                                {
                                    "authorId": "152549864",
                                    "name": "Charles Sutton"
                                },
                                {
                                    "authorId": "3159346",
                                    "name": "Sebastian Gehrmann"
                                },
                                {
                                    "authorId": "2620528",
                                    "name": "Parker Schuh"
                                },
                                {
                                    "authorId": "2362367",
                                    "name": "Kensen Shi"
                                },
                                {
                                    "authorId": "2160888237",
                                    "name": "Sasha Tsvyashchenko"
                                },
                                {
                                    "authorId": "2124977868",
                                    "name": "Joshua Maynez"
                                },
                                {
                                    "authorId": "1484043592",
                                    "name": "Abhishek Rao"
                                },
                                {
                                    "authorId": "80940648",
                                    "name": "Parker Barnes"
                                },
                                {
                                    "authorId": "144447820",
                                    "name": "Yi Tay"
                                },
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                },
                                {
                                    "authorId": "3331141",
                                    "name": "Vinodkumar Prabhakaran"
                                },
                                {
                                    "authorId": "49849144",
                                    "name": "Emily Reif"
                                },
                                {
                                    "authorId": "2140321952",
                                    "name": "Nan Du"
                                },
                                {
                                    "authorId": "2044655623",
                                    "name": "Ben Hutchinson"
                                },
                                {
                                    "authorId": "2161431901",
                                    "name": "Reiner Pope"
                                },
                                {
                                    "authorId": "2065251344",
                                    "name": "James Bradbury"
                                },
                                {
                                    "authorId": "2058365883",
                                    "name": "Jacob Austin"
                                },
                                {
                                    "authorId": "2090818",
                                    "name": "M. Isard"
                                },
                                {
                                    "authorId": "2284681044",
                                    "name": "Guy Gur-Ari"
                                },
                                {
                                    "authorId": "38253388",
                                    "name": "Pengcheng Yin"
                                },
                                {
                                    "authorId": "2145151992",
                                    "name": "Toju Duke"
                                },
                                {
                                    "authorId": "6639036",
                                    "name": "Anselm Levskaya"
                                },
                                {
                                    "authorId": "1780892",
                                    "name": "S. Ghemawat"
                                },
                                {
                                    "authorId": "50991767",
                                    "name": "Sunipa Dev"
                                },
                                {
                                    "authorId": "47407464",
                                    "name": "H. Michalewski"
                                },
                                {
                                    "authorId": "143936294",
                                    "name": "Xavier Garc\u00eda"
                                },
                                {
                                    "authorId": "40055795",
                                    "name": "Vedant Misra"
                                },
                                {
                                    "authorId": "2148473059",
                                    "name": "Kevin Robinson"
                                },
                                {
                                    "authorId": "2096916416",
                                    "name": "L. Fedus"
                                },
                                {
                                    "authorId": "65855107",
                                    "name": "Denny Zhou"
                                },
                                {
                                    "authorId": "7975935",
                                    "name": "Daphne Ippolito"
                                },
                                {
                                    "authorId": "150970919",
                                    "name": "D. Luan"
                                },
                                {
                                    "authorId": "8939217",
                                    "name": "Hyeontaek Lim"
                                },
                                {
                                    "authorId": "2368067",
                                    "name": "Barret Zoph"
                                },
                                {
                                    "authorId": "1572884723",
                                    "name": "A. Spiridonov"
                                },
                                {
                                    "authorId": "35474601",
                                    "name": "Ryan Sepassi"
                                },
                                {
                                    "authorId": "35363891",
                                    "name": "David Dohan"
                                },
                                {
                                    "authorId": "3504647",
                                    "name": "Shivani Agrawal"
                                },
                                {
                                    "authorId": "3175815",
                                    "name": "Mark Omernick"
                                },
                                {
                                    "authorId": "2555924",
                                    "name": "Andrew M. Dai"
                                },
                                {
                                    "authorId": "2598683",
                                    "name": "Thanumalayan Sankaranarayana Pillai"
                                },
                                {
                                    "authorId": "97905921",
                                    "name": "Marie Pellat"
                                },
                                {
                                    "authorId": "102549875",
                                    "name": "Aitor Lewkowycz"
                                },
                                {
                                    "authorId": "2057453483",
                                    "name": "Erica Moreira"
                                },
                                {
                                    "authorId": "48422824",
                                    "name": "R. Child"
                                },
                                {
                                    "authorId": "2636739",
                                    "name": "Oleksandr Polozov"
                                },
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "2198519",
                                    "name": "Zongwei Zhou"
                                },
                                {
                                    "authorId": "1524732527",
                                    "name": "Xuezhi Wang"
                                },
                                {
                                    "authorId": "4125424",
                                    "name": "Brennan Saeta"
                                },
                                {
                                    "authorId": "2152965375",
                                    "name": "Mark D\u00edaz"
                                },
                                {
                                    "authorId": "2345617",
                                    "name": "Orhan Firat"
                                },
                                {
                                    "authorId": "1754926",
                                    "name": "Michele Catasta"
                                },
                                {
                                    "authorId": "119640649",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "1398655031",
                                    "name": "K. Meier-Hellstern"
                                },
                                {
                                    "authorId": "2396681",
                                    "name": "D. Eck"
                                },
                                {
                                    "authorId": "48448318",
                                    "name": "J. Dean"
                                },
                                {
                                    "authorId": "1754497",
                                    "name": "Slav Petrov"
                                },
                                {
                                    "authorId": "22640071",
                                    "name": "Noah Fiedel"
                                }
                            ],
                            "year": 2022,
                            "venue": "Journal of machine learning research",
                            "n_citations": 6293
                        },
                        "score": 0
                    },
                    {
                        "id": "(Xie et al., 2023)",
                        "snippets": [
                            "The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to set the domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% points over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no knowledge of downstream tasks, even matches the performance of using domain weights tuned on downstream tasks."
                        ],
                        "paper": {
                            "corpus_id": 258741043,
                            "title": "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining",
                            "authors": [
                                {
                                    "authorId": "46215055",
                                    "name": "Sang Michael Xie"
                                },
                                {
                                    "authorId": "143950636",
                                    "name": "Hieu Pham"
                                },
                                {
                                    "authorId": "9929684",
                                    "name": "Xuanyi Dong"
                                },
                                {
                                    "authorId": "2140321952",
                                    "name": "Nan Du"
                                },
                                {
                                    "authorId": "2391802",
                                    "name": "Hanxiao Liu"
                                },
                                {
                                    "authorId": "2141538599",
                                    "name": "Yifeng Lu"
                                },
                                {
                                    "authorId": "145419642",
                                    "name": "Percy Liang"
                                },
                                {
                                    "authorId": "1397917613",
                                    "name": "Quoc V. Le"
                                },
                                {
                                    "authorId": "2114186424",
                                    "name": "Tengyu Ma"
                                },
                                {
                                    "authorId": "40625240",
                                    "name": "Adams Wei Yu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 203
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Challenges and Limitations",
                "tldr": "Despite its benefits, curriculum learning faces significant challenges including computational overhead for sample evaluation, failure to address data redundancy, and the counterintuitive finding that quality-focused selection doesn't always improve performance. Balancing data diversity with difficulty ranking remains a complex optimization problem. (6 sources)",
                "text": "\n* **Computational Overhead**: Traditional online batch selection methods that select hard samples based on high loss or gradient norm require a second forward/backward pass, introducing substantial computational costs that reduce scalability for large language models <Paper corpusId=\"264439064\" paperTitle=\"(Fan et al., 2023)\" isShortName></Paper> <Paper corpusId=\"3663876\" paperTitle=\"(Katharopoulos et al., 2018)\" isShortName></Paper>.\n\n* **Redundancy Management**: While curriculum learning enhances convergence by prioritizing easy points with low label noise, traditional approaches fail to address the issue of redundant training points that have already been learned <Paper corpusId=\"261048902\" paperTitle=\"(Deng et al., 2023)\" isShortName></Paper>.\n\n* **Balancing Diversity and Difficulty**: Optimizing for data diversity tends to bias coresets toward easier samples, whereas selection based on difficulty ranking often omits easy samples that are necessary for effective training. These complementary factors must be jointly considered for optimal coreset selection <Paper corpusId=\"263909051\" paperTitle=\"(Maharana et al., 2023)\" isShortName></Paper>.\n\n* **Counter-intuitive Quality Effects**: Contrary to intuition, selecting data based on similarity to \"high-quality\" sources may not improve\u2014and can sometimes hurt\u2014performance compared to random data selection. This challenges the assumption that filtering for human notions of data quality necessarily improves model behavior <Paper corpusId=\"267094971\" paperTitle=\"(Engstrom et al., 2024)\" isShortName></Paper>.\n\n* **Combinatorial Effects**: Most data selection methods focus on evaluating individual sample quality while neglecting the combinatorial effects among samples. Even perfectly curated individual samples may create suboptimal learning conditions due to intrinsic homogeneity or contradictions when combined <Paper corpusId=\"271064746\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>.\n\n* **Domain-Specific Limitations**: Linguistic-based curriculum learning approaches have failed to demonstrate improvements in causal language model pretraining, suggesting that not all curriculum strategies transfer effectively across different language modeling tasks <Paper corpusId=\"264439064\" paperTitle=\"(Fan et al., 2023)\" isShortName></Paper>.\n\n* **Resource Constraints**: Implementing sophisticated curriculum learning approaches often requires additional computational resources for sample evaluation and curriculum design, creating a trade-off between the benefits of improved training and the costs of implementing the curriculum <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
                "citations": [
                    {
                        "id": "(Fan et al., 2023)",
                        "snippets": [
                            "Nevertheless, few research have succeeded applying traditional sample-level selection schemes for large langauge model pretraining. Online batch selection methods [Loshchilov and Hutter, 2015(Katharopoulos et al., 2018), Jiang et al., 2019, Schaul et al., 2015] select hard samples with high loss or high gradient norm, which require a second forward/backward pass. That introduces large extra computation costs when the model size is large, which hurts the scalability. On the other side, Campos [2021] shows that the linguistic-based curriculum learning failed to improve on causal language model pretraining."
                        ],
                        "paper": {
                            "corpus_id": 264439064,
                            "title": "Irreducible Curriculum for Language Model Pretraining",
                            "authors": [
                                {
                                    "authorId": "2261456109",
                                    "name": "Simin Fan"
                                },
                                {
                                    "authorId": "2256984280",
                                    "name": "Martin Jaggi"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 11
                        },
                        "score": 0.88525390625
                    },
                    {
                        "id": "(Katharopoulos et al., 2018)",
                        "snippets": [
                            "Deep neural network training spends most of the computation on examples that are properly handled, and could be ignored. We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on \"informative\" examples, and reduces the variance of the stochastic gradients during training. Our contribution is twofold: first, we derive a tractable upper bound to the per-sample gradient norm, and second we derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup. The resulting scheme can be used by changing a few lines of code in a standard SGD procedure, and we demonstrate experimentally, on image classification, CNN fine-tuning, and RNN training, that for a fixed wall-clock time budget, it provides a reduction of the train losses of up to an order of magnitude and a relative improvement of test errors between 5% and 17%."
                        ],
                        "paper": {
                            "corpus_id": 3663876,
                            "title": "Not All Samples Are Created Equal: Deep Learning with Importance Sampling",
                            "authors": [
                                {
                                    "authorId": "3493855",
                                    "name": "Angelos Katharopoulos"
                                },
                                {
                                    "authorId": "2721983",
                                    "name": "F. Fleuret"
                                }
                            ],
                            "year": 2018,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 522
                        },
                        "score": 0
                    },
                    {
                        "id": "(Deng et al., 2023)",
                        "snippets": [
                            "Curriculum learning, as advocated by (Bengio et al., 2009), prioritizes easy points with low label noise before uniformly training on all data points. While this strategy enhances convergence, it fails to address the issue of skipping redundant points already learned."
                        ],
                        "paper": {
                            "corpus_id": 261048902,
                            "title": "Towards Accelerated Model Training via Bayesian Data Selection",
                            "authors": [
                                {
                                    "authorId": "145114723",
                                    "name": "Zhijie Deng"
                                },
                                {
                                    "authorId": "2153522384",
                                    "name": "Peng Cui"
                                },
                                {
                                    "authorId": "145254043",
                                    "name": "Jun Zhu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 5
                        },
                        "score": 0.7568359375
                    },
                    {
                        "id": "(Maharana et al., 2023)",
                        "snippets": [
                            "Analytical theories suggest that higher-quality data can lead to lower test errors in models trained on a fixed data budget. Moreover, a model can be trained on a lower compute budget without compromising performance if a dataset can be stripped of its redundancies. Coreset selection (or data pruning) seeks to select a subset of the training data so as to maximize the performance of models trained on this subset, also referred to as coreset. There are two dominant approaches: (1) geometry-based data selection for maximizing data diversity in the coreset, and (2) functions that assign difficulty scores to samples based on training dynamics. Optimizing for data diversity leads to a coreset that is biased towards easier samples, whereas, selection by difficulty ranking omits easy samples that are necessary for the training of deep learning models. This demonstrates that data diversity and importance scores are two complementary factors that need to be jointly considered during coreset selection."
                        ],
                        "paper": {
                            "corpus_id": 263909051,
                            "title": "D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning",
                            "authors": [
                                {
                                    "authorId": "8785371",
                                    "name": "Adyasha Maharana"
                                },
                                {
                                    "authorId": "46841632",
                                    "name": "Prateek Yadav"
                                },
                                {
                                    "authorId": "2253396640",
                                    "name": "Mohit Bansal"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 34
                        },
                        "score": 0.74755859375
                    },
                    {
                        "id": "(Engstrom et al., 2024)",
                        "snippets": [
                            "When selecting data for training large-scale models, standard practice is to filter for examples that match human notions of data quality. Such filtering yields qualitatively clean datapoints that intuitively should improve model behavior. However, in practice the opposite can often happen: we find that selecting according to similarity with\"high quality\"data sources may not increase (and can even hurt) performance compared to randomly selecting data."
                        ],
                        "paper": {
                            "corpus_id": 267094971,
                            "title": "DsDm: Model-Aware Dataset Selection with Datamodels",
                            "authors": [
                                {
                                    "authorId": "39468283",
                                    "name": "Logan Engstrom"
                                },
                                {
                                    "authorId": "2280334462",
                                    "name": "Axel Feldmann"
                                },
                                {
                                    "authorId": "143826246",
                                    "name": "A. Ma\u0327dry"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 61
                        },
                        "score": 0.9541015625
                    },
                    {
                        "id": "(Yin et al., 2024)",
                        "snippets": [
                            "Data is the cornerstone of large language models (LLMs), but not all data is useful for model learning. Carefully selected data can better elicit the capabilities of LLMs with much less computational overhead. Most methods concentrate on evaluating the quality of individual samples in data selection, while the combinatorial effects among samples are neglected. Even if each sample is of perfect quality, their combinations may be suboptimal in teaching LLMs due to their intrinsic homogeneity or contradiction."
                        ],
                        "paper": {
                            "corpus_id": 271064746,
                            "title": "Entropy Law: The Story Behind Data Compression and LLM Performance",
                            "authors": [
                                {
                                    "authorId": "2260649859",
                                    "name": "Mingjia Yin"
                                },
                                {
                                    "authorId": "2276003321",
                                    "name": "Chuhan Wu"
                                },
                                {
                                    "authorId": "2283529247",
                                    "name": "Yufei Wang"
                                },
                                {
                                    "authorId": "2256768674",
                                    "name": "Hao Wang"
                                },
                                {
                                    "authorId": "2260810851",
                                    "name": "Wei Guo"
                                },
                                {
                                    "authorId": "2282544603",
                                    "name": "Yasheng Wang"
                                },
                                {
                                    "authorId": "2293683997",
                                    "name": "Yong Liu"
                                },
                                {
                                    "authorId": "2284295184",
                                    "name": "Ruiming Tang"
                                },
                                {
                                    "authorId": "1862782",
                                    "name": "Defu Lian"
                                },
                                {
                                    "authorId": "2113754294",
                                    "name": "Enhong Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 27
                        },
                        "score": 0.85693359375
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Recent Advances",
                "tldr": "Recent advances in curriculum learning for language models include ordered skill acquisition frameworks, SmallToLarge training methods, and domain optimization strategies that significantly improve data efficiency. These innovations span from specialized algorithms for training progression to automated data selection techniques that reduce training resources while maintaining or improving model performance. (15 sources)",
                "text": "\nThe field of curriculum learning for language models has seen significant advancements in recent years, with researchers developing increasingly sophisticated approaches to data selection and training scheduling. One notable innovation is the \"ordered skill acquisition\" framework proposed by Chen et al., which builds on the hypothesis that language models, like humans, acquire interdependent skills in a natural order <Paper corpusId=\"260203057\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. This approach formalizes the notion of skills and their dependencies, demonstrating that teaching prerequisite skills first enables more advanced skills to be learned with less data. Their Skill-It algorithm, which samples data based on prerequisite skills, achieved 36.5 points higher accuracy than random sampling in continual pre-training and reduced validation loss by 13.6% in fine-tuning scenarios <Paper corpusId=\"260203057\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper> <Paper corpusId=\"276250237\" paperTitle=\"(Sow et al., 2025)\" isShortName></Paper>.\n\nBuilding on this concept, the Orion-14B model implemented a staged training approach that intentionally organized data to increase complexity incrementally. Initial stages focused on common knowledge sources like web pages and news articles, gradually incorporating more complex sources such as textbooks, academic papers, and source code as training progressed <Paper corpusId=\"267095066\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>. This structured approach to curriculum design reflects the growing understanding that the strategic ordering of training data can significantly enhance model performance.\n\nFor supervised fine-tuning of large language models, the SmallToLarge (S2L) method represents another important advance. This approach leverages training trajectories from small models to guide data selection for larger models, making the fine-tuning process more efficient <Paper corpusId=\"268363364\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. This cross-model knowledge transfer strategy aligns with broader research showing that high-quality data selection can dramatically reduce data requirements, as demonstrated by the LIMA project, which achieved remarkable performance by carefully curating just 1,000 high-quality examples <Paper corpusId=\"270620248\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"274130626\" paperTitle=\"(Khera et al., 2024)\" isShortName></Paper>.\n\nThe Domain Reweighting with Minimax Optimization (DoReMi) approach represents another significant advance in curriculum learning. This method trains a small proxy model using group distributionally robust optimization over domains to produce domain weights without knowledge of downstream tasks <Paper corpusId=\"273233719\" paperTitle=\"(Bai et al., 2024)\" isShortName></Paper>. In experiments, DoReMi improved average few-shot downstream accuracy by 6.5 percentage points over baseline models and reached baseline accuracy with 2.6 times fewer training steps, demonstrating substantial efficiency gains <Paper corpusId=\"273233719\" paperTitle=\"(Bai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258741043\" paperTitle=\"(Xie et al., 2023)\" isShortName></Paper>.\n\nAnother notable advance is the Online Data Mixing (ODM) algorithm, which optimizes data mixing proportions during training using multi-armed bandit algorithms. This approach combines elements from both data selection and data mixing to adapt to changing training dynamics, reaching the final perplexity of comparable methods with 19% fewer training iterations <Paper corpusId=\"273502726\" paperTitle=\"(Na et al., 2024)\" isShortName></Paper> <Paper corpusId=\"265658930\" paperTitle=\"(Albalak et al., 2023)\" isShortName></Paper>.\n\nFor data quality assessment, the QuRating method introduces a sophisticated approach that uses language models to rate pretraining data based on writing style, required expertise, factual content, and educational value <Paper corpusId=\"273233719\" paperTitle=\"(Bai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"267681974\" paperTitle=\"(Wettig et al., 2024)\" isShortName></Paper>. This method creates a training curriculum that improves performance without changing the training dataset, demonstrating how quality-focused selection can enhance model capabilities.\n\nAt a more practical level, researchers have begun to consider different granularities of data selection, from token-level to sample-level to group-level approaches <Paper corpusId=\"276575595\" paperTitle=\"(Belenki et al., 2025)\" isShortName></Paper>. Group-level data selection, which considers samples in large groups with common characteristics derived from metadata such as web domains or source collection names, has proven particularly effective for large-scale training <Paper corpusId=\"273350576\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper>.\n\nThe field continues to evolve with innovations like the Reducible Holdout Loss Selection (RHO-LOSS) technique, which selects points that most reduce the model's generalization loss, addressing the weaknesses of existing methods that focus solely on \"hard\" or \"easy\" examples <Paper corpusId=\"273350576\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"235743024\" paperTitle=\"(Mindermann et al., 2022)\" isShortName></Paper>. This approach selects points that are learnable, worth learning, and not yet learned, training in far fewer steps than prior approaches while improving accuracy <Paper corpusId=\"273350576\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"235743024\" paperTitle=\"(Mindermann et al., 2022)\" isShortName></Paper>.\n\nThese advances collectively demonstrate how curriculum learning continues to evolve from its conceptual foundations to increasingly sophisticated implementations that deliver measurable improvements in model performance and training efficiency. As the field progresses, the integration of Bayesian optimization to learn data selection measures represents a promising direction, with early work showing significant improvements across models, domains, and tasks <Paper corpusId=\"7403346\" paperTitle=\"(Ruder et al., 2017)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training."
                        ],
                        "paper": {
                            "corpus_id": 260203057,
                            "title": "Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models",
                            "authors": [
                                {
                                    "authorId": "48622329",
                                    "name": "Mayee F. Chen"
                                },
                                {
                                    "authorId": "2069037355",
                                    "name": "Nicholas Roberts"
                                },
                                {
                                    "authorId": "144383716",
                                    "name": "K. Bhatia"
                                },
                                {
                                    "authorId": "39597242",
                                    "name": "Jue Wang"
                                },
                                {
                                    "authorId": "1776014",
                                    "name": "Ce Zhang"
                                },
                                {
                                    "authorId": "2186982588",
                                    "name": "Frederic Sala"
                                },
                                {
                                    "authorId": "1803218",
                                    "name": "Christopher R\u00e9"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 65
                        },
                        "score": 0.916015625
                    },
                    {
                        "id": "(Sow et al., 2025)",
                        "snippets": [
                            "Training Data Re-weighting/Selection for LLMs. Several recent studies (Xie et al., 2023;(Chen et al., 2023)Fan et al., 2023;Thakkar et al., 2023) have explored various reweighting techniques to enhance the generalization and efficiency of language models pretraining. For instance, Xie et al. ( 2023) and Fan et al. (2023) optimize the composition of pretraining corpora to achieve better performance across pretraining domains or for out-of-domain generalization. (Chen et al., 2023) introduce a framework for ordered skill learning, optimizing data selection based on how effectively it teaches interdependent skills for continual pretraining and fine-tuning regimes. Although effective, these techniques operate at the group level, whereas our work explores reweighting at the instance level, offering finer control over how individual samples are treated based on their loss values",
                            ".The approach in Fan & Jaggi (2023) offers a curriculum learning framework that prioritizes samples with a higher learnability score, which is precomputed using another auxiliary model similar to DoReMi and DoGE."
                        ],
                        "paper": {
                            "corpus_id": 276250237,
                            "title": "Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining",
                            "authors": [
                                {
                                    "authorId": "143827145",
                                    "name": "Daouda Sow"
                                },
                                {
                                    "authorId": "2220057323",
                                    "name": "Herbert Woisetschl\u00e4ger"
                                },
                                {
                                    "authorId": "35693339",
                                    "name": "Saikiran Bulusu"
                                },
                                {
                                    "authorId": "2255363698",
                                    "name": "Shiqiang Wang"
                                },
                                {
                                    "authorId": "2254179381",
                                    "name": "Hans-Arno Jacobsen"
                                },
                                {
                                    "authorId": "2344954540",
                                    "name": "Yingbin Liang"
                                }
                            ],
                            "year": 2025,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 6
                        },
                        "score": 0.81494140625
                    },
                    {
                        "id": "(Chen et al., 2024)",
                        "snippets": [
                            "Considering that humans acquire knowledge in a deliberate order (Evanson et al., 2023), it is plausible that language models might also benefit from a structured learning order when processing training data. Curriculum learning (Bengio et al., 2009) has been suggested as a method to organize the learning process by progressively increasing the complexity of the training data. However, most prior studies have concentrated on sample-level data and smaller datasets. Chen et al. ( 2023) employed a skills-based framework for training data selection and continuous pretraining with a 3B-parameter language model. This approach achieved greater accuracy compared to the baseline method of uniform data source sampling, suggesting the potential efficacy of strategic data scheduling.\n\nIn training the Orion-14B model, we intentionally develop a data scheduling strategy that organizes training data to incrementally increase its complexity. We divide the training data into several stages based on the data sources and their complexity. These stages are differentiated by the mix ratios of data sources. Initial stages primarily include data with common knowledge, such as web pages and news articles. In the subsequent stages, we gradually augment the proportion of data containing more complex knowledge, including textbooks, academic papers, and source code."
                        ],
                        "paper": {
                            "corpus_id": 267095066,
                            "title": "Orion-14B: Open-source Multilingual Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2280380985",
                                    "name": "Du Chen"
                                },
                                {
                                    "authorId": "2280373513",
                                    "name": "Yi Huang"
                                },
                                {
                                    "authorId": "2280377400",
                                    "name": "Xiaopu Li"
                                },
                                {
                                    "authorId": "2280380419",
                                    "name": "Yongqiang Li"
                                },
                                {
                                    "authorId": "2280377865",
                                    "name": "Yongqiang Liu"
                                },
                                {
                                    "authorId": "1557313084",
                                    "name": "Haihui Pan"
                                },
                                {
                                    "authorId": "2280411885",
                                    "name": "Leichao Xu"
                                },
                                {
                                    "authorId": "2109546143",
                                    "name": "Dacheng Zhang"
                                },
                                {
                                    "authorId": "2280368716",
                                    "name": "Zhipeng Zhang"
                                },
                                {
                                    "authorId": "2281415463",
                                    "name": "Kun Han"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.83740234375
                    },
                    {
                        "id": "(Yang et al., 2024)",
                        "snippets": [
                            "Despite the effectiveness of data selection for large language models (LLMs) during pretraining and instruction fine-tuning phases, improving data efficiency in supervised fine-tuning (SFT) for specialized domains poses significant challenges due to the complexity of fine-tuning data. To bridge this gap, we introduce an effective and scalable data selection method for SFT, SmallToLarge (S2L), which leverages training trajectories from small models to guide the data selection for larger models."
                        ],
                        "paper": {
                            "corpus_id": 268363364,
                            "title": "SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models",
                            "authors": [
                                {
                                    "authorId": "2291083514",
                                    "name": "Yu Yang"
                                },
                                {
                                    "authorId": "2290975932",
                                    "name": "Siddhartha Mishra"
                                },
                                {
                                    "authorId": "2290912016",
                                    "name": "Jeffrey N Chiang"
                                },
                                {
                                    "authorId": "2389094",
                                    "name": "Baharan Mirzasoleiman"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 23
                        },
                        "score": 0.86669921875
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "Recent research highlights that data quality is more critical than data quantity for effective fine-tuning (Nakkiran et al., 2019)(Shumailov et al., 2024)(Zhou et al., 2023)Jindal et al., 2024). As a result, several data curation techniques have been proposed, such as data selection (Chen et al., 2023)(Li et al., 2023), data evolution (Wang et al., 2022)Xu et al., 2023), and data reflection (Mukherjee et al., 2023)(Yin et al., 2023). Data selection, in particular, involves choosing a high-quality subset from a candidate dataset based on specific selection criteria, enhancing the model's performance while improving training efficiency by reducing the number of samples."
                        ],
                        "paper": {
                            "corpus_id": 270620248,
                            "title": "Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2243875495",
                                    "name": "Ziche Liu"
                                },
                                {
                                    "authorId": "2307471917",
                                    "name": "Rui Ke"
                                },
                                {
                                    "authorId": "2303951713",
                                    "name": "Feng Jiang"
                                },
                                {
                                    "authorId": "2218230700",
                                    "name": "Haizhou Li"
                                }
                            ],
                            "year": 2024,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 2
                        },
                        "score": 0.9599609375
                    },
                    {
                        "id": "(Khera et al., 2024)",
                        "snippets": [
                            "Data selection is a well known problem in literature with many algorithms such as filtering, coresets, importance sampling and more working towards the same goal [22]. Here, we present data selection in the domain of LLMs and natural language and how they can enable data efficient training. \n\nLIMA, proposed by [30] employs a small high quality dataset for fine-tuning. They show that by carefully curating only a 1000 data points, they are able to achieve remarkable performance which is generalizable to unseen data as well, thereby suggesting limited high quality tuning data is sufficient. \n\nAnother prominent study by [16] demonstrates that sample quality can reduce the data requirement without compromising the downstream performance. They investigate data engineering strategies in the fine-tuning paradigm from multiple facets to identify the characterises of good instruction tuning data. DEITA, the model family tuned by their proposed strategy to automatically select a complex and high quality dataset achieves comparable performance to open source models while using only a tenth of the data."
                        ],
                        "paper": {
                            "corpus_id": 274130626,
                            "title": "Efficient Alignment of Large Language Models via Data Sampling",
                            "authors": [
                                {
                                    "authorId": "21302492",
                                    "name": "Amrit Khera"
                                },
                                {
                                    "authorId": "2213553962",
                                    "name": "Rajat Ghosh"
                                },
                                {
                                    "authorId": "2267726934",
                                    "name": "Debojyoti Dutta"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.96337890625
                    },
                    {
                        "id": "(Bai et al., 2024)",
                        "snippets": [
                            "Efficient data selection is crucial for the pretraining of large language models (LLMs), as the quality of training data significantly impacts the statistical efficiency of the training procedure and the model performance (Brown, 2020;(Du et al., 2021)(Chowdhery et al., 2022). Recently, we have witnessed numerous approaches, such as filtering high-quality data (Xie et al., 2023)(Wettig et al., 2024), mixing data from multiple domains (Xie et al., 2023)Liu et al., 2024), and selecting data that optimally boosts downstream task performance dynamically (Engstrom et al., 2024;Yu et al., 2024), which aim to improve data efficiency by prioritizing more informative training samples."
                        ],
                        "paper": {
                            "corpus_id": 273233719,
                            "title": "Efficient Pretraining Data Selection for Language Models via Multi-Actor Collaboration",
                            "authors": [
                                {
                                    "authorId": "2318978696",
                                    "name": "Tianyi Bai"
                                },
                                {
                                    "authorId": "2302788310",
                                    "name": "Ling Yang"
                                },
                                {
                                    "authorId": "2325175258",
                                    "name": "Zhen Hao Wong"
                                },
                                {
                                    "authorId": "2367277175",
                                    "name": "Fupeng Sun"
                                },
                                {
                                    "authorId": "2233445161",
                                    "name": "Jiahui Peng"
                                },
                                {
                                    "authorId": "2366068443",
                                    "name": "Xinlin Zhuang"
                                },
                                {
                                    "authorId": "2325489246",
                                    "name": "Chi Zhang"
                                },
                                {
                                    "authorId": "2325195689",
                                    "name": "Lijun Wu"
                                },
                                {
                                    "authorId": "2289911484",
                                    "name": "Jiantao Qiu"
                                },
                                {
                                    "authorId": "2302813081",
                                    "name": "Wentao Zhang"
                                },
                                {
                                    "authorId": "2303407552",
                                    "name": "Binhang Yuan"
                                },
                                {
                                    "authorId": "2291040348",
                                    "name": "Conghui He"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 6
                        },
                        "score": 0.96337890625
                    },
                    {
                        "id": "(Xie et al., 2023)",
                        "snippets": [
                            "The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to set the domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% points over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no knowledge of downstream tasks, even matches the performance of using domain weights tuned on downstream tasks."
                        ],
                        "paper": {
                            "corpus_id": 258741043,
                            "title": "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining",
                            "authors": [
                                {
                                    "authorId": "46215055",
                                    "name": "Sang Michael Xie"
                                },
                                {
                                    "authorId": "143950636",
                                    "name": "Hieu Pham"
                                },
                                {
                                    "authorId": "9929684",
                                    "name": "Xuanyi Dong"
                                },
                                {
                                    "authorId": "2140321952",
                                    "name": "Nan Du"
                                },
                                {
                                    "authorId": "2391802",
                                    "name": "Hanxiao Liu"
                                },
                                {
                                    "authorId": "2141538599",
                                    "name": "Yifeng Lu"
                                },
                                {
                                    "authorId": "145419642",
                                    "name": "Percy Liang"
                                },
                                {
                                    "authorId": "1397917613",
                                    "name": "Quoc V. Le"
                                },
                                {
                                    "authorId": "2114186424",
                                    "name": "Tengyu Ma"
                                },
                                {
                                    "authorId": "40625240",
                                    "name": "Adams Wei Yu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 203
                        },
                        "score": 0
                    },
                    {
                        "id": "(Na et al., 2024)",
                        "snippets": [
                            "Efficient data selection Given that data ablations on large language models is expensive, one class of approaches relies on approximating them on smaller models. Relevant work studies scaling laws for model parameters vs training tokens (Hoffmann et al., 2022;(Biderman et al., 2023), empirical effects of including or excluding different sources of data (Longpre et al., 2023), and the effects of training over multiple epochs vs new training tokens (Muennighoff et al., 2023). Previous work has also explored improving domain-specific fit via continued pre-training (Gururangan et al., 2020), predicting domain fit using lexical features (Reid et al., 2022), or improving general test-time adaptation via dynamic data selection, either by distributionally robust optimization with a small proxy model (Oren et al., 2019)Xie et al., 2023) or online using a multi-armed bandit approach (Albalak et al., 2023). Additional previous works aim to adapt to known downstream tasks via data selection, including at the individual example level (Wang et al., 2020) or even by explicitly fine-tuning models on many tasks (Aghajanyan et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 273502726,
                            "title": "Scalable Data Ablation Approximations for Language Models through Modular Training and Merging",
                            "authors": [
                                {
                                    "authorId": "2166313248",
                                    "name": "Clara Na"
                                },
                                {
                                    "authorId": "2124977543",
                                    "name": "Ian Magnusson"
                                },
                                {
                                    "authorId": "47286118",
                                    "name": "A. Jha"
                                },
                                {
                                    "authorId": "20662387",
                                    "name": "Tom Sherborne"
                                },
                                {
                                    "authorId": "2268272",
                                    "name": "Emma Strubell"
                                },
                                {
                                    "authorId": "2285322061",
                                    "name": "Jesse Dodge"
                                },
                                {
                                    "authorId": "2697425",
                                    "name": "Pradeep Dasigi"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 5
                        },
                        "score": 0.87158203125
                    },
                    {
                        "id": "(Albalak et al., 2023)",
                        "snippets": [
                            "The data used to pretrain large language models has a decisive impact on a model's downstream performance, which has led to a large body of work on data selection methods that aim to automatically determine the most suitable data to use for pretraining. Existing data selection methods suffer from slow and computationally expensive processes, a problem amplified by the increasing size of models and of pretraining datasets. Data mixing, on the other hand, reduces the complexity of data selection by grouping data points together and determining sampling probabilities across entire groups. However, data mixing proportions are typically fixed before training and therefore cannot adapt to changing training dynamics. To address these limitations, we develop an efficient algorithm for Online Data Mixing (ODM) that combines elements from both data selection and data mixing. Based on multi-armed bandit algorithms, our online approach optimizes the data mixing proportions during training. Remarkably, our method trains a model that reaches the final perplexity of the next best method with 19\\% fewer training iterations, and improves performance on the 5-shot MMLU benchmark by 1.9% relative accuracy, while adding negligible wall-clock time during pretraining."
                        ],
                        "paper": {
                            "corpus_id": 265658930,
                            "title": "Efficient Online Data Mixing For Language Model Pre-Training",
                            "authors": [
                                {
                                    "authorId": "2044198106",
                                    "name": "Alon Albalak"
                                },
                                {
                                    "authorId": "2256983134",
                                    "name": "Liangming Pan"
                                },
                                {
                                    "authorId": "2269733851",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "2257130314",
                                    "name": "W. Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 45
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wettig et al., 2024)",
                        "snippets": [
                            "Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that can capture human intuitions about data quality. In this paper, we investigate four qualities - writing style, required expertise, facts&trivia, and educational value - and find that LLMs are able to discern these qualities, especially when making pairwise judgments of texts. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity. When we sample using quality ratings as logits over documents, our models obtain lower perplexity and stronger in-context learning performance than baselines. Our best model is based on educational value and performs similarly to a model trained with uniform sampling for 50% more steps. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications."
                        ],
                        "paper": {
                            "corpus_id": 267681974,
                            "title": "QuRating: Selecting High-Quality Data for Training Language Models",
                            "authors": [
                                {
                                    "authorId": "2127066887",
                                    "name": "Alexander Wettig"
                                },
                                {
                                    "authorId": "2284268826",
                                    "name": "Aatmik Gupta"
                                },
                                {
                                    "authorId": "2323513320",
                                    "name": "Saumya Malik"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 79
                        },
                        "score": 0.94970703125
                    },
                    {
                        "id": "(Belenki et al., 2025)",
                        "snippets": [
                            "There is an extensive body of work on data selection and mixture optimization for pretraining language models. Albalak et al. (2024) offer a comprehensive recent survey. Approaches for data selection and cleaning consider different granularities of data, such as token-level, sample-level (individual documents or sentences can be selected or weighted), and group-level (where we consider samples in large groups assumed to have common characteristics, often derived from meta-data such as the web domain (like Wikipedia) or source collection name (such as C4)."
                        ],
                        "paper": {
                            "corpus_id": 276575595,
                            "title": "Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert Models",
                            "authors": [
                                {
                                    "authorId": "2316558829",
                                    "name": "Lior Belenki"
                                },
                                {
                                    "authorId": "2274120058",
                                    "name": "Alekh Agarwal"
                                },
                                {
                                    "authorId": "2346974785",
                                    "name": "Tianze Shi"
                                },
                                {
                                    "authorId": "2288931206",
                                    "name": "Kristina Toutanova"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.90380859375
                    },
                    {
                        "id": "(Jiang et al., 2024)",
                        "snippets": [
                            "Data curation and selection. For current large language models, compute often poses a greater constraint than data availability, making data selection crucial (Albalak et al., 2024). A widely used approach is data filtering (Soboleva et al., 2023;Penedo et al., 2023;2024), where undesirable data points are removed based on heuristics like perplexity, repetition (Lee et al., 2021), or semantic similarity (Abbas et al., 2023). This filtering process is foundational for constructing most largescale datasets today. After filtering, data is often categorized into subsets or domains (e.g., code, books), and deciding how much data to use from each domain becomes an important step. \n\nTwo primary strategies for data selection are prevalent: one focuses on deciding whether individual data points should be included based on various criteria (Mindermann et al., 2022)Engstrom et al., 2024), and the other uses all available data but samples from different domains with varying probabilities (Xie et al., 2024;Fan et al., 2023;(Albalak et al., 2023). While data selection aims to enhance training efficiency, these methods may introduce considerable computational overhead (Xie et al., 2024;(Chen et al., 2023)Fan et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 273350576,
                            "title": "Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws",
                            "authors": [
                                {
                                    "authorId": "2257132704",
                                    "name": "Yiding Jiang"
                                },
                                {
                                    "authorId": "2325951293",
                                    "name": "Allan Zhou"
                                },
                                {
                                    "authorId": "2261439316",
                                    "name": "Zhili Feng"
                                },
                                {
                                    "authorId": "49288855",
                                    "name": "Sadhika Malladi"
                                },
                                {
                                    "authorId": "2242257227",
                                    "name": "J. Kolter"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 22
                        },
                        "score": 0.9326171875
                    },
                    {
                        "id": "(Mindermann et al., 2022)",
                        "snippets": [
                            "Training on web-scale data can take months. But most computation and time is wasted on redundant and noisy points that are already learnt or not learnable. To accelerate training, we introduce Reducible Holdout Loss Selection (RHO-LOSS), a simple but principled technique which selects approximately those points for training that most reduce the model's generalization loss. As a result, RHO-LOSS mitigates the weaknesses of existing data selection methods: techniques from the optimization literature typically select 'hard' (e.g. high loss) points, but such points are often noisy (not learnable) or less task-relevant. Conversely, curriculum learning prioritizes 'easy' points, but such points need not be trained on once learned. In contrast, RHO-LOSS selects points that are learnable, worth learning, and not yet learnt. RHO-LOSS trains in far fewer steps than prior art, improves accuracy, and speeds up training on a wide range of datasets, hyperparameters, and architectures (MLPs, CNNs, and BERT). On the large web-scraped image dataset Clothing-1M, RHO-LOSS trains in 18x fewer steps and reaches 2% higher final accuracy than uniform data shuffling."
                        ],
                        "paper": {
                            "corpus_id": 235743024,
                            "title": "Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt",
                            "authors": [
                                {
                                    "authorId": "32777162",
                                    "name": "S. Mindermann"
                                },
                                {
                                    "authorId": "2117637436",
                                    "name": "Muhammed Razzak"
                                },
                                {
                                    "authorId": "2110536989",
                                    "name": "Winnie Xu"
                                },
                                {
                                    "authorId": "145408381",
                                    "name": "Andreas Kirsch"
                                },
                                {
                                    "authorId": "1429832706",
                                    "name": "Mrinank Sharma"
                                },
                                {
                                    "authorId": "1833773702",
                                    "name": "Adrien Morisot"
                                },
                                {
                                    "authorId": "19177000",
                                    "name": "Aidan N. Gomez"
                                },
                                {
                                    "authorId": "33859827",
                                    "name": "Sebastian Farquhar"
                                },
                                {
                                    "authorId": "38732223",
                                    "name": "J. Brauner"
                                },
                                {
                                    "authorId": "2681954",
                                    "name": "Y. Gal"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 164
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ruder et al., 2017)",
                        "snippets": [
                            "Inspired by work on curriculum learning, we propose to learn data selection measures using Bayesian Optimization and evaluate them across models, domains and tasks. Our learned measures outperform existing domain similarity measures significantly on three tasks: sentiment analysis, part-of-speech tagging, and parsing. We show the importance of complementing similarity with diversity, and that learned measures are\u2013to some degree\u2013transferable across models, domains, and even tasks."
                        ],
                        "paper": {
                            "corpus_id": 7403346,
                            "title": "Learning to select data for transfer learning with Bayesian Optimization",
                            "authors": [
                                {
                                    "authorId": "2884561",
                                    "name": "Sebastian Ruder"
                                },
                                {
                                    "authorId": "2022124",
                                    "name": "Barbara Plank"
                                }
                            ],
                            "year": 2017,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 186
                        },
                        "score": 0.88427734375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.272943
    }
}