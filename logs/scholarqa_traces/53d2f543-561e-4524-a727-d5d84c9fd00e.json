{
    "query": "What are recent advances in graph-based neural approaches for abstractive multi-document summarization, and which papers have demonstrated state-of-the-art results?",
    "user_id": "lib_user",
    "task_id": "53d2f543-561e-4524-a727-d5d84c9fd00e",
    "timestamp": "2025-06-24T00:00:28.006940",
    "n_retrieval": 256,
    "n_retrieved": 263,
    "n_candidates": 29,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.47950800000000005,
    "decomposed_query": {
        "rewritten_query": "Advances in graph-based neural approaches for abstractive multi-document summarization, and papers demonstrating state-of-the-art results.",
        "keyword_query": "graph-based neural approaches abstractive multi-document summarization state-of-the-art",
        "search_filters": {
            "year": "2022-2025",
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010065,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
            "venue": "IEEE Access",
            "year": 2024,
            "reference_count": 56,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2024.3443730",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2024.3443730?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2024.3443730, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2058964352",
                    "name": "A. Khaliq"
                },
                {
                    "authorId": "2149073250",
                    "name": "Atif Khan"
                },
                {
                    "authorId": "2316766711",
                    "name": "Salman Afsar Awan"
                },
                {
                    "authorId": "2316764100",
                    "name": "Salman Jan"
                },
                {
                    "authorId": "2257111507",
                    "name": "Muhammad Umair"
                },
                {
                    "authorId": "38509860",
                    "name": "M. Zuhairi"
                }
            ],
            "abstract": "The development of abstractive summarization methods is a crucial task in Natural Language Processing (NLP) that presents challenges, which require the creation of intelligent systems that are capable of extracting the main idea from text effectively and generate coherent summary. Numerous existing abstractive approaches do not take into account the importance of the broader context or fail to capture the global semantics in identifying salient content for summary. Moreover, there is lack of studies that extensively evaluated abstractive summarization models for specific domains, such as medical scientific document summarization. With this motivation behind, this paper developed an integrated framework for abstractive summarization of medical scientific documents that integrates topic-aware Heterogeneous Graph Neural Network with a Transformer model. The suggested framework uses Latent Dirichlet Allocation (LDA) for topic modeling to uncover latent topics and global information, thus preserving document-level attributes important for creation of effective summaries. In addition to topic modeling, the framework utilized a Heterogeneous Graph Neural Network (HGNN), capable of capturing the relationship between sentences through graph-based document representation, and allows for the concurrent updating of both local and global information. Finally, the framework is integrated with a Transformer decoder, which greatly enhances the ability of model to produce accurate and informative abstractive summaries. The performance of proposed framework is evaluated on publicly available PubMed dataset related to medical scientific papers. Experimental results illustrate that the suggested framework for abstractive summarization showed superior performance as compared to the state-of-the-art models, achieving high F1-Scores: 46.03 for Rouge-1, 21.42 for Rouge-2, and 39.71 for Rouge-L. Our research makes a significant contribution to the field of natural language processing, particularly in the area of medical scientific document summarization. It demonstrates superior performance and provides a deeper understanding of document structure, and has the potential to impact various applications by offering efficient access to information.",
            "corpus_id": 271938110,
            "sentences": [
                {
                    "corpus_id": "271938110",
                    "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
                    "text": "In recent years, there have been notable achievements in applying Graph Neural Networks (GNNs) or Graph Attention Networks (GAT) [11] to summarize documents [4], [12], [13], [14], [15], [16], [17]. This success arises from their ability to capture complex inter-sentence relationships within documents. Specifically, GNN models demonstrate proficiency in representing complex structural data by encapsulating semantic units (nodes) and their interconnections (edges). Due to these capabilities, there is growing interest in leveraging the GAT framework alongside topic modeling for abstractive summarization tasks. The researchers in [12] proposed a heterogeneous neural graph structure using BERT to obtain contextual sentence representations. They simultaneously trained with latent topics using the Neural Topic Model (NTM) to model global information. The study of [18] proposed an expanded model that employed NTM for abstractive text summarization. \n\nThe authors of [15] introduced a graph structure enriched with latent topical information. They employed K-Means and Gaussian Mixture Models (GMM) for topic extraction. The study of [3] proposed an innovative Graph-Based Topic-Aware abstractive text summarization model. Initially, documents were encoded with BERT to generate sentence representations. Concurrently, NTM identified potential topics, while GAT refined these sentences and topic nodes. Finally, the sentence embeddings, enriched with topic information, were fed into a Transformer-driven decoder to generate summaries. \n\nAnother study by [19] introduced a heterogeneous graph that incorporated topical information for abstractive summarization of Chinese complaint reports. To achieve this, GAT was formulated using sentence context embeddings and latent topics. Both the document context embeddings and topic data were updated concurrently, thereby addressing the issue of semantic fragmentation [3]. \n\nDespite the effectiveness of existing methods, there remains a need for a comprehensive, topic-aware neural graph model that integrates global semantic information and is coupled with an advanced decoder designed specifically for abstractive summarization of medical scientific documents. \n\nThis study introduces an innovative Topic-aware Heterogenous Graph Neural Network (HGNN) model aimed at summarizing medical documents.",
                    "score": 0.5528985147593463,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 2288,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 197
                        },
                        {
                            "start": 198,
                            "end": 302
                        },
                        {
                            "start": 303,
                            "end": 467
                        },
                        {
                            "start": 468,
                            "end": 614
                        },
                        {
                            "start": 615,
                            "end": 744
                        },
                        {
                            "start": 745,
                            "end": 855
                        },
                        {
                            "start": 856,
                            "end": 954
                        },
                        {
                            "start": 957,
                            "end": 1047
                        },
                        {
                            "start": 1048,
                            "end": 1125
                        },
                        {
                            "start": 1126,
                            "end": 1227
                        },
                        {
                            "start": 1228,
                            "end": 1309
                        },
                        {
                            "start": 1310,
                            "end": 1407
                        },
                        {
                            "start": 1408,
                            "end": 1540
                        },
                        {
                            "start": 1543,
                            "end": 1695
                        },
                        {
                            "start": 1696,
                            "end": 1784
                        },
                        {
                            "start": 1785,
                            "end": 1923
                        },
                        {
                            "start": 1926,
                            "end": 2214
                        },
                        {
                            "start": 2217,
                            "end": 2351
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 168,
                            "end": 172,
                            "matchedPaperCorpusId": "220045815"
                        },
                        {
                            "start": 174,
                            "end": 178,
                            "matchedPaperCorpusId": "252818946"
                        },
                        {
                            "start": 180,
                            "end": 184,
                            "matchedPaperCorpusId": "252670538"
                        },
                        {
                            "start": 186,
                            "end": 190,
                            "matchedPaperCorpusId": "250301954"
                        },
                        {
                            "start": 192,
                            "end": 196,
                            "matchedPaperCorpusId": "266285531"
                        },
                        {
                            "start": 972,
                            "end": 976,
                            "matchedPaperCorpusId": "252670538"
                        },
                        {
                            "start": 1139,
                            "end": 1142,
                            "matchedPaperCorpusId": "250012667"
                        },
                        {
                            "start": 1560,
                            "end": 1564,
                            "matchedPaperCorpusId": "264064249"
                        },
                        {
                            "start": 1919,
                            "end": 1922,
                            "matchedPaperCorpusId": "250012667"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9892578125
                },
                {
                    "corpus_id": "271938110",
                    "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
                    "text": "Modern text summarization research has shifted from homogeneous graphs with static nodes to heterogeneous networks. These networks allow for the inclusion of diverse node types, which represent wide range of textual elements. In addition, they allow for dynamic updates during the summarization process. The approach of [4] constructed a graph neural network based on word co-occurrences within the document to capture word-level relationships. The study in [42] utilized syntactic graph convolutional networks (GCNs) to model the non-Euclidean structure of documents. This approach effectively captured long-range dependencies beyond simple word order. In addition, an attention mechanism is integrated to focus on relevant content for summarization. The authors of [43] employed a network with three distinct node types: sentences, Elementary Discourse Units (EDUs), and entities. They utilized RST discourse parsing to grasp the relationships between EDUs, which provides a deeper comprehension of the document's structure. \n\nThe research in [44] presented a heterogeneous graph network that incorporated information from both words and sentences. The model focused on redundancy dependencies between sentences, and iteratively refined sentence representations through a redundancy-aware graph. This iterative process aimed to enhance the model's ability to capture the essential meaning of each sentence. Recent research examined novel network structures and attention mechanisms for extractive summarization of long documents [45]. One approach utilized a transformer-based architecture within a heterogeneous network and included distinct node types for tokens, entities, and sentences. The study discovered that a multi-granularity sparse attention mechanism assisted in focusing on important relationships between these diverse nodes during the summarization process. \n\nAnother study proposed a novel Multiplex Graph Convolutional Network (M-GCN) architecture [46], which were effective at capturing various kinds of relationships between words and sentences. The model took into account both relationships within sentences (intra-sentential) and relationships between sentences (inter-sentential) to produce better summaries, particularly for long documents. \n\nMoreover, the work of [14] improved the current heterogeneous graph approaches by adding passage nodes alongside word and sentence nodes.",
                    "score": 0.49506449754752213,
                    "section_title": "C. NEURAL GRAPH-BASED TEXT SUMMARIZATION",
                    "char_start_offset": 12480,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 115
                        },
                        {
                            "start": 116,
                            "end": 225
                        },
                        {
                            "start": 226,
                            "end": 303
                        },
                        {
                            "start": 304,
                            "end": 444
                        },
                        {
                            "start": 445,
                            "end": 568
                        },
                        {
                            "start": 569,
                            "end": 653
                        },
                        {
                            "start": 654,
                            "end": 751
                        },
                        {
                            "start": 752,
                            "end": 882
                        },
                        {
                            "start": 883,
                            "end": 1026
                        },
                        {
                            "start": 1029,
                            "end": 1150
                        },
                        {
                            "start": 1151,
                            "end": 1297
                        },
                        {
                            "start": 1298,
                            "end": 1408
                        },
                        {
                            "start": 1409,
                            "end": 1536
                        },
                        {
                            "start": 1537,
                            "end": 1692
                        },
                        {
                            "start": 1693,
                            "end": 1875
                        },
                        {
                            "start": 1878,
                            "end": 2067
                        },
                        {
                            "start": 2068,
                            "end": 2267
                        },
                        {
                            "start": 2270,
                            "end": 2407
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 458,
                            "end": 462,
                            "matchedPaperCorpusId": "212747858"
                        },
                        {
                            "start": 767,
                            "end": 771,
                            "matchedPaperCorpusId": "233189611"
                        },
                        {
                            "start": 1045,
                            "end": 1049,
                            "matchedPaperCorpusId": "226262332"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.978515625
                },
                {
                    "corpus_id": "271938110",
                    "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
                    "text": "This study proposed a novel Topic-aware Graph Neural Abstractive Summarization model particularly developed for lengthy scientific medical texts. Our model surpassed sentence-level neural graphs by considering additional semantic elements like words and latent topic nodes. This supplements the graph structure, leading a deeper understanding of the text. The model utilizes a powerful technique called BERT to encode the entire document. This exhaustive understanding of the text enables the model to better grasp the overall semantics and relationships between concepts. LDA is also employed to identify hidden topics within the text and recognizes its underlying themes. This thematic comprehension further improved the model's ability to generate summaries that precisely capture the gist of the document. \n\nIn addition, a Heterogeneous Graph Neural Network is integrated into the framework to handle the complexity of scientific medical text by capturing meaningful connections between words, sentences, and latent topics within the document. The network can effectively model the complex connections within the text using the diverse node types. Finally, a Transformer decoder is utilized to ensure that the generated summaries are accurate, clear, and closely reflect the original text. Moreover, the decoder produced high-quality summaries using the comprehensive understanding and rich relationships captured in the previous stages. \n\nWe evaluated our model against various methods using the publicly available PubMed dataset of medical research papers. The results revealed that our approach outperformed most traditional models and achieved performance closer to the leading methods For future research, we propose several directions to further improve our model: We will investigate the integration of more complex semantic units into the model to enhance its performance and robustness. We plan to explore advanced topic modeling techniques that are effective even in resource-limited environments, aiming to maintain simplicity without sacrificing performance. Additionally, we will examine the potential of advanced, sophisticated decoder components that could more effectively synergize with other neural components.",
                    "score": 0.4731360815716472,
                    "section_title": "V. CONCLUSION AND FUTURE DIRECTIONS",
                    "char_start_offset": 40178,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 145
                        },
                        {
                            "start": 146,
                            "end": 273
                        },
                        {
                            "start": 274,
                            "end": 355
                        },
                        {
                            "start": 356,
                            "end": 438
                        },
                        {
                            "start": 439,
                            "end": 572
                        },
                        {
                            "start": 573,
                            "end": 673
                        },
                        {
                            "start": 674,
                            "end": 809
                        },
                        {
                            "start": 812,
                            "end": 1047
                        },
                        {
                            "start": 1048,
                            "end": 1151
                        },
                        {
                            "start": 1152,
                            "end": 1293
                        },
                        {
                            "start": 1294,
                            "end": 1441
                        },
                        {
                            "start": 1444,
                            "end": 1562
                        },
                        {
                            "start": 1563,
                            "end": 1899
                        },
                        {
                            "start": 1900,
                            "end": 2074
                        },
                        {
                            "start": 2075,
                            "end": 2232
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9736328125
                },
                {
                    "corpus_id": "271938110",
                    "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
                    "text": "This section illustrates the methodology of our proposed Heterogeneous Graph Neural Network framework, enriched with topical information, for abstractive summarization as shown in Fig. 1. The proposed framework consists of four core trainable modules: a BERT Document Encoder, an LDA Topic Extractor, a Graph Attention Layer, and a Transformer Decoder. Given a corpus of scientific medical document with n documents, D = [D 1 , D 2 , . . . . . . ., D n ], and the corresponding 'M' human generated gold summaries Y = [Y 1 , Y 2 , . . . . . ., Y m ], our model aims to create abstractive summaries of the documents (D) that capture the essential meaning of the documents using their own phrasing, achieving quality comparable to human-written summaries (Y). The proposed framework utilizes a multi-step process to generate abstractive summaries for scientific medical documents. First, the BERT Document Encoder processes each document to generate contextualized embeddings for each sentence, capturing its semantic meaning. These embeddings become the foundation for sentence and word nodes within the document graph. Next, the LDA model identifies a set of latent topics that represent the underlying thematic structure of each document. These topics are also incorporated as nodes in the document graph. A heterogeneous document graph is then constructed, where nodes represent sentences, words, and topics and the edges capture the relationships between these nodes, and are represented by TF-IDF weights. \n\nThe Graph Attention Layer then operates on heterogeneous document graph. It Utilizes TF-IDF weights on the edges to emphasize connections between important words and sentences within the document. It leverages an attention mechanism to selectively focus on crucial relationships within the graph, refining the representation of each node. Finally, the refined document graph is fed into a Transformer decoder. The decoder utilizes these refined node representations to generate an abstractive summary that captures the essential meaning of the document using its own phrasing. The details behind each module are discussed below as follows. \n\nA. DOCUMENT ENCODER BERT's encoder leverages an attention-based architecture, offering significant advantages compared to standard language models.",
                    "score": 0.5796693513754119,
                    "section_title": "III. METHODOLOGY",
                    "char_start_offset": 15890,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 187
                        },
                        {
                            "start": 188,
                            "end": 352
                        },
                        {
                            "start": 353,
                            "end": 445
                        },
                        {
                            "start": 446,
                            "end": 539
                        },
                        {
                            "start": 540,
                            "end": 756
                        },
                        {
                            "start": 757,
                            "end": 877
                        },
                        {
                            "start": 878,
                            "end": 1023
                        },
                        {
                            "start": 1024,
                            "end": 1117
                        },
                        {
                            "start": 1118,
                            "end": 1238
                        },
                        {
                            "start": 1239,
                            "end": 1305
                        },
                        {
                            "start": 1306,
                            "end": 1508
                        },
                        {
                            "start": 1511,
                            "end": 1583
                        },
                        {
                            "start": 1584,
                            "end": 1707
                        },
                        {
                            "start": 1708,
                            "end": 1849
                        },
                        {
                            "start": 1850,
                            "end": 1920
                        },
                        {
                            "start": 1921,
                            "end": 2087
                        },
                        {
                            "start": 2088,
                            "end": 2150
                        },
                        {
                            "start": 2153,
                            "end": 2300
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97119140625
                },
                {
                    "corpus_id": "271938110",
                    "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
                    "text": "Our research includes three key areas: 1. A review of fundamental methods used for abstractive summarization. \n\n2. An examination of topic-aware models that combine topic modeling with other techniques to improve summarization. \n\n3. An exploration of how neural graph-based networks are used for both extractive and abstractive summarization tasks.",
                    "score": 0.5528985147593463,
                    "section_title": "II. RELATED WORK",
                    "char_start_offset": 7446,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 41
                        },
                        {
                            "start": 42,
                            "end": 109
                        },
                        {
                            "start": 112,
                            "end": 227
                        },
                        {
                            "start": 230,
                            "end": 348
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97119140625
                },
                {
                    "corpus_id": "271938110",
                    "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
                    "text": "Our model achieved near-identical results in R-1, comparable results in R-2, and surpassed Topic-GraphSum in R-L. This demonstrates the effectiveness of our model, which combined a GAT layer with a Transformer model, for abstractive summarization task. Topic-GraphSum, a state-ofthe-art model for extractive summarization of long scientific documents, employed an NTM model for topic modeling along with BERT and GAT networks. GraphSum utilizing an NTM model contributes to its higher R-1 score compared to our proposed model. This is because NTM can be jointly optimized with the document encoder and graph networks. However, NTM training configurations are more complex [15], and aligning it with a graph neural network is more challenging compared to the simpler and easier-to-train LDA model. It is important to note that extractive summarization focuses on selecting the most important sentences from a document, which often leads to higher ROUGE scores compared to abstractive methods [50]. \n\nOur analysis also compares our model with two other advanced models: HeterGraphLongSum and GTASum. \n\nHeterGraphLongSum: This graph-based model focuses on extracting summaries from long scientific papers. It leverages three semantic units -words, sentences, and passage nodes -within its graph structure. Notably, the inclusion of passage nodes as high-level semantic units contributes to HeterGraphLongSum's stronger performance on ROUGE scores compared to our proposed abstractive model. Another reason for this performance difference is that extractive summarization, by design, selects the most important sentences from a document, which often leads to higher ROUGE scores compared to abstractive methods. \n\nGTASum: This Graph-Based Topic-Aware abstractive summarization model is a direct competitor. GTASum employs a combination of techniques, including a BERT encoder, NTM for topic modeling, GAT, and a Transformer decoder. Our model surpassed GTASum in R-1 score, while achieving comparable results in R-2 and R-L scores. \n\nOur model differs from GTASum in its use of semantic units within the graph structure.",
                    "score": 0.5080688508903602,
                    "section_title": "5) GRAPH BASED MODELS",
                    "char_start_offset": 34342,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 113
                        },
                        {
                            "start": 114,
                            "end": 252
                        },
                        {
                            "start": 253,
                            "end": 426
                        },
                        {
                            "start": 427,
                            "end": 526
                        },
                        {
                            "start": 527,
                            "end": 617
                        },
                        {
                            "start": 618,
                            "end": 796
                        },
                        {
                            "start": 797,
                            "end": 996
                        },
                        {
                            "start": 999,
                            "end": 1097
                        },
                        {
                            "start": 1100,
                            "end": 1202
                        },
                        {
                            "start": 1203,
                            "end": 1302
                        },
                        {
                            "start": 1303,
                            "end": 1487
                        },
                        {
                            "start": 1488,
                            "end": 1707
                        },
                        {
                            "start": 1710,
                            "end": 1802
                        },
                        {
                            "start": 1803,
                            "end": 1928
                        },
                        {
                            "start": 1929,
                            "end": 2027
                        },
                        {
                            "start": 2030,
                            "end": 2116
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 672,
                            "end": 676,
                            "matchedPaperCorpusId": "252670538"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.970703125
                },
                {
                    "corpus_id": "271938110",
                    "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
                    "text": "This category includes models like Topic-GraphSum [12], SSN-DM [57] HeterGraphLongSum [14] and GTASum [3] that utilize graph structures to represent document information and relationships between sentences or topics, facilitating summarization. \n\nTable 2 in this study shows the Rouge F-scores for various models. The first segment of Table 2 demonstrates results for traditional summarization models and Oracle. The second segment presents the results of neural abstractive summarization models. The third segment reveal the results of Match-Sum, a sophisticated BERT-based summarizing model. The fourth segment illustrates the results achieved by recent advanced graph-based models for abstractive summarization. The last segment of Table 2 presents the results of our proposed model. \n\nOur model significantly outperformed conventional extractive summarization models on all ROUGE metrics, as shown in Table 2. This achievement extends to state-of-the-art neural abstractive and extractive models, where our approach also achieved higher ROUGE scores. This improvement highlights the benefit of integrating global semantic information with a dedicated graph layer. This layer facilitates the model's ability to generate summaries by considering the broader context within the document. \n\nAdditionally, traditional sequence-to-sequence (seq2seq) models with attention and pointer networks often struggle with lengthy scientific documents. This limitation arises from the challenges encoders face in capturing long-range dependencies within long texts [57]. Our model overcomes this limitation. Furthermore, our model surpasses the advanced BERT-based Match-Sum model, which experiences performance drops on the PubMed scientific dataset. Match-Sum's difficulty lies in grasping semantic and global information, hindering its ability to interpret the meaning of sentences and summaries. In contrast, our model can learn semantic information and leverage latent topics to focus on salient in long documents. \n\nOur model's performance is compared with highly advanced neural graph-based models for both extractive and abstractive summarization, which utilize rich semantic information. Our model achieved near-identical results in R-1, comparable results in R-2, and surpassed Topic-GraphSum in R-L.",
                    "score": 0.7653193041829127,
                    "section_title": "5) GRAPH BASED MODELS",
                    "char_start_offset": 32157,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 244
                        },
                        {
                            "start": 247,
                            "end": 313
                        },
                        {
                            "start": 314,
                            "end": 412
                        },
                        {
                            "start": 413,
                            "end": 496
                        },
                        {
                            "start": 497,
                            "end": 593
                        },
                        {
                            "start": 594,
                            "end": 714
                        },
                        {
                            "start": 715,
                            "end": 786
                        },
                        {
                            "start": 789,
                            "end": 913
                        },
                        {
                            "start": 914,
                            "end": 1054
                        },
                        {
                            "start": 1055,
                            "end": 1167
                        },
                        {
                            "start": 1168,
                            "end": 1288
                        },
                        {
                            "start": 1291,
                            "end": 1440
                        },
                        {
                            "start": 1441,
                            "end": 1558
                        },
                        {
                            "start": 1559,
                            "end": 1595
                        },
                        {
                            "start": 1596,
                            "end": 1739
                        },
                        {
                            "start": 1740,
                            "end": 1887
                        },
                        {
                            "start": 1888,
                            "end": 2007
                        },
                        {
                            "start": 2010,
                            "end": 2184
                        },
                        {
                            "start": 2185,
                            "end": 2298
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 63,
                            "end": 67,
                            "matchedPaperCorpusId": "235097475"
                        },
                        {
                            "start": 86,
                            "end": 90,
                            "matchedPaperCorpusId": "252818946"
                        },
                        {
                            "start": 102,
                            "end": 105,
                            "matchedPaperCorpusId": "250012667"
                        },
                        {
                            "start": 1553,
                            "end": 1557,
                            "matchedPaperCorpusId": "235097475"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9658203125
                },
                {
                    "corpus_id": "271938110",
                    "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
                    "text": "The development of abstractive summarization methods is a crucial task in Natural Language Processing (NLP) that presents challenges, which require the creation of intelligent systems that are capable of extracting the main idea from text effectively and generate coherent summary. Numerous existing abstractive approaches do not take into account the importance of the broader context or fail to capture the global semantics in identifying salient content for summary. Moreover, there is lack of studies that extensively evaluated abstractive summarization models for specific domains, such as medical scientific document summarization. With this motivation behind, this paper developed an integrated framework for abstractive summarization of medical scientific documents that integrates topic-aware Heterogeneous Graph Neural Network with a Transformer model. The suggested framework uses Latent Dirichlet Allocation (LDA) for topic modeling to uncover latent topics and global information, thus preserving document-level attributes important for creation of effective summaries. In addition to topic modeling, the framework utilized a Heterogeneous Graph Neural Network (HGNN), capable of capturing the relationship between sentences through graph-based document representation, and allows for the concurrent updating of both local and global information. Finally, the framework is integrated with a Transformer decoder, which greatly enhances the ability of model to produce accurate and informative abstractive summaries. The performance of proposed framework is evaluated on publicly available PubMed dataset related to medical scientific papers. Experimental results illustrate that the suggested framework for abstractive summarization showed superior performance as compared to the state-of-the-art models, achieving high F1-Scores: 46.03 for Rouge-1, 21.42 for Rouge-2, and 39.71 for Rouge-L. Our research makes a significant contribution to the field of natural language processing, particularly in the area of medical scientific document summarization. It demonstrates superior performance and provides a deeper understanding of document structure, and has the potential to impact various applications by offering efficient access to information.",
                    "score": 0.5914987155932159,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.958984375
                },
                {
                    "corpus_id": "271938110",
                    "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
                    "text": "This study introduces an innovative Topic-aware Heterogenous Graph Neural Network (HGNN) model aimed at summarizing medical documents. Instead of focusing solely on creating a sentence-level graph, the model integrates additional semantic components such as words and latent topic nodes by incorporating them as auxiliary elements within the graph. The initial phase involves using a BERT-Encoder (Bidirectional Encoder Representations) [20] to encode the entire document, thereby generating sentence and word nodes. Following this, latent topic nodes are identified using a well-established generative statistical topic modeling technique known as Latent Dirichlet Allocation (LDA) [21]. \n\nIn the next phase, a heterogeneous document graph is constructed, comprising sentences, words, and global topic nodes linked by edge features quantified as TF-IDF values. The representations of these nodes are then refined and updated using GAT. Ultimately, the sentence representations, enriched with topic information, are fed into a Transformer-based decoder to generate abstractive summaries. Our approach differs from previous works. For instance, the research by [3] relies on the Neural Topic Model (NTM) for topical information, which requires complex training setups [15]. Similarly, our method differs from work conducted by [19], which focused on constructing a heterogeneous graph solely for word and topic nodes, specifically for summarizing Chinese complaint reports. \n\nThe key contributions of our model are outlined as follows: \n\n\u2022 This study introduces a novel Neural Graph structure enriched with topical information for abstractive summarization of medical scientific documents. This approach integrates the LDA Topic Extractor, BERT-Encoder, Graph Attention Network, and Transformer model into a cohesive framework. \n\n113856 VOLUME 12, 2024 Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply. \n\n\u2022 The framework effectively captures global semantic information, guiding the summary generation process and addressing the challenge of limited semantic context in traditional summarization methods. \n\n\u2022 Leveraging data from the publicly available PubMed dataset, our research demonstrates consistent and favorable results compared to state-of-the-art heterogeneous graph structures used for both extractive and abstractive summarization tasks.",
                    "score": 0.541749456549423,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 4505,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 134
                        },
                        {
                            "start": 135,
                            "end": 348
                        },
                        {
                            "start": 349,
                            "end": 516
                        },
                        {
                            "start": 517,
                            "end": 688
                        },
                        {
                            "start": 691,
                            "end": 861
                        },
                        {
                            "start": 862,
                            "end": 936
                        },
                        {
                            "start": 937,
                            "end": 1087
                        },
                        {
                            "start": 1088,
                            "end": 1129
                        },
                        {
                            "start": 1130,
                            "end": 1272
                        },
                        {
                            "start": 1273,
                            "end": 1472
                        },
                        {
                            "start": 1475,
                            "end": 1534
                        },
                        {
                            "start": 1537,
                            "end": 1688
                        },
                        {
                            "start": 1689,
                            "end": 1826
                        },
                        {
                            "start": 1829,
                            "end": 1943
                        },
                        {
                            "start": 1944,
                            "end": 1963
                        },
                        {
                            "start": 1966,
                            "end": 2165
                        },
                        {
                            "start": 2168,
                            "end": 2410
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 683,
                            "end": 687,
                            "matchedPaperCorpusId": "753304"
                        },
                        {
                            "start": 1160,
                            "end": 1163,
                            "matchedPaperCorpusId": "250012667"
                        },
                        {
                            "start": 1267,
                            "end": 1271,
                            "matchedPaperCorpusId": "252670538"
                        },
                        {
                            "start": 1326,
                            "end": 1330,
                            "matchedPaperCorpusId": "264064249"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.958984375
                }
            ],
            "relevance_judgement": 0.9892578125,
            "relevance_judgment_input_expanded": "# Title: Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization\n# Venue: IEEE Access\n# Authors: A. Khaliq, Atif Khan, Salman Afsar Awan, Salman Jan, Muhammad Umair, M. Zuhairi\n## Abstract\nThe development of abstractive summarization methods is a crucial task in Natural Language Processing (NLP) that presents challenges, which require the creation of intelligent systems that are capable of extracting the main idea from text effectively and generate coherent summary. Numerous existing abstractive approaches do not take into account the importance of the broader context or fail to capture the global semantics in identifying salient content for summary. Moreover, there is lack of studies that extensively evaluated abstractive summarization models for specific domains, such as medical scientific document summarization. With this motivation behind, this paper developed an integrated framework for abstractive summarization of medical scientific documents that integrates topic-aware Heterogeneous Graph Neural Network with a Transformer model. The suggested framework uses Latent Dirichlet Allocation (LDA) for topic modeling to uncover latent topics and global information, thus preserving document-level attributes important for creation of effective summaries. In addition to topic modeling, the framework utilized a Heterogeneous Graph Neural Network (HGNN), capable of capturing the relationship between sentences through graph-based document representation, and allows for the concurrent updating of both local and global information. Finally, the framework is integrated with a Transformer decoder, which greatly enhances the ability of model to produce accurate and informative abstractive summaries. The performance of proposed framework is evaluated on publicly available PubMed dataset related to medical scientific papers. Experimental results illustrate that the suggested framework for abstractive summarization showed superior performance as compared to the state-of-the-art models, achieving high F1-Scores: 46.03 for Rouge-1, 21.42 for Rouge-2, and 39.71 for Rouge-L. Our research makes a significant contribution to the field of natural language processing, particularly in the area of medical scientific document summarization. It demonstrates superior performance and provides a deeper understanding of document structure, and has the potential to impact various applications by offering efficient access to information.\n## I. INTRODUCTION\nIn recent years, there have been notable achievements in applying Graph Neural Networks (GNNs) or Graph Attention Networks (GAT) [11] to summarize documents [4], [12], [13], [14], [15], [16], [17]. This success arises from their ability to capture complex inter-sentence relationships within documents. Specifically, GNN models demonstrate proficiency in representing complex structural data by encapsulating semantic units (nodes) and their interconnections (edges). Due to these capabilities, there is growing interest in leveraging the GAT framework alongside topic modeling for abstractive summarization tasks. The researchers in [12] proposed a heterogeneous neural graph structure using BERT to obtain contextual sentence representations. They simultaneously trained with latent topics using the Neural Topic Model (NTM) to model global information. The study of [18] proposed an expanded model that employed NTM for abstractive text summarization. \n\nThe authors of [15] introduced a graph structure enriched with latent topical information. They employed K-Means and Gaussian Mixture Models (GMM) for topic extraction. The study of [3] proposed an innovative Graph-Based Topic-Aware abstractive text summarization model. Initially, documents were encoded with BERT to generate sentence representations. Concurrently, NTM identified potential topics, while GAT refined these sentences and topic nodes. Finally, the sentence embeddings, enriched with topic information, were fed into a Transformer-driven decoder to generate summaries. \n\nAnother study by [19] introduced a heterogeneous graph that incorporated topical information for abstractive summarization of Chinese complaint reports. To achieve this, GAT was formulated using sentence context embeddings and latent topics. Both the document context embeddings and topic data were updated concurrently, thereby addressing the issue of semantic fragmentation [3]. \n\nDespite the effectiveness of existing methods, there remains a need for a comprehensive, topic-aware neural graph model that integrates global semantic information and is coupled with an advanced decoder designed specifically for abstractive summarization of medical scientific documents. \n\nThis study introduces an innovative Topic-aware Heterogenous Graph Neural Network (HGNN) model aimed at summarizing medical documents.\n...\nThis study introduces an innovative Topic-aware Heterogenous Graph Neural Network (HGNN) model aimed at summarizing medical documents. Instead of focusing solely on creating a sentence-level graph, the model integrates additional semantic components such as words and latent topic nodes by incorporating them as auxiliary elements within the graph. The initial phase involves using a BERT-Encoder (Bidirectional Encoder Representations) [20] to encode the entire document, thereby generating sentence and word nodes. Following this, latent topic nodes are identified using a well-established generative statistical topic modeling technique known as Latent Dirichlet Allocation (LDA) [21]. \n\nIn the next phase, a heterogeneous document graph is constructed, comprising sentences, words, and global topic nodes linked by edge features quantified as TF-IDF values. The representations of these nodes are then refined and updated using GAT. Ultimately, the sentence representations, enriched with topic information, are fed into a Transformer-based decoder to generate abstractive summaries. Our approach differs from previous works. For instance, the research by [3] relies on the Neural Topic Model (NTM) for topical information, which requires complex training setups [15]. Similarly, our method differs from work conducted by [19], which focused on constructing a heterogeneous graph solely for word and topic nodes, specifically for summarizing Chinese complaint reports. \n\nThe key contributions of our model are outlined as follows: \n\n\u2022 This study introduces a novel Neural Graph structure enriched with topical information for abstractive summarization of medical scientific documents. This approach integrates the LDA Topic Extractor, BERT-Encoder, Graph Attention Network, and Transformer model into a cohesive framework. \n\n113856 VOLUME 12, 2024 Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply. \n\n\u2022 The framework effectively captures global semantic information, guiding the summary generation process and addressing the challenge of limited semantic context in traditional summarization methods. \n\n\u2022 Leveraging data from the publicly available PubMed dataset, our research demonstrates consistent and favorable results compared to state-of-the-art heterogeneous graph structures used for both extractive and abstractive summarization tasks.\n\n## II. RELATED WORK\nOur research includes three key areas: 1. A review of fundamental methods used for abstractive summarization. \n\n2. An examination of topic-aware models that combine topic modeling with other techniques to improve summarization. \n\n3. An exploration of how neural graph-based networks are used for both extractive and abstractive summarization tasks.\n\n## C. NEURAL GRAPH-BASED TEXT SUMMARIZATION\nModern text summarization research has shifted from homogeneous graphs with static nodes to heterogeneous networks. These networks allow for the inclusion of diverse node types, which represent wide range of textual elements. In addition, they allow for dynamic updates during the summarization process. The approach of [4] constructed a graph neural network based on word co-occurrences within the document to capture word-level relationships. The study in [42] utilized syntactic graph convolutional networks (GCNs) to model the non-Euclidean structure of documents. This approach effectively captured long-range dependencies beyond simple word order. In addition, an attention mechanism is integrated to focus on relevant content for summarization. The authors of [43] employed a network with three distinct node types: sentences, Elementary Discourse Units (EDUs), and entities. They utilized RST discourse parsing to grasp the relationships between EDUs, which provides a deeper comprehension of the document's structure. \n\nThe research in [44] presented a heterogeneous graph network that incorporated information from both words and sentences. The model focused on redundancy dependencies between sentences, and iteratively refined sentence representations through a redundancy-aware graph. This iterative process aimed to enhance the model's ability to capture the essential meaning of each sentence. Recent research examined novel network structures and attention mechanisms for extractive summarization of long documents [45]. One approach utilized a transformer-based architecture within a heterogeneous network and included distinct node types for tokens, entities, and sentences. The study discovered that a multi-granularity sparse attention mechanism assisted in focusing on important relationships between these diverse nodes during the summarization process. \n\nAnother study proposed a novel Multiplex Graph Convolutional Network (M-GCN) architecture [46], which were effective at capturing various kinds of relationships between words and sentences. The model took into account both relationships within sentences (intra-sentential) and relationships between sentences (inter-sentential) to produce better summaries, particularly for long documents. \n\nMoreover, the work of [14] improved the current heterogeneous graph approaches by adding passage nodes alongside word and sentence nodes.\n\n## III. METHODOLOGY\nThis section illustrates the methodology of our proposed Heterogeneous Graph Neural Network framework, enriched with topical information, for abstractive summarization as shown in Fig. 1. The proposed framework consists of four core trainable modules: a BERT Document Encoder, an LDA Topic Extractor, a Graph Attention Layer, and a Transformer Decoder. Given a corpus of scientific medical document with n documents, D = [D 1 , D 2 , . . . . . . ., D n ], and the corresponding 'M' human generated gold summaries Y = [Y 1 , Y 2 , . . . . . ., Y m ], our model aims to create abstractive summaries of the documents (D) that capture the essential meaning of the documents using their own phrasing, achieving quality comparable to human-written summaries (Y). The proposed framework utilizes a multi-step process to generate abstractive summaries for scientific medical documents. First, the BERT Document Encoder processes each document to generate contextualized embeddings for each sentence, capturing its semantic meaning. These embeddings become the foundation for sentence and word nodes within the document graph. Next, the LDA model identifies a set of latent topics that represent the underlying thematic structure of each document. These topics are also incorporated as nodes in the document graph. A heterogeneous document graph is then constructed, where nodes represent sentences, words, and topics and the edges capture the relationships between these nodes, and are represented by TF-IDF weights. \n\nThe Graph Attention Layer then operates on heterogeneous document graph. It Utilizes TF-IDF weights on the edges to emphasize connections between important words and sentences within the document. It leverages an attention mechanism to selectively focus on crucial relationships within the graph, refining the representation of each node. Finally, the refined document graph is fed into a Transformer decoder. The decoder utilizes these refined node representations to generate an abstractive summary that captures the essential meaning of the document using its own phrasing. The details behind each module are discussed below as follows. \n\nA. DOCUMENT ENCODER BERT's encoder leverages an attention-based architecture, offering significant advantages compared to standard language models.\n\n## 5) GRAPH BASED MODELS\nThis category includes models like Topic-GraphSum [12], SSN-DM [57] HeterGraphLongSum [14] and GTASum [3] that utilize graph structures to represent document information and relationships between sentences or topics, facilitating summarization. \n\nTable 2 in this study shows the Rouge F-scores for various models. The first segment of Table 2 demonstrates results for traditional summarization models and Oracle. The second segment presents the results of neural abstractive summarization models. The third segment reveal the results of Match-Sum, a sophisticated BERT-based summarizing model. The fourth segment illustrates the results achieved by recent advanced graph-based models for abstractive summarization. The last segment of Table 2 presents the results of our proposed model. \n\nOur model significantly outperformed conventional extractive summarization models on all ROUGE metrics, as shown in Table 2. This achievement extends to state-of-the-art neural abstractive and extractive models, where our approach also achieved higher ROUGE scores. This improvement highlights the benefit of integrating global semantic information with a dedicated graph layer. This layer facilitates the model's ability to generate summaries by considering the broader context within the document. \n\nAdditionally, traditional sequence-to-sequence (seq2seq) models with attention and pointer networks often struggle with lengthy scientific documents. This limitation arises from the challenges encoders face in capturing long-range dependencies within long texts [57]. Our model overcomes this limitation. Furthermore, our model surpasses the advanced BERT-based Match-Sum model, which experiences performance drops on the PubMed scientific dataset. Match-Sum's difficulty lies in grasping semantic and global information, hindering its ability to interpret the meaning of sentences and summaries. In contrast, our model can learn semantic information and leverage latent topics to focus on salient in long documents. \n\nOur model's performance is compared with highly advanced neural graph-based models for both extractive and abstractive summarization, which utilize rich semantic information. Our model achieved near-identical results in R-1, comparable results in R-2, and surpassed Topic-GraphSum in R-L.\n...\nOur model achieved near-identical results in R-1, comparable results in R-2, and surpassed Topic-GraphSum in R-L. This demonstrates the effectiveness of our model, which combined a GAT layer with a Transformer model, for abstractive summarization task. Topic-GraphSum, a state-ofthe-art model for extractive summarization of long scientific documents, employed an NTM model for topic modeling along with BERT and GAT networks. GraphSum utilizing an NTM model contributes to its higher R-1 score compared to our proposed model. This is because NTM can be jointly optimized with the document encoder and graph networks. However, NTM training configurations are more complex [15], and aligning it with a graph neural network is more challenging compared to the simpler and easier-to-train LDA model. It is important to note that extractive summarization focuses on selecting the most important sentences from a document, which often leads to higher ROUGE scores compared to abstractive methods [50]. \n\nOur analysis also compares our model with two other advanced models: HeterGraphLongSum and GTASum. \n\nHeterGraphLongSum: This graph-based model focuses on extracting summaries from long scientific papers. It leverages three semantic units -words, sentences, and passage nodes -within its graph structure. Notably, the inclusion of passage nodes as high-level semantic units contributes to HeterGraphLongSum's stronger performance on ROUGE scores compared to our proposed abstractive model. Another reason for this performance difference is that extractive summarization, by design, selects the most important sentences from a document, which often leads to higher ROUGE scores compared to abstractive methods. \n\nGTASum: This Graph-Based Topic-Aware abstractive summarization model is a direct competitor. GTASum employs a combination of techniques, including a BERT encoder, NTM for topic modeling, GAT, and a Transformer decoder. Our model surpassed GTASum in R-1 score, while achieving comparable results in R-2 and R-L scores. \n\nOur model differs from GTASum in its use of semantic units within the graph structure.\n\n## V. CONCLUSION AND FUTURE DIRECTIONS\nThis study proposed a novel Topic-aware Graph Neural Abstractive Summarization model particularly developed for lengthy scientific medical texts. Our model surpassed sentence-level neural graphs by considering additional semantic elements like words and latent topic nodes. This supplements the graph structure, leading a deeper understanding of the text. The model utilizes a powerful technique called BERT to encode the entire document. This exhaustive understanding of the text enables the model to better grasp the overall semantics and relationships between concepts. LDA is also employed to identify hidden topics within the text and recognizes its underlying themes. This thematic comprehension further improved the model's ability to generate summaries that precisely capture the gist of the document. \n\nIn addition, a Heterogeneous Graph Neural Network is integrated into the framework to handle the complexity of scientific medical text by capturing meaningful connections between words, sentences, and latent topics within the document. The network can effectively model the complex connections within the text using the diverse node types. Finally, a Transformer decoder is utilized to ensure that the generated summaries are accurate, clear, and closely reflect the original text. Moreover, the decoder produced high-quality summaries using the comprehensive understanding and rich relationships captured in the previous stages. \n\nWe evaluated our model against various methods using the publicly available PubMed dataset of medical research papers. The results revealed that our approach outperformed most traditional models and achieved performance closer to the leading methods For future research, we propose several directions to further improve our model: We will investigate the integration of more complex semantic units into the model to enhance its performance and robustness. We plan to explore advanced topic modeling techniques that are effective even in resource-limited environments, aiming to maintain simplicity without sacrificing performance. Additionally, we will examine the potential of advanced, sophisticated decoder components that could more effectively synergize with other neural components.",
            "reference_string": "[271938110 | Khaliq et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Cross-Document Distillation via Graph-Based Summarization of Extracted Essential Knowledge",
            "venue": "IEEE Transactions on Audio, Speech, and Language Processing",
            "year": 2025,
            "reference_count": 89,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/taslp.2024.3490375",
                "status": "HYBRID",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TASLP.2024.3490375?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TASLP.2024.3490375, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "134327204",
                    "name": "Luca Ragazzi"
                },
                {
                    "authorId": "2237646991",
                    "name": "Gianluca Moro"
                },
                {
                    "authorId": "2132084411",
                    "name": "Lorenzo Valgimigli"
                },
                {
                    "authorId": "2328891652",
                    "name": "Riccardo Fiorani"
                }
            ],
            "abstract": "Abstractive multi-document summarization aims to generate a comprehensive summary that encapsulates crucial content derived from multiple input documents. Despite the proficiency exhibited by language models in text summarization, challenges persist in capturing and aggregating salient information dispersed across a cluster of lengthy sources. To accommodate more input, existing solutions prioritize sparse attention mechanisms, relying on sequence truncation without incorporating graph-based modeling of multiple semantic units to locate essential facets. Furthermore, the limited availability of training examples adversely impacts performance, thereby compromising summarization quality in real-world few-shot scenarios. In this paper, we present G-Seek-2, a graph-enhanced approach designed to distill multiple topic-related documents by pinpointing and processing solely the pertinent information. We use a heterogeneous graph to model the input cluster, interconnecting various encoded entities via informative semantic edges. Then, a graph neural network locates the most salient sentences that are provided to a language model to generate the summary. We extensively evaluate G-Seek-2 across seven datasets spanning various domains\u2014including news articles, lawsuits, government reports, and scientific texts\u2014under few-shot settings with a limited training sample size of only 100 examples. The experimental findings demonstrate that our model consistently outperforms advanced summarization baselines, achieving improvements as measured by syntactic and semantic metrics.",
            "corpus_id": 273784877,
            "sentences": [
                {
                    "corpus_id": "273784877",
                    "title": "Cross-Document Distillation via Graph-Based Summarization of Extracted Essential Knowledge",
                    "text": "Abstractive multi-document summarization aims to generate a comprehensive summary that encapsulates crucial content derived from multiple input documents. Despite the proficiency exhibited by language models in text summarization, challenges persist in capturing and aggregating salient information dispersed across a cluster of lengthy sources. To accommodate more input, existing solutions prioritize sparse attention mechanisms, relying on sequence truncation without incorporating graph-based modeling of multiple semantic units to locate essential facets. Furthermore, the limited availability of training examples adversely impacts performance, thereby compromising summarization quality in real-world few-shot scenarios. In this paper, we present G-Seek-2, a graph-enhanced approach designed to distill multiple topic-related documents by pinpointing and processing solely the pertinent information. We use a heterogeneous graph to model the input cluster, interconnecting various encoded entities via informative semantic edges. Then, a graph neural network locates the most salient sentences that are provided to a language model to generate the summary. We extensively evaluate G-Seek-2 across seven datasets spanning various domains\u2014including news articles, lawsuits, government reports, and scientific texts\u2014under few-shot settings with a limited training sample size of only 100 examples. The experimental findings demonstrate that our model consistently outperforms advanced summarization baselines, achieving improvements as measured by syntactic and semantic metrics.",
                    "score": 0.6647292555132034,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.986328125
                },
                {
                    "corpus_id": "273784877",
                    "title": "Cross-Document Distillation via Graph-Based Summarization of Extracted Essential Knowledge",
                    "text": "Cross-Document Distillation via Graph-Based Summarization of Extracted Essential Knowledge Luca Ragazzi , Gianluca Moro , Lorenzo Valgimigli, and Riccardo Fiorani Abstract-Abstractive multi-document summarization aims to generate a comprehensive summary that encapsulates crucial content derived from multiple input documents. Despite the proficiency exhibited by language models in text summarization, challenges persist in capturing and aggregating salient information dispersed across a cluster of lengthy sources. To accommodate more input, existing solutions prioritize sparse attention mechanisms, relying on sequence truncation without incorporating graph-based modeling of multiple semantic units to locate essential facets. Furthermore, the limited availability of training examples adversely impacts performance, thereby compromising summarization quality in real-world few-shot scenarios. In this paper, we present G-SEEK-2, a graph-enhanced approach designed to distill multiple topic-related documents by pinpointing and processing solely the pertinent information. We use a heterogeneous graph to model the input cluster, interconnecting various encoded entities via informative semantic edges. Then, a graph neural network locates the most salient sentences that are provided to a language model to generate the summary. We extensively evaluate G-SEEK-2 across seven datasets spanning various domains-including news articles, lawsuits, government reports, and scientific texts-under few-shot settings with a limited training sample size of only 100 examples. The experimental findings demonstrate that our model consistently outperforms advanced summarization baselines, achieving improvements as measured by syntactic and semantic metrics. \n\nIndex Terms-Multi-document summarization, graphenhanced transformer, few-shot learning.",
                    "score": 0.7189857944159705,
                    "section_title": "body",
                    "char_start_offset": 1,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 326
                        },
                        {
                            "start": 327,
                            "end": 517
                        },
                        {
                            "start": 518,
                            "end": 732
                        },
                        {
                            "start": 733,
                            "end": 899
                        },
                        {
                            "start": 900,
                            "end": 1078
                        },
                        {
                            "start": 1079,
                            "end": 1208
                        },
                        {
                            "start": 1209,
                            "end": 1335
                        },
                        {
                            "start": 1336,
                            "end": 1573
                        },
                        {
                            "start": 1574,
                            "end": 1755
                        },
                        {
                            "start": 1758,
                            "end": 1845
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.982421875
                }
            ],
            "relevance_judgement": 0.986328125,
            "relevance_judgment_input_expanded": "# Title: Cross-Document Distillation via Graph-Based Summarization of Extracted Essential Knowledge\n# Venue: IEEE Transactions on Audio, Speech, and Language Processing\n# Authors: Luca Ragazzi, Gianluca Moro, Lorenzo Valgimigli, Riccardo Fiorani\n## Abstract\nAbstractive multi-document summarization aims to generate a comprehensive summary that encapsulates crucial content derived from multiple input documents. Despite the proficiency exhibited by language models in text summarization, challenges persist in capturing and aggregating salient information dispersed across a cluster of lengthy sources. To accommodate more input, existing solutions prioritize sparse attention mechanisms, relying on sequence truncation without incorporating graph-based modeling of multiple semantic units to locate essential facets. Furthermore, the limited availability of training examples adversely impacts performance, thereby compromising summarization quality in real-world few-shot scenarios. In this paper, we present G-Seek-2, a graph-enhanced approach designed to distill multiple topic-related documents by pinpointing and processing solely the pertinent information. We use a heterogeneous graph to model the input cluster, interconnecting various encoded entities via informative semantic edges. Then, a graph neural network locates the most salient sentences that are provided to a language model to generate the summary. We extensively evaluate G-Seek-2 across seven datasets spanning various domains\u2014including news articles, lawsuits, government reports, and scientific texts\u2014under few-shot settings with a limited training sample size of only 100 examples. The experimental findings demonstrate that our model consistently outperforms advanced summarization baselines, achieving improvements as measured by syntactic and semantic metrics.\n## body\nCross-Document Distillation via Graph-Based Summarization of Extracted Essential Knowledge Luca Ragazzi , Gianluca Moro , Lorenzo Valgimigli, and Riccardo Fiorani Abstract-Abstractive multi-document summarization aims to generate a comprehensive summary that encapsulates crucial content derived from multiple input documents. Despite the proficiency exhibited by language models in text summarization, challenges persist in capturing and aggregating salient information dispersed across a cluster of lengthy sources. To accommodate more input, existing solutions prioritize sparse attention mechanisms, relying on sequence truncation without incorporating graph-based modeling of multiple semantic units to locate essential facets. Furthermore, the limited availability of training examples adversely impacts performance, thereby compromising summarization quality in real-world few-shot scenarios. In this paper, we present G-SEEK-2, a graph-enhanced approach designed to distill multiple topic-related documents by pinpointing and processing solely the pertinent information. We use a heterogeneous graph to model the input cluster, interconnecting various encoded entities via informative semantic edges. Then, a graph neural network locates the most salient sentences that are provided to a language model to generate the summary. We extensively evaluate G-SEEK-2 across seven datasets spanning various domains-including news articles, lawsuits, government reports, and scientific texts-under few-shot settings with a limited training sample size of only 100 examples. The experimental findings demonstrate that our model consistently outperforms advanced summarization baselines, achieving improvements as measured by syntactic and semantic metrics. \n\nIndex Terms-Multi-document summarization, graphenhanced transformer, few-shot learning.",
            "reference_string": "[273784877 | Ragazzi et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Contrastive Hierarchical Discourse Graph for Scientific Document Summarization",
            "venue": "CODI",
            "year": 2023,
            "reference_count": 51,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.00177",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.00177, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2135688409",
                    "name": "Haopeng Zhang"
                },
                {
                    "authorId": "2218494165",
                    "name": "Xiao Liu"
                },
                {
                    "authorId": "2168548350",
                    "name": "Jiawei Zhang"
                }
            ],
            "abstract": "The extended structural context has made scientific paper summarization a challenging task. This paper proposes CHANGES, a contrastive hierarchical graph neural network for extractive scientific paper summarization. CHANGES represents a scientific paper with a hierarchical discourse graph and learns effective sentence representations with dedicated designed hierarchical graph information aggregation. We also propose a graph contrastive learning module to learn global theme-aware sentence representations. Extensive experiments on the PubMed and arXiv benchmark datasets prove the effectiveness of CHANGES and the importance of capturing hierarchical structure information in modeling scientific papers.",
            "corpus_id": 258999996,
            "sentences": [
                {
                    "corpus_id": "258999996",
                    "title": "Contrastive Hierarchical Discourse Graph for Scientific Document Summarization",
                    "text": "Thus, sparse Transformer models like BigBird (Zaheer et al., 2020) and Longformer (Beltagy et al., 2020) are proposed. \n\nRecently, researchers have also turned to graph neural networks (GNN) as an alternative approach. Graph neural networks have been demonstrated to be effective at tasks with rich relational structure and can preserve global structure information (Yao et al., 2019;Xu et al., 2019;Zhang and Zhang, 2020). By representing a document as a graph, GNNs update and learn sentence representations by message passing, and turn extractive summarization into a node classification problem. Among all attempts, one popular way is to construct crosssentence similarity graphs (Erkan and Radev, 2004;Zheng and Lapata, 2019), which uses sentence representation cosine similarity as edge weights to model cross-sentence semantic relations. Xu et al. (2019) proposed using Rhetorical Structure Theory (RST) trees and coreference mentions to capture cross-sentence discourse relations. Wang et al. (2020) proposed constructing a word-document het-erogeneous graph by using words as the intermediary between sentences. Despite their success, how to construct an effective graph to capture the hierarchical structure for academic papers remains an open question. \n\nTo address the above challenges, we propose CHANGES (Contrastive HierArchical Graph neural network for Extractive Summarization), a hierarchical graph neural network model to fully exploit the section structure of scientific papers. CHANGES first constructs a sentence-section hierarchical graph for a scientific paper, and then learns hierarchical sentence representations by dedicated designed information aggregation with iterative intra-section and inter-section message passing. Inspired by recent advances in contrastive learning (Liu and Liu, 2021;Chen et al., 2020), we also propose a graph contrastive learning module to learn global theme-aware sentence representations and provide fine-grained discriminative information. The local sentence and global section representations are then fused for salient sentence prediction. We validate CHANGES with extensive experiments and analyses on two scientific paper summarization datasets. Experimental results demonstrate the effectiveness of our proposed method. Our main contributions are as follows: \n\n\u2022 We propose a hierarchical graph-based model for long scientific paper extractive summarization.",
                    "score": 0.6501439029980846,
                    "section_title": "Introduction",
                    "char_start_offset": 2113,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 118
                        },
                        {
                            "start": 121,
                            "end": 218
                        },
                        {
                            "start": 219,
                            "end": 423
                        },
                        {
                            "start": 424,
                            "end": 599
                        },
                        {
                            "start": 600,
                            "end": 844
                        },
                        {
                            "start": 845,
                            "end": 988
                        },
                        {
                            "start": 989,
                            "end": 1120
                        },
                        {
                            "start": 1121,
                            "end": 1263
                        },
                        {
                            "start": 1266,
                            "end": 1498
                        },
                        {
                            "start": 1499,
                            "end": 1749
                        },
                        {
                            "start": 1750,
                            "end": 1998
                        },
                        {
                            "start": 1999,
                            "end": 2100
                        },
                        {
                            "start": 2101,
                            "end": 2208
                        },
                        {
                            "start": 2209,
                            "end": 2283
                        },
                        {
                            "start": 2284,
                            "end": 2322
                        },
                        {
                            "start": 2325,
                            "end": 2422
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 45,
                            "end": 66,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 366,
                            "end": 384,
                            "matchedPaperCorpusId": "52284222"
                        },
                        {
                            "start": 400,
                            "end": 422,
                            "matchedPaperCorpusId": "226262336"
                        },
                        {
                            "start": 684,
                            "end": 707,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 1821,
                            "end": 1839,
                            "matchedPaperCorpusId": "211096730"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98486328125
                },
                {
                    "corpus_id": "258999996",
                    "title": "Contrastive Hierarchical Discourse Graph for Scientific Document Summarization",
                    "text": "Graph models have been widely applied to extractive summarization due to the capability of modeling cross-sentence relations within a document. The sparsity nature of graph structure also brings scalability and flexibility, making it a good fit for long documents. Graph neural networks' memory costs are generally linear with regard to the input size compared to the quadratic self-attention mechanism. \n\nResearchers have explored supervised graph neural network methods for summarization (Cui and Hu, 2021;Jia et al., 2020;Huang and Kurohashi, 2021;Xie et al., 2022;Phan et al., 2022). Yasunaga et al. (2017) first proposed to use Graph Convolutional Network (GCN) on the approximate discourse graph. Xu et al. (2019) then applied GCN on structural discourse graphs based on RST trees and coreference mentions. Recently, Wang et al. (2020) proposed constructing a word-document heterogeneous graph by using words as the intermediary Figure 1: The overall model architecture of CHANGES. We first construct a hierarchical graph for an input document, and then learn representations with a graph contrastive module and hierarchical graph layers. The concatenation representations of sentence node and its section node will be fused for summary sentence selection. between sentences. Zhang et al. (2022a) proposed to use hypergraph to capture the high-order sentence relations within the document. Our paper follows the series of work but incorporates hierarchical graphs for scientific paper discourse structure modeling and graph contrastive learning for theme-aware sentence representation learning.",
                    "score": 0.5642473185802919,
                    "section_title": "Graph-based Summarization",
                    "char_start_offset": 6565,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 143
                        },
                        {
                            "start": 144,
                            "end": 264
                        },
                        {
                            "start": 265,
                            "end": 403
                        },
                        {
                            "start": 406,
                            "end": 587
                        },
                        {
                            "start": 588,
                            "end": 702
                        },
                        {
                            "start": 703,
                            "end": 812
                        },
                        {
                            "start": 813,
                            "end": 987
                        },
                        {
                            "start": 988,
                            "end": 1144
                        },
                        {
                            "start": 1145,
                            "end": 1262
                        },
                        {
                            "start": 1263,
                            "end": 1281
                        },
                        {
                            "start": 1282,
                            "end": 1395
                        },
                        {
                            "start": 1396,
                            "end": 1600
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 508,
                            "end": 525,
                            "matchedPaperCorpusId": "226262332"
                        },
                        {
                            "start": 525,
                            "end": 551,
                            "matchedPaperCorpusId": "233189611"
                        },
                        {
                            "start": 568,
                            "end": 586,
                            "matchedPaperCorpusId": "252818946"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9736328125
                }
            ],
            "relevance_judgement": 0.98486328125,
            "relevance_judgment_input_expanded": "# Title: Contrastive Hierarchical Discourse Graph for Scientific Document Summarization\n# Venue: CODI\n# Authors: Haopeng Zhang, Xiao Liu, Jiawei Zhang\n## Abstract\nThe extended structural context has made scientific paper summarization a challenging task. This paper proposes CHANGES, a contrastive hierarchical graph neural network for extractive scientific paper summarization. CHANGES represents a scientific paper with a hierarchical discourse graph and learns effective sentence representations with dedicated designed hierarchical graph information aggregation. We also propose a graph contrastive learning module to learn global theme-aware sentence representations. Extensive experiments on the PubMed and arXiv benchmark datasets prove the effectiveness of CHANGES and the importance of capturing hierarchical structure information in modeling scientific papers.\n## Introduction\nThus, sparse Transformer models like BigBird (Zaheer et al., 2020) and Longformer (Beltagy et al., 2020) are proposed. \n\nRecently, researchers have also turned to graph neural networks (GNN) as an alternative approach. Graph neural networks have been demonstrated to be effective at tasks with rich relational structure and can preserve global structure information (Yao et al., 2019;Xu et al., 2019;Zhang and Zhang, 2020). By representing a document as a graph, GNNs update and learn sentence representations by message passing, and turn extractive summarization into a node classification problem. Among all attempts, one popular way is to construct crosssentence similarity graphs (Erkan and Radev, 2004;Zheng and Lapata, 2019), which uses sentence representation cosine similarity as edge weights to model cross-sentence semantic relations. Xu et al. (2019) proposed using Rhetorical Structure Theory (RST) trees and coreference mentions to capture cross-sentence discourse relations. Wang et al. (2020) proposed constructing a word-document het-erogeneous graph by using words as the intermediary between sentences. Despite their success, how to construct an effective graph to capture the hierarchical structure for academic papers remains an open question. \n\nTo address the above challenges, we propose CHANGES (Contrastive HierArchical Graph neural network for Extractive Summarization), a hierarchical graph neural network model to fully exploit the section structure of scientific papers. CHANGES first constructs a sentence-section hierarchical graph for a scientific paper, and then learns hierarchical sentence representations by dedicated designed information aggregation with iterative intra-section and inter-section message passing. Inspired by recent advances in contrastive learning (Liu and Liu, 2021;Chen et al., 2020), we also propose a graph contrastive learning module to learn global theme-aware sentence representations and provide fine-grained discriminative information. The local sentence and global section representations are then fused for salient sentence prediction. We validate CHANGES with extensive experiments and analyses on two scientific paper summarization datasets. Experimental results demonstrate the effectiveness of our proposed method. Our main contributions are as follows: \n\n\u2022 We propose a hierarchical graph-based model for long scientific paper extractive summarization.\n\n## Graph-based Summarization\nGraph models have been widely applied to extractive summarization due to the capability of modeling cross-sentence relations within a document. The sparsity nature of graph structure also brings scalability and flexibility, making it a good fit for long documents. Graph neural networks' memory costs are generally linear with regard to the input size compared to the quadratic self-attention mechanism. \n\nResearchers have explored supervised graph neural network methods for summarization (Cui and Hu, 2021;Jia et al., 2020;Huang and Kurohashi, 2021;Xie et al., 2022;Phan et al., 2022). Yasunaga et al. (2017) first proposed to use Graph Convolutional Network (GCN) on the approximate discourse graph. Xu et al. (2019) then applied GCN on structural discourse graphs based on RST trees and coreference mentions. Recently, Wang et al. (2020) proposed constructing a word-document heterogeneous graph by using words as the intermediary Figure 1: The overall model architecture of CHANGES. We first construct a hierarchical graph for an input document, and then learn representations with a graph contrastive module and hierarchical graph layers. The concatenation representations of sentence node and its section node will be fused for summary sentence selection. between sentences. Zhang et al. (2022a) proposed to use hypergraph to capture the high-order sentence relations within the document. Our paper follows the series of work but incorporates hierarchical graphs for scientific paper discourse structure modeling and graph contrastive learning for theme-aware sentence representation learning.",
            "reference_string": "[258999996 | Zhang et al. | 2023 | Citations: 12]"
        },
        {
            "title": "Abstractive text summarization: State of the art, challenges, and improvements",
            "venue": "Neurocomputing",
            "year": 2024,
            "reference_count": 224,
            "citation_count": 24,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.02413, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2300173312",
                    "name": "Hassan Shakil"
                },
                {
                    "authorId": "2313554804",
                    "name": "Ahmad Farooq"
                },
                {
                    "authorId": "2261083539",
                    "name": "Jugal K. Kalita"
                }
            ],
            "abstract": null,
            "corpus_id": 271525553,
            "sentences": [
                {
                    "corpus_id": "271525553",
                    "title": "Abstractive text summarization: State of the art, challenges, and improvements",
                    "text": "This method strengthens the model's capacity to generate coherent summaries and help tackle the issues of OOV words. On the Gigaword and DUC-2004 [52] datasets, the authors evaluated the performance of the model in comparison to a number of benchmark models. The results demonstrated that the suggested modifications improved the functionality of the basic Seq2Seq model, leading to more precise and insightful abstractive summaries. A graph-based attention mechanism for abstractive document summarization was introduced by Tan et al. [53] that takes into account the relationships between sentences in a document. Traditional Seq2Seq models with attention mechanisms frequently concentrate on the words within a sentence but are unable to recognize the inter-sentence dependencies. The authors aimed to overcome this restriction by embedding the structural information of the document into the attention mechanism. The suggested approach first creates a sentence graph that represents the document, where the nodes are the sentences and the edges are the relationships between them. The attention mechanism then works on this graph, permitting the model to focus on both local and global sentence-level information. The model consists of a bidirectional RNN encoder to capture a representation of the input sentence and a decoder with a graph-based attention mechanism for generating the summary. On the CNN/Daily Mail and DUC-2004 datasets, the authors assessed the performance of the model in comparison with various state-of-the-art abstrac-tive and extractive summarization models. The results showed that by precisely capturing the connections between sentences in a document, the graph-based attention mechanism improved the quality of generated summaries.",
                    "score": 0.6492219533767885,
                    "section_title": "Attention Mechanisms",
                    "char_start_offset": 24259,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 117,
                            "end": 258
                        },
                        {
                            "start": 259,
                            "end": 433
                        },
                        {
                            "start": 434,
                            "end": 615
                        },
                        {
                            "start": 616,
                            "end": 783
                        },
                        {
                            "start": 784,
                            "end": 916
                        },
                        {
                            "start": 917,
                            "end": 1084
                        },
                        {
                            "start": 1085,
                            "end": 1217
                        },
                        {
                            "start": 1218,
                            "end": 1398
                        },
                        {
                            "start": 1399,
                            "end": 1587
                        },
                        {
                            "start": 1588,
                            "end": 1764
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 536,
                            "end": 540,
                            "matchedPaperCorpusId": "26698484"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.984375
                }
            ],
            "relevance_judgement": 0.984375,
            "relevance_judgment_input_expanded": "# Title: Abstractive text summarization: State of the art, challenges, and improvements\n# Venue: Neurocomputing\n# Authors: Hassan Shakil, Ahmad Farooq, Jugal K. Kalita\n## Abstract\nNone\n## Attention Mechanisms\nThis method strengthens the model's capacity to generate coherent summaries and help tackle the issues of OOV words. On the Gigaword and DUC-2004 [52] datasets, the authors evaluated the performance of the model in comparison to a number of benchmark models. The results demonstrated that the suggested modifications improved the functionality of the basic Seq2Seq model, leading to more precise and insightful abstractive summaries. A graph-based attention mechanism for abstractive document summarization was introduced by Tan et al. [53] that takes into account the relationships between sentences in a document. Traditional Seq2Seq models with attention mechanisms frequently concentrate on the words within a sentence but are unable to recognize the inter-sentence dependencies. The authors aimed to overcome this restriction by embedding the structural information of the document into the attention mechanism. The suggested approach first creates a sentence graph that represents the document, where the nodes are the sentences and the edges are the relationships between them. The attention mechanism then works on this graph, permitting the model to focus on both local and global sentence-level information. The model consists of a bidirectional RNN encoder to capture a representation of the input sentence and a decoder with a graph-based attention mechanism for generating the summary. On the CNN/Daily Mail and DUC-2004 datasets, the authors assessed the performance of the model in comparison with various state-of-the-art abstrac-tive and extractive summarization models. The results showed that by precisely capturing the connections between sentences in a document, the graph-based attention mechanism improved the quality of generated summaries.",
            "reference_string": "[271525553 | Shakil et al. | 2024 | Citations: 24]"
        },
        {
            "title": "Multi-Document Scientific Summarization from a Knowledge Graph-Centric View",
            "venue": "International Conference on Computational Linguistics",
            "year": 2022,
            "reference_count": 34,
            "citation_count": 15,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2209.04319",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2209.04319, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2073437",
                    "name": "Pancheng Wang"
                },
                {
                    "authorId": "2145340498",
                    "name": "Shasha Li"
                },
                {
                    "authorId": "2116489",
                    "name": "Kunyuan Pang"
                },
                {
                    "authorId": "50670961",
                    "name": "Liangliang He"
                },
                {
                    "authorId": "2108821455",
                    "name": "Dong Li"
                },
                {
                    "authorId": "1762106",
                    "name": "Jintao Tang"
                },
                {
                    "authorId": "38972135",
                    "name": "Ting Wang"
                }
            ],
            "abstract": "Multi-Document Scientific Summarization (MDSS) aims to produce coherent and concise summaries for clusters of topic-relevant scientific papers. This task requires precise understanding of paper content and accurate modeling of cross-paper relationships. Knowledge graphs convey compact and interpretable structured information for documents, which makes them ideal for content modeling and relationship modeling. In this paper, we present KGSum, an MDSS model centred on knowledge graphs during both the encoding and decoding process. Specifically, in the encoding process, two graph-based modules are proposed to incorporate knowledge graph information into paper encoding, while in the decoding process, we propose a two-stage decoder by first generating knowledge graph information of summary in the form of descriptive sentences, followed by generating the final summary. Empirical results show that the proposed architecture brings substantial improvements over baselines on the Multi-Xscience dataset.",
            "corpus_id": 252185277,
            "sentences": [
                {
                    "corpus_id": "252185277",
                    "title": "Multi-Document Scientific Summarization from a Knowledge Graph-Centric View",
                    "text": "In this work, we propose a knowledge graph-centric Transformer-based model for MDSS. Our model is able to incorporate knowledge graph information into the paper encoding process with a graph updater and an entity-sentence updater, and introduce a two-stage decoder including a KGtext generator and a summary generator to guide the summary decoding with knowledge graph information. Experiments show that the proposed model significantly outperforms all strong baselines and achieves the best result on the Multi-Xscience dataset. \n\nIn the future, we will explore other more intuitive and effective methods to incorporate graph information in both the encoding and decoding phase of summary generation. 2020) is a heterogeneous graph-based extractive model with semantic nodes of different granularity. HiMAP (Fabbri et al., 2019) expands the pointergenerator network (See et al., 2017) into a hierarchical network and integrates an MMR module. HierSumm (Liu and Lapata, 2019a) is a Transformer based model with an attention mechanism to share information cross-document for abstractive multi-document summarization. MGSum (Jin et al., 2020) is a multi-granularity interaction network for abstractive multi-document summarization. We also consider evaluating on single document summarization models by concatenating multiple papers into a long sequence. GraphSum (Li et al., 2020) is a neural multi-document summarization model that leverages well-known graphs to produce abstractive summaries. We use TF-IDF graph as the input graph. PEGASUS (Zhang et al., 2020) is a sequence-to-sequence model with gapsentences generation as a pre-training objective tailored for abstractive summarization. Pointer-Generator (See et al., 2017) is an RNN based model with an attention mechanism and allows the system to copy words from the source via pointing for abstractive summarization. BertABS (Liu and Lapata, 2019b) uses a pretrained BERT (Devlin et al., 2019) as the encoder for abstractive summarization. We also report the performance of BertABS with an encoder (SciBertABS) pretrained on scientific articles.",
                    "score": 0.66835169672834,
                    "section_title": "Conclusion",
                    "char_start_offset": 25793,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 84
                        },
                        {
                            "start": 85,
                            "end": 381
                        },
                        {
                            "start": 382,
                            "end": 529
                        },
                        {
                            "start": 532,
                            "end": 701
                        },
                        {
                            "start": 702,
                            "end": 801
                        },
                        {
                            "start": 802,
                            "end": 943
                        },
                        {
                            "start": 944,
                            "end": 1115
                        },
                        {
                            "start": 1116,
                            "end": 1229
                        },
                        {
                            "start": 1230,
                            "end": 1352
                        },
                        {
                            "start": 1353,
                            "end": 1493
                        },
                        {
                            "start": 1494,
                            "end": 1533
                        },
                        {
                            "start": 1534,
                            "end": 1691
                        },
                        {
                            "start": 1692,
                            "end": 1874
                        },
                        {
                            "start": 1875,
                            "end": 1997
                        },
                        {
                            "start": 1998,
                            "end": 2103
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 867,
                            "end": 885,
                            "matchedPaperCorpusId": "8314118"
                        },
                        {
                            "start": 1122,
                            "end": 1140,
                            "matchedPaperCorpusId": "220045815"
                        },
                        {
                            "start": 1362,
                            "end": 1378,
                            "matchedPaperCorpusId": "218718706"
                        },
                        {
                            "start": 1542,
                            "end": 1562,
                            "matchedPaperCorpusId": "209405420"
                        },
                        {
                            "start": 1710,
                            "end": 1728,
                            "matchedPaperCorpusId": "8314118"
                        },
                        {
                            "start": 1930,
                            "end": 1951,
                            "matchedPaperCorpusId": "52967399"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98095703125
                }
            ],
            "relevance_judgement": 0.98095703125,
            "relevance_judgment_input_expanded": "# Title: Multi-Document Scientific Summarization from a Knowledge Graph-Centric View\n# Venue: International Conference on Computational Linguistics\n# Authors: Pancheng Wang, Shasha Li, Kunyuan Pang, Liangliang He, Dong Li, Jintao Tang, Ting Wang\n## Abstract\nMulti-Document Scientific Summarization (MDSS) aims to produce coherent and concise summaries for clusters of topic-relevant scientific papers. This task requires precise understanding of paper content and accurate modeling of cross-paper relationships. Knowledge graphs convey compact and interpretable structured information for documents, which makes them ideal for content modeling and relationship modeling. In this paper, we present KGSum, an MDSS model centred on knowledge graphs during both the encoding and decoding process. Specifically, in the encoding process, two graph-based modules are proposed to incorporate knowledge graph information into paper encoding, while in the decoding process, we propose a two-stage decoder by first generating knowledge graph information of summary in the form of descriptive sentences, followed by generating the final summary. Empirical results show that the proposed architecture brings substantial improvements over baselines on the Multi-Xscience dataset.\n## Conclusion\nIn this work, we propose a knowledge graph-centric Transformer-based model for MDSS. Our model is able to incorporate knowledge graph information into the paper encoding process with a graph updater and an entity-sentence updater, and introduce a two-stage decoder including a KGtext generator and a summary generator to guide the summary decoding with knowledge graph information. Experiments show that the proposed model significantly outperforms all strong baselines and achieves the best result on the Multi-Xscience dataset. \n\nIn the future, we will explore other more intuitive and effective methods to incorporate graph information in both the encoding and decoding phase of summary generation. 2020) is a heterogeneous graph-based extractive model with semantic nodes of different granularity. HiMAP (Fabbri et al., 2019) expands the pointergenerator network (See et al., 2017) into a hierarchical network and integrates an MMR module. HierSumm (Liu and Lapata, 2019a) is a Transformer based model with an attention mechanism to share information cross-document for abstractive multi-document summarization. MGSum (Jin et al., 2020) is a multi-granularity interaction network for abstractive multi-document summarization. We also consider evaluating on single document summarization models by concatenating multiple papers into a long sequence. GraphSum (Li et al., 2020) is a neural multi-document summarization model that leverages well-known graphs to produce abstractive summaries. We use TF-IDF graph as the input graph. PEGASUS (Zhang et al., 2020) is a sequence-to-sequence model with gapsentences generation as a pre-training objective tailored for abstractive summarization. Pointer-Generator (See et al., 2017) is an RNN based model with an attention mechanism and allows the system to copy words from the source via pointing for abstractive summarization. BertABS (Liu and Lapata, 2019b) uses a pretrained BERT (Devlin et al., 2019) as the encoder for abstractive summarization. We also report the performance of BertABS with an encoder (SciBertABS) pretrained on scientific articles.",
            "reference_string": "[252185277 | Wang et al. | 2022 | Citations: 15]"
        },
        {
            "title": "Enhanced Topic-Aware Summarization Using Statistical Graph Neural Networks",
            "venue": "Computers, Materials &amp; Continua",
            "year": 2024,
            "reference_count": 57,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://file.techscience.com/files/cmc/2024/TSP_CMC-80-2/TSP_CMC_53488/TSP_CMC_53488.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.32604/cmc.2024.053488?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.32604/cmc.2024.053488, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2058964352",
                    "name": "A. Khaliq"
                },
                {
                    "authorId": "2269634624",
                    "name": "Salman Afsar Awan"
                },
                {
                    "authorId": "2315394721",
                    "name": "Fahad Ahmad"
                },
                {
                    "authorId": "2315423888",
                    "name": "Muhammad Azam Zia"
                },
                {
                    "authorId": "2255632961",
                    "name": "Muhammad Zafar Iqbal"
                }
            ],
            "abstract": "The rapid expansion of online content and big data has precipitated an urgent need for efficient summarization techniques to swiftly comprehend vast textual documents without compromising their original integrity. Current approaches in Extractive Text Summarization (ETS) leverage the modeling of inter-sentence relationships, a task of paramount importance in producing coherent summaries. This study introduces an innovative model that integrates Graph Attention Networks (GATs) with Transformer-based Bidirectional Encoder Representations from Transformers (BERT) and Latent Dirichlet Allocation (LDA), further enhanced by Term Frequency-Inverse Document Frequency (TF-IDF) values, to improve sentence selection by capturing comprehensive topical information. Our approach constructs a graph with nodes representing sentences, words, and topics, thereby elevating the interconnectivity and enabling a more refined understanding of text structures. This model is stretched to Multi-Document Summarization (MDS) from Single-Document Summarization, offering significant improvements over existing models such as THGS-GMM and Topic-GraphSum, as demonstrated by empirical evaluations on benchmark news datasets like Cable News Network (CNN)/Daily Mail (DM) and Multi-News. The results consistently demonstrate superior performance, showcasing the model\u2019s robustness in handling complex summarization tasks across single and multi-document contexts. This research not only advances the integration of BERT and LDA within a GATs but also emphasizes our model\u2019s capacity to effectively manage global information and adapt to diverse summarization challenges.",
            "corpus_id": 271791043,
            "sentences": [
                {
                    "corpus_id": "271791043",
                    "title": "Enhanced Topic-Aware Summarization Using Statistical Graph Neural Networks",
                    "text": "The rapid expansion of online content and big data has precipitated an urgent need for efficient summarization techniques to swiftly comprehend vast textual documents without compromising their original integrity. Current approaches in Extractive Text Summarization (ETS) leverage the modeling of inter-sentence relationships, a task of paramount importance in producing coherent summaries. This study introduces an innovative model that integrates Graph Attention Networks (GATs) with Transformer-based Bidirectional Encoder Representations from Transformers (BERT) and Latent Dirichlet Allocation (LDA), further enhanced by Term Frequency-Inverse Document Frequency (TF-IDF) values, to improve sentence selection by capturing comprehensive topical information. Our approach constructs a graph with nodes representing sentences, words, and topics, thereby elevating the interconnectivity and enabling a more refined understanding of text structures. This model is stretched to Multi-Document Summarization (MDS) from Single-Document Summarization, offering significant improvements over existing models such as THGS-GMM and Topic-GraphSum, as demonstrated by empirical evaluations on benchmark news datasets like Cable News Network (CNN)/Daily Mail (DM) and Multi-News. The results consistently demonstrate superior performance, showcasing the model\u2019s robustness in handling complex summarization tasks across single and multi-document contexts. This research not only advances the integration of BERT and LDA within a GATs but also emphasizes our model\u2019s capacity to effectively manage global information and adapt to diverse summarization challenges.",
                    "score": 0.5926377154680638,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97802734375
                },
                {
                    "corpus_id": "271791043",
                    "title": "Enhanced Topic-Aware Summarization Using Statistical Graph Neural Networks",
                    "text": "This research evaluation strategy is focused on rigorously testing the proposed model's effectiveness in extractive summarization against contemporary state-of-the-art models. The performance assessment utilizes the CNN/DM and Multi-News datasets, which are standards in the field for SDS and MDS tasks, respectively. This deliberate choice confirms a broad and challenging test environment, reflecting real-world applications and the complexity of integrating multiple document contexts. \n\nThrough systematic comparisons, proposed model is tested against well-known benchmarks and recent advances in neural graph-based models. These assessments are not just limited to performance metrics like ROUGE scores but also extend to an examination of how well each model assimilates and processes linguistic and thematic information across different document types. This research examines the performance in relation to various node configurations and graph structures used in competing models, providing a comprehensive view of the proposed model's abilities. Furthermore, the assessment comprises a detailed review of the implementation settings, such as vocabulary size, tokenization standards, and hyperparameters like learning rates and dropout settings. These particulars underscore the tailored approach of proposed model in handling the refinements of extractive summarization, highlighting its innovative integration of advanced neural network techniques and its adaptability to both SDS and MDS summarization challenges. This laborious evaluation framework ensures conclusions are well-founded and validate clear advancements over existing methodologies.",
                    "score": 0.5682286825042536,
                    "section_title": "Performance Evaluation",
                    "char_start_offset": 29267,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 317
                        },
                        {
                            "start": 318,
                            "end": 488
                        },
                        {
                            "start": 491,
                            "end": 627
                        },
                        {
                            "start": 628,
                            "end": 859
                        },
                        {
                            "start": 860,
                            "end": 1054
                        },
                        {
                            "start": 1055,
                            "end": 1253
                        },
                        {
                            "start": 1254,
                            "end": 1524
                        },
                        {
                            "start": 1525,
                            "end": 1658
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9765625
                }
            ],
            "relevance_judgement": 0.97802734375,
            "relevance_judgment_input_expanded": "# Title: Enhanced Topic-Aware Summarization Using Statistical Graph Neural Networks\n# Venue: Computers, Materials &amp; Continua\n# Authors: A. Khaliq, Salman Afsar Awan, Fahad Ahmad, Muhammad Azam Zia, Muhammad Zafar Iqbal\n## Abstract\nThe rapid expansion of online content and big data has precipitated an urgent need for efficient summarization techniques to swiftly comprehend vast textual documents without compromising their original integrity. Current approaches in Extractive Text Summarization (ETS) leverage the modeling of inter-sentence relationships, a task of paramount importance in producing coherent summaries. This study introduces an innovative model that integrates Graph Attention Networks (GATs) with Transformer-based Bidirectional Encoder Representations from Transformers (BERT) and Latent Dirichlet Allocation (LDA), further enhanced by Term Frequency-Inverse Document Frequency (TF-IDF) values, to improve sentence selection by capturing comprehensive topical information. Our approach constructs a graph with nodes representing sentences, words, and topics, thereby elevating the interconnectivity and enabling a more refined understanding of text structures. This model is stretched to Multi-Document Summarization (MDS) from Single-Document Summarization, offering significant improvements over existing models such as THGS-GMM and Topic-GraphSum, as demonstrated by empirical evaluations on benchmark news datasets like Cable News Network (CNN)/Daily Mail (DM) and Multi-News. The results consistently demonstrate superior performance, showcasing the model\u2019s robustness in handling complex summarization tasks across single and multi-document contexts. This research not only advances the integration of BERT and LDA within a GATs but also emphasizes our model\u2019s capacity to effectively manage global information and adapt to diverse summarization challenges.\n## Performance Evaluation\nThis research evaluation strategy is focused on rigorously testing the proposed model's effectiveness in extractive summarization against contemporary state-of-the-art models. The performance assessment utilizes the CNN/DM and Multi-News datasets, which are standards in the field for SDS and MDS tasks, respectively. This deliberate choice confirms a broad and challenging test environment, reflecting real-world applications and the complexity of integrating multiple document contexts. \n\nThrough systematic comparisons, proposed model is tested against well-known benchmarks and recent advances in neural graph-based models. These assessments are not just limited to performance metrics like ROUGE scores but also extend to an examination of how well each model assimilates and processes linguistic and thematic information across different document types. This research examines the performance in relation to various node configurations and graph structures used in competing models, providing a comprehensive view of the proposed model's abilities. Furthermore, the assessment comprises a detailed review of the implementation settings, such as vocabulary size, tokenization standards, and hyperparameters like learning rates and dropout settings. These particulars underscore the tailored approach of proposed model in handling the refinements of extractive summarization, highlighting its innovative integration of advanced neural network techniques and its adaptability to both SDS and MDS summarization challenges. This laborious evaluation framework ensures conclusions are well-founded and validate clear advancements over existing methodologies.",
            "reference_string": "[271791043 | Khaliq et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Hierarchical Attention Graph for Scientific Document Summarization in Global and Local Level",
            "venue": "NAACL-HLT",
            "year": 2024,
            "reference_count": 39,
            "citation_count": 5,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.10202, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294460391",
                    "name": "Chenlong Zhao"
                },
                {
                    "authorId": "2294514721",
                    "name": "Xiwen Zhou"
                },
                {
                    "authorId": "2295151257",
                    "name": "Xiaopeng Xie"
                },
                {
                    "authorId": "2294558459",
                    "name": "Yong Zhang"
                }
            ],
            "abstract": "Scientific document summarization has been a challenging task due to the long structure of the input text. The long input hinders the simultaneous effective modeling of both global high-order relations between sentences and local intra-sentence relations which is the most critical step in extractive summarization. However, existing methods mostly focus on one type of relation, neglecting the simultaneous effective modeling of both relations, which can lead to insufficient learning of semantic representations. In this paper, we propose HAESum, a novel approach utilizing graph neural networks to locally and globally model documents based on their hierarchical discourse structure. First, intra-sentence relations are learned using a local heterogeneous graph. Subsequently, a novel hypergraph self-attention layer is introduced to further enhance the characterization of high-order inter-sentence relations. We validate our approach on two benchmark datasets, and the experimental results demonstrate the effectiveness of HAESum and the importance of considering hierarchical structures in modeling long scientific documents. Our code will be available at \\url{https://github.com/MoLICHENXI/HAESum}",
            "corpus_id": 269791107,
            "sentences": [
                {
                    "corpus_id": "269791107",
                    "title": "Hierarchical Attention Graph for Scientific Document Summarization in Global and Local Level",
                    "text": "Graph neural networks have been widely used for extractive summarization due to their flexibility and scalability.Dong et al. (2020) proposed an unsupervised graph-based model that combines both sentence similarity and hierarchical discourse structure to rank sentences.Cui et al. (2020) injected latent topic information into graph neural networks to further improve performance.Wang et al. (2020) constructed a word-document heterogeneous graph using word nodes as intermediate to connect sentences.Zhang et al. (2022) proposed a hypergraph transformer to model long-distance dependency while emphasizing the importance of high-order inter-sentence relations in extraction summarization.Our paper follows this line of work, but the main difference is that our approach combines both intra-sentence relations and high-order crosssentence relations and efficiently leverages the hierarchical discourse structure of scientific documents to learn sentence representations that incorporate both local and global information.",
                    "score": 0.5473260482125347,
                    "section_title": "Graph based Summarization",
                    "char_start_offset": 7172,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 114
                        },
                        {
                            "start": 114,
                            "end": 270
                        },
                        {
                            "start": 270,
                            "end": 380
                        },
                        {
                            "start": 380,
                            "end": 501
                        },
                        {
                            "start": 501,
                            "end": 689
                        },
                        {
                            "start": 689,
                            "end": 1021
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 380,
                            "end": 398,
                            "matchedPaperCorpusId": "216552978"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97802734375
                }
            ],
            "relevance_judgement": 0.97802734375,
            "relevance_judgment_input_expanded": "# Title: Hierarchical Attention Graph for Scientific Document Summarization in Global and Local Level\n# Venue: NAACL-HLT\n# Authors: Chenlong Zhao, Xiwen Zhou, Xiaopeng Xie, Yong Zhang\n## Abstract\nScientific document summarization has been a challenging task due to the long structure of the input text. The long input hinders the simultaneous effective modeling of both global high-order relations between sentences and local intra-sentence relations which is the most critical step in extractive summarization. However, existing methods mostly focus on one type of relation, neglecting the simultaneous effective modeling of both relations, which can lead to insufficient learning of semantic representations. In this paper, we propose HAESum, a novel approach utilizing graph neural networks to locally and globally model documents based on their hierarchical discourse structure. First, intra-sentence relations are learned using a local heterogeneous graph. Subsequently, a novel hypergraph self-attention layer is introduced to further enhance the characterization of high-order inter-sentence relations. We validate our approach on two benchmark datasets, and the experimental results demonstrate the effectiveness of HAESum and the importance of considering hierarchical structures in modeling long scientific documents. Our code will be available at \\url{https://github.com/MoLICHENXI/HAESum}\n## Graph based Summarization\nGraph neural networks have been widely used for extractive summarization due to their flexibility and scalability.Dong et al. (2020) proposed an unsupervised graph-based model that combines both sentence similarity and hierarchical discourse structure to rank sentences.Cui et al. (2020) injected latent topic information into graph neural networks to further improve performance.Wang et al. (2020) constructed a word-document heterogeneous graph using word nodes as intermediate to connect sentences.Zhang et al. (2022) proposed a hypergraph transformer to model long-distance dependency while emphasizing the importance of high-order inter-sentence relations in extraction summarization.Our paper follows this line of work, but the main difference is that our approach combines both intra-sentence relations and high-order crosssentence relations and efficiently leverages the hierarchical discourse structure of scientific documents to learn sentence representations that incorporate both local and global information.",
            "reference_string": "[269791107 | Zhao et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Two-phase Multi-document Event Summarization on Core Event Graphs",
            "venue": "Journal of Artificial Intelligence Research",
            "year": 2022,
            "reference_count": 54,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://jair.org/index.php/jair/article/download/13267/26820",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1613/jair.1.13267?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1613/jair.1.13267, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48354529",
                    "name": "Zengjian Chen"
                },
                {
                    "authorId": "2116315442",
                    "name": "Jin Xu"
                },
                {
                    "authorId": "145865588",
                    "name": "M. Liao"
                },
                {
                    "authorId": "2138967543",
                    "name": "Tong Xue"
                },
                {
                    "authorId": "2190820519",
                    "name": "Kun He"
                }
            ],
            "abstract": "Succinct event description based on multiple documents is critical to news systems as well as search engines. Different from existing summarization or event tasks, Multi-document Event Summarization (MES) aims at the query-level event sequence generation, which has extra constraints on event expression and conciseness. Identifying and summarizing the key event from a set of related articles is a challenging task that has not been sufficiently studied, mainly because online articles exhibit characteristics of redundancy and sparsity, and a perfect event summarization needs high level information fusion among diverse sentences and articles. To address these challenges, we propose a two-phase framework for the MES task, that first performs event semantic graph construction and dominant event detection via graph-sequence matching, then summarizes the extracted key event by an event-aware pointer generator. For experiments in the new task, we construct two large-scale real-world datasets for training and assessment. Extensive evaluations show that the proposed framework significantly outperforms the related baseline methods, with the most dominant event of the articles effectively identified and correctly summarized.",
            "corpus_id": 250374734,
            "sentences": [
                {
                    "corpus_id": "250374734",
                    "title": "Two-phase Multi-document Event Summarization on Core Event Graphs",
                    "text": "Existing multi-document summarization methods mainly focus on sentence-level summarization and can be categorized into extractive and abstractive methods. Most extractive methods are operated over graph-based representations of sentences or passages with edge weights computed by tf-idf (Erkan et al., 2004), discourse relations (Christensen et al., 2013) or sentence embeddings (Yasunaga et al., 2017b) and then a specific algorithm is further adopted for ranking text units for inclusion in the final summary. More recently, some extractive summarization works also utilize graph convolutional networks for salient sentences estimation (Kipf et al., 2016) and sentence ordering (Yin et al., 2019). Abstractive models, especially neural abstractive ones, have achieved promising results on single-document summarization (See et al., 2017;Paulus et al., 2018;Lewis et al., 2019). However, the extension of sequence-to-sequence architectures to multi-document summarization is less straightforward due to the lack of sufficient training data and the computational challenge of processing multiple documents. Intuitively, graph-based extractive methods are suitable to identify relationship of different sentences or documents and extract salient information, while neural sequence-to-sequence architectures are effective in abstraction and content rewriting. Hence, abstractive models based on graphs gain much attention (Yasunaga et al., 2017b;Li et al., 2020). Our model, which also combines a graph-based extractive module and neural abstractive sequence-to-sequence architecture, is a higher level summarization and focuses on the core event summarization from multiple documents, which is more challenging.",
                    "score": 0.7984654416181444,
                    "section_title": "Multi-document Summarization",
                    "char_start_offset": 5936,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 154
                        },
                        {
                            "start": 155,
                            "end": 511
                        },
                        {
                            "start": 512,
                            "end": 699
                        },
                        {
                            "start": 700,
                            "end": 879
                        },
                        {
                            "start": 880,
                            "end": 1106
                        },
                        {
                            "start": 1107,
                            "end": 1357
                        },
                        {
                            "start": 1358,
                            "end": 1461
                        },
                        {
                            "start": 1462,
                            "end": 1710
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 287,
                            "end": 307,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 821,
                            "end": 839,
                            "matchedPaperCorpusId": "8314118"
                        },
                        {
                            "start": 839,
                            "end": 859,
                            "matchedPaperCorpusId": "21850704"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9775390625
                }
            ],
            "relevance_judgement": 0.9775390625,
            "relevance_judgment_input_expanded": "# Title: Two-phase Multi-document Event Summarization on Core Event Graphs\n# Venue: Journal of Artificial Intelligence Research\n# Authors: Zengjian Chen, Jin Xu, M. Liao, Tong Xue, Kun He\n## Abstract\nSuccinct event description based on multiple documents is critical to news systems as well as search engines. Different from existing summarization or event tasks, Multi-document Event Summarization (MES) aims at the query-level event sequence generation, which has extra constraints on event expression and conciseness. Identifying and summarizing the key event from a set of related articles is a challenging task that has not been sufficiently studied, mainly because online articles exhibit characteristics of redundancy and sparsity, and a perfect event summarization needs high level information fusion among diverse sentences and articles. To address these challenges, we propose a two-phase framework for the MES task, that first performs event semantic graph construction and dominant event detection via graph-sequence matching, then summarizes the extracted key event by an event-aware pointer generator. For experiments in the new task, we construct two large-scale real-world datasets for training and assessment. Extensive evaluations show that the proposed framework significantly outperforms the related baseline methods, with the most dominant event of the articles effectively identified and correctly summarized.\n## Multi-document Summarization\nExisting multi-document summarization methods mainly focus on sentence-level summarization and can be categorized into extractive and abstractive methods. Most extractive methods are operated over graph-based representations of sentences or passages with edge weights computed by tf-idf (Erkan et al., 2004), discourse relations (Christensen et al., 2013) or sentence embeddings (Yasunaga et al., 2017b) and then a specific algorithm is further adopted for ranking text units for inclusion in the final summary. More recently, some extractive summarization works also utilize graph convolutional networks for salient sentences estimation (Kipf et al., 2016) and sentence ordering (Yin et al., 2019). Abstractive models, especially neural abstractive ones, have achieved promising results on single-document summarization (See et al., 2017;Paulus et al., 2018;Lewis et al., 2019). However, the extension of sequence-to-sequence architectures to multi-document summarization is less straightforward due to the lack of sufficient training data and the computational challenge of processing multiple documents. Intuitively, graph-based extractive methods are suitable to identify relationship of different sentences or documents and extract salient information, while neural sequence-to-sequence architectures are effective in abstraction and content rewriting. Hence, abstractive models based on graphs gain much attention (Yasunaga et al., 2017b;Li et al., 2020). Our model, which also combines a graph-based extractive module and neural abstractive sequence-to-sequence architecture, is a higher level summarization and focuses on the core event summarization from multiple documents, which is more challenging.",
            "reference_string": "[250374734 | Chen et al. | 2022 | Citations: 2]"
        },
        {
            "title": "GATSum: Graph-Based Topic-Aware Abstract Text Summarization",
            "venue": "Information Technology and Control",
            "year": 2022,
            "reference_count": 44,
            "citation_count": 11,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://itc.ktu.lt/index.php/ITC/article/download/30796/15387",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.5755/j01.itc.51.2.30796?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5755/j01.itc.51.2.30796, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2040863712",
                    "name": "Ming Jiang"
                },
                {
                    "authorId": "2034370007",
                    "name": "Yifan Zou"
                },
                {
                    "authorId": "2110980173",
                    "name": "Jian Xu"
                },
                {
                    "authorId": "2156053270",
                    "name": "Min Zhang"
                }
            ],
            "abstract": "The purpose of text summarization is to compress a text document into a summary containing key information. abstract approaches are challenging tasks, it is necessary to design a mechanism to effectively extract salient information from the source text, and then generate a summary. However, most of the existing abstract approaches are difficult to capture global semantics, ignoring the impact of global information on obtaining important content. To solve this problem, this paper proposes a Graph-Based Topic Aware abstract Text Summarization (GTASum) framework. Specifically, GTASum seamlessly incorporates a neural topic model to discover potential topic information, which can provide document-level features for generating summaries. In addition, the model integrates the graph neural network which can effectively capture the relationship between sentences through the document representation of graph structure, and simultaneously update the local and global information. The further discussion showed that latent topics can help the model capture salient content. We conducted experiments on two datasets, and the result shows that GTASum is superior to many extractive and abstract approaches in terms of ROUGE measurement. The result of the ablation study proves that the model has the ability to capture the original subject and the correct information and improve the factual accuracy of the summarization.",
            "corpus_id": 250012667,
            "sentences": [
                {
                    "corpus_id": "250012667",
                    "title": "GATSum: Graph-Based Topic-Aware Abstract Text Summarization",
                    "text": "In this study, we integrate the global semantic structure of the text by proposing a novel graph-based topic-aware abstract text summarization model GTA-Sum and investigate the abstract text summarization problem. In particular, neural topic models, BERT, and graph neural networks are combined for summarization. Extensive experiments are conducted on two real-world datasets to compare GTASum with several methods. It turns out that GTASum outperforms the vast majority of classical models, with performance approaching industry-leading methods. In future work, we will continue to explore incorporating more types of high-level semantic units into the model to improve the performance and robustness of the model.",
                    "score": 0.6413846483905024,
                    "section_title": "Conclusion and Future Work",
                    "char_start_offset": 47429,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 213
                        },
                        {
                            "start": 214,
                            "end": 313
                        },
                        {
                            "start": 314,
                            "end": 416
                        },
                        {
                            "start": 417,
                            "end": 547
                        },
                        {
                            "start": 548,
                            "end": 716
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9765625
                },
                {
                    "corpus_id": "250012667",
                    "title": "GATSum: Graph-Based Topic-Aware Abstract Text Summarization",
                    "text": "The purpose of text summarization is to compress a text document into a summary containing key information. abstract approaches are challenging tasks, it is necessary to design a mechanism to effectively extract salient information from the source text, and then generate a summary. However, most of the existing abstract approaches are difficult to capture global semantics, ignoring the impact of global information on obtaining important content. To solve this problem, this paper proposes a Graph-Based Topic Aware abstract Text Summarization (GTASum) framework. Specifically, GTASum seamlessly incorporates a neural topic model to discover potential topic information, which can provide document-level features for generating summaries. In addition, the model integrates the graph neural network which can effectively capture the relationship between sentences through the document representation of graph structure, and simultaneously update the local and global information. The further discussion showed that latent topics can help the model capture salient content. We conducted experiments on two datasets, and the result shows that GTASum is superior to many extractive and abstract approaches in terms of ROUGE measurement. The result of the ablation study proves that the model has the ability to capture the original subject and the correct information and improve the factual accuracy of the summarization.",
                    "score": 0.5860706532355354,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9697265625
                },
                {
                    "corpus_id": "250012667",
                    "title": "GATSum: Graph-Based Topic-Aware Abstract Text Summarization",
                    "text": "This attracted our attention, and GNN (Graph Neural Networks) was considered to solve this problem. Lately, GNN has been universally used for cross-sentence relation modeling for summarization tasks [15]. Several studies [35,37] set up document graphs according to discourse analysis. But, this method relies on external tools, which possibly bring out semantically fragmented outputs [20]. To sum up, GAT (Graph Attention Network) is constructed using sentence context representation and topic information, and the docu-ment context vector and topic information are updated simultaneously. This can not only jointly update local semantics and global semantics, but also reduce the problem of semantic fragmentation. \n\nIn this work, we suggest a novel GTASum model (Graph-Based Topic-Aware abstract text summarization). First, the document is encoded with BERT [6] (Bidirectional Encoder Representation from Transformers) to obtain contextual sentence representations; meanwhile, NTM [7] is used to learn the potential topic of the document. Second, construct a heterogeneous document graph that contains sentence representation nodes and potential topic nodes, then revise them using a adapted GAT [30]. Finally, the sentence representations containing topic information are fed into a Transformer-based decoder to generate summaries. Overall, the primary contributions of us are as follows: \n\n1 A novel Graph-Based Topic-Aware abstract text summarization model is proposed, which helps to capture global semantic information and provides guidance in the procedure of making a summary. This solves the problem of text summarization lacking global semantic information. \n\n2 A heterogeneous document graph is designed to jointly update local semantics and global semantics and reduce the problem of semantic fragmentation. \n\n3 GTASum is evaluated on two standard datasets (CNN/DailyMail and XSum) and outperforms many existing extractive and abstract models in terms of ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measure.",
                    "score": 0.5642473185802919,
                    "section_title": "Introduction",
                    "char_start_offset": 2280,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 99
                        },
                        {
                            "start": 100,
                            "end": 204
                        },
                        {
                            "start": 205,
                            "end": 284
                        },
                        {
                            "start": 285,
                            "end": 390
                        },
                        {
                            "start": 391,
                            "end": 590
                        },
                        {
                            "start": 591,
                            "end": 716
                        },
                        {
                            "start": 719,
                            "end": 819
                        },
                        {
                            "start": 820,
                            "end": 1041
                        },
                        {
                            "start": 1042,
                            "end": 1204
                        },
                        {
                            "start": 1205,
                            "end": 1335
                        },
                        {
                            "start": 1336,
                            "end": 1392
                        },
                        {
                            "start": 1395,
                            "end": 1586
                        },
                        {
                            "start": 1587,
                            "end": 1669
                        },
                        {
                            "start": 1672,
                            "end": 1821
                        },
                        {
                            "start": 1824,
                            "end": 2035
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 199,
                            "end": 203,
                            "matchedPaperCorpusId": "102354588"
                        },
                        {
                            "start": 221,
                            "end": 225,
                            "matchedPaperCorpusId": "59599804"
                        },
                        {
                            "start": 385,
                            "end": 389,
                            "matchedPaperCorpusId": "208331728"
                        },
                        {
                            "start": 861,
                            "end": 864,
                            "matchedPaperCorpusId": "226096901"
                        },
                        {
                            "start": 984,
                            "end": 987,
                            "matchedPaperCorpusId": "52182945"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95703125
                }
            ],
            "relevance_judgement": 0.9765625,
            "relevance_judgment_input_expanded": "# Title: GATSum: Graph-Based Topic-Aware Abstract Text Summarization\n# Venue: Information Technology and Control\n# Authors: Ming Jiang, Yifan Zou, Jian Xu, Min Zhang\n## Abstract\nThe purpose of text summarization is to compress a text document into a summary containing key information. abstract approaches are challenging tasks, it is necessary to design a mechanism to effectively extract salient information from the source text, and then generate a summary. However, most of the existing abstract approaches are difficult to capture global semantics, ignoring the impact of global information on obtaining important content. To solve this problem, this paper proposes a Graph-Based Topic Aware abstract Text Summarization (GTASum) framework. Specifically, GTASum seamlessly incorporates a neural topic model to discover potential topic information, which can provide document-level features for generating summaries. In addition, the model integrates the graph neural network which can effectively capture the relationship between sentences through the document representation of graph structure, and simultaneously update the local and global information. The further discussion showed that latent topics can help the model capture salient content. We conducted experiments on two datasets, and the result shows that GTASum is superior to many extractive and abstract approaches in terms of ROUGE measurement. The result of the ablation study proves that the model has the ability to capture the original subject and the correct information and improve the factual accuracy of the summarization.\n## Introduction\nThis attracted our attention, and GNN (Graph Neural Networks) was considered to solve this problem. Lately, GNN has been universally used for cross-sentence relation modeling for summarization tasks [15]. Several studies [35,37] set up document graphs according to discourse analysis. But, this method relies on external tools, which possibly bring out semantically fragmented outputs [20]. To sum up, GAT (Graph Attention Network) is constructed using sentence context representation and topic information, and the docu-ment context vector and topic information are updated simultaneously. This can not only jointly update local semantics and global semantics, but also reduce the problem of semantic fragmentation. \n\nIn this work, we suggest a novel GTASum model (Graph-Based Topic-Aware abstract text summarization). First, the document is encoded with BERT [6] (Bidirectional Encoder Representation from Transformers) to obtain contextual sentence representations; meanwhile, NTM [7] is used to learn the potential topic of the document. Second, construct a heterogeneous document graph that contains sentence representation nodes and potential topic nodes, then revise them using a adapted GAT [30]. Finally, the sentence representations containing topic information are fed into a Transformer-based decoder to generate summaries. Overall, the primary contributions of us are as follows: \n\n1 A novel Graph-Based Topic-Aware abstract text summarization model is proposed, which helps to capture global semantic information and provides guidance in the procedure of making a summary. This solves the problem of text summarization lacking global semantic information. \n\n2 A heterogeneous document graph is designed to jointly update local semantics and global semantics and reduce the problem of semantic fragmentation. \n\n3 GTASum is evaluated on two standard datasets (CNN/DailyMail and XSum) and outperforms many existing extractive and abstract models in terms of ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measure.\n\n## Conclusion and Future Work\nIn this study, we integrate the global semantic structure of the text by proposing a novel graph-based topic-aware abstract text summarization model GTA-Sum and investigate the abstract text summarization problem. In particular, neural topic models, BERT, and graph neural networks are combined for summarization. Extensive experiments are conducted on two real-world datasets to compare GTASum with several methods. It turns out that GTASum outperforms the vast majority of classical models, with performance approaching industry-leading methods. In future work, we will continue to explore incorporating more types of high-level semantic units into the model to improve the performance and robustness of the model.",
            "reference_string": "[250012667 | Jiang et al. | 2022 | Citations: 11]"
        },
        {
            "title": "Global and Local Context Fusion in Heterogeneous Graph Neural Network for Summarizing Lengthy Scientific Documents",
            "venue": "IEEE Access",
            "year": 2025,
            "reference_count": 49,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2025.3553755",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2025.3553755?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2025.3553755, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257111507",
                    "name": "Muhammad Umair"
                },
                {
                    "authorId": "2149073250",
                    "name": "Atif Khan"
                },
                {
                    "authorId": "47122876",
                    "name": "Fasee Ullah"
                },
                {
                    "authorId": "2282843278",
                    "name": "Atef Masmoudi"
                },
                {
                    "authorId": "2299236773",
                    "name": "Muhammed Faheem"
                }
            ],
            "abstract": "The primary objective of text summarization is to condense a document\u2019s length while preserving its essential content. Extractive summarization methods are commonly used due to their effectiveness and straightforward presentation. However, a significant challenge lies in segmenting documents into distinct concepts and understanding how sentences interact, especially in complex materials such as scientific articles. This process entails identifying relationships between sentences and determining the most significant and informative content within extensive text collections. Traditional techniques often utilize pre-trained models like BERT, known for their ability to capture word context. Nonetheless, these models have limitations, including constrained input lengths and the computational intensity of self-attention mechanisms, which hinder their effectiveness in processing large-scale scientific texts. To address these challenges, we propose a computationally efficient Heterogeneous Graph Neural Network (HGNN) for the extractive summarization of lengthy scientific texts. This framework combines GloVe embeddings with Convolutional Neural Networks (CNNs) and Bidirectional Long Short-Term Memory (BiLSTM) encoders. GloVe offers simple yet effective word embeddings, CNNs focus on capturing local word structures, and BiLSTMs identify long-range dependencies, allowing for flexible encoding of extensive texts. For global context and topic modeling, we utilize an enhanced version of Latent Dirichlet Allocation (LDA) to retain essential document attributes. In this model, words, sentences, and topics are represented as nodes in a heterogeneous graph, with TF-IDF values illustrating the relationships between edges. The graph is processed using a Graph Attention Network (GAT), which refines node representations by integrating both local and global information. This study represents the first instance of combining LDA with CNN and BiLSTM encoders in a Graph Attention-based model for summarizing scientific texts. Experimental results demonstrate that the proposed framework outperforms both BERT-based and non-BERT approaches on publicly available datasets from arXiv and PubMed. Our model achieves a ROUGE-1 score of 46.31, a ROUGE-2 score of 19.98, and a ROUGE-L score of 40.21 on the arXiv dataset. It performs even better on the PubMed dataset, attaining a ROUGE-1 score of 48.85, a ROUGE-2 score of 21.78, and a ROUGE-L score of 42.12.",
            "corpus_id": 277284940,
            "sentences": [
                {
                    "corpus_id": "277284940",
                    "title": "Global and Local Context Fusion in Heterogeneous Graph Neural Network for Summarizing Lengthy Scientific Documents",
                    "text": "Conventional truncation models frequently lose crucial meaning, whereas hierarchical and distributed representations fail to effectively capture both global and local contextual dependencies. Graph-based models that incorporate heterogeneous structures aim to improve interconnectivity, but they still face challenges in effectively modeling fine-grained semantic relationships. \n\nDomain-specific approaches like SciBERTSUM and citation graph-based methods often fail to generalize effectively across diverse document types. Additionally, cluster-based and pre-training methods, despite their potential, involve substantial computational overhead, making them impractical for real-time summarization. These challenges emphasize the need for a more efficient and scalable solution that combines semantic-rich representations with a graph-based framework to advance extractive summarization for long scientific documents. \n\nThis study aims to fill this gap by proposing a computationally efficient Heterogeneous Graph Neural Network (HGNN) for extractive summarization of scientific texts. The framework combines GloVe embeddings, CNNs, and BiLSTMs for local and long-range dependencies, enhanced LDA for global context, and a Graph Attention Network (GAT) to refine node representations in a heterogeneous graph. This is the first work to integrate LDA, CNN, and BiLSTM encoders in a GAT-based model for scientific text summarization.",
                    "score": 0.4757768092357937,
                    "section_title": "C. LONG DOCUMENT EXTRACTIVE SUMMARIZATION",
                    "char_start_offset": 18898,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 191
                        },
                        {
                            "start": 192,
                            "end": 378
                        },
                        {
                            "start": 381,
                            "end": 524
                        },
                        {
                            "start": 525,
                            "end": 700
                        },
                        {
                            "start": 701,
                            "end": 919
                        },
                        {
                            "start": 922,
                            "end": 1087
                        },
                        {
                            "start": 1088,
                            "end": 1311
                        },
                        {
                            "start": 1312,
                            "end": 1433
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9765625
                },
                {
                    "corpus_id": "277284940",
                    "title": "Global and Local Context Fusion in Heterogeneous Graph Neural Network for Summarizing Lengthy Scientific Documents",
                    "text": "Long document summarization has recently gained popularity due to the increasing complexity of modeling large documents [30]. Currently, the most common approach to addressing this issue involves truncating documents into smaller, fixed-length sequences and using sliding window techniques to process the document incrementally [31]. However, this approach results in significant information loss, making it unsuitable for the Extractive Document Summarization (EDS) task. EDS requires an understanding of the informational relationships among the extracted sentences [32]. In response to this challenge, several promising approaches have been developed specifically for long document summarization. \n\nThe study conducted by [33] proposed a hierarchical encoder designed to extract the syntactic structure of the input document, coupled with a discourse aware decoding module for abstractive summarization. The research team in [13] advances the task of long text summarization by using distributed representations to capture the global context of the entire document and the local context of specific sections or topics. In the domain of graph-based approaches, [3] implement a modified version of the GAT designed to identify inter-sentence relationships. To enhance the summarization of lengthy documents, they implemented Neural Topic Modeling (NTM) to identify latent topics. This approach introduced NTM as a new node type, which was then integrated with existing sentence nodes to improve the overall structure and accuracy of the summarization process. \n\nThe researchers of [7] integrated passage nodes into the heterogeneous graph structure alongside the existing word and sentence nodes. In the study [34], the authors attempted to redefine the task of review formalization, focusing on scientific paper summarization using citation graphs. The CGSUM model included information from both the source paper and its references, and enhanced the contextual relevance of summarization. The authors also built the Semantic Scholar Network (SSN) from 141K research papers and 661K citation relations with a broad view of scientific domains. The researchers of [35] explored the effect of pre-training on a BERT-based extractive summarization system created specifically for scientific documents. They gained significant performance improvements by adding an intermediate pre-training phase that makes use of pre-existing summarization datasets.",
                    "score": 0.5495629685562042,
                    "section_title": "C. LONG DOCUMENT EXTRACTIVE SUMMARIZATION",
                    "char_start_offset": 14209,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 125
                        },
                        {
                            "start": 126,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 472
                        },
                        {
                            "start": 473,
                            "end": 573
                        },
                        {
                            "start": 574,
                            "end": 699
                        },
                        {
                            "start": 702,
                            "end": 906
                        },
                        {
                            "start": 907,
                            "end": 1121
                        },
                        {
                            "start": 1122,
                            "end": 1257
                        },
                        {
                            "start": 1258,
                            "end": 1380
                        },
                        {
                            "start": 1381,
                            "end": 1560
                        },
                        {
                            "start": 1563,
                            "end": 1697
                        },
                        {
                            "start": 1698,
                            "end": 1850
                        },
                        {
                            "start": 1851,
                            "end": 1990
                        },
                        {
                            "start": 1991,
                            "end": 2143
                        },
                        {
                            "start": 2144,
                            "end": 2298
                        },
                        {
                            "start": 2299,
                            "end": 2447
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 120,
                            "end": 124,
                            "matchedPaperCorpusId": "196184038"
                        },
                        {
                            "start": 328,
                            "end": 332,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 1582,
                            "end": 1585,
                            "matchedPaperCorpusId": "252818946"
                        },
                        {
                            "start": 1711,
                            "end": 1715,
                            "matchedPaperCorpusId": "233169121"
                        },
                        {
                            "start": 2163,
                            "end": 2167,
                            "matchedPaperCorpusId": "235097196"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9658203125
                },
                {
                    "corpus_id": "277284940",
                    "title": "Global and Local Context Fusion in Heterogeneous Graph Neural Network for Summarizing Lengthy Scientific Documents",
                    "text": "The proposed Global and Local Context Fusion in Heterogeneous Graph Neural Networks introduces an innovative approach to extractive summarization of lengthy scientific documents. By leveraging GloVe embeddings, CNN, BiL-STM encoders, and an advanced graph-based architecture, the model effectively integrates both global and local semantic contexts. This method outperforms current state-of-the-art models on benchmark datasets such as arXiv and PubMed. Its ability to process long documents with complex structures while maintaining relatively low computational complexity makes it a promising tool for future applications in fields like medical record analysis and scientific research. \n\nThe proposed approach has several limitations that require further investigation. While LDA is a relatively simple and effective topic modeling technique with low complexity, it may not be well-suited for handling flexible, dynamic, and diverse document structures. Additionally, there is significant room for improvement in terms of scalability, particularly for very large datasets, as well as in generalizing the approach to work effectively across multiple languages, including low-resource languages. Furthermore, although the model incorporates both local and global semantics of the contexts, it occasionally prioritizes more prominent information over diverse content, which can reduce the representativeness of the generated summaries. \n\nThe future direction aims to build upon these discoveries and address the limitations mentioned earlier. To enhance the performance and flexibility of topic modeling, we plan to extend existing approaches based on neural topic models, such as the Neural Variational Document Model (NVDM). The model's effectiveness could be further improved for multilingual summarization by incorporating language-specific embeddings or leveraging pre-trained multilingual transformers like XLM-R. Additionally, exploring dynamic graph architectures or hierarchical attention mechanisms-such as optimizing for real-time processing through techniques like model pruning and quantization-could yield significant advancements. \n\nIn conclusion, integrating the model with abstractive summarization capabilities and evaluating it across diverse datasets would strengthen the robustness of the results. This would also demonstrate that the proposed model is a versatile and advanced solution for summarizing long documents. \n\nMUHAMMAD UMAIR , photograph and biography not available at the time of publication. \n\nATIF KHAN , photograph and biography not available at the time of publication.",
                    "score": 0.5246068791433227,
                    "section_title": "V. CONCLUSION AND FUTURE DIRECTIONS",
                    "char_start_offset": 56989,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 178
                        },
                        {
                            "start": 179,
                            "end": 349
                        },
                        {
                            "start": 350,
                            "end": 453
                        },
                        {
                            "start": 454,
                            "end": 687
                        },
                        {
                            "start": 690,
                            "end": 771
                        },
                        {
                            "start": 772,
                            "end": 955
                        },
                        {
                            "start": 956,
                            "end": 1195
                        },
                        {
                            "start": 1196,
                            "end": 1434
                        },
                        {
                            "start": 1437,
                            "end": 1541
                        },
                        {
                            "start": 1542,
                            "end": 1725
                        },
                        {
                            "start": 1726,
                            "end": 1918
                        },
                        {
                            "start": 1919,
                            "end": 2144
                        },
                        {
                            "start": 2147,
                            "end": 2317
                        },
                        {
                            "start": 2318,
                            "end": 2438
                        },
                        {
                            "start": 2441,
                            "end": 2524
                        },
                        {
                            "start": 2527,
                            "end": 2605
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.958984375
                }
            ],
            "relevance_judgement": 0.9765625,
            "relevance_judgment_input_expanded": "# Title: Global and Local Context Fusion in Heterogeneous Graph Neural Network for Summarizing Lengthy Scientific Documents\n# Venue: IEEE Access\n# Authors: Muhammad Umair, Atif Khan, Fasee Ullah, Atef Masmoudi, Muhammed Faheem\n## Abstract\nThe primary objective of text summarization is to condense a document\u2019s length while preserving its essential content. Extractive summarization methods are commonly used due to their effectiveness and straightforward presentation. However, a significant challenge lies in segmenting documents into distinct concepts and understanding how sentences interact, especially in complex materials such as scientific articles. This process entails identifying relationships between sentences and determining the most significant and informative content within extensive text collections. Traditional techniques often utilize pre-trained models like BERT, known for their ability to capture word context. Nonetheless, these models have limitations, including constrained input lengths and the computational intensity of self-attention mechanisms, which hinder their effectiveness in processing large-scale scientific texts. To address these challenges, we propose a computationally efficient Heterogeneous Graph Neural Network (HGNN) for the extractive summarization of lengthy scientific texts. This framework combines GloVe embeddings with Convolutional Neural Networks (CNNs) and Bidirectional Long Short-Term Memory (BiLSTM) encoders. GloVe offers simple yet effective word embeddings, CNNs focus on capturing local word structures, and BiLSTMs identify long-range dependencies, allowing for flexible encoding of extensive texts. For global context and topic modeling, we utilize an enhanced version of Latent Dirichlet Allocation (LDA) to retain essential document attributes. In this model, words, sentences, and topics are represented as nodes in a heterogeneous graph, with TF-IDF values illustrating the relationships between edges. The graph is processed using a Graph Attention Network (GAT), which refines node representations by integrating both local and global information. This study represents the first instance of combining LDA with CNN and BiLSTM encoders in a Graph Attention-based model for summarizing scientific texts. Experimental results demonstrate that the proposed framework outperforms both BERT-based and non-BERT approaches on publicly available datasets from arXiv and PubMed. Our model achieves a ROUGE-1 score of 46.31, a ROUGE-2 score of 19.98, and a ROUGE-L score of 40.21 on the arXiv dataset. It performs even better on the PubMed dataset, attaining a ROUGE-1 score of 48.85, a ROUGE-2 score of 21.78, and a ROUGE-L score of 42.12.\n## C. LONG DOCUMENT EXTRACTIVE SUMMARIZATION\nLong document summarization has recently gained popularity due to the increasing complexity of modeling large documents [30]. Currently, the most common approach to addressing this issue involves truncating documents into smaller, fixed-length sequences and using sliding window techniques to process the document incrementally [31]. However, this approach results in significant information loss, making it unsuitable for the Extractive Document Summarization (EDS) task. EDS requires an understanding of the informational relationships among the extracted sentences [32]. In response to this challenge, several promising approaches have been developed specifically for long document summarization. \n\nThe study conducted by [33] proposed a hierarchical encoder designed to extract the syntactic structure of the input document, coupled with a discourse aware decoding module for abstractive summarization. The research team in [13] advances the task of long text summarization by using distributed representations to capture the global context of the entire document and the local context of specific sections or topics. In the domain of graph-based approaches, [3] implement a modified version of the GAT designed to identify inter-sentence relationships. To enhance the summarization of lengthy documents, they implemented Neural Topic Modeling (NTM) to identify latent topics. This approach introduced NTM as a new node type, which was then integrated with existing sentence nodes to improve the overall structure and accuracy of the summarization process. \n\nThe researchers of [7] integrated passage nodes into the heterogeneous graph structure alongside the existing word and sentence nodes. In the study [34], the authors attempted to redefine the task of review formalization, focusing on scientific paper summarization using citation graphs. The CGSUM model included information from both the source paper and its references, and enhanced the contextual relevance of summarization. The authors also built the Semantic Scholar Network (SSN) from 141K research papers and 661K citation relations with a broad view of scientific domains. The researchers of [35] explored the effect of pre-training on a BERT-based extractive summarization system created specifically for scientific documents. They gained significant performance improvements by adding an intermediate pre-training phase that makes use of pre-existing summarization datasets.\n...\nConventional truncation models frequently lose crucial meaning, whereas hierarchical and distributed representations fail to effectively capture both global and local contextual dependencies. Graph-based models that incorporate heterogeneous structures aim to improve interconnectivity, but they still face challenges in effectively modeling fine-grained semantic relationships. \n\nDomain-specific approaches like SciBERTSUM and citation graph-based methods often fail to generalize effectively across diverse document types. Additionally, cluster-based and pre-training methods, despite their potential, involve substantial computational overhead, making them impractical for real-time summarization. These challenges emphasize the need for a more efficient and scalable solution that combines semantic-rich representations with a graph-based framework to advance extractive summarization for long scientific documents. \n\nThis study aims to fill this gap by proposing a computationally efficient Heterogeneous Graph Neural Network (HGNN) for extractive summarization of scientific texts. The framework combines GloVe embeddings, CNNs, and BiLSTMs for local and long-range dependencies, enhanced LDA for global context, and a Graph Attention Network (GAT) to refine node representations in a heterogeneous graph. This is the first work to integrate LDA, CNN, and BiLSTM encoders in a GAT-based model for scientific text summarization.\n\n## V. CONCLUSION AND FUTURE DIRECTIONS\nThe proposed Global and Local Context Fusion in Heterogeneous Graph Neural Networks introduces an innovative approach to extractive summarization of lengthy scientific documents. By leveraging GloVe embeddings, CNN, BiL-STM encoders, and an advanced graph-based architecture, the model effectively integrates both global and local semantic contexts. This method outperforms current state-of-the-art models on benchmark datasets such as arXiv and PubMed. Its ability to process long documents with complex structures while maintaining relatively low computational complexity makes it a promising tool for future applications in fields like medical record analysis and scientific research. \n\nThe proposed approach has several limitations that require further investigation. While LDA is a relatively simple and effective topic modeling technique with low complexity, it may not be well-suited for handling flexible, dynamic, and diverse document structures. Additionally, there is significant room for improvement in terms of scalability, particularly for very large datasets, as well as in generalizing the approach to work effectively across multiple languages, including low-resource languages. Furthermore, although the model incorporates both local and global semantics of the contexts, it occasionally prioritizes more prominent information over diverse content, which can reduce the representativeness of the generated summaries. \n\nThe future direction aims to build upon these discoveries and address the limitations mentioned earlier. To enhance the performance and flexibility of topic modeling, we plan to extend existing approaches based on neural topic models, such as the Neural Variational Document Model (NVDM). The model's effectiveness could be further improved for multilingual summarization by incorporating language-specific embeddings or leveraging pre-trained multilingual transformers like XLM-R. Additionally, exploring dynamic graph architectures or hierarchical attention mechanisms-such as optimizing for real-time processing through techniques like model pruning and quantization-could yield significant advancements. \n\nIn conclusion, integrating the model with abstractive summarization capabilities and evaluating it across diverse datasets would strengthen the robustness of the results. This would also demonstrate that the proposed model is a versatile and advanced solution for summarizing long documents. \n\nMUHAMMAD UMAIR , photograph and biography not available at the time of publication. \n\nATIF KHAN , photograph and biography not available at the time of publication.",
            "reference_string": "[277284940 | Umair et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Abstractive Summarization Guided by Latent Hierarchical Document Structure",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 60,
            "citation_count": 13,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2211.09458",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.09458, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2159539050",
                    "name": "Yifu Qiu"
                },
                {
                    "authorId": "40146204",
                    "name": "Shay B. Cohen"
                }
            ],
            "abstract": "Sequential abstractive neural summarizers often do not use the underlying structure in the input article or dependencies between the input sentences. This structure is essential to integrate and consolidate information from different parts of the text. To address this shortcoming, we propose a hierarchy-aware graph neural network (HierGNN) which captures such dependencies through three main steps: 1) learning a hierarchical document structure through a latent structure tree learned by a sparse matrix-tree computation; 2) propagating sentence information over this structure using a novel message-passing node propagation mechanism to identify salient information; 3) using graph-level attention to concentrate the decoder on salient information. Experiments confirm HierGNN improves strong sequence models such as BART, with a 0.55 and 0.75 margin in average ROUGE-1/2/L for CNN/DM and XSum. Further human evaluation demonstrates that summaries produced by our model are more relevant and less redundant than the baselines, into which HierGNN is incorporated. We also find HierGNN synthesizes summaries by fusing multiple source sentences more, rather than compressing a single source sentence, and that it processes long inputs more effectively.",
            "corpus_id": 253155963,
            "sentences": [
                {
                    "corpus_id": "253155963",
                    "title": "Abstractive Summarization Guided by Latent Hierarchical Document Structure",
                    "text": "Sequential abstractive neural summarizers often do not use the underlying structure in the input article or dependencies between the input sentences. This structure is essential to integrate and consolidate information from different parts of the text. To address this shortcoming, we propose a hierarchy-aware graph neural network (HierGNN) which captures such dependencies through three main steps: 1) learning a hierarchical document structure through a latent structure tree learned by a sparse matrix-tree computation; 2) propagating sentence information over this structure using a novel message-passing node propagation mechanism to identify salient information; 3) using graph-level attention to concentrate the decoder on salient information. Experiments confirm HierGNN improves strong sequence models such as BART, with a 0.55 and 0.75 margin in average ROUGE-1/2/L for CNN/DM and XSum. Further human evaluation demonstrates that summaries produced by our model are more relevant and less redundant than the baselines, into which HierGNN is incorporated. We also find HierGNN synthesizes summaries by fusing multiple source sentences more, rather than compressing a single source sentence, and that it processes long inputs more effectively.",
                    "score": 0.5473260482125347,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97607421875
                },
                {
                    "corpus_id": "253155963",
                    "title": "Abstractive Summarization Guided by Latent Hierarchical Document Structure",
                    "text": "Neural Abstractive Summarization Rush et al. (2015) first proposed to use a sequence-to-sequence model with an attention mechanism to perform sentence compression. Mendes et al. (2019) demonstrated the advantages and limitations of neural methods based on sentence compression. The pointer-generator networks (PGN; See et al. 2017) enhances the attention model with a copying functionality. PGN has also been further extended to create summarization systems by incorporating the topic information (Liu et al., 2019b), document structural information (Song et al., 2018), semantic information (Hardy and Vlachos, 2018), and was improved by replacing the plain LSTM module with the more advanced Transformer model to overcome the difficulty in modeling long sequence input (Pilault et al., 2020;Wang et al., 2021;Fonseca et al., 2022). For the pretrained models, BERTSum (Liu and Lapata, 2019) adopted the BERT encoder for the summarizer, with a randomly initialized decoder. Lewis et al. (2020) presented BART which pre-trains both the underlying encoder and decoder. Our approach is related to the structural attention model (Balachandran et al., 2021;Liu et al., 2019a), but differs in two major ways: (i) we introduce an adaptive sparse matrix-tree construction to learn a latent hierarchical graph and a novel propagation rule; (ii) we investigate to use the structure information both with the encoder and the decoder for abstractive summarization, and not just the encoder. These shows to be more effective for unsupervised learning of the latent hierarchical structure while can defeat the approach that leverages external graph constructor (Balachandran et al., 2021).",
                    "score": 0.4839985067575868,
                    "section_title": "Related Work",
                    "char_start_offset": 5245,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 163
                        },
                        {
                            "start": 164,
                            "end": 277
                        },
                        {
                            "start": 278,
                            "end": 390
                        },
                        {
                            "start": 391,
                            "end": 833
                        },
                        {
                            "start": 834,
                            "end": 973
                        },
                        {
                            "start": 974,
                            "end": 1066
                        },
                        {
                            "start": 1067,
                            "end": 1478
                        },
                        {
                            "start": 1479,
                            "end": 1675
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 33,
                            "end": 51,
                            "matchedPaperCorpusId": "1918428"
                        },
                        {
                            "start": 164,
                            "end": 184,
                            "matchedPaperCorpusId": "102352252"
                        },
                        {
                            "start": 315,
                            "end": 330,
                            "matchedPaperCorpusId": "8314118"
                        },
                        {
                            "start": 497,
                            "end": 516,
                            "matchedPaperCorpusId": "203641742"
                        },
                        {
                            "start": 550,
                            "end": 569,
                            "matchedPaperCorpusId": "46936631"
                        },
                        {
                            "start": 592,
                            "end": 617,
                            "matchedPaperCorpusId": "52111191"
                        },
                        {
                            "start": 771,
                            "end": 793,
                            "matchedPaperCorpusId": "202541012"
                        },
                        {
                            "start": 793,
                            "end": 811,
                            "matchedPaperCorpusId": "229171633"
                        },
                        {
                            "start": 811,
                            "end": 832,
                            "matchedPaperCorpusId": "249062562"
                        },
                        {
                            "start": 974,
                            "end": 993,
                            "matchedPaperCorpusId": "204960716"
                        },
                        {
                            "start": 1125,
                            "end": 1152,
                            "matchedPaperCorpusId": "231934225"
                        },
                        {
                            "start": 1152,
                            "end": 1170,
                            "matchedPaperCorpusId": "174800499"
                        },
                        {
                            "start": 1647,
                            "end": 1674,
                            "matchedPaperCorpusId": "231934225"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96044921875
                }
            ],
            "relevance_judgement": 0.97607421875,
            "relevance_judgment_input_expanded": "# Title: Abstractive Summarization Guided by Latent Hierarchical Document Structure\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Yifu Qiu, Shay B. Cohen\n## Abstract\nSequential abstractive neural summarizers often do not use the underlying structure in the input article or dependencies between the input sentences. This structure is essential to integrate and consolidate information from different parts of the text. To address this shortcoming, we propose a hierarchy-aware graph neural network (HierGNN) which captures such dependencies through three main steps: 1) learning a hierarchical document structure through a latent structure tree learned by a sparse matrix-tree computation; 2) propagating sentence information over this structure using a novel message-passing node propagation mechanism to identify salient information; 3) using graph-level attention to concentrate the decoder on salient information. Experiments confirm HierGNN improves strong sequence models such as BART, with a 0.55 and 0.75 margin in average ROUGE-1/2/L for CNN/DM and XSum. Further human evaluation demonstrates that summaries produced by our model are more relevant and less redundant than the baselines, into which HierGNN is incorporated. We also find HierGNN synthesizes summaries by fusing multiple source sentences more, rather than compressing a single source sentence, and that it processes long inputs more effectively.\n## Related Work\nNeural Abstractive Summarization Rush et al. (2015) first proposed to use a sequence-to-sequence model with an attention mechanism to perform sentence compression. Mendes et al. (2019) demonstrated the advantages and limitations of neural methods based on sentence compression. The pointer-generator networks (PGN; See et al. 2017) enhances the attention model with a copying functionality. PGN has also been further extended to create summarization systems by incorporating the topic information (Liu et al., 2019b), document structural information (Song et al., 2018), semantic information (Hardy and Vlachos, 2018), and was improved by replacing the plain LSTM module with the more advanced Transformer model to overcome the difficulty in modeling long sequence input (Pilault et al., 2020;Wang et al., 2021;Fonseca et al., 2022). For the pretrained models, BERTSum (Liu and Lapata, 2019) adopted the BERT encoder for the summarizer, with a randomly initialized decoder. Lewis et al. (2020) presented BART which pre-trains both the underlying encoder and decoder. Our approach is related to the structural attention model (Balachandran et al., 2021;Liu et al., 2019a), but differs in two major ways: (i) we introduce an adaptive sparse matrix-tree construction to learn a latent hierarchical graph and a novel propagation rule; (ii) we investigate to use the structure information both with the encoder and the decoder for abstractive summarization, and not just the encoder. These shows to be more effective for unsupervised learning of the latent hierarchical structure while can defeat the approach that leverages external graph constructor (Balachandran et al., 2021).",
            "reference_string": "[253155963 | Qiu et al. | 2022 | Citations: 13]"
        },
        {
            "title": "Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive Review",
            "venue": "Discret. Math. Algorithms Appl.",
            "year": 2023,
            "reference_count": 205,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.09411, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2258949345",
                    "name": "Guanghua Wang"
                },
                {
                    "authorId": "2258731583",
                    "name": "Weili Wu"
                }
            ],
            "abstract": "In recent years, deep learning has revolutionized natural language processing (NLP) by enabling the development of models that can learn complex representations of language data, leading to significant improvements in performance across a wide range of NLP tasks. Deep learning models for NLP typically use large amounts of data to train deep neural networks, allowing them to learn the patterns and relationships in language data. This is in contrast to traditional NLP approaches, which rely on hand-engineered features and rules to perform NLP tasks. The ability of deep neural networks to learn hierarchical representations of language data, handle variable-length input sequences, and perform well on large datasets makes them well-suited for NLP applications. Driven by the exponential growth of textual data and the increasing demand for condensed, coherent, and informative summaries, text summarization has been a critical research area in the field of NLP. Applying deep learning to text summarization refers to the use of deep neural networks to perform text summarization tasks. In this survey, we begin with a review of fashionable text summarization tasks in recent years, including extractive, abstractive, multi-document, and so on. Next, we discuss most deep learning-based models and their experimental results on these tasks. The paper also covers datasets and data representation for summarization tasks. Finally, we delve into the opportunities and challenges associated with summarization tasks and their corresponding methodologies, aiming to inspire future research efforts to advance the field further. A goal of our survey is to explain how these methods differ in their requirements as understanding them is essential for choosing a technique suited for a specific setting.",
            "corpus_id": 264146402,
            "sentences": [
                {
                    "corpus_id": "264146402",
                    "title": "Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive Review",
                    "text": "Although Graph Neural Networks (GNN) [62,114,145,158] are primarily used in domains where data naturally exhibits graph structures, such as social networks, molecular structures, and knowledge graphs, they can also be adapted for NLP tasks. In NLP, GNNs are typically used to model relationships between words, sentences, or documents by representing them as nodes in a graph, with edges representing the relationships between these nodes. A GNN model learns to propagate information through the graph by iteratively updating the node representations based on the information from their neighbors. The core building blocks of GNNs are graph convolutional layers, which are designed to aggregate information from neighboring nodes and update the node features. \n\nJing et al. [75] present a novel Multiplex Graph Convolutional Network (Multi-GCN) approach for extractive summarization. Multi-GCN learns node embedding of different relations among sentences and words separately and combines them to produce a final embedding. This approach helps to mitigate the over-smoothing and vanishing gradient problems of the original GCN. \n\nA heterogeneous GNN, HETERSUMGRAPH [179] is introduced for extractive document summarization. This network includes nodes of different granularity levels apart from sentences, which act as intermediaries and enrich cross-sentence relations. This approach allows different sentences to interact considering overlapping word information. Moreover, the graph network can accommodate additional node types, such as document nodes for multi-document summarization. Doan et al. [40] propose a method for long document summarization by applying Heterogeneous Graph Neural Networks (HeterGNN) and introducing a homogeneous graph structure (HomoGNN). The HomoGNN focuses on sentence-level nodes to create a graph structure, enriching inter-sentence connections. Simultaneously, the HeterGNN explores the complex relationships between words and sentences, tackling intra-sentence connections. Both networks are constructed and updated using a Graph Attention Network model. In the HomoGNN, a BERT model is used for the initial encoding of sentences, while the HeterGNN uses a combination of CNN and BiLSTM for node feature extraction.",
                    "score": 0.5684432941095042,
                    "section_title": "Graph neural networks",
                    "char_start_offset": 38428,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 240
                        },
                        {
                            "start": 241,
                            "end": 439
                        },
                        {
                            "start": 440,
                            "end": 597
                        },
                        {
                            "start": 598,
                            "end": 759
                        },
                        {
                            "start": 762,
                            "end": 883
                        },
                        {
                            "start": 884,
                            "end": 1023
                        },
                        {
                            "start": 1024,
                            "end": 1127
                        },
                        {
                            "start": 1130,
                            "end": 1223
                        },
                        {
                            "start": 1224,
                            "end": 1370
                        },
                        {
                            "start": 1371,
                            "end": 1465
                        },
                        {
                            "start": 1466,
                            "end": 1589
                        },
                        {
                            "start": 1590,
                            "end": 1771
                        },
                        {
                            "start": 1772,
                            "end": 1882
                        },
                        {
                            "start": 1883,
                            "end": 2012
                        },
                        {
                            "start": 2013,
                            "end": 2093
                        },
                        {
                            "start": 2094,
                            "end": 2254
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 37,
                            "end": 41,
                            "matchedPaperCorpusId": "207238980"
                        },
                        {
                            "start": 41,
                            "end": 45,
                            "matchedPaperCorpusId": "17486263"
                        },
                        {
                            "start": 45,
                            "end": 49,
                            "matchedPaperCorpusId": "3051291"
                        },
                        {
                            "start": 49,
                            "end": 53,
                            "matchedPaperCorpusId": "206756462"
                        },
                        {
                            "start": 774,
                            "end": 778,
                            "matchedPaperCorpusId": "237353135"
                        },
                        {
                            "start": 1165,
                            "end": 1170,
                            "matchedPaperCorpusId": "216552978"
                        },
                        {
                            "start": 1602,
                            "end": 1606,
                            "matchedPaperCorpusId": "252819335"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.974609375
                }
            ],
            "relevance_judgement": 0.974609375,
            "relevance_judgment_input_expanded": "# Title: Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive Review\n# Venue: Discret. Math. Algorithms Appl.\n# Authors: Guanghua Wang, Weili Wu\n## Abstract\nIn recent years, deep learning has revolutionized natural language processing (NLP) by enabling the development of models that can learn complex representations of language data, leading to significant improvements in performance across a wide range of NLP tasks. Deep learning models for NLP typically use large amounts of data to train deep neural networks, allowing them to learn the patterns and relationships in language data. This is in contrast to traditional NLP approaches, which rely on hand-engineered features and rules to perform NLP tasks. The ability of deep neural networks to learn hierarchical representations of language data, handle variable-length input sequences, and perform well on large datasets makes them well-suited for NLP applications. Driven by the exponential growth of textual data and the increasing demand for condensed, coherent, and informative summaries, text summarization has been a critical research area in the field of NLP. Applying deep learning to text summarization refers to the use of deep neural networks to perform text summarization tasks. In this survey, we begin with a review of fashionable text summarization tasks in recent years, including extractive, abstractive, multi-document, and so on. Next, we discuss most deep learning-based models and their experimental results on these tasks. The paper also covers datasets and data representation for summarization tasks. Finally, we delve into the opportunities and challenges associated with summarization tasks and their corresponding methodologies, aiming to inspire future research efforts to advance the field further. A goal of our survey is to explain how these methods differ in their requirements as understanding them is essential for choosing a technique suited for a specific setting.\n## Graph neural networks\nAlthough Graph Neural Networks (GNN) [62,114,145,158] are primarily used in domains where data naturally exhibits graph structures, such as social networks, molecular structures, and knowledge graphs, they can also be adapted for NLP tasks. In NLP, GNNs are typically used to model relationships between words, sentences, or documents by representing them as nodes in a graph, with edges representing the relationships between these nodes. A GNN model learns to propagate information through the graph by iteratively updating the node representations based on the information from their neighbors. The core building blocks of GNNs are graph convolutional layers, which are designed to aggregate information from neighboring nodes and update the node features. \n\nJing et al. [75] present a novel Multiplex Graph Convolutional Network (Multi-GCN) approach for extractive summarization. Multi-GCN learns node embedding of different relations among sentences and words separately and combines them to produce a final embedding. This approach helps to mitigate the over-smoothing and vanishing gradient problems of the original GCN. \n\nA heterogeneous GNN, HETERSUMGRAPH [179] is introduced for extractive document summarization. This network includes nodes of different granularity levels apart from sentences, which act as intermediaries and enrich cross-sentence relations. This approach allows different sentences to interact considering overlapping word information. Moreover, the graph network can accommodate additional node types, such as document nodes for multi-document summarization. Doan et al. [40] propose a method for long document summarization by applying Heterogeneous Graph Neural Networks (HeterGNN) and introducing a homogeneous graph structure (HomoGNN). The HomoGNN focuses on sentence-level nodes to create a graph structure, enriching inter-sentence connections. Simultaneously, the HeterGNN explores the complex relationships between words and sentences, tackling intra-sentence connections. Both networks are constructed and updated using a Graph Attention Network model. In the HomoGNN, a BERT model is used for the initial encoding of sentences, while the HeterGNN uses a combination of CNN and BiLSTM for node feature extraction.",
            "reference_string": "[264146402 | Wang et al. | 2023 | Citations: 4]"
        },
        {
            "title": "Multi-granularity adaptive extractive document summarization with heterogeneous graph neural networks",
            "venue": "PeerJ Computer Science",
            "year": 2023,
            "reference_count": 55,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.7717/peerj-cs.1737",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10773739, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2274342252",
                    "name": "Wu Su"
                },
                {
                    "authorId": "2274904693",
                    "name": "Jin Jiang"
                },
                {
                    "authorId": "2274639188",
                    "name": "Kaihui Huang"
                }
            ],
            "abstract": "The crucial aspect of extractive document summarization lies in understanding the interrelations between sentences. Documents inherently comprise a multitude of sentences, and sentence-level models frequently fail to consider the relationships between distantly-placed sentences, resulting in the omission of significant information in the summary. Moreover, information within documents tends to be distributed sparsely, challenging the efficacy of sentence-level models. In the realm of heterogeneous graph neural networks, it has been observed that semantic nodes with varying levels of granularity encapsulate distinct semantic connections. Initially, the incorporation of edge features into the computation of dynamic graph attention networks is performed to account for node relationships. Subsequently, given the multiplicity of topics in a document or a set of documents, a topic model is employed to extract topic-specific features and the probability distribution linking these topics with sentence nodes. Last but not least, the model defines nodes with different levels of granularity\u2014ranging from documents and topics to sentences\u2014and these various nodes necessitate different propagation widths and depths for capturing intricate relationships in the information being disseminated. Adaptive measures are taken to learn the importance and correlation between nodes of different granularities in terms of both width and depth. Experimental evidence from two benchmark datasets highlights the superior performance of the proposed model, as assessed by ROUGE metrics, in comparison to existing approaches, even in the absence of pre-trained language models. Additionally, an ablation study confirms the positive impact of each individual module on the model's ROUGE scores.",
            "corpus_id": 266244733,
            "sentences": [
                {
                    "corpus_id": "266244733",
                    "title": "Multi-granularity adaptive extractive document summarization with heterogeneous graph neural networks",
                    "text": "The research introduces a multi-granularity adaptive automatic summarization model predicated on a heterogeneous graph neural network. This approach innovatively incorporates topic semantic units into the heterogeneous graph, thereby enriching the complexity of inter-sentence relations. Attention is also paid to the relative constancy of attention coefficients and the significance of edge features. Adaptive techniques are employed to update nodal characteristics across varying granularities, both in terms of breadth and depth. The proposed model further allows for effortless inclusion of document-level nodes into a single-document framework, thereby simplifying the construction of multi-document summarization models. The designed graph structureincorporating sentences, topic words, and individual words-can be seamlessly integrated as a module within existing generative models. This integrated module efficiently encodes sentence-level information and channels it into broader linguistic and thematic structures through the graph architecture. Our model, serving as an encoding module, can be applied to other datasets by simply extracting sentences, topics, and words from the dataset and incorporating them into the corresponding graph structure nodes. Performance assessments on the CNN/DailyMail and Multi-News datasets reveal that the model delivers commendable results, particularly when benchmarked against models not leveraging BERT.",
                    "score": 0.5473260482125347,
                    "section_title": "CONCLUSION",
                    "char_start_offset": 35331,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 134
                        },
                        {
                            "start": 135,
                            "end": 287
                        },
                        {
                            "start": 288,
                            "end": 401
                        },
                        {
                            "start": 402,
                            "end": 532
                        },
                        {
                            "start": 533,
                            "end": 726
                        },
                        {
                            "start": 727,
                            "end": 889
                        },
                        {
                            "start": 890,
                            "end": 1055
                        },
                        {
                            "start": 1056,
                            "end": 1266
                        },
                        {
                            "start": 1267,
                            "end": 1453
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97314453125
                },
                {
                    "corpus_id": "266244733",
                    "title": "Multi-granularity adaptive extractive document summarization with heterogeneous graph neural networks",
                    "text": "The critical component of extractive summarization involves an in-depth analysis of the source text to produce condensed, semantically clear sentences that serve as a summary of the document. Prevailing models often utilize an encoder-decoder framework based on attention mechanisms and yield satisfactory outcomes in the realm of short text summarization. Recurrent neural networks (RNNs) (Cheng & Lapata, 2016;Nallapati, Zhai & Zhou, 2017;Zhou et al., 2018) commonly serve the purpose of elucidating crosssentence relationships. Nonetheless, these RNN-based frameworks often fall short in addressing long-distance dependencies, particularly when handling extensive or multidocument texts. An alternative avenue worth exploring is the utilization of graph structures for modeling inter-sentence connections. Yet, the effective design of these graph structures for automating document summarization remains an area warranting further research. \n\nDocuments inherently comprise a multitude of sentences, and sentence-level models frequently fail to consider the relationships between distantly-placed sentences, resulting in the omission of significant information in the summary. Moreover, information within documents tends to be distributed sparsely, challenging the efficacy of sentence-level models. To address these shortcomings, a fused multi-granularity node graph model is proposed. This model integrates a topic model to isolate keyword features, and leverages both TF-IDF and topic probabilities for the generation of edge features, thus strengthening the interconnections among nodes of varying granularities. Through iterative node information propagation, links are established between sentences, topics, and words, facilitating enhanced information transfer to both proximal and distant nodes. Additionally, an adaptive mechanism for adjusting the depth and breadth of the graph is incorporated, progressively improving the model's capacity to handle long-distance dependencies. \n\nSemantic units in the model are composed of words in conjunction with topics. Each sentence is interconnected with the words and topics it encompasses, yet subjects and words lack direct edges between them. Edge features are incorporated into the calculations of the graph attention layer, facilitating information transfer between nodes. Given the varying requirements for propagation width and depth across nodes of different granularities, adaptability in both dimensions is introduced to enhance the model's performance.",
                    "score": 0.5101122198941723,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 191
                        },
                        {
                            "start": 192,
                            "end": 356
                        },
                        {
                            "start": 357,
                            "end": 530
                        },
                        {
                            "start": 531,
                            "end": 690
                        },
                        {
                            "start": 691,
                            "end": 808
                        },
                        {
                            "start": 809,
                            "end": 943
                        },
                        {
                            "start": 946,
                            "end": 1178
                        },
                        {
                            "start": 1179,
                            "end": 1302
                        },
                        {
                            "start": 1303,
                            "end": 1389
                        },
                        {
                            "start": 1390,
                            "end": 1619
                        },
                        {
                            "start": 1620,
                            "end": 1806
                        },
                        {
                            "start": 1807,
                            "end": 1991
                        },
                        {
                            "start": 1994,
                            "end": 2071
                        },
                        {
                            "start": 2072,
                            "end": 2200
                        },
                        {
                            "start": 2201,
                            "end": 2332
                        },
                        {
                            "start": 2333,
                            "end": 2518
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 412,
                            "end": 441,
                            "matchedPaperCorpusId": "6405271"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95947265625
                }
            ],
            "relevance_judgement": 0.97314453125,
            "relevance_judgment_input_expanded": "# Title: Multi-granularity adaptive extractive document summarization with heterogeneous graph neural networks\n# Venue: PeerJ Computer Science\n# Authors: Wu Su, Jin Jiang, Kaihui Huang\n## Abstract\nThe crucial aspect of extractive document summarization lies in understanding the interrelations between sentences. Documents inherently comprise a multitude of sentences, and sentence-level models frequently fail to consider the relationships between distantly-placed sentences, resulting in the omission of significant information in the summary. Moreover, information within documents tends to be distributed sparsely, challenging the efficacy of sentence-level models. In the realm of heterogeneous graph neural networks, it has been observed that semantic nodes with varying levels of granularity encapsulate distinct semantic connections. Initially, the incorporation of edge features into the computation of dynamic graph attention networks is performed to account for node relationships. Subsequently, given the multiplicity of topics in a document or a set of documents, a topic model is employed to extract topic-specific features and the probability distribution linking these topics with sentence nodes. Last but not least, the model defines nodes with different levels of granularity\u2014ranging from documents and topics to sentences\u2014and these various nodes necessitate different propagation widths and depths for capturing intricate relationships in the information being disseminated. Adaptive measures are taken to learn the importance and correlation between nodes of different granularities in terms of both width and depth. Experimental evidence from two benchmark datasets highlights the superior performance of the proposed model, as assessed by ROUGE metrics, in comparison to existing approaches, even in the absence of pre-trained language models. Additionally, an ablation study confirms the positive impact of each individual module on the model's ROUGE scores.\n## INTRODUCTION\nThe critical component of extractive summarization involves an in-depth analysis of the source text to produce condensed, semantically clear sentences that serve as a summary of the document. Prevailing models often utilize an encoder-decoder framework based on attention mechanisms and yield satisfactory outcomes in the realm of short text summarization. Recurrent neural networks (RNNs) (Cheng & Lapata, 2016;Nallapati, Zhai & Zhou, 2017;Zhou et al., 2018) commonly serve the purpose of elucidating crosssentence relationships. Nonetheless, these RNN-based frameworks often fall short in addressing long-distance dependencies, particularly when handling extensive or multidocument texts. An alternative avenue worth exploring is the utilization of graph structures for modeling inter-sentence connections. Yet, the effective design of these graph structures for automating document summarization remains an area warranting further research. \n\nDocuments inherently comprise a multitude of sentences, and sentence-level models frequently fail to consider the relationships between distantly-placed sentences, resulting in the omission of significant information in the summary. Moreover, information within documents tends to be distributed sparsely, challenging the efficacy of sentence-level models. To address these shortcomings, a fused multi-granularity node graph model is proposed. This model integrates a topic model to isolate keyword features, and leverages both TF-IDF and topic probabilities for the generation of edge features, thus strengthening the interconnections among nodes of varying granularities. Through iterative node information propagation, links are established between sentences, topics, and words, facilitating enhanced information transfer to both proximal and distant nodes. Additionally, an adaptive mechanism for adjusting the depth and breadth of the graph is incorporated, progressively improving the model's capacity to handle long-distance dependencies. \n\nSemantic units in the model are composed of words in conjunction with topics. Each sentence is interconnected with the words and topics it encompasses, yet subjects and words lack direct edges between them. Edge features are incorporated into the calculations of the graph attention layer, facilitating information transfer between nodes. Given the varying requirements for propagation width and depth across nodes of different granularities, adaptability in both dimensions is introduced to enhance the model's performance.\n\n## CONCLUSION\nThe research introduces a multi-granularity adaptive automatic summarization model predicated on a heterogeneous graph neural network. This approach innovatively incorporates topic semantic units into the heterogeneous graph, thereby enriching the complexity of inter-sentence relations. Attention is also paid to the relative constancy of attention coefficients and the significance of edge features. Adaptive techniques are employed to update nodal characteristics across varying granularities, both in terms of breadth and depth. The proposed model further allows for effortless inclusion of document-level nodes into a single-document framework, thereby simplifying the construction of multi-document summarization models. The designed graph structureincorporating sentences, topic words, and individual words-can be seamlessly integrated as a module within existing generative models. This integrated module efficiently encodes sentence-level information and channels it into broader linguistic and thematic structures through the graph architecture. Our model, serving as an encoding module, can be applied to other datasets by simply extracting sentences, topics, and words from the dataset and incorporating them into the corresponding graph structure nodes. Performance assessments on the CNN/DailyMail and Multi-News datasets reveal that the model delivers commendable results, particularly when benchmarked against models not leveraging BERT.",
            "reference_string": "[266244733 | Su et al. | 2023 | Citations: 1]"
        },
        {
            "title": "Pruned Graph Neural Network for Short Story Ordering",
            "venue": "International Conference of the Italian Association for Artificial Intelligence",
            "year": 2022,
            "reference_count": 52,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2203.06778",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.06778, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2124373879",
                    "name": "Melika Golestani"
                },
                {
                    "authorId": "19218725",
                    "name": "Zeinab Borhanifard"
                },
                {
                    "authorId": "51133923",
                    "name": "Farnaz Tahmasebian"
                },
                {
                    "authorId": "3054779",
                    "name": "Heshaam Faili"
                }
            ],
            "abstract": "Text coherence is a fundamental problem in natural language generation and understanding. Organizing sentences into an order that maximizes coherence is known as sentence ordering. This paper is proposing a new approach based on the graph neural network approach to encode a set of sentences and learn orderings of short stories. We propose a new method for constructing sentence-entity graphs of short stories to create the edges between sentences and reduce noise in our graph by replacing the pronouns with their referring entities. We improve the sentence ordering by introducing an aggregation method based on majority voting of state-of-the-art methods and our proposed one. Our approach employs a BERT-based model to learn semantic representations of the sentences. The results demonstrate that the proposed method significantly outperforms existing baselines on a corpus of short stories with a new state-of-the-art performance in terms of Perfect Match Ratio (PMR) and Kendall's Tau (Tau) metrics. More precisely, our method increases PMR and Tau criteria by more than 5% and 4.3%, respectively. These outcomes highlight the benefit of forming the edges between sentences based on their cosine similarity. We also observe that replacing pronouns with their referring entities effectively encodes sentences in sentence-entity graphs.",
            "corpus_id": 247446769,
            "sentences": [
                {
                    "corpus_id": "247446769",
                    "title": "Pruned Graph Neural Network for Short Story Ordering",
                    "text": "Graph neural networks (GNN) have shown to be effective in NLP applications, including syntactic dependency trees (Marcheggiani and Titov, 2017), neural machine translation (Beck et al., 2018), knowledge graphs (Wang et al., 2018), semantic graphs (Song et al., 2018), sequence-to-graph learning (Johnson, 2017), graph-to-sequence learning (Beck et al., 2018), sentence ordering (Yin et al., 2019), and multi-document summarization (Christensen et al., 2013;Yasunaga et al., 2017). \n\nIn particular, text classification is a common application of GNNs in natural language processing. A GNN infers document labels based on the relationships among documents or words (Hamilton et al., 2017). Christensen et al. (2013) used a GNN in multi-document summarization. They create multi-document graphs which determine pairwise ordering constraints of sentences based on the discourse relationship between them. Kipf and Welling (2017) proposed Graph Convolutional Networks (GCN), which is used in Yasunaga et al. (2017) to generate sentence relation graphs. The final sentence embeddings indicate the graph representation and are utilized as inputs to achieve satisfactory results on multi-document summarization. \n\nAnother method is presented in Marcheggiani and Titov (2017) where a syntactic GCN is developed with a CNN/RNN as sentence encoder. The GCN indicates syntactic relations between words in a sentence. In a more recent work, Yin et al. (2019) proposed a graph-based neural network for sentence ordering, in which paragraphs are modeled as graphs where sentences and entities are the nodes. The method showed improvement in evaluation metrics for sentence ordering task. In this work, we explore the use of GRN for NLP tasks, es-pecially to perform sentence-ordering on a corpus of short stories.",
                    "score": 0.5642473185802919,
                    "section_title": "Graph Neural Networks in NLP",
                    "char_start_offset": 7547,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 480
                        },
                        {
                            "start": 483,
                            "end": 581
                        },
                        {
                            "start": 582,
                            "end": 687
                        },
                        {
                            "start": 688,
                            "end": 757
                        },
                        {
                            "start": 758,
                            "end": 900
                        },
                        {
                            "start": 901,
                            "end": 1047
                        },
                        {
                            "start": 1048,
                            "end": 1203
                        },
                        {
                            "start": 1206,
                            "end": 1337
                        },
                        {
                            "start": 1338,
                            "end": 1404
                        },
                        {
                            "start": 1405,
                            "end": 1592
                        },
                        {
                            "start": 1593,
                            "end": 1672
                        },
                        {
                            "start": 1673,
                            "end": 1798
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 113,
                            "end": 143,
                            "matchedPaperCorpusId": "16839291"
                        },
                        {
                            "start": 172,
                            "end": 191,
                            "matchedPaperCorpusId": "49430686"
                        },
                        {
                            "start": 210,
                            "end": 229,
                            "matchedPaperCorpusId": "53082628"
                        },
                        {
                            "start": 247,
                            "end": 266,
                            "matchedPaperCorpusId": "52115592"
                        },
                        {
                            "start": 295,
                            "end": 310,
                            "matchedPaperCorpusId": "29504454"
                        },
                        {
                            "start": 339,
                            "end": 358,
                            "matchedPaperCorpusId": "49430686"
                        },
                        {
                            "start": 378,
                            "end": 396,
                            "matchedPaperCorpusId": "199466313"
                        },
                        {
                            "start": 457,
                            "end": 479,
                            "matchedPaperCorpusId": "6532096"
                        },
                        {
                            "start": 663,
                            "end": 686,
                            "matchedPaperCorpusId": "4755450"
                        },
                        {
                            "start": 987,
                            "end": 1009,
                            "matchedPaperCorpusId": "6532096"
                        },
                        {
                            "start": 1237,
                            "end": 1266,
                            "matchedPaperCorpusId": "16839291"
                        },
                        {
                            "start": 1428,
                            "end": 1445,
                            "matchedPaperCorpusId": "199466313"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97265625
                }
            ],
            "relevance_judgement": 0.97265625,
            "relevance_judgment_input_expanded": "# Title: Pruned Graph Neural Network for Short Story Ordering\n# Venue: International Conference of the Italian Association for Artificial Intelligence\n# Authors: Melika Golestani, Zeinab Borhanifard, Farnaz Tahmasebian, Heshaam Faili\n## Abstract\nText coherence is a fundamental problem in natural language generation and understanding. Organizing sentences into an order that maximizes coherence is known as sentence ordering. This paper is proposing a new approach based on the graph neural network approach to encode a set of sentences and learn orderings of short stories. We propose a new method for constructing sentence-entity graphs of short stories to create the edges between sentences and reduce noise in our graph by replacing the pronouns with their referring entities. We improve the sentence ordering by introducing an aggregation method based on majority voting of state-of-the-art methods and our proposed one. Our approach employs a BERT-based model to learn semantic representations of the sentences. The results demonstrate that the proposed method significantly outperforms existing baselines on a corpus of short stories with a new state-of-the-art performance in terms of Perfect Match Ratio (PMR) and Kendall's Tau (Tau) metrics. More precisely, our method increases PMR and Tau criteria by more than 5% and 4.3%, respectively. These outcomes highlight the benefit of forming the edges between sentences based on their cosine similarity. We also observe that replacing pronouns with their referring entities effectively encodes sentences in sentence-entity graphs.\n## Graph Neural Networks in NLP\nGraph neural networks (GNN) have shown to be effective in NLP applications, including syntactic dependency trees (Marcheggiani and Titov, 2017), neural machine translation (Beck et al., 2018), knowledge graphs (Wang et al., 2018), semantic graphs (Song et al., 2018), sequence-to-graph learning (Johnson, 2017), graph-to-sequence learning (Beck et al., 2018), sentence ordering (Yin et al., 2019), and multi-document summarization (Christensen et al., 2013;Yasunaga et al., 2017). \n\nIn particular, text classification is a common application of GNNs in natural language processing. A GNN infers document labels based on the relationships among documents or words (Hamilton et al., 2017). Christensen et al. (2013) used a GNN in multi-document summarization. They create multi-document graphs which determine pairwise ordering constraints of sentences based on the discourse relationship between them. Kipf and Welling (2017) proposed Graph Convolutional Networks (GCN), which is used in Yasunaga et al. (2017) to generate sentence relation graphs. The final sentence embeddings indicate the graph representation and are utilized as inputs to achieve satisfactory results on multi-document summarization. \n\nAnother method is presented in Marcheggiani and Titov (2017) where a syntactic GCN is developed with a CNN/RNN as sentence encoder. The GCN indicates syntactic relations between words in a sentence. In a more recent work, Yin et al. (2019) proposed a graph-based neural network for sentence ordering, in which paragraphs are modeled as graphs where sentences and entities are the nodes. The method showed improvement in evaluation metrics for sentence ordering task. In this work, we explore the use of GRN for NLP tasks, es-pecially to perform sentence-ordering on a corpus of short stories.",
            "reference_string": "[247446769 | Golestani et al. | 2022 | Citations: 0]"
        },
        {
            "title": "Topic-Selective Graph Network for Topic-Focused Summarization",
            "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining",
            "year": 2023,
            "reference_count": 35,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2302.13106",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.13106, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1764406",
                    "name": "Shi Zesheng"
                },
                {
                    "authorId": "2110348767",
                    "name": "Yucheng Zhou"
                }
            ],
            "abstract": "Due to the success of the pre-trained language model (PLM), existing PLM-based summarization models show their powerful generative capability. However, these models are trained on general-purpose summarization datasets, leading to generated summaries failing to satisfy the needs of different readers. To generate summaries with topics, many efforts have been made on topic-focused summarization. However, these works generate a summary only guided by a prompt comprising topic words. Despite their success, these methods still ignore the disturbance of sentences with non-relevant topics and only conduct cross-interaction between tokens by attention module. To address this issue, we propose a topic-arc recognition objective and topic-selective graph network. First, the topic-arc recognition objective is used to model training, which endows the capability to discriminate topics for the model. Moreover, the topic-selective graph network can conduct topic-guided cross-interaction on sentences based on the results of topic-arc recognition. In the experiments, we conduct extensive evaluations on NEWTS and COVIDET datasets. Results show that our methods achieve state-of-the-art performance.",
            "corpus_id": 257219819,
            "sentences": [
                {
                    "corpus_id": "257219819",
                    "title": "Topic-Selective Graph Network for Topic-Focused Summarization",
                    "text": "Graph neural network [20] has been valued in the field of deep learning for its excellent processing ability on unstructured data and node-centric information aggregation mode. With the advance of graph neural networks, there are many graph networks with special structures, e.g., GCN [21], GAT [27], HAN [30] and r-GCN [21]. Moreover, GNN is often used for downstream tasks such as text classification, information extraction, and text generation. In text summarization, Wang et al. [28] propose a heterogeneous graph-based neural network for extracting summaries, which contains semantic nodes of different granularity levels except sentences. These extra nodes act as \"intermediaries\" between sentences and enrich cross-sentence relations. The introduction of document nodes allows the graph structure to be flexibly extended from a single document setup to multiple documents. Another work [8] proposes a multiplex graph summary (Multi-GraS) model based on multiplex graph convolutional networks that can be used to extract text summaries. This model not only considers Various types of intersentential relations (such as semantic similarity and natural connection), and intra-sentential relations (such as semantic and syntactic relations between words) are also modeled.",
                    "score": 0.6282457414339021,
                    "section_title": "Graph Neural Network",
                    "char_start_offset": 7591,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 176
                        },
                        {
                            "start": 177,
                            "end": 325
                        },
                        {
                            "start": 326,
                            "end": 448
                        },
                        {
                            "start": 449,
                            "end": 645
                        },
                        {
                            "start": 646,
                            "end": 742
                        },
                        {
                            "start": 743,
                            "end": 880
                        },
                        {
                            "start": 881,
                            "end": 1043
                        },
                        {
                            "start": 1044,
                            "end": 1276
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 21,
                            "end": 25,
                            "matchedPaperCorpusId": "4675456"
                        },
                        {
                            "start": 285,
                            "end": 289,
                            "matchedPaperCorpusId": "5458500"
                        },
                        {
                            "start": 295,
                            "end": 299,
                            "matchedPaperCorpusId": "3292002"
                        },
                        {
                            "start": 305,
                            "end": 309,
                            "matchedPaperCorpusId": "6857205"
                        },
                        {
                            "start": 320,
                            "end": 324,
                            "matchedPaperCorpusId": "5458500"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97119140625
                }
            ],
            "relevance_judgement": 0.97119140625,
            "relevance_judgment_input_expanded": "# Title: Topic-Selective Graph Network for Topic-Focused Summarization\n# Venue: Pacific-Asia Conference on Knowledge Discovery and Data Mining\n# Authors: Shi Zesheng, Yucheng Zhou\n## Abstract\nDue to the success of the pre-trained language model (PLM), existing PLM-based summarization models show their powerful generative capability. However, these models are trained on general-purpose summarization datasets, leading to generated summaries failing to satisfy the needs of different readers. To generate summaries with topics, many efforts have been made on topic-focused summarization. However, these works generate a summary only guided by a prompt comprising topic words. Despite their success, these methods still ignore the disturbance of sentences with non-relevant topics and only conduct cross-interaction between tokens by attention module. To address this issue, we propose a topic-arc recognition objective and topic-selective graph network. First, the topic-arc recognition objective is used to model training, which endows the capability to discriminate topics for the model. Moreover, the topic-selective graph network can conduct topic-guided cross-interaction on sentences based on the results of topic-arc recognition. In the experiments, we conduct extensive evaluations on NEWTS and COVIDET datasets. Results show that our methods achieve state-of-the-art performance.\n## Graph Neural Network\nGraph neural network [20] has been valued in the field of deep learning for its excellent processing ability on unstructured data and node-centric information aggregation mode. With the advance of graph neural networks, there are many graph networks with special structures, e.g., GCN [21], GAT [27], HAN [30] and r-GCN [21]. Moreover, GNN is often used for downstream tasks such as text classification, information extraction, and text generation. In text summarization, Wang et al. [28] propose a heterogeneous graph-based neural network for extracting summaries, which contains semantic nodes of different granularity levels except sentences. These extra nodes act as \"intermediaries\" between sentences and enrich cross-sentence relations. The introduction of document nodes allows the graph structure to be flexibly extended from a single document setup to multiple documents. Another work [8] proposes a multiplex graph summary (Multi-GraS) model based on multiplex graph convolutional networks that can be used to extract text summaries. This model not only considers Various types of intersentential relations (such as semantic similarity and natural connection), and intra-sentential relations (such as semantic and syntactic relations between words) are also modeled.",
            "reference_string": "[257219819 | Zesheng et al. | 2023 | Citations: 4]"
        },
        {
            "title": "HEGEL: Hypergraph Transformer for Long Document Summarization",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 44,
            "citation_count": 45,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2210.04126",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.04126, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2135688409",
                    "name": "Haopeng Zhang"
                },
                {
                    "authorId": null,
                    "name": "Xiao Liu"
                },
                {
                    "authorId": "2168548350",
                    "name": "Jiawei Zhang"
                }
            ],
            "abstract": "Extractive summarization for long documents is challenging due to the extended structured input context. The long-distance sentence dependency hinders cross-sentence relations modeling, the critical step of extractive summarization. This paper proposes HEGEL, a hypergraph neural network for long document summarization by capturing high-order cross-sentence relations. HEGEL updates and learns effective sentence representations with hypergraph transformer layers and fuses different types of sentence dependencies, including latent topics, keywords coreference, and section structure. We validate HEGEL by conducting extensive experiments on two benchmark datasets, and experimental results demonstrate the effectiveness and efficiency of HEGEL.",
            "corpus_id": 252780923,
            "sentences": [
                {
                    "corpus_id": "252780923",
                    "title": "HEGEL: Hypergraph Transformer for Long Document Summarization",
                    "text": "Graph-based models have been exploited for extractive summarization to capture cross-sentence dependencies. Unsupervised graph summarization methods rely on graph connectivity to score and rank sentences Zheng and Lapata, 2019;Dong et al., 2020). Researchers also explore supervised graph neural networks for summarization. Yasunaga et al. (2017) applied Graph Convolutional Network (GCN) on the approximate discourse graph. Xu et al. (2019) proposed to apply GCN on structural discourse graphs based on RST trees and coreference mentions. Cui et al. (2020) leveraged topical information by building topicsentence graphs. Recently, Wang et al. (2020) proposed to construct word-document heterogeneous graphs and use word nodes as the intermediary between sentences. Jing et al. (2021) proposed to use multiplex graph to consider different sentence relations. Our paper follows this line of work on developing novel graph neural networks for single document extractive summarization. The main difference is that we construct a hypergraph from a document that could capture high-order crosssentence relations instead of pairwise relations, and fuse different types of sentence dependencies, including section structure, latent topics, and keywords coreference.",
                    "score": 0.5310019991518748,
                    "section_title": "Graph based summarization",
                    "char_start_offset": 5898,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97119140625
                },
                {
                    "corpus_id": "252780923",
                    "title": "HEGEL: Hypergraph Transformer for Long Document Summarization",
                    "text": "in (Xu et al., 2019), approximate discourse graph in (Yasunaga et al., 2017), topicsentence graph in (Cui and Hu, 2021) and worddocument heterogeneous graph in (Wang et al., 2020). However, the usability of these approaches is limited by the following two aspects: (1) These methods only model the pairwise interaction between sentences, while sentence interactions could be triadic, tetradic, or of a higher-order in natural language (Ding et al., 2020). How to capture high-order cross-sentence relations for extractive summarization is still an open question.\n\n(2) These graph-based approaches rely on either semantic or discourses structure cross-sentence relation but are incapable of fusing sentence interactions from different perspectives. Sentences within a document could have various types of interactions, such as embedding similarity, keywords coreference, topical modeling from the semantic perspective, and section or rhetorical structure from the discourse perspective. Capturing multi-type crosssentence relations could benefit sentence representation learning and sentence salience modeling. Figure 1 is an illustration showing different types of sentence interactions provide different connectivity for document graph construction, which covers both local and global context information.\n\nTo address the above issues, we propose HEGEL (HypErGraph transformer for Extractive Long document summarization), a graph-based model designed for summarizing long documents with rich discourse information. To better model high-order cross-sentence relations, we represent a document as a hypergraph, a generalization of graph structure, in which an edge can join any number of vertices. We then introduce three types of hyperedges that model sentence relations from different perspectives, including section structure, latent topic, and keywords coreference, respectively. We also propose hypergraph transformer layers to update and learn effective sentence embeddings on hypergraphs. We validate HEGEL by conducting extensive experiments and analyses on two benchmark datasets, and experimental results demonstrate the effectiveness and efficiency of HEGEL. We highlight our contributions as follows:\n\n(i) We propose a hypergraph neural model, HEGEL, for long document summarization. To the best of our knowledge, we are the first to model high-order cross-sentence relations with hypergraphs",
                    "score": 0.6314765615072985,
                    "section_title": "Introduction",
                    "char_start_offset": 2030,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 101,
                            "end": 119,
                            "matchedPaperCorpusId": "239050558"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96484375
                }
            ],
            "relevance_judgement": 0.97119140625,
            "relevance_judgment_input_expanded": "# Title: HEGEL: Hypergraph Transformer for Long Document Summarization\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Haopeng Zhang, Xiao Liu, Jiawei Zhang\n## Abstract\nExtractive summarization for long documents is challenging due to the extended structured input context. The long-distance sentence dependency hinders cross-sentence relations modeling, the critical step of extractive summarization. This paper proposes HEGEL, a hypergraph neural network for long document summarization by capturing high-order cross-sentence relations. HEGEL updates and learns effective sentence representations with hypergraph transformer layers and fuses different types of sentence dependencies, including latent topics, keywords coreference, and section structure. We validate HEGEL by conducting extensive experiments on two benchmark datasets, and experimental results demonstrate the effectiveness and efficiency of HEGEL.\n## Introduction\nin (Xu et al., 2019), approximate discourse graph in (Yasunaga et al., 2017), topicsentence graph in (Cui and Hu, 2021) and worddocument heterogeneous graph in (Wang et al., 2020). However, the usability of these approaches is limited by the following two aspects: (1) These methods only model the pairwise interaction between sentences, while sentence interactions could be triadic, tetradic, or of a higher-order in natural language (Ding et al., 2020). How to capture high-order cross-sentence relations for extractive summarization is still an open question.\n\n(2) These graph-based approaches rely on either semantic or discourses structure cross-sentence relation but are incapable of fusing sentence interactions from different perspectives. Sentences within a document could have various types of interactions, such as embedding similarity, keywords coreference, topical modeling from the semantic perspective, and section or rhetorical structure from the discourse perspective. Capturing multi-type crosssentence relations could benefit sentence representation learning and sentence salience modeling. Figure 1 is an illustration showing different types of sentence interactions provide different connectivity for document graph construction, which covers both local and global context information.\n\nTo address the above issues, we propose HEGEL (HypErGraph transformer for Extractive Long document summarization), a graph-based model designed for summarizing long documents with rich discourse information. To better model high-order cross-sentence relations, we represent a document as a hypergraph, a generalization of graph structure, in which an edge can join any number of vertices. We then introduce three types of hyperedges that model sentence relations from different perspectives, including section structure, latent topic, and keywords coreference, respectively. We also propose hypergraph transformer layers to update and learn effective sentence embeddings on hypergraphs. We validate HEGEL by conducting extensive experiments and analyses on two benchmark datasets, and experimental results demonstrate the effectiveness and efficiency of HEGEL. We highlight our contributions as follows:\n\n(i) We propose a hypergraph neural model, HEGEL, for long document summarization. To the best of our knowledge, we are the first to model high-order cross-sentence relations with hypergraphs\n\n## Graph based summarization\nGraph-based models have been exploited for extractive summarization to capture cross-sentence dependencies. Unsupervised graph summarization methods rely on graph connectivity to score and rank sentences Zheng and Lapata, 2019;Dong et al., 2020). Researchers also explore supervised graph neural networks for summarization. Yasunaga et al. (2017) applied Graph Convolutional Network (GCN) on the approximate discourse graph. Xu et al. (2019) proposed to apply GCN on structural discourse graphs based on RST trees and coreference mentions. Cui et al. (2020) leveraged topical information by building topicsentence graphs. Recently, Wang et al. (2020) proposed to construct word-document heterogeneous graphs and use word nodes as the intermediary between sentences. Jing et al. (2021) proposed to use multiplex graph to consider different sentence relations. Our paper follows this line of work on developing novel graph neural networks for single document extractive summarization. The main difference is that we construct a hypergraph from a document that could capture high-order crosssentence relations instead of pairwise relations, and fuse different types of sentence dependencies, including section structure, latent topics, and keywords coreference.",
            "reference_string": "[252780923 | Zhang et al. | 2022 | Citations: 45]"
        },
        {
            "title": "Compressed Heterogeneous Graph for Abstractive Multi-Document Summarization",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2023,
            "reference_count": 35,
            "citation_count": 11,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2303.06565",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.06565, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2143544766",
                    "name": "Miao Li"
                },
                {
                    "authorId": "2149459181",
                    "name": "Jianzhong Qi"
                },
                {
                    "authorId": "1800564",
                    "name": "Jey Han Lau"
                }
            ],
            "abstract": "Multi-document summarization (MDS) aims to generate a summary for a number of related documents. We propose HGSum \u2014 an MDS model that extends an encoder-decoder architecture to incorporate a heterogeneous graph to represent different semantic units (e.g., words and sentences) of the documents. This contrasts with existing MDS models which do not consider different edge types of graphs and as such do not capture the diversity of relationships in the documents. To preserve only key information and relationships of the documents in the heterogeneous graph, HGSum uses graph pooling to compress the input graph. And to guide HGSum to learn the compression, we introduce an additional objective that maximizes the similarity between the compressed graph and the graph constructed from the ground-truth summary during training. HGSum is trained end-to-end with the graph similarity and standard cross-entropy objectives. Experimental results over Multi-News, WCEP-100, and Arxiv show that HGSum outperforms state-of-the-art MDS models. The code for our model and experiments is available at: https://github.com/oaimli/HGSum.",
            "corpus_id": 257496469,
            "sentences": [
                {
                    "corpus_id": "257496469",
                    "title": "Compressed Heterogeneous Graph for Abstractive Multi-Document Summarization",
                    "text": "Abstractive Multi-Document Summarization PLM-Based Models Recent PLM-based models have shown strong performance for abstractive text summarization tasks. These models follow a Transformerbased (Vaswani et al. 2017) encoder-decoder architecture. For example, general-purpose PLMs such as T5 (Raffel et al. 2020), BART (Lewis et al. 2020), and LED (Beltagy, Peters, and Cohan 2020) can be fine-tuned for abstractive text summarization. PEGASUS (Zhang et al. 2020a) is a strong PLM-based model pre-trained with an objective that predicts gap sentences as a pseudo summary. These models can be used for MDS by concatenating the source documents into a single document. PRIMERA (Xiao et al. 2022) has the same architecture as LED, but is designed for MDS specifically in that it is pre-trained to generate pseudo summaries -text spans that are automatically extracted based on the entity salience. Although these models show impressive performances and can even handle zero-shot cases, they use a flat concatenation of the input documents, which limits their capability in learning the cross-document relationships among different semantic units. \n\nGraph-Based Models Although graphs are commonly used to boost text summarization (Wu et al. 2021b;You et al. 2022;Song and King 2022), there are only a handful of models which have been proposed to use graphs to encode the documents in abstractive MDS (Li et al. 2020;Jin, Wang, and Wan 2020;Li and Zhuge 2021;Cui and Hu 2021). Most of these models only leverage homogeneous graphs as they do not consider different edge types of graphs. For example, MGSum (Jin, Wang, and Wan 2020) constructs a threelevel (i.e., document, sentence, and word levels) hierarchical graph and learns semantics with a multi-level interaction network. GraphSum (Li et al. 2020) constructs a similarity graph over the paragraphs. It learns a graph representation for the paragraphs and uses a hierarchical graph attention mechanism to guide the summary generation process.",
                    "score": 0.5715745543620938,
                    "section_title": "Related Work",
                    "char_start_offset": 23964,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 153
                        },
                        {
                            "start": 154,
                            "end": 244
                        },
                        {
                            "start": 245,
                            "end": 433
                        },
                        {
                            "start": 434,
                            "end": 569
                        },
                        {
                            "start": 570,
                            "end": 664
                        },
                        {
                            "start": 665,
                            "end": 892
                        },
                        {
                            "start": 893,
                            "end": 1141
                        },
                        {
                            "start": 1144,
                            "end": 1471
                        },
                        {
                            "start": 1472,
                            "end": 1581
                        },
                        {
                            "start": 1582,
                            "end": 1774
                        },
                        {
                            "start": 1775,
                            "end": 1851
                        },
                        {
                            "start": 1852,
                            "end": 1994
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 290,
                            "end": 310,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 442,
                            "end": 461,
                            "matchedPaperCorpusId": "209405420"
                        },
                        {
                            "start": 673,
                            "end": 690,
                            "matchedPaperCorpusId": "247519084"
                        },
                        {
                            "start": 1225,
                            "end": 1242,
                            "matchedPaperCorpusId": "235187330"
                        },
                        {
                            "start": 1242,
                            "end": 1258,
                            "matchedPaperCorpusId": "250390945"
                        },
                        {
                            "start": 1258,
                            "end": 1277,
                            "matchedPaperCorpusId": "250301954"
                        },
                        {
                            "start": 1396,
                            "end": 1412,
                            "matchedPaperCorpusId": "218718706"
                        },
                        {
                            "start": 1412,
                            "end": 1436,
                            "matchedPaperCorpusId": "220045815"
                        },
                        {
                            "start": 1436,
                            "end": 1454,
                            "matchedPaperCorpusId": "196170254"
                        },
                        {
                            "start": 1454,
                            "end": 1470,
                            "matchedPaperCorpusId": "239050558"
                        },
                        {
                            "start": 1601,
                            "end": 1626,
                            "matchedPaperCorpusId": "220045815"
                        },
                        {
                            "start": 1784,
                            "end": 1799,
                            "matchedPaperCorpusId": "218718706"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97021484375
                }
            ],
            "relevance_judgement": 0.97021484375,
            "relevance_judgment_input_expanded": "# Title: Compressed Heterogeneous Graph for Abstractive Multi-Document Summarization\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Miao Li, Jianzhong Qi, Jey Han Lau\n## Abstract\nMulti-document summarization (MDS) aims to generate a summary for a number of related documents. We propose HGSum \u2014 an MDS model that extends an encoder-decoder architecture to incorporate a heterogeneous graph to represent different semantic units (e.g., words and sentences) of the documents. This contrasts with existing MDS models which do not consider different edge types of graphs and as such do not capture the diversity of relationships in the documents. To preserve only key information and relationships of the documents in the heterogeneous graph, HGSum uses graph pooling to compress the input graph. And to guide HGSum to learn the compression, we introduce an additional objective that maximizes the similarity between the compressed graph and the graph constructed from the ground-truth summary during training. HGSum is trained end-to-end with the graph similarity and standard cross-entropy objectives. Experimental results over Multi-News, WCEP-100, and Arxiv show that HGSum outperforms state-of-the-art MDS models. The code for our model and experiments is available at: https://github.com/oaimli/HGSum.\n## Related Work\nAbstractive Multi-Document Summarization PLM-Based Models Recent PLM-based models have shown strong performance for abstractive text summarization tasks. These models follow a Transformerbased (Vaswani et al. 2017) encoder-decoder architecture. For example, general-purpose PLMs such as T5 (Raffel et al. 2020), BART (Lewis et al. 2020), and LED (Beltagy, Peters, and Cohan 2020) can be fine-tuned for abstractive text summarization. PEGASUS (Zhang et al. 2020a) is a strong PLM-based model pre-trained with an objective that predicts gap sentences as a pseudo summary. These models can be used for MDS by concatenating the source documents into a single document. PRIMERA (Xiao et al. 2022) has the same architecture as LED, but is designed for MDS specifically in that it is pre-trained to generate pseudo summaries -text spans that are automatically extracted based on the entity salience. Although these models show impressive performances and can even handle zero-shot cases, they use a flat concatenation of the input documents, which limits their capability in learning the cross-document relationships among different semantic units. \n\nGraph-Based Models Although graphs are commonly used to boost text summarization (Wu et al. 2021b;You et al. 2022;Song and King 2022), there are only a handful of models which have been proposed to use graphs to encode the documents in abstractive MDS (Li et al. 2020;Jin, Wang, and Wan 2020;Li and Zhuge 2021;Cui and Hu 2021). Most of these models only leverage homogeneous graphs as they do not consider different edge types of graphs. For example, MGSum (Jin, Wang, and Wan 2020) constructs a threelevel (i.e., document, sentence, and word levels) hierarchical graph and learns semantics with a multi-level interaction network. GraphSum (Li et al. 2020) constructs a similarity graph over the paragraphs. It learns a graph representation for the paragraphs and uses a hierarchical graph attention mechanism to guide the summary generation process.",
            "reference_string": "[257496469 | Li et al. | 2023 | Citations: 11]"
        },
        {
            "title": "Hierarchical Heterogeneous Graph Attention Network for Syntax-Aware Summarization",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2022,
            "reference_count": 52,
            "citation_count": 26,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/21385/21134",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v36i10.21385?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v36i10.21385, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2114791973",
                    "name": "Zixing Song"
                },
                {
                    "authorId": "145310663",
                    "name": "Irwin King"
                }
            ],
            "abstract": "The task of summarization often requires a non-trivial understanding of the given text at the semantic level. In this work, we essentially incorporate the constituent structure into the single document summarization via the Graph Neural Networks to learn the semantic meaning of tokens. More specifically, we propose a novel hierarchical heterogeneous graph attention network over constituency-based parse trees for syntax-aware summarization. This approach reflects psychological findings that humans will pinpoint specific selection patterns to construct summaries hierarchically. Extensive experiments demonstrate that our model is effective for both the abstractive and extractive summarization tasks on five benchmark datasets from various domains. Moreover, further performance improvement can be obtained by virtue of state-of-the-art pre-trained models.",
            "corpus_id": 250301954,
            "sentences": [
                {
                    "corpus_id": "250301954",
                    "title": "Hierarchical Heterogeneous Graph Attention Network for Syntax-Aware Summarization",
                    "text": "The task of summarization often requires a non-trivial understanding of the given text at the semantic level. In this work, we essentially incorporate the constituent structure into the single document summarization via the Graph Neural Networks to learn the semantic meaning of tokens. More specifically, we propose a novel hierarchical heterogeneous graph attention network over constituency-based parse trees for syntax-aware summarization. This approach reflects psychological findings that humans will pinpoint specific selection patterns to construct summaries hierarchically. Extensive experiments demonstrate that our model is effective for both the abstractive and extractive summarization tasks on five benchmark datasets from various domains. Moreover, further performance improvement can be obtained by virtue of state-of-the-art pre-trained models.",
                    "score": 0.6235911521508446,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96875
                }
            ],
            "relevance_judgement": 0.96875,
            "relevance_judgment_input_expanded": "# Title: Hierarchical Heterogeneous Graph Attention Network for Syntax-Aware Summarization\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Zixing Song, Irwin King\n## Abstract\nThe task of summarization often requires a non-trivial understanding of the given text at the semantic level. In this work, we essentially incorporate the constituent structure into the single document summarization via the Graph Neural Networks to learn the semantic meaning of tokens. More specifically, we propose a novel hierarchical heterogeneous graph attention network over constituency-based parse trees for syntax-aware summarization. This approach reflects psychological findings that humans will pinpoint specific selection patterns to construct summaries hierarchically. Extensive experiments demonstrate that our model is effective for both the abstractive and extractive summarization tasks on five benchmark datasets from various domains. Moreover, further performance improvement can be obtained by virtue of state-of-the-art pre-trained models.\n",
            "reference_string": "[250301954 | Song et al. | 2022 | Citations: 26]"
        },
        {
            "title": "GoSum: Extractive Summarization of Long Documents by Reinforcement Learning and Graph Organized discourse state",
            "venue": "Knowledge and Information Systems",
            "year": 2022,
            "reference_count": 37,
            "citation_count": 12,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2211.10247",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.10247, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "14889056",
                    "name": "Junyi Bian"
                },
                {
                    "authorId": "2145264230",
                    "name": "Xiaodi Huang"
                },
                {
                    "authorId": "2157473395",
                    "name": "Hong Zhou"
                },
                {
                    "authorId": "7472263",
                    "name": "Shanfeng Zhu"
                }
            ],
            "abstract": "Extracting summaries from long documents can be regarded as sentence classification using the structural information of the documents. How to use such structural information to summarize a document is challenging. In this paper, we propose GoSum, a novel graph and reinforcement learning based extractive model for long-paper summarization. In particular, GoSum encodes sentence states in reinforcement learning by building a heterogeneous graph for each input document at different discourse levels. An edge in the graph reflects the discourse hierarchy of a document for restraining the semantic drifts across section boundaries. We evaluate GoSum on two datasets of scientific articles summarization: PubMed and arXiv. The experimental results have demonstrated that GoSum achieve state-of-the-art results compared with strong baselines of both extractive and abstractive models. The ablation studies further validate that the performance of our GoSum benefits from the use of discourse information.",
            "corpus_id": 253708345,
            "sentences": [
                {
                    "corpus_id": "253708345",
                    "title": "GoSum: Extractive Summarization of Long Documents by Reinforcement Learning and Graph Organized discourse state",
                    "text": "In this paper, we have presented a novel approach called GoSum for extracting summaries from long documents. It effectively integrates reinforcement learning with a graph neural network. In particular, we have shown how graph-organized discourse information can be applied in reinforcement learning-based extractive summarization. Experimental results on the arXiv and PubMed datasets have demonstrated that GoSum achieves state-ofthe-art performance. The ablation experiments examine the effect of discourse information on Go-Sum. The results show that the performance of GoSum comes from the use of the hierarchical attribution of sentences and the semantic information about section titles of documents. With achieving satisfactory results in scientific literature, GoSum requires hierarchical discourse information about long texts as its inputs. In the future, we will attempt to automatically generate discourse information from documents.",
                    "score": 0.5114910977249096,
                    "section_title": "Conclusion",
                    "char_start_offset": 21927,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 108
                        },
                        {
                            "start": 109,
                            "end": 186
                        },
                        {
                            "start": 187,
                            "end": 330
                        },
                        {
                            "start": 331,
                            "end": 451
                        },
                        {
                            "start": 452,
                            "end": 531
                        },
                        {
                            "start": 532,
                            "end": 706
                        },
                        {
                            "start": 707,
                            "end": 850
                        },
                        {
                            "start": 851,
                            "end": 945
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96875
                }
            ],
            "relevance_judgement": 0.96875,
            "relevance_judgment_input_expanded": "# Title: GoSum: Extractive Summarization of Long Documents by Reinforcement Learning and Graph Organized discourse state\n# Venue: Knowledge and Information Systems\n# Authors: Junyi Bian, Xiaodi Huang, Hong Zhou, Shanfeng Zhu\n## Abstract\nExtracting summaries from long documents can be regarded as sentence classification using the structural information of the documents. How to use such structural information to summarize a document is challenging. In this paper, we propose GoSum, a novel graph and reinforcement learning based extractive model for long-paper summarization. In particular, GoSum encodes sentence states in reinforcement learning by building a heterogeneous graph for each input document at different discourse levels. An edge in the graph reflects the discourse hierarchy of a document for restraining the semantic drifts across section boundaries. We evaluate GoSum on two datasets of scientific articles summarization: PubMed and arXiv. The experimental results have demonstrated that GoSum achieve state-of-the-art results compared with strong baselines of both extractive and abstractive models. The ablation studies further validate that the performance of our GoSum benefits from the use of discourse information.\n## Conclusion\nIn this paper, we have presented a novel approach called GoSum for extracting summaries from long documents. It effectively integrates reinforcement learning with a graph neural network. In particular, we have shown how graph-organized discourse information can be applied in reinforcement learning-based extractive summarization. Experimental results on the arXiv and PubMed datasets have demonstrated that GoSum achieves state-ofthe-art performance. The ablation experiments examine the effect of discourse information on Go-Sum. The results show that the performance of GoSum comes from the use of the hierarchical attribution of sentences and the semantic information about section titles of documents. With achieving satisfactory results in scientific literature, GoSum requires hierarchical discourse information about long texts as its inputs. In the future, we will attempt to automatically generate discourse information from documents.",
            "reference_string": "[253708345 | Bian et al. | 2022 | Citations: 12]"
        },
        {
            "title": "Leveraging Knowledge-aware Methodologies for Multi-document Summarization",
            "venue": "The Web Conference",
            "year": 2024,
            "reference_count": 27,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3589335.3651262",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3589335.3651262?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3589335.3651262, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2163451228",
                    "name": "Yutong Qu"
                }
            ],
            "abstract": "With the development of information technology, a large amount of information and corpora has been incrementally sparked from the Web, stimulating an increasingly high demand for summarizing. Document Summarization is one of Natural Language Processing tasks, which aims to generate abridged versions of a given single or multiple documents as concise and coherent as possible while preserving salient information from the source texts. Recent research in the area has started to use knowledge graphs as they can capture more factual and applicable information from more facets along with source information, benefiting fact consistency and informativeness of generated summaries, rather than just from a linguistic perspective. However, there is no explicit investigation of the effects of different kinds of knowledge graphs on document summarization. The proposed method is to use structured informative and knowledgeable auxiliary information, especially knowledge graphs, into pre-trained summarization models, advancing summary qualities. Expected outcomes are exploring knowledge and knowledge graph incorporation for multi-document summarization, and achieving more informative, coherent, and factually consistent summaries.",
            "corpus_id": 269762702,
            "sentences": [
                {
                    "corpus_id": "269762702",
                    "title": "Leveraging Knowledge-aware Methodologies for Multi-document Summarization",
                    "text": "Along with the prosperity of knowledge-aware research in the natural language processing field, more and more document summarization models attempted to incorporate knowledge graphs to enhance the quality of generated summaries.SDS with KG Gunel et al. [6]  MDS with KG Zhou et al. [26] presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP.Pasunuru et al. [18] presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News [5] and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model [13] with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. [18] utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features.",
                    "score": 0.5926342385333716,
                    "section_title": "STATE OF THE ART",
                    "char_start_offset": 6022,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 228
                        },
                        {
                            "start": 228,
                            "end": 547
                        },
                        {
                            "start": 547,
                            "end": 740
                        },
                        {
                            "start": 740,
                            "end": 879
                        },
                        {
                            "start": 879,
                            "end": 1129
                        },
                        {
                            "start": 1129,
                            "end": 1335
                        },
                        {
                            "start": 1335,
                            "end": 1466
                        },
                        {
                            "start": 1466,
                            "end": 1729
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 253,
                            "end": 256,
                            "matchedPaperCorpusId": "204735695"
                        },
                        {
                            "start": 282,
                            "end": 286,
                            "matchedPaperCorpusId": "236478143"
                        },
                        {
                            "start": 895,
                            "end": 899,
                            "matchedPaperCorpusId": "235097309"
                        },
                        {
                            "start": 1112,
                            "end": 1115,
                            "matchedPaperCorpusId": "174799390"
                        },
                        {
                            "start": 1210,
                            "end": 1214,
                            "matchedPaperCorpusId": "204960716"
                        },
                        {
                            "start": 1525,
                            "end": 1529,
                            "matchedPaperCorpusId": "235097309"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96484375
                }
            ],
            "relevance_judgement": 0.96484375,
            "relevance_judgment_input_expanded": "# Title: Leveraging Knowledge-aware Methodologies for Multi-document Summarization\n# Venue: The Web Conference\n# Authors: Yutong Qu\n## Abstract\nWith the development of information technology, a large amount of information and corpora has been incrementally sparked from the Web, stimulating an increasingly high demand for summarizing. Document Summarization is one of Natural Language Processing tasks, which aims to generate abridged versions of a given single or multiple documents as concise and coherent as possible while preserving salient information from the source texts. Recent research in the area has started to use knowledge graphs as they can capture more factual and applicable information from more facets along with source information, benefiting fact consistency and informativeness of generated summaries, rather than just from a linguistic perspective. However, there is no explicit investigation of the effects of different kinds of knowledge graphs on document summarization. The proposed method is to use structured informative and knowledgeable auxiliary information, especially knowledge graphs, into pre-trained summarization models, advancing summary qualities. Expected outcomes are exploring knowledge and knowledge graph incorporation for multi-document summarization, and achieving more informative, coherent, and factually consistent summaries.\n## STATE OF THE ART\nAlong with the prosperity of knowledge-aware research in the natural language processing field, more and more document summarization models attempted to incorporate knowledge graphs to enhance the quality of generated summaries.SDS with KG Gunel et al. [6]  MDS with KG Zhou et al. [26] presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP.Pasunuru et al. [18] presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News [5] and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model [13] with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. [18] utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features.",
            "reference_string": "[269762702 | Qu | 2024 | Citations: 0]"
        },
        {
            "title": "GraphLSS: Integrating Lexical, Structural, and Semantic Features for Long Document Extractive Summarization",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 30,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.21315, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "72840732",
                    "name": "M. Bugue\u00f1o"
                },
                {
                    "authorId": "2328087303",
                    "name": "Hazem Abou Hamdan"
                },
                {
                    "authorId": "2304438177",
                    "name": "Gerard de Melo"
                }
            ],
            "abstract": "Heterogeneous graph neural networks have recently gained attention for long document summarization, modeling the extraction as a node classification task. Although effective, these models often require external tools or additional machine learning models to define graph components, producing highly complex and less intuitive structures. We present GraphLSS, a heterogeneous graph construction for long document extractive summarization, incorporating Lexical, Structural, and Semantic features. It defines two levels of information (words and sentences) and four types of edges (sentence semantic similarity, sentence occurrence order, word in sentence, and word semantic similarity) without any need for auxiliary learning models. Experiments on two benchmark datasets show that GraphLSS is competitive with top-performing graph-based methods, outperforming recent non-graph models. We release our code on GitHub.",
            "corpus_id": 273662136,
            "sentences": [
                {
                    "corpus_id": "273662136",
                    "title": "GraphLSS: Integrating Lexical, Structural, and Semantic Features for Long Document Extractive Summarization",
                    "text": "Graph Structure Developing an effective graph structure for summarization has been challenging, leading to a proliferation of diverse approaches. Wang et al. (2020) proposed connecting sentence nodes to word nodes by establishing undirected associations with the contained words. Subsequently, Jia et al. (2020) extended this by introducing named entity nodes and three other edge types: directed edges for tracking subsequent named entities and words in a sentence, directed edges for entities and words within a sentence, and undirected edges for sentence pairs with trigram overlap. \n\nTopic-GraphSum (Cui et al., 2020) was one of the first attempts to apply graph strategies to long document extractive summarization. It integrated a joint neural topic model to discover latent topics in a document, defining these as intermediate nodes to capture inter-sentence relationships across various genres and lengths. SSN (Cui and Hu, 2021) defined a sliding selector network with dynamic memory. SSN splits a given document into multiple segments, encodes them with BERT (Devlin et al., 2019), and selects salient sentences. Instead of representing the document as a graph, it uses a graph-based memory module, updated iteratively with a GAT (Veli\u010dkovi\u0107 et al., 2018), to allow information to flow across different windows. Heter-GraphLongSum (Phan et al., 2022) utilized words, sentences, and passages as nodes, while considering undirected edges for words in sentences, and directed edges for words in passages and passage to sentences. Instead of pre-trained embeddings, it used CNNs and bidirectional LSTMs for node encoding, yielding outstanding results. MTGNN-SUM (Doan et al., 2022) achieved similar results by capturing both inter and intra-sentence information when combining a homogeneous graph of sentence nodes with a heterogeneous graph of words and sentences, as in Wang et al. (2020). \n\nRecent studies underscore the importance of structural information in long document summarization. HEGEL (Zhang et al., 2022) modeled documents as hypergraphs, capturing semantic edges like keyword coreference, section structure, and latent topics.",
                    "score": 0.48051836594576036,
                    "section_title": "Previous Work",
                    "char_start_offset": 2025,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 145
                        },
                        {
                            "start": 146,
                            "end": 279
                        },
                        {
                            "start": 280,
                            "end": 585
                        },
                        {
                            "start": 588,
                            "end": 720
                        },
                        {
                            "start": 721,
                            "end": 914
                        },
                        {
                            "start": 915,
                            "end": 993
                        },
                        {
                            "start": 994,
                            "end": 1122
                        },
                        {
                            "start": 1123,
                            "end": 1321
                        },
                        {
                            "start": 1322,
                            "end": 1536
                        },
                        {
                            "start": 1537,
                            "end": 1657
                        },
                        {
                            "start": 1658,
                            "end": 1897
                        },
                        {
                            "start": 1900,
                            "end": 1998
                        },
                        {
                            "start": 1999,
                            "end": 2148
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 146,
                            "end": 164,
                            "matchedPaperCorpusId": "216552978"
                        },
                        {
                            "start": 294,
                            "end": 311,
                            "matchedPaperCorpusId": "226262332"
                        },
                        {
                            "start": 603,
                            "end": 621,
                            "matchedPaperCorpusId": "222310577"
                        },
                        {
                            "start": 919,
                            "end": 937,
                            "matchedPaperCorpusId": "235097475"
                        },
                        {
                            "start": 1069,
                            "end": 1090,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1240,
                            "end": 1265,
                            "matchedPaperCorpusId": "3292002"
                        },
                        {
                            "start": 1341,
                            "end": 1360,
                            "matchedPaperCorpusId": "252818946"
                        },
                        {
                            "start": 1668,
                            "end": 1687,
                            "matchedPaperCorpusId": "252819335"
                        },
                        {
                            "start": 1878,
                            "end": 1896,
                            "matchedPaperCorpusId": "216552978"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96484375
                },
                {
                    "corpus_id": "273662136",
                    "title": "GraphLSS: Integrating Lexical, Structural, and Semantic Features for Long Document Extractive Summarization",
                    "text": "Heterogeneous graph neural networks have recently gained attention for long document summarization, modeling the extraction as a node classification task. Although effective, these models often require external tools or additional machine learning models to define graph components, producing highly complex and less intuitive structures. We present GraphLSS, a heterogeneous graph construction for long document extractive summarization, incorporating Lexical, Structural, and Semantic features. It defines two levels of information (words and sentences) and four types of edges (sentence semantic similarity, sentence occurrence order, word in sentence, and word semantic similarity) without any need for auxiliary learning models. Experiments on two benchmark datasets show that GraphLSS is competitive with top-performing graph-based methods, outperforming recent non-graph models. We release our code on GitHub.",
                    "score": 0.5204356573180369,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9599609375
                }
            ],
            "relevance_judgement": 0.96484375,
            "relevance_judgment_input_expanded": "# Title: GraphLSS: Integrating Lexical, Structural, and Semantic Features for Long Document Extractive Summarization\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: M. Bugue\u00f1o, Hazem Abou Hamdan, Gerard de Melo\n## Abstract\nHeterogeneous graph neural networks have recently gained attention for long document summarization, modeling the extraction as a node classification task. Although effective, these models often require external tools or additional machine learning models to define graph components, producing highly complex and less intuitive structures. We present GraphLSS, a heterogeneous graph construction for long document extractive summarization, incorporating Lexical, Structural, and Semantic features. It defines two levels of information (words and sentences) and four types of edges (sentence semantic similarity, sentence occurrence order, word in sentence, and word semantic similarity) without any need for auxiliary learning models. Experiments on two benchmark datasets show that GraphLSS is competitive with top-performing graph-based methods, outperforming recent non-graph models. We release our code on GitHub.\n## Previous Work\nGraph Structure Developing an effective graph structure for summarization has been challenging, leading to a proliferation of diverse approaches. Wang et al. (2020) proposed connecting sentence nodes to word nodes by establishing undirected associations with the contained words. Subsequently, Jia et al. (2020) extended this by introducing named entity nodes and three other edge types: directed edges for tracking subsequent named entities and words in a sentence, directed edges for entities and words within a sentence, and undirected edges for sentence pairs with trigram overlap. \n\nTopic-GraphSum (Cui et al., 2020) was one of the first attempts to apply graph strategies to long document extractive summarization. It integrated a joint neural topic model to discover latent topics in a document, defining these as intermediate nodes to capture inter-sentence relationships across various genres and lengths. SSN (Cui and Hu, 2021) defined a sliding selector network with dynamic memory. SSN splits a given document into multiple segments, encodes them with BERT (Devlin et al., 2019), and selects salient sentences. Instead of representing the document as a graph, it uses a graph-based memory module, updated iteratively with a GAT (Veli\u010dkovi\u0107 et al., 2018), to allow information to flow across different windows. Heter-GraphLongSum (Phan et al., 2022) utilized words, sentences, and passages as nodes, while considering undirected edges for words in sentences, and directed edges for words in passages and passage to sentences. Instead of pre-trained embeddings, it used CNNs and bidirectional LSTMs for node encoding, yielding outstanding results. MTGNN-SUM (Doan et al., 2022) achieved similar results by capturing both inter and intra-sentence information when combining a homogeneous graph of sentence nodes with a heterogeneous graph of words and sentences, as in Wang et al. (2020). \n\nRecent studies underscore the importance of structural information in long document summarization. HEGEL (Zhang et al., 2022) modeled documents as hypergraphs, capturing semantic edges like keyword coreference, section structure, and latent topics.",
            "reference_string": "[273662136 | Bugueno et al. | 2024 | Citations: 0]"
        },
        {
            "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 22,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2205.03978",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.03978, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2064325789",
                    "name": "Aiswarya Sankar"
                },
                {
                    "authorId": "145934595",
                    "name": "Ankit R. Chadha"
                }
            ],
            "abstract": "Abstractive multi document summarization has evolved as a task through the basic sequence to sequence approaches to transformer and graph based techniques. Each of these approaches has primarily focused on the issues of multi document information synthe-sis and attention based approaches to extract salient information. A challenge that arises with multi document summarization which is not prevalent in single document summarization is the need to effectively summarize multiple documents that might have con\ufb02icting polarity, sentiment or subjective information about a given topic. In this paper we propose ACM, attribute conditioned multi document summarization,a model that incorporates attribute conditioning modules in order to decouple con\ufb02icting information by conditioning for a certain attribute in the output summary. This approach shows strong gains in ROUGE score over baseline multi document summarization approaches and shows gains in \ufb02uency, informativeness and reduction in repetitiveness as shown through a human annotation analysis study.",
            "corpus_id": 248571519,
            "sentences": [
                {
                    "corpus_id": "248571519",
                    "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization",
                    "text": "Abstractive multi document summarization is the task of writing a single summary of the key points and content in multiple related documents. This task has evolved from research in single document abstractive and extractive summarization; however, it faces unique challenges due to input documents having duplicate and conflicting content as well a larger body of text. (Radev et al., 2000). This task has evolved from early approaches using sequence to sequence (Seq2Seq) neural architectures to transformer based architectures with the introduction of large-scale datasets (Liu et al., 2018), (Fabbri et al., 2019). Beyond the introduction of approaches now commonly used for single document abstractive summarization, cross document attention and graphs that capture relations between text in various documents have further improved the state of the art for multi document summarization tasks. (Liu et al., 2018), (Li et al., 2014). These graphs aim to better represent the inter dependencies between articles by representing text spans as nodes in the graph and capturing the relations between these sentences as edge weights. \n\nDespite the advances made with these approaches, a significant challenge remains in multi document summarization with respect to how to deal with contradictory information present in the multiple source documents. It is critical to both learn the relationships between different documents as well as to extract salient information that is consistent with the output viewpoint. This is a situation often faced with summarizing multiple news articles where different viewpoints on an issue can significantly change the semantic structure of the content present in each article making it challenging for the abstractive summarization model to learn the relationships between inconsistent or conflicting information. In this work we define conflicting opinions as a combined measure of the polarity and sentiment of text. By this definition, two pieces of text on the same topic that have a differing sentiment and/ or polarity are determined to have different viewpoints. This definition is used throughout the paper. \n\nThis paper proposes ACM, attribute conditioned multi document summarization, a novel approach to multi document summarization that incorporates an attribute conditioning module with an abstractive multi document summarization model in order to condition for a particular attribute when generating the multi document summary.",
                    "score": 0.7961666082314961,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 141
                        },
                        {
                            "start": 142,
                            "end": 369
                        },
                        {
                            "start": 370,
                            "end": 391
                        },
                        {
                            "start": 392,
                            "end": 617
                        },
                        {
                            "start": 618,
                            "end": 896
                        },
                        {
                            "start": 897,
                            "end": 935
                        },
                        {
                            "start": 936,
                            "end": 1130
                        },
                        {
                            "start": 1133,
                            "end": 1346
                        },
                        {
                            "start": 1347,
                            "end": 1509
                        },
                        {
                            "start": 1510,
                            "end": 1845
                        },
                        {
                            "start": 1846,
                            "end": 1950
                        },
                        {
                            "start": 1951,
                            "end": 2101
                        },
                        {
                            "start": 2102,
                            "end": 2147
                        },
                        {
                            "start": 2150,
                            "end": 2474
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 370,
                            "end": 390,
                            "matchedPaperCorpusId": "1320"
                        },
                        {
                            "start": 917,
                            "end": 934,
                            "matchedPaperCorpusId": "10112929"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96435546875
                },
                {
                    "corpus_id": "248571519",
                    "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization",
                    "text": "Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013). These papers primarily aim to address de-duplicating information and learning relationships between the different topics shared across documents however none of these architectures are built to deal with conflicting information.",
                    "score": 0.6179287379772154,
                    "section_title": "Multi Document Summarization",
                    "char_start_offset": 8653,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 109
                        },
                        {
                            "start": 110,
                            "end": 276
                        },
                        {
                            "start": 277,
                            "end": 452
                        },
                        {
                            "start": 453,
                            "end": 619
                        },
                        {
                            "start": 620,
                            "end": 994
                        },
                        {
                            "start": 995,
                            "end": 1150
                        },
                        {
                            "start": 1151,
                            "end": 1363
                        },
                        {
                            "start": 1364,
                            "end": 1615
                        },
                        {
                            "start": 1616,
                            "end": 1844
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 205,
                            "end": 231,
                            "matchedPaperCorpusId": "577937"
                        },
                        {
                            "start": 241,
                            "end": 264,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 491,
                            "end": 508,
                            "matchedPaperCorpusId": "10112929"
                        },
                        {
                            "start": 1345,
                            "end": 1362,
                            "matchedPaperCorpusId": "218718706"
                        },
                        {
                            "start": 1448,
                            "end": 1465,
                            "matchedPaperCorpusId": "218718706"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96142578125
                }
            ],
            "relevance_judgement": 0.96435546875,
            "relevance_judgment_input_expanded": "# Title: ACM - Attribute Conditioning for Abstractive Multi Document Summarization\n# Venue: arXiv.org\n# Authors: Aiswarya Sankar, Ankit R. Chadha\n## Abstract\nAbstractive multi document summarization has evolved as a task through the basic sequence to sequence approaches to transformer and graph based techniques. Each of these approaches has primarily focused on the issues of multi document information synthe-sis and attention based approaches to extract salient information. A challenge that arises with multi document summarization which is not prevalent in single document summarization is the need to effectively summarize multiple documents that might have con\ufb02icting polarity, sentiment or subjective information about a given topic. In this paper we propose ACM, attribute conditioned multi document summarization,a model that incorporates attribute conditioning modules in order to decouple con\ufb02icting information by conditioning for a certain attribute in the output summary. This approach shows strong gains in ROUGE score over baseline multi document summarization approaches and shows gains in \ufb02uency, informativeness and reduction in repetitiveness as shown through a human annotation analysis study.\n## Introduction\nAbstractive multi document summarization is the task of writing a single summary of the key points and content in multiple related documents. This task has evolved from research in single document abstractive and extractive summarization; however, it faces unique challenges due to input documents having duplicate and conflicting content as well a larger body of text. (Radev et al., 2000). This task has evolved from early approaches using sequence to sequence (Seq2Seq) neural architectures to transformer based architectures with the introduction of large-scale datasets (Liu et al., 2018), (Fabbri et al., 2019). Beyond the introduction of approaches now commonly used for single document abstractive summarization, cross document attention and graphs that capture relations between text in various documents have further improved the state of the art for multi document summarization tasks. (Liu et al., 2018), (Li et al., 2014). These graphs aim to better represent the inter dependencies between articles by representing text spans as nodes in the graph and capturing the relations between these sentences as edge weights. \n\nDespite the advances made with these approaches, a significant challenge remains in multi document summarization with respect to how to deal with contradictory information present in the multiple source documents. It is critical to both learn the relationships between different documents as well as to extract salient information that is consistent with the output viewpoint. This is a situation often faced with summarizing multiple news articles where different viewpoints on an issue can significantly change the semantic structure of the content present in each article making it challenging for the abstractive summarization model to learn the relationships between inconsistent or conflicting information. In this work we define conflicting opinions as a combined measure of the polarity and sentiment of text. By this definition, two pieces of text on the same topic that have a differing sentiment and/ or polarity are determined to have different viewpoints. This definition is used throughout the paper. \n\nThis paper proposes ACM, attribute conditioned multi document summarization, a novel approach to multi document summarization that incorporates an attribute conditioning module with an abstractive multi document summarization model in order to condition for a particular attribute when generating the multi document summary.\n\n## Multi Document Summarization\nMulti document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013). These papers primarily aim to address de-duplicating information and learning relationships between the different topics shared across documents however none of these architectures are built to deal with conflicting information.",
            "reference_string": "[248571519 | Sankar et al. | 2022 | Citations: 0]"
        },
        {
            "title": "A Multi-Granularity Heterogeneous Graph for Extractive Text Summarization",
            "venue": "Electronics",
            "year": 2023,
            "reference_count": 23,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2079-9292/12/10/2184/pdf?version=1683790161",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics12102184?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics12102184, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2217089383",
                    "name": "Henghui Zhao"
                },
                {
                    "authorId": "2118071874",
                    "name": "Wensheng Zhang"
                },
                {
                    "authorId": "2118492864",
                    "name": "Mengxing Huang"
                },
                {
                    "authorId": "3073247",
                    "name": "Siling Feng"
                },
                {
                    "authorId": "2145066102",
                    "name": "Yuanyuan Wu"
                }
            ],
            "abstract": "Extractive text summarization selects the most important sentences from a document, preserves their original meaning, and produces an objective and fact-based summary. It is faster and less computationally intensive than abstract summarization techniques. Learning cross-sentence relationships is crucial for extractive text summarization. However, most of the language models currently in use process text data sequentially, which makes it difficult to capture such inter-sentence relations, especially in long documents. This paper proposes an extractive summarization model based on the graph neural network (GNN) to address this problem. The model effectively represents cross-sentence relationships using a graph-structured document representation. In addition to sentence nodes, we introduce two nodes with different granularity in the graph structure, words and topics, which bring different levels of semantic information. The node representations are updated by the graph attention network (GAT). The final summary is obtained using the binary classification of the sentence nodes. Our text summarization method was demonstrated to be highly effective, as supported by the results of our experiments on the CNN/DM and NYT datasets. To be specific, our approach outperformed baseline models of the same type in terms of ROUGE scores on both datasets, indicating the potential of our proposed model for enhancing text summarization tasks.",
            "corpus_id": 258654556,
            "sentences": [
                {
                    "corpus_id": "258654556",
                    "title": "A Multi-Granularity Heterogeneous Graph for Extractive Text Summarization",
                    "text": "Extractive text summarization selects the most important sentences from a document, preserves their original meaning, and produces an objective and fact-based summary. It is faster and less computationally intensive than abstract summarization techniques. Learning cross-sentence relationships is crucial for extractive text summarization. However, most of the language models currently in use process text data sequentially, which makes it difficult to capture such inter-sentence relations, especially in long documents. This paper proposes an extractive summarization model based on the graph neural network (GNN) to address this problem. The model effectively represents cross-sentence relationships using a graph-structured document representation. In addition to sentence nodes, we introduce two nodes with different granularity in the graph structure, words and topics, which bring different levels of semantic information. The node representations are updated by the graph attention network (GAT). The final summary is obtained using the binary classification of the sentence nodes. Our text summarization method was demonstrated to be highly effective, as supported by the results of our experiments on the CNN/DM and NYT datasets. To be specific, our approach outperformed baseline models of the same type in terms of ROUGE scores on both datasets, indicating the potential of our proposed model for enhancing text summarization tasks.",
                    "score": 0.669685781064558,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96435546875
                }
            ],
            "relevance_judgement": 0.96435546875,
            "relevance_judgment_input_expanded": "# Title: A Multi-Granularity Heterogeneous Graph for Extractive Text Summarization\n# Venue: Electronics\n# Authors: Henghui Zhao, Wensheng Zhang, Mengxing Huang, Siling Feng, Yuanyuan Wu\n## Abstract\nExtractive text summarization selects the most important sentences from a document, preserves their original meaning, and produces an objective and fact-based summary. It is faster and less computationally intensive than abstract summarization techniques. Learning cross-sentence relationships is crucial for extractive text summarization. However, most of the language models currently in use process text data sequentially, which makes it difficult to capture such inter-sentence relations, especially in long documents. This paper proposes an extractive summarization model based on the graph neural network (GNN) to address this problem. The model effectively represents cross-sentence relationships using a graph-structured document representation. In addition to sentence nodes, we introduce two nodes with different granularity in the graph structure, words and topics, which bring different levels of semantic information. The node representations are updated by the graph attention network (GAT). The final summary is obtained using the binary classification of the sentence nodes. Our text summarization method was demonstrated to be highly effective, as supported by the results of our experiments on the CNN/DM and NYT datasets. To be specific, our approach outperformed baseline models of the same type in terms of ROUGE scores on both datasets, indicating the potential of our proposed model for enhancing text summarization tasks.\n",
            "reference_string": "[258654556 | Zhao et al. | 2023 | Citations: 7]"
        },
        {
            "title": "Generating Extractive and Abstractive Summaries in Parallel from Scientific Articles Incorporating Citing Statements",
            "venue": "NEWSUM",
            "year": 2023,
            "reference_count": 44,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.newsum-1.8.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.newsum-1.8, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2164696118",
                    "name": "Sudipta Singha Roy"
                },
                {
                    "authorId": "2269458234",
                    "name": "Robert E. Mercer"
                }
            ],
            "abstract": "Summarization of scientific articles often overlooks insights from citing papers, focusing solely on the document\u2019s content. To incorporate citation contexts, we develop a model to summarize a scientific document using the information in the source and citing documents. It concurrently generates abstractive and extractive summaries, each enhancing the other. The extractive summarizer utilizes a blend of heterogeneous graph-based neural networks and graph attention networks, while the abstractive summarizer employs an autoregressive decoder. These modules exchange control signals through the loss function, ensuring the creation of high-quality summaries in both styles.",
            "corpus_id": 265607988,
            "sentences": [
                {
                    "corpus_id": "265607988",
                    "title": "Generating Extractive and Abstractive Summaries in Parallel from Scientific Articles Incorporating Citing Statements",
                    "text": "Another approach is to construct a sentence-level fully connected graph. Zhong et al. (Zhong et al., 2019) and Liu et al. (Liu and Lapata, 2019) used transformer (Vaswani et al., 2017) encoders to determine how sentences interact with each other. Wang et al. (Wang et al., 2020) introduced an heterogeneous graph neural network for extractive summarization which used additional semantic units (words) as intermediate nodes to construct relationships between sentences. \n\nAbstractive summarizers focus heavily on form, with the goal of producing a generalized summary, which tends to necessitate complex languagegenerating models. These models are typically based on sequence-to-sequence (seq2seq) architectures, in which a source document is seen as one sequence whereas its summary as another. The majority of previous research on neural abstractive summarization depended on large-scale, high-quality datasets of supervised document-summarization pairings (See et al., 2017). Recently, state-ofthe-art solutions on abstractive summarization are built upon the transformer (Vaswani et al., 2017) and BERT (Devlin et al., 2019) models. These attention-based abstractive models are being used in different fields like clinical note summariza-tion (Kanwal and Rizzo, 2022), scientific document summarization (Zhang et al., 2022), and layabstract generation (Yu et al., 2020). \n\nIn this paper, addressing the above-mentioned issues, we have built a standalone summarization model which can generate both extractive and abstractive summaries from scientific documents incorporating the citation network. Analyzing the citation network, citing statements from the citing articles are accumulated with the original text document to incorporate the research community's observation on that particular cited manuscript. These summaries are the abstracts of the original papers with additional information reflecting the research community's view. After that, we run the Long-Former (Beltagy et al., 2020) encoder to generate sentence and word representations and train extractive and abstractive summarizers together.",
                    "score": 0.7531684553059691,
                    "section_title": "Introduction",
                    "char_start_offset": 4141,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 72
                        },
                        {
                            "start": 73,
                            "end": 246
                        },
                        {
                            "start": 247,
                            "end": 469
                        },
                        {
                            "start": 472,
                            "end": 630
                        },
                        {
                            "start": 631,
                            "end": 795
                        },
                        {
                            "start": 796,
                            "end": 978
                        },
                        {
                            "start": 979,
                            "end": 1136
                        },
                        {
                            "start": 1137,
                            "end": 1374
                        },
                        {
                            "start": 1377,
                            "end": 1600
                        },
                        {
                            "start": 1601,
                            "end": 1812
                        },
                        {
                            "start": 1813,
                            "end": 1939
                        },
                        {
                            "start": 1940,
                            "end": 2110
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 122,
                            "end": 144,
                            "matchedPaperCorpusId": "201304248"
                        },
                        {
                            "start": 162,
                            "end": 184,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 247,
                            "end": 278,
                            "matchedPaperCorpusId": "216552978"
                        },
                        {
                            "start": 959,
                            "end": 977,
                            "matchedPaperCorpusId": "8314118"
                        },
                        {
                            "start": 1075,
                            "end": 1097,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 1107,
                            "end": 1128,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1247,
                            "end": 1271,
                            "matchedPaperCorpusId": "233296914"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.962890625
                }
            ],
            "relevance_judgement": 0.962890625,
            "relevance_judgment_input_expanded": "# Title: Generating Extractive and Abstractive Summaries in Parallel from Scientific Articles Incorporating Citing Statements\n# Venue: NEWSUM\n# Authors: Sudipta Singha Roy, Robert E. Mercer\n## Abstract\nSummarization of scientific articles often overlooks insights from citing papers, focusing solely on the document\u2019s content. To incorporate citation contexts, we develop a model to summarize a scientific document using the information in the source and citing documents. It concurrently generates abstractive and extractive summaries, each enhancing the other. The extractive summarizer utilizes a blend of heterogeneous graph-based neural networks and graph attention networks, while the abstractive summarizer employs an autoregressive decoder. These modules exchange control signals through the loss function, ensuring the creation of high-quality summaries in both styles.\n## Introduction\nAnother approach is to construct a sentence-level fully connected graph. Zhong et al. (Zhong et al., 2019) and Liu et al. (Liu and Lapata, 2019) used transformer (Vaswani et al., 2017) encoders to determine how sentences interact with each other. Wang et al. (Wang et al., 2020) introduced an heterogeneous graph neural network for extractive summarization which used additional semantic units (words) as intermediate nodes to construct relationships between sentences. \n\nAbstractive summarizers focus heavily on form, with the goal of producing a generalized summary, which tends to necessitate complex languagegenerating models. These models are typically based on sequence-to-sequence (seq2seq) architectures, in which a source document is seen as one sequence whereas its summary as another. The majority of previous research on neural abstractive summarization depended on large-scale, high-quality datasets of supervised document-summarization pairings (See et al., 2017). Recently, state-ofthe-art solutions on abstractive summarization are built upon the transformer (Vaswani et al., 2017) and BERT (Devlin et al., 2019) models. These attention-based abstractive models are being used in different fields like clinical note summariza-tion (Kanwal and Rizzo, 2022), scientific document summarization (Zhang et al., 2022), and layabstract generation (Yu et al., 2020). \n\nIn this paper, addressing the above-mentioned issues, we have built a standalone summarization model which can generate both extractive and abstractive summaries from scientific documents incorporating the citation network. Analyzing the citation network, citing statements from the citing articles are accumulated with the original text document to incorporate the research community's observation on that particular cited manuscript. These summaries are the abstracts of the original papers with additional information reflecting the research community's view. After that, we run the Long-Former (Beltagy et al., 2020) encoder to generate sentence and word representations and train extractive and abstractive summarizers together.",
            "reference_string": "[265607988 | Roy et al. | 2023 | Citations: 0]"
        },
        {
            "title": "Target-aware Abstractive Related Work Generation with Contrastive Learning",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2022,
            "reference_count": 43,
            "citation_count": 29,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3477495.3532065",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.13339, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46772896",
                    "name": "Xiuying Chen"
                },
                {
                    "authorId": "1753416672",
                    "name": "Hind Alamro"
                },
                {
                    "authorId": "9255404",
                    "name": "Li Mingzhe"
                },
                {
                    "authorId": "2112311595",
                    "name": "Shen Gao"
                },
                {
                    "authorId": "2055864368",
                    "name": "Rui Yan"
                },
                {
                    "authorId": "2118502950",
                    "name": "Xin Gao"
                },
                {
                    "authorId": "2928371",
                    "name": "Xiangliang Zhang"
                }
            ],
            "abstract": "The related work section is an important component of a scientific paper, which highlights the contribution of the target paper in the context of the reference papers. Authors can save their time and effort by using the automatically generated related work section as a draft to complete the final related work. Most of the existing related work section generation methods rely on extracting off-the-shelf sentences to make a comparative discussion about the target work and the reference papers. However, such sentences need to be written in advance and are hard to obtain in practice. Hence, in this paper, we propose an abstractive target-aware related work generator (TAG), which can generate related work sections consisting of new sentences. Concretely, we first propose a target-aware graph encoder, which models the relationships between reference papers and the target paper with target-centered attention mechanisms. In the decoding process, we propose a hierarchical decoder that attends to the nodes of different levels in the graph with keyphrases as semantic indicators. Finally, to generate a more informative related work, we propose multi-level contrastive optimization objectives, which aim to maximize the mutual information between the generated related work with the references and minimize that with non-references. Extensive experiments on two public scholar datasets show that the proposed model brings substantial improvements over several strong baselines in terms of automatic and tailored human evaluations.",
            "corpus_id": 249097545,
            "sentences": [
                {
                    "corpus_id": "249097545",
                    "title": "Target-aware Abstractive Related Work Generation with Contrastive Learning",
                    "text": "We evaluate our models against popular and state-of-the-art multidocument summarization baselines and related work generation techniques. We adopt the LEAD baseline which selects the first sentence of each reference as the summary as a baseline, and ORACLE as an upper bound of extractive summarization systems. \n\nOur extractive baselines include: LexRank [11]: a graph-based extractive model that computes sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. \n\nNES [37]: a neural data-driven summarizer that extracts sentences from reference papers based on target paper. A joint contextdriven attention mechanism is proposed to measure the contextual relevance within full texts. \n\nBertSumEXT [25]: an extractive summarization model with BERT encoder, which is able to express the semantics of a document and obtain representations for its sentences. \n\nAbstractive baseline models include: BertSumABS [25]: an abstractive summarization system built on BERT with a new fine-tuning schedule that adopts different optimizers for the encoder and the decoder. \n\nMGSum [20]: a multi-granularity interaction network for abstractive multi-document summarization. The multi granularities include document-level, sentence-level, and word-level. \n\nEMS [42]: an entity-aware model for abstractive multi-document summarization with BERT encoder. It augments the classical Transformer based encoder-decoder framework with a graph consisting of paragraph nodes and entity nodes. EMS uses the same keyphrase extraction tool as ours. \n\nRRG [5]: an abstractive related work generator based on reference papers. It augments the RNN encoder with a relation-aware graph between multiple references. The relation graph and the document representation are refined iteratively. \n\nThe essential difference between our model and baselines is that we explicitly model the relationship between the target work and the references. To make a fair comparison, for baselines that have relatively good performance, we give an additional with.Target (w.T ) version which takes references and target paper as input. Thus, the baseline models treat the references and the target paper as multiple inputs without distinguishing them.",
                    "score": 0.611568779454541,
                    "section_title": "Baselines",
                    "char_start_offset": 23597,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 137
                        },
                        {
                            "start": 138,
                            "end": 311
                        },
                        {
                            "start": 314,
                            "end": 513
                        },
                        {
                            "start": 516,
                            "end": 626
                        },
                        {
                            "start": 627,
                            "end": 735
                        },
                        {
                            "start": 738,
                            "end": 906
                        },
                        {
                            "start": 909,
                            "end": 1110
                        },
                        {
                            "start": 1113,
                            "end": 1210
                        },
                        {
                            "start": 1211,
                            "end": 1290
                        },
                        {
                            "start": 1293,
                            "end": 1388
                        },
                        {
                            "start": 1389,
                            "end": 1519
                        },
                        {
                            "start": 1520,
                            "end": 1572
                        },
                        {
                            "start": 1575,
                            "end": 1648
                        },
                        {
                            "start": 1649,
                            "end": 1733
                        },
                        {
                            "start": 1734,
                            "end": 1809
                        },
                        {
                            "start": 1812,
                            "end": 1957
                        },
                        {
                            "start": 1958,
                            "end": 2136
                        },
                        {
                            "start": 2137,
                            "end": 2252
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 356,
                            "end": 360,
                            "matchedPaperCorpusId": "506350"
                        },
                        {
                            "start": 520,
                            "end": 524,
                            "matchedPaperCorpusId": "53083244"
                        },
                        {
                            "start": 749,
                            "end": 753,
                            "matchedPaperCorpusId": "201304248"
                        },
                        {
                            "start": 957,
                            "end": 961,
                            "matchedPaperCorpusId": "201304248"
                        },
                        {
                            "start": 1119,
                            "end": 1123,
                            "matchedPaperCorpusId": "220045815"
                        },
                        {
                            "start": 1297,
                            "end": 1301,
                            "matchedPaperCorpusId": "236478143"
                        },
                        {
                            "start": 1579,
                            "end": 1582,
                            "matchedPaperCorpusId": "236460044"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.962890625
                },
                {
                    "corpus_id": "249097545",
                    "title": "Target-aware Abstractive Related Work Generation with Contrastive Learning",
                    "text": "For example, Zhao et al. [41] converted the original documents to a sentence graph, taking both linguistic and deep representation into account, and Liu et al. [23] constructed sentence graph based on both the similarities and relative distances in the neighborhood of each sentence. Due to the success of neural abstractive models on single-document summarization, abstractive models are adapted to multi-document summarization. Jin et al. [20] proposed a multi-granularity interaction network for extractive and abstractive multi-document summarization, and Zhou et al. [42] augmented the classical Transformer [35] with a heterogeneous graph consisting of entity nodes. However, MDS aims at synthesizing the similar and removing the redundant information among multiple input documents, while RWG needs to find the specific contributions of individual papers even if their research directions are the same and arrange them in comparative discussion [19]. \n\nOur model is also related to contrastive learning, which has boosted unsupervised representation learning in recommendation [31,40], computer vision [21,30], and natural language processing [29,43]. In text generation, Logeswaran and Lee [27] proposed to learn better sentence representations by using a classifier to distinguish context sentences from other contrastive sentences. Bui et al. [3] introduced contrastive learning into code summarization, where the model is asked to recognize similar and dissimilar code snippets through contrastive learning objectives. In scientific paper related studies, Cohan et al. [8] explored contrastive learning and designed a loss function that trains the Transformer model to learn closer representations for papers when one cites the other. The contrastive learning mechanism in our paper is also based on citation graph relatedness but is employed differently for generating comparative related work discussion.",
                    "score": 0.5924006559842446,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 8465,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 283
                        },
                        {
                            "start": 284,
                            "end": 429
                        },
                        {
                            "start": 430,
                            "end": 672
                        },
                        {
                            "start": 673,
                            "end": 957
                        },
                        {
                            "start": 960,
                            "end": 1158
                        },
                        {
                            "start": 1159,
                            "end": 1341
                        },
                        {
                            "start": 1342,
                            "end": 1529
                        },
                        {
                            "start": 1530,
                            "end": 1745
                        },
                        {
                            "start": 1746,
                            "end": 1917
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 25,
                            "end": 29,
                            "matchedPaperCorpusId": "220633461"
                        },
                        {
                            "start": 160,
                            "end": 164,
                            "matchedPaperCorpusId": "235792514"
                        },
                        {
                            "start": 441,
                            "end": 445,
                            "matchedPaperCorpusId": "220045815"
                        },
                        {
                            "start": 572,
                            "end": 576,
                            "matchedPaperCorpusId": "236478143"
                        },
                        {
                            "start": 613,
                            "end": 617,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 952,
                            "end": 956,
                            "matchedPaperCorpusId": "1948673"
                        },
                        {
                            "start": 1084,
                            "end": 1088,
                            "matchedPaperCorpusId": "235792396"
                        },
                        {
                            "start": 1088,
                            "end": 1091,
                            "matchedPaperCorpusId": "235792358"
                        },
                        {
                            "start": 1113,
                            "end": 1116,
                            "matchedPaperCorpusId": "235490270"
                        },
                        {
                            "start": 1150,
                            "end": 1154,
                            "matchedPaperCorpusId": "248780405"
                        },
                        {
                            "start": 1154,
                            "end": 1157,
                            "matchedPaperCorpusId": "237277941"
                        },
                        {
                            "start": 1198,
                            "end": 1202,
                            "matchedPaperCorpusId": "3525802"
                        },
                        {
                            "start": 1580,
                            "end": 1583,
                            "matchedPaperCorpusId": "215768677"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96240234375
                }
            ],
            "relevance_judgement": 0.962890625,
            "relevance_judgment_input_expanded": "# Title: Target-aware Abstractive Related Work Generation with Contrastive Learning\n# Venue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\n# Authors: Xiuying Chen, Hind Alamro, Li Mingzhe, Shen Gao, Rui Yan, Xin Gao, Xiangliang Zhang\n## Abstract\nThe related work section is an important component of a scientific paper, which highlights the contribution of the target paper in the context of the reference papers. Authors can save their time and effort by using the automatically generated related work section as a draft to complete the final related work. Most of the existing related work section generation methods rely on extracting off-the-shelf sentences to make a comparative discussion about the target work and the reference papers. However, such sentences need to be written in advance and are hard to obtain in practice. Hence, in this paper, we propose an abstractive target-aware related work generator (TAG), which can generate related work sections consisting of new sentences. Concretely, we first propose a target-aware graph encoder, which models the relationships between reference papers and the target paper with target-centered attention mechanisms. In the decoding process, we propose a hierarchical decoder that attends to the nodes of different levels in the graph with keyphrases as semantic indicators. Finally, to generate a more informative related work, we propose multi-level contrastive optimization objectives, which aim to maximize the mutual information between the generated related work with the references and minimize that with non-references. Extensive experiments on two public scholar datasets show that the proposed model brings substantial improvements over several strong baselines in terms of automatic and tailored human evaluations.\n## RELATED WORK\nFor example, Zhao et al. [41] converted the original documents to a sentence graph, taking both linguistic and deep representation into account, and Liu et al. [23] constructed sentence graph based on both the similarities and relative distances in the neighborhood of each sentence. Due to the success of neural abstractive models on single-document summarization, abstractive models are adapted to multi-document summarization. Jin et al. [20] proposed a multi-granularity interaction network for extractive and abstractive multi-document summarization, and Zhou et al. [42] augmented the classical Transformer [35] with a heterogeneous graph consisting of entity nodes. However, MDS aims at synthesizing the similar and removing the redundant information among multiple input documents, while RWG needs to find the specific contributions of individual papers even if their research directions are the same and arrange them in comparative discussion [19]. \n\nOur model is also related to contrastive learning, which has boosted unsupervised representation learning in recommendation [31,40], computer vision [21,30], and natural language processing [29,43]. In text generation, Logeswaran and Lee [27] proposed to learn better sentence representations by using a classifier to distinguish context sentences from other contrastive sentences. Bui et al. [3] introduced contrastive learning into code summarization, where the model is asked to recognize similar and dissimilar code snippets through contrastive learning objectives. In scientific paper related studies, Cohan et al. [8] explored contrastive learning and designed a loss function that trains the Transformer model to learn closer representations for papers when one cites the other. The contrastive learning mechanism in our paper is also based on citation graph relatedness but is employed differently for generating comparative related work discussion.\n\n## Baselines\nWe evaluate our models against popular and state-of-the-art multidocument summarization baselines and related work generation techniques. We adopt the LEAD baseline which selects the first sentence of each reference as the summary as a baseline, and ORACLE as an upper bound of extractive summarization systems. \n\nOur extractive baselines include: LexRank [11]: a graph-based extractive model that computes sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. \n\nNES [37]: a neural data-driven summarizer that extracts sentences from reference papers based on target paper. A joint contextdriven attention mechanism is proposed to measure the contextual relevance within full texts. \n\nBertSumEXT [25]: an extractive summarization model with BERT encoder, which is able to express the semantics of a document and obtain representations for its sentences. \n\nAbstractive baseline models include: BertSumABS [25]: an abstractive summarization system built on BERT with a new fine-tuning schedule that adopts different optimizers for the encoder and the decoder. \n\nMGSum [20]: a multi-granularity interaction network for abstractive multi-document summarization. The multi granularities include document-level, sentence-level, and word-level. \n\nEMS [42]: an entity-aware model for abstractive multi-document summarization with BERT encoder. It augments the classical Transformer based encoder-decoder framework with a graph consisting of paragraph nodes and entity nodes. EMS uses the same keyphrase extraction tool as ours. \n\nRRG [5]: an abstractive related work generator based on reference papers. It augments the RNN encoder with a relation-aware graph between multiple references. The relation graph and the document representation are refined iteratively. \n\nThe essential difference between our model and baselines is that we explicitly model the relationship between the target work and the references. To make a fair comparison, for baselines that have relatively good performance, we give an additional with.Target (w.T ) version which takes references and target paper as input. Thus, the baseline models treat the references and the target paper as multiple inputs without distinguishing them.",
            "reference_string": "[249097545 | Chen et al. | 2022 | Citations: 29]"
        },
        {
            "title": "Large-Scale Multi-Document Summarization with Information Extraction and Compression",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 20,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.00548, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2152171283",
                    "name": "Ning Wang"
                },
                {
                    "authorId": "2140162523",
                    "name": "Han Liu"
                },
                {
                    "authorId": "1753376",
                    "name": "D. Klabjan"
                }
            ],
            "abstract": "We develop an abstractive summarization framework independent of labeled data for multiple heterogeneous documents. Unlike existing multi-document summarization methods, our framework processes documents telling different stories instead of documents on the same topic. We also enhance an existing sentence fusion method with a uni-directional language model to prioritize fused sentences with higher sentence probability with the goal of increasing readability. Lastly, we construct a total of twelve dataset variations based on CNN/Daily Mail and the NewsRoom datasets, where each document group contains a large and diverse collection of documents to evaluate the performance of our model in comparison with other baseline systems. Our experiments demonstrate that our framework outperforms current state-of-the-art methods in this more generic setting.",
            "corpus_id": 248496597,
            "sentences": [
                {
                    "corpus_id": "248496597",
                    "title": "Large-Scale Multi-Document Summarization with Information Extraction and Compression",
                    "text": "Dohare et al. (2018) propose to generate abstractive summaries by first generating AMR graphs (abstract meaning representation) for corresponding input stories, extract summary graphs from the AMR graphs, and lastly create summaries from the summary graphs. \n\nIn the field of unsupervised multi-document summarization, Chu and Liu (2019) propose an end-to-end, unsupervised, abstractive, neural summarization model. Their model consists of two parts: (i) an LSTM-based auto-encoder that learns a representation of each input text and (ii) a summarization module that learns to generate summaries based on the representation encoded by the auto-encoder. The content of all input texts is obtained as the mean of all last hidden and cell states of the LSTM network. This information is then fed into the decoder of the auto-encoder to decode a summary. Lastly, they evaluate average similarity of summary to all input texts by encoding the summary again with the encoder of the auto-encoder and computing the average cosine distance between the hidden states of each encoded input text and the hidden state of the encoded summary. \n\nTheir approach requires no labeled training data to summarize multiple documents. However, it is also based on an assumption that all input texts to be summarized contain similar information. This is often not the case in real world as often times documents to be summarized have different gist. In contrast to neural-based summarization methods, Nayeem et al. (2018) propose an unsupervised multi-document summarization system comprising of word graph-based (Filippova, 2010) sentence fusion and integer linear programming (ILP)-based sentence ranking. They first apply hierarchical agglomerative clustering with complete linkage to all sentences in the documents, where the distance between two sentences are based on their continuous representations. For each cluster of related sentences, they generate abstractive fused sentences by first constructing a word graph and then apply k-shortest path on it. They not only merge similar sentences into a single one but also substitute such lexicals as verbs or nouns with ones having functional similarities, making the resulting sentence more abstractive.",
                    "score": 0.8423400779107223,
                    "section_title": "Related Work",
                    "char_start_offset": 6251,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 257
                        },
                        {
                            "start": 260,
                            "end": 415
                        },
                        {
                            "start": 416,
                            "end": 652
                        },
                        {
                            "start": 653,
                            "end": 763
                        },
                        {
                            "start": 764,
                            "end": 850
                        },
                        {
                            "start": 851,
                            "end": 1128
                        },
                        {
                            "start": 1131,
                            "end": 1212
                        },
                        {
                            "start": 1213,
                            "end": 1322
                        },
                        {
                            "start": 1323,
                            "end": 1426
                        },
                        {
                            "start": 1427,
                            "end": 1684
                        },
                        {
                            "start": 1685,
                            "end": 1884
                        },
                        {
                            "start": 1885,
                            "end": 2038
                        },
                        {
                            "start": 2039,
                            "end": 2236
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 319,
                            "end": 337,
                            "matchedPaperCorpusId": "59413781"
                        },
                        {
                            "start": 1478,
                            "end": 1498,
                            "matchedPaperCorpusId": "52011473"
                        },
                        {
                            "start": 1590,
                            "end": 1607,
                            "matchedPaperCorpusId": "14750088"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96240234375
                }
            ],
            "relevance_judgement": 0.96240234375,
            "relevance_judgment_input_expanded": "# Title: Large-Scale Multi-Document Summarization with Information Extraction and Compression\n# Venue: arXiv.org\n# Authors: Ning Wang, Han Liu, D. Klabjan\n## Abstract\nWe develop an abstractive summarization framework independent of labeled data for multiple heterogeneous documents. Unlike existing multi-document summarization methods, our framework processes documents telling different stories instead of documents on the same topic. We also enhance an existing sentence fusion method with a uni-directional language model to prioritize fused sentences with higher sentence probability with the goal of increasing readability. Lastly, we construct a total of twelve dataset variations based on CNN/Daily Mail and the NewsRoom datasets, where each document group contains a large and diverse collection of documents to evaluate the performance of our model in comparison with other baseline systems. Our experiments demonstrate that our framework outperforms current state-of-the-art methods in this more generic setting.\n## Related Work\nDohare et al. (2018) propose to generate abstractive summaries by first generating AMR graphs (abstract meaning representation) for corresponding input stories, extract summary graphs from the AMR graphs, and lastly create summaries from the summary graphs. \n\nIn the field of unsupervised multi-document summarization, Chu and Liu (2019) propose an end-to-end, unsupervised, abstractive, neural summarization model. Their model consists of two parts: (i) an LSTM-based auto-encoder that learns a representation of each input text and (ii) a summarization module that learns to generate summaries based on the representation encoded by the auto-encoder. The content of all input texts is obtained as the mean of all last hidden and cell states of the LSTM network. This information is then fed into the decoder of the auto-encoder to decode a summary. Lastly, they evaluate average similarity of summary to all input texts by encoding the summary again with the encoder of the auto-encoder and computing the average cosine distance between the hidden states of each encoded input text and the hidden state of the encoded summary. \n\nTheir approach requires no labeled training data to summarize multiple documents. However, it is also based on an assumption that all input texts to be summarized contain similar information. This is often not the case in real world as often times documents to be summarized have different gist. In contrast to neural-based summarization methods, Nayeem et al. (2018) propose an unsupervised multi-document summarization system comprising of word graph-based (Filippova, 2010) sentence fusion and integer linear programming (ILP)-based sentence ranking. They first apply hierarchical agglomerative clustering with complete linkage to all sentences in the documents, where the distance between two sentences are based on their continuous representations. For each cluster of related sentences, they generate abstractive fused sentences by first constructing a word graph and then apply k-shortest path on it. They not only merge similar sentences into a single one but also substitute such lexicals as verbs or nouns with ones having functional similarities, making the resulting sentence more abstractive.",
            "reference_string": "[248496597 | Wang et al. | 2022 | Citations: 1]"
        },
        {
            "title": "Graph-to-Text Generation with Dynamic Structure Pruning",
            "venue": "International Conference on Computational Linguistics",
            "year": 2022,
            "reference_count": 48,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2209.07258",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2209.07258, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2154884699",
                    "name": "Liang Li"
                },
                {
                    "authorId": "9706609",
                    "name": "Ruiying Geng"
                },
                {
                    "authorId": "2132475886",
                    "name": "Bowen Li"
                },
                {
                    "authorId": "2112563315",
                    "name": "Can Ma"
                },
                {
                    "authorId": "35755264",
                    "name": "Yinliang Yue"
                },
                {
                    "authorId": "66200440",
                    "name": "Binhua Li"
                },
                {
                    "authorId": "2323761746",
                    "name": "Yongbin Li"
                }
            ],
            "abstract": "Most graph-to-text works are built on the encoder-decoder framework with cross-attention mechanism. Recent studies have shown that explicitly modeling the input graph structure can significantly improve the performance. However, the vanilla structural encoder cannot capture all specialized information in a single forward pass for all decoding steps, resulting in inaccurate semantic representations. Meanwhile, the input graph is flatted as an unordered sequence in the cross attention, ignoring the original graph structure. As a result, the obtained input graph context vector in the decoder may be flawed. To address these issues, we propose a Structure-Aware Cross-Attention (SACA) mechanism to re-encode the input graph representation conditioning on the newly generated context at each decoding step in a structure aware manner. We further adapt SACA and introduce its variant Dynamic Graph Pruning (DGP) mechanism to dynamically drop irrelevant nodes in the decoding process. We achieve new state-of-the-art results on two graph-to-text datasets, LDC2020T02 and ENT-DESC, with only minor increase on computational cost.",
            "corpus_id": 252280335,
            "sentences": [
                {
                    "corpus_id": "252280335",
                    "title": "Graph-to-Text Generation with Dynamic Structure Pruning",
                    "text": "Graph-to-text is a challenging task which aims at generating a descriptive text from the structured knowledge, such Knowledge Graph (KG), and Abstract Meaning Representation (AMR) graphs. It is helpful for interpretability of KGs in general (Schmitt et al., 2020) and knowledge-based question answering (Hui et al., 2022;Wang et al., 2022;Fu et al., 2020;Qin et al., 2022). \n\nIn recent years, most graph-to-text methods have been built based on the encoder-decoder architecture. This kind of method usually consists of a structural encoder and a decoder. The structural encoder aims to model the structure information into the representation of the input graph. Song et al. (2018) first propose the graph recurrent networks (GRNs) to encode the AMR node directly. And then, some works (Shi et al., 2020;Chen et al., 2020) introduce the Graph Neural Networks (GNNs) as the structural encoder, which updates the representations of nodes based on their immediate neighbors. To integrate both local and non-local features and learn a better structural representation of a graph, Guo et al. (2019) introduce the dense connection, allowing deeper GCNs. Unlike the local information aggregation scheme, Zhu et al. (2019); Cai and Lam (2020) propose the Graph Transformer that uses explicit relation encoding and allows direct communication between two distant nodes. \n\nA recently proposed neural abstractive Multi-Document Summarization (MDS) model, Graph-Summ (Li et al., 2020), also considers the input graph structure during decoding. The biggest difference between Graphsum and our proposed SACA is that the former only introduces one graph attention layer in each decoder layer. SACA, on the other hand, injects graph structure into decoding by re-encoding the input graph. Specifically, it re-computes the input graph representation by conditioning it on the newly generated text at each decoding step. \n\nRecent approaches try to apply the Pre-trained Language Models (PLMs) (Kenton and Toutanova, 2019;Raffel et al., 2019) into the graph-to-text generation.",
                    "score": 0.5360579305721715,
                    "section_title": "Related Works",
                    "char_start_offset": 5273,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 187
                        },
                        {
                            "start": 188,
                            "end": 373
                        },
                        {
                            "start": 376,
                            "end": 478
                        },
                        {
                            "start": 479,
                            "end": 554
                        },
                        {
                            "start": 555,
                            "end": 661
                        },
                        {
                            "start": 662,
                            "end": 763
                        },
                        {
                            "start": 764,
                            "end": 970
                        },
                        {
                            "start": 971,
                            "end": 1146
                        },
                        {
                            "start": 1147,
                            "end": 1359
                        },
                        {
                            "start": 1362,
                            "end": 1530
                        },
                        {
                            "start": 1531,
                            "end": 1676
                        },
                        {
                            "start": 1677,
                            "end": 1771
                        },
                        {
                            "start": 1772,
                            "end": 1901
                        },
                        {
                            "start": 1904,
                            "end": 2057
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 241,
                            "end": 263,
                            "matchedPaperCorpusId": "218470233"
                        },
                        {
                            "start": 303,
                            "end": 321,
                            "matchedPaperCorpusId": "265038300"
                        },
                        {
                            "start": 321,
                            "end": 339,
                            "matchedPaperCorpusId": "250089255"
                        },
                        {
                            "start": 662,
                            "end": 680,
                            "matchedPaperCorpusId": "25111673"
                        },
                        {
                            "start": 785,
                            "end": 803,
                            "matchedPaperCorpusId": "220730154"
                        },
                        {
                            "start": 1196,
                            "end": 1213,
                            "matchedPaperCorpusId": "202540640"
                        },
                        {
                            "start": 1215,
                            "end": 1233,
                            "matchedPaperCorpusId": "208138227"
                        },
                        {
                            "start": 1454,
                            "end": 1471,
                            "matchedPaperCorpusId": "218718706"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9609375
                }
            ],
            "relevance_judgement": 0.9609375,
            "relevance_judgment_input_expanded": "# Title: Graph-to-Text Generation with Dynamic Structure Pruning\n# Venue: International Conference on Computational Linguistics\n# Authors: Liang Li, Ruiying Geng, Bowen Li, Can Ma, Yinliang Yue, Binhua Li, Yongbin Li\n## Abstract\nMost graph-to-text works are built on the encoder-decoder framework with cross-attention mechanism. Recent studies have shown that explicitly modeling the input graph structure can significantly improve the performance. However, the vanilla structural encoder cannot capture all specialized information in a single forward pass for all decoding steps, resulting in inaccurate semantic representations. Meanwhile, the input graph is flatted as an unordered sequence in the cross attention, ignoring the original graph structure. As a result, the obtained input graph context vector in the decoder may be flawed. To address these issues, we propose a Structure-Aware Cross-Attention (SACA) mechanism to re-encode the input graph representation conditioning on the newly generated context at each decoding step in a structure aware manner. We further adapt SACA and introduce its variant Dynamic Graph Pruning (DGP) mechanism to dynamically drop irrelevant nodes in the decoding process. We achieve new state-of-the-art results on two graph-to-text datasets, LDC2020T02 and ENT-DESC, with only minor increase on computational cost.\n## Related Works\nGraph-to-text is a challenging task which aims at generating a descriptive text from the structured knowledge, such Knowledge Graph (KG), and Abstract Meaning Representation (AMR) graphs. It is helpful for interpretability of KGs in general (Schmitt et al., 2020) and knowledge-based question answering (Hui et al., 2022;Wang et al., 2022;Fu et al., 2020;Qin et al., 2022). \n\nIn recent years, most graph-to-text methods have been built based on the encoder-decoder architecture. This kind of method usually consists of a structural encoder and a decoder. The structural encoder aims to model the structure information into the representation of the input graph. Song et al. (2018) first propose the graph recurrent networks (GRNs) to encode the AMR node directly. And then, some works (Shi et al., 2020;Chen et al., 2020) introduce the Graph Neural Networks (GNNs) as the structural encoder, which updates the representations of nodes based on their immediate neighbors. To integrate both local and non-local features and learn a better structural representation of a graph, Guo et al. (2019) introduce the dense connection, allowing deeper GCNs. Unlike the local information aggregation scheme, Zhu et al. (2019); Cai and Lam (2020) propose the Graph Transformer that uses explicit relation encoding and allows direct communication between two distant nodes. \n\nA recently proposed neural abstractive Multi-Document Summarization (MDS) model, Graph-Summ (Li et al., 2020), also considers the input graph structure during decoding. The biggest difference between Graphsum and our proposed SACA is that the former only introduces one graph attention layer in each decoder layer. SACA, on the other hand, injects graph structure into decoding by re-encoding the input graph. Specifically, it re-computes the input graph representation by conditioning it on the newly generated text at each decoding step. \n\nRecent approaches try to apply the Pre-trained Language Models (PLMs) (Kenton and Toutanova, 2019;Raffel et al., 2019) into the graph-to-text generation.",
            "reference_string": "[252280335 | Li et al. | 2022 | Citations: 2]"
        },
        {
            "title": "Parallel Hierarchical Transformer with Attention Alignment for Abstractive Multi-Document Summarization",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 38,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2208.07845",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2208.07845, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51439912",
                    "name": "Ye Ma"
                },
                {
                    "authorId": "48373979",
                    "name": "Lu Zong"
                }
            ],
            "abstract": "In comparison to single-document summarization, abstractive Multi-Document Summarization (MDS) brings challenges on the representation and coverage of its lengthy and linked sources. This study develops a Parallel Hierarchical Transformer (PHT) with attention alignment for MDS. By incorporating word- and paragraph-level multi-head attentions, the hierarchical architecture of PHT allows better processing of dependencies at both token and document levels. To guide the decoding towards a better coverage of the source documents, the attention-alignment mechanism is then introduced to calibrate beam search with predicted optimal attention distributions. Based on the WikiSum data, a comprehensive evaluation is conducted to test improvements on MDS by the proposed architecture. By better handling the inner- and cross-document information, results in both ROUGE and human evaluation suggest that our hierarchical model generates summaries of higher quality relative to other Transformer-based baselines at relatively low computational cost.",
            "corpus_id": 251594480,
            "sentences": [
                {
                    "corpus_id": "251594480",
                    "title": "Parallel Hierarchical Transformer with Attention Alignment for Abstractive Multi-Document Summarization",
                    "text": "Since Sutskever et al. (2014) propose the sequence-to-sequence (seq2seq) model for machine translation, the development of NLP applications has been almost inseparable from this framework. In the field of abstractive summarization, the seq2seq model is first applied by Rush et al. (2015) to summarize sentences. With respect to the recent bloom of the attention mechanism and pre-trained models, great effort has been made to improve neural machine summarization upon extensions of seq2seq (Gehrmann et al., 2018;See et al., 2017;Zhang et al., 2019). With the promising results on single documents (See et al., 2017;Gehrmann et al., 2018;Lewis et al., 2020;Pradhan et al., 2021;Liao et al., 2021;Liang et al., 2021), there are increasing recent attempts to study abstractive multi-document summarization (MDS) in the seq2seq framework (Liu et al., 2018;Lebanoff et al., 2018;Fabbri et al., 2019;Liu and Lapata, 2019;Ma et al., 2020). \n\nThis study makes an exploratory attempt to improve the established abstractive summarization models for multi-document summarization (MDS) utilizing the Transformer (Vaswani et al., 2017) architecture. In comparison to single-document summarization, MDS brings challenges on the representation and coverage of its lengthy and linked sources. Liu et al. (2018) propose a two-stage model to first extractively select the important paragraphs, then train the concatenated flat sequences using the Transformer-decoder with memory compressed attention (T-DMCA). Although the two-stage approach effectively reduces redundant information of source documents and retains salient information as inputs, it fails to take into account the cross-document relationship in its summaries. On the other hand, Liu and Lapata (2019) propose a Hierarchical Transformer (HT) with local and global encoder layers to represent cross-token and cross-document information.",
                    "score": 0.4776206877742628,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 188
                        },
                        {
                            "start": 189,
                            "end": 312
                        },
                        {
                            "start": 313,
                            "end": 551
                        },
                        {
                            "start": 552,
                            "end": 934
                        },
                        {
                            "start": 937,
                            "end": 1138
                        },
                        {
                            "start": 1139,
                            "end": 1278
                        },
                        {
                            "start": 1279,
                            "end": 1493
                        },
                        {
                            "start": 1494,
                            "end": 1710
                        },
                        {
                            "start": 1711,
                            "end": 1885
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 270,
                            "end": 288,
                            "matchedPaperCorpusId": "1918428"
                        },
                        {
                            "start": 491,
                            "end": 514,
                            "matchedPaperCorpusId": "52144157"
                        },
                        {
                            "start": 514,
                            "end": 531,
                            "matchedPaperCorpusId": "8314118"
                        },
                        {
                            "start": 531,
                            "end": 550,
                            "matchedPaperCorpusId": "155100086"
                        },
                        {
                            "start": 599,
                            "end": 617,
                            "matchedPaperCorpusId": "8314118"
                        },
                        {
                            "start": 617,
                            "end": 639,
                            "matchedPaperCorpusId": "52144157"
                        },
                        {
                            "start": 639,
                            "end": 658,
                            "matchedPaperCorpusId": "204960716"
                        },
                        {
                            "start": 658,
                            "end": 679,
                            "matchedPaperCorpusId": "228889686"
                        },
                        {
                            "start": 679,
                            "end": 697,
                            "matchedPaperCorpusId": "233849756"
                        },
                        {
                            "start": 697,
                            "end": 716,
                            "matchedPaperCorpusId": "226319324"
                        },
                        {
                            "start": 854,
                            "end": 876,
                            "matchedPaperCorpusId": "52053741"
                        },
                        {
                            "start": 876,
                            "end": 896,
                            "matchedPaperCorpusId": "174799390"
                        },
                        {
                            "start": 896,
                            "end": 917,
                            "matchedPaperCorpusId": "170079112"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9609375
                }
            ],
            "relevance_judgement": 0.9609375,
            "relevance_judgment_input_expanded": "# Title: Parallel Hierarchical Transformer with Attention Alignment for Abstractive Multi-Document Summarization\n# Venue: arXiv.org\n# Authors: Ye Ma, Lu Zong\n## Abstract\nIn comparison to single-document summarization, abstractive Multi-Document Summarization (MDS) brings challenges on the representation and coverage of its lengthy and linked sources. This study develops a Parallel Hierarchical Transformer (PHT) with attention alignment for MDS. By incorporating word- and paragraph-level multi-head attentions, the hierarchical architecture of PHT allows better processing of dependencies at both token and document levels. To guide the decoding towards a better coverage of the source documents, the attention-alignment mechanism is then introduced to calibrate beam search with predicted optimal attention distributions. Based on the WikiSum data, a comprehensive evaluation is conducted to test improvements on MDS by the proposed architecture. By better handling the inner- and cross-document information, results in both ROUGE and human evaluation suggest that our hierarchical model generates summaries of higher quality relative to other Transformer-based baselines at relatively low computational cost.\n## Introduction\nSince Sutskever et al. (2014) propose the sequence-to-sequence (seq2seq) model for machine translation, the development of NLP applications has been almost inseparable from this framework. In the field of abstractive summarization, the seq2seq model is first applied by Rush et al. (2015) to summarize sentences. With respect to the recent bloom of the attention mechanism and pre-trained models, great effort has been made to improve neural machine summarization upon extensions of seq2seq (Gehrmann et al., 2018;See et al., 2017;Zhang et al., 2019). With the promising results on single documents (See et al., 2017;Gehrmann et al., 2018;Lewis et al., 2020;Pradhan et al., 2021;Liao et al., 2021;Liang et al., 2021), there are increasing recent attempts to study abstractive multi-document summarization (MDS) in the seq2seq framework (Liu et al., 2018;Lebanoff et al., 2018;Fabbri et al., 2019;Liu and Lapata, 2019;Ma et al., 2020). \n\nThis study makes an exploratory attempt to improve the established abstractive summarization models for multi-document summarization (MDS) utilizing the Transformer (Vaswani et al., 2017) architecture. In comparison to single-document summarization, MDS brings challenges on the representation and coverage of its lengthy and linked sources. Liu et al. (2018) propose a two-stage model to first extractively select the important paragraphs, then train the concatenated flat sequences using the Transformer-decoder with memory compressed attention (T-DMCA). Although the two-stage approach effectively reduces redundant information of source documents and retains salient information as inputs, it fails to take into account the cross-document relationship in its summaries. On the other hand, Liu and Lapata (2019) propose a Hierarchical Transformer (HT) with local and global encoder layers to represent cross-token and cross-document information.",
            "reference_string": "[251594480 | Ma et al. | 2022 | Citations: 0]"
        },
        {
            "title": "N-GPETS: Neural Attention Graph-Based Pretrained Statistical Model for Extractive Text Summarization",
            "venue": "Computational Intelligence and Neuroscience",
            "year": 2022,
            "reference_count": 67,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://downloads.hindawi.com/journals/cin/2022/6241373.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9708337, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257111507",
                    "name": "Muhammad Umair"
                },
                {
                    "authorId": "46487765",
                    "name": "I. Alam"
                },
                {
                    "authorId": "2149073250",
                    "name": "Atif Khan"
                },
                {
                    "authorId": "143835958",
                    "name": "Inayat Khan"
                },
                {
                    "authorId": "2175498951",
                    "name": "Niamat Ullah"
                },
                {
                    "authorId": "2191729128",
                    "name": "Mohammad Yusuf Momand"
                }
            ],
            "abstract": "The extractive summarization approach involves selecting the source document's salient sentences to build a summary. One of the most important aspects of extractive summarization is learning and modelling cross-sentence associations. Inspired by the popularity of Transformer-based Bidirectional Encoder Representations (BERT) pretrained linguistic model and graph attention network (GAT) having a sophisticated network that captures intersentence associations, this research work proposes a novel neural model N-GPETS by combining heterogeneous graph attention network with BERT model along with statistical approach using TF-IDF values for extractive summarization task. Apart from sentence nodes, N-GPETS also works with different semantic word nodes of varying granularity levels that serve as a link between sentences, improving intersentence interaction. Furthermore, proposed N-GPETS becomes more improved and feature-rich by integrating graph layer with BERT encoder at graph initialization step rather than employing other neural network encoders such as CNN or LSTM. To the best of our knowledge, this work is the first attempt to combine the BERT encoder and TF-IDF values of the entire document with a heterogeneous attention graph structure for the extractive summarization task. The empirical outcomes on benchmark news data sets CNN/DM show that the proposed model N-GPETS gets favorable results in comparison with other heterogeneous graph structures employing the BERT model and graph structures without the BERT model.",
            "corpus_id": 253763913,
            "sentences": [
                {
                    "corpus_id": "253763913",
                    "title": "N-GPETS: Neural Attention Graph-Based Pretrained Statistical Model for Extractive Text Summarization",
                    "text": "Te authors of [50] developed a hybrid method for producing summaries of long scientifc texts that combined the benefts of both extractive and abstractive designs. Te authors in [51,52] use the deep learning model BERT and RISTECB model to answer important questions related to the COVID-19 research articles. Te authors of [44] demonstrated an excellent tuning-based approach for extractive summarization using the BERT model. Te BERT model was also used by the authors of [7,8,16,36,46] for contextual representation in summarization tasks. Te authors in [53] use the BERT model to automatically generate titles from a huge set of published literature or related work. Additionally, extractive summarization tasks using graph structures have been carried out by exploiting linguistic and statistical information included in sentences [9]. Recent research has combined neural networks with graphs, or (GNNs), and used the encoder-decoder structure for extractive summarization [13,54]. Many researchers nowadays use a heterogeneous graph neural network with multiple updated nodes rather than a homogeneous graph structure with no updated nodes for extractive summarization tasks. Te study [55] proposed a bipartite graph attention network for multihop reading comprehension (RC) across documents that encoded diferent documents and entities together. Te authors in [48] presented an approach that modeled redundancy-aware heterogeneous graphs and refned sentence representation using neural networks for extractive summarization. Te studies [9,56]  4 \n\nComputational Intelligence and Neuroscience between sentences are learned. Te work done by [14] built a sentence-level graph-based model, using BERT for sentence encoding and joint neural network model (NTM) for discovering latent topic information. Te authors in [15] proposed a heterogeneous graph structure for modelling crosssentence relationship between sentences. To represent the relationships between the EDUs, they used three diferent types of nodes, including sentence nodes, EDU nodes, and entity nodes, and RST discourse parsing and leverage external discourse expertise to enhance the model's performance. Te next section goes over the unique model N-GPETS methodology that is proposed in this study in depth.",
                    "score": 0.4874881987267635,
                    "section_title": "Extractive Text Summarization Approaches and",
                    "char_start_offset": 16057,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 162
                        },
                        {
                            "start": 163,
                            "end": 308
                        },
                        {
                            "start": 309,
                            "end": 426
                        },
                        {
                            "start": 427,
                            "end": 541
                        },
                        {
                            "start": 542,
                            "end": 669
                        },
                        {
                            "start": 670,
                            "end": 839
                        },
                        {
                            "start": 840,
                            "end": 985
                        },
                        {
                            "start": 986,
                            "end": 1180
                        },
                        {
                            "start": 1181,
                            "end": 1351
                        },
                        {
                            "start": 1352,
                            "end": 1530
                        },
                        {
                            "start": 1531,
                            "end": 1551
                        },
                        {
                            "start": 1554,
                            "end": 1628
                        },
                        {
                            "start": 1629,
                            "end": 1803
                        },
                        {
                            "start": 1804,
                            "end": 1923
                        },
                        {
                            "start": 1924,
                            "end": 2172
                        },
                        {
                            "start": 2173,
                            "end": 2276
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 177,
                            "end": 181,
                            "matchedPaperCorpusId": "246287960"
                        },
                        {
                            "start": 181,
                            "end": 184,
                            "matchedPaperCorpusId": "246418639"
                        },
                        {
                            "start": 323,
                            "end": 327,
                            "matchedPaperCorpusId": "201304248"
                        },
                        {
                            "start": 476,
                            "end": 478,
                            "matchedPaperCorpusId": "6405271"
                        },
                        {
                            "start": 478,
                            "end": 481,
                            "matchedPaperCorpusId": "247041797"
                        },
                        {
                            "start": 481,
                            "end": 484,
                            "matchedPaperCorpusId": "221845203"
                        },
                        {
                            "start": 484,
                            "end": 487,
                            "matchedPaperCorpusId": "219036690"
                        },
                        {
                            "start": 556,
                            "end": 560,
                            "matchedPaperCorpusId": "237588401"
                        },
                        {
                            "start": 835,
                            "end": 838,
                            "matchedPaperCorpusId": "49656757"
                        },
                        {
                            "start": 977,
                            "end": 981,
                            "matchedPaperCorpusId": "222310577"
                        },
                        {
                            "start": 981,
                            "end": 984,
                            "matchedPaperCorpusId": "6532096"
                        },
                        {
                            "start": 1190,
                            "end": 1194,
                            "matchedPaperCorpusId": "158046817"
                        },
                        {
                            "start": 1366,
                            "end": 1370,
                            "matchedPaperCorpusId": "250012576"
                        },
                        {
                            "start": 1542,
                            "end": 1545,
                            "matchedPaperCorpusId": "49656757"
                        },
                        {
                            "start": 1545,
                            "end": 1548,
                            "matchedPaperCorpusId": "246588027"
                        },
                        {
                            "start": 1645,
                            "end": 1649,
                            "matchedPaperCorpusId": "233189611"
                        },
                        {
                            "start": 1818,
                            "end": 1822,
                            "matchedPaperCorpusId": "220586224"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95947265625
                }
            ],
            "relevance_judgement": 0.95947265625,
            "relevance_judgment_input_expanded": "# Title: N-GPETS: Neural Attention Graph-Based Pretrained Statistical Model for Extractive Text Summarization\n# Venue: Computational Intelligence and Neuroscience\n# Authors: Muhammad Umair, I. Alam, Atif Khan, Inayat Khan, Niamat Ullah, Mohammad Yusuf Momand\n## Abstract\nThe extractive summarization approach involves selecting the source document's salient sentences to build a summary. One of the most important aspects of extractive summarization is learning and modelling cross-sentence associations. Inspired by the popularity of Transformer-based Bidirectional Encoder Representations (BERT) pretrained linguistic model and graph attention network (GAT) having a sophisticated network that captures intersentence associations, this research work proposes a novel neural model N-GPETS by combining heterogeneous graph attention network with BERT model along with statistical approach using TF-IDF values for extractive summarization task. Apart from sentence nodes, N-GPETS also works with different semantic word nodes of varying granularity levels that serve as a link between sentences, improving intersentence interaction. Furthermore, proposed N-GPETS becomes more improved and feature-rich by integrating graph layer with BERT encoder at graph initialization step rather than employing other neural network encoders such as CNN or LSTM. To the best of our knowledge, this work is the first attempt to combine the BERT encoder and TF-IDF values of the entire document with a heterogeneous attention graph structure for the extractive summarization task. The empirical outcomes on benchmark news data sets CNN/DM show that the proposed model N-GPETS gets favorable results in comparison with other heterogeneous graph structures employing the BERT model and graph structures without the BERT model.\n## Extractive Text Summarization Approaches and\nTe authors of [50] developed a hybrid method for producing summaries of long scientifc texts that combined the benefts of both extractive and abstractive designs. Te authors in [51,52] use the deep learning model BERT and RISTECB model to answer important questions related to the COVID-19 research articles. Te authors of [44] demonstrated an excellent tuning-based approach for extractive summarization using the BERT model. Te BERT model was also used by the authors of [7,8,16,36,46] for contextual representation in summarization tasks. Te authors in [53] use the BERT model to automatically generate titles from a huge set of published literature or related work. Additionally, extractive summarization tasks using graph structures have been carried out by exploiting linguistic and statistical information included in sentences [9]. Recent research has combined neural networks with graphs, or (GNNs), and used the encoder-decoder structure for extractive summarization [13,54]. Many researchers nowadays use a heterogeneous graph neural network with multiple updated nodes rather than a homogeneous graph structure with no updated nodes for extractive summarization tasks. Te study [55] proposed a bipartite graph attention network for multihop reading comprehension (RC) across documents that encoded diferent documents and entities together. Te authors in [48] presented an approach that modeled redundancy-aware heterogeneous graphs and refned sentence representation using neural networks for extractive summarization. Te studies [9,56]  4 \n\nComputational Intelligence and Neuroscience between sentences are learned. Te work done by [14] built a sentence-level graph-based model, using BERT for sentence encoding and joint neural network model (NTM) for discovering latent topic information. Te authors in [15] proposed a heterogeneous graph structure for modelling crosssentence relationship between sentences. To represent the relationships between the EDUs, they used three diferent types of nodes, including sentence nodes, EDU nodes, and entity nodes, and RST discourse parsing and leverage external discourse expertise to enhance the model's performance. Te next section goes over the unique model N-GPETS methodology that is proposed in this study in depth.",
            "reference_string": "[253763913 | Umair et al. | 2022 | Citations: 3]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "248496597",
            "title": "Large-Scale Multi-Document Summarization with Information Extraction and Compression",
            "text": "Dohare et al. (2018) propose to generate abstractive summaries by first generating AMR graphs (abstract meaning representation) for corresponding input stories, extract summary graphs from the AMR graphs, and lastly create summaries from the summary graphs. \n\nIn the field of unsupervised multi-document summarization, Chu and Liu (2019) propose an end-to-end, unsupervised, abstractive, neural summarization model. Their model consists of two parts: (i) an LSTM-based auto-encoder that learns a representation of each input text and (ii) a summarization module that learns to generate summaries based on the representation encoded by the auto-encoder. The content of all input texts is obtained as the mean of all last hidden and cell states of the LSTM network. This information is then fed into the decoder of the auto-encoder to decode a summary. Lastly, they evaluate average similarity of summary to all input texts by encoding the summary again with the encoder of the auto-encoder and computing the average cosine distance between the hidden states of each encoded input text and the hidden state of the encoded summary. \n\nTheir approach requires no labeled training data to summarize multiple documents. However, it is also based on an assumption that all input texts to be summarized contain similar information. This is often not the case in real world as often times documents to be summarized have different gist. In contrast to neural-based summarization methods, Nayeem et al. (2018) propose an unsupervised multi-document summarization system comprising of word graph-based (Filippova, 2010) sentence fusion and integer linear programming (ILP)-based sentence ranking. They first apply hierarchical agglomerative clustering with complete linkage to all sentences in the documents, where the distance between two sentences are based on their continuous representations. For each cluster of related sentences, they generate abstractive fused sentences by first constructing a word graph and then apply k-shortest path on it. They not only merge similar sentences into a single one but also substitute such lexicals as verbs or nouns with ones having functional similarities, making the resulting sentence more abstractive.",
            "score": 0.8423400779107223,
            "section_title": "Related Work",
            "char_start_offset": 6251,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 260,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1128
                },
                {
                    "start": 1131,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2236
                }
            ],
            "ref_mentions": [
                {
                    "start": 319,
                    "end": 337,
                    "matchedPaperCorpusId": "59413781"
                },
                {
                    "start": 1478,
                    "end": 1498,
                    "matchedPaperCorpusId": "52011473"
                },
                {
                    "start": 1590,
                    "end": 1607,
                    "matchedPaperCorpusId": "14750088"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96240234375
        },
        {
            "corpus_id": "259292631",
            "title": "Cogito Ergo Summ: Abstractive Summarization of Biomedical Papers via Semantic Parsing Graphs and Consistency Rewards",
            "text": "Abstractive Summarization Sequence-to-sequence architectures founded on self-supervised pre-trained transformers have been responsible for a keen impetus in implicitly learning abstractive summarization procedures (Zhang et al. 2020a), comprising multi-document (Moro et al. 2022) and low-resource settings (Moro and Ragazzi 2022;Moro et al. 2023). Nevertheless, modern solutions are highly prone to hallucinating content (Cao et al. 2018;Maynez et al. 2020) or falling back on extraction (See, Liu, and Manning 2017). Gaining an understanding of semantics and context is becoming a prerogative, but a model trained purely on form cannot learn meaning (Bender and Koller 2020)-even with more data and huge architectural dimensions. \n\nGraph-enhanced Summarization Graph structures have long been studied for implementing summarization subtasks (i.e., information extraction, content selection, sur-1 Inspired by the first principle of Ren\u00e9 Descartes's philosophy, we coin the name Cogito Ergo Summ to emphasize the researched neural network capacity of \"thinking\" about the inner semantics of the text-via joint text-graph reasoning-before summarizing. face realization), registering distinct benefits depending on their composition. Early techniques for extractive summarization build intra- (Mihalcea and Tarau 2004) and interdocument (Wan 2008) cosine similarity connectivity networks to identify salient sentences. Late hybrid neural systems mainly stand on the shoulders of run-of-the-mill GNNs (Wu et al. 2021a), exploiting graph-based attention (Tan, Wan, and Xiao 2017) and heterogeneous word-/sentencelevel nodes (Wang et al. 2020). As for abstractive summarization, the community has attempted a medley of graphical document representations, from dependency (Wu et al. 2021b), sentiment (Moro et al. 2018), and coreference links (Balachandran et al. 2021) to latent co-occurrences (Frisoni and Moro 2020), discourse relations (Li et al. 2020; Chen and Yang 2021) and citation networks (An et al. 2021).",
            "score": 0.826107303999808,
            "section_title": "Related Work",
            "char_start_offset": 5255,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 731
                },
                {
                    "start": 734,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 2011
                }
            ],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 234,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 422,
                    "end": 439,
                    "matchedPaperCorpusId": "19198109"
                },
                {
                    "start": 489,
                    "end": 517,
                    "matchedPaperCorpusId": "202541447"
                },
                {
                    "start": 652,
                    "end": 676,
                    "matchedPaperCorpusId": "211029226"
                },
                {
                    "start": 1292,
                    "end": 1316,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 1499,
                    "end": 1515,
                    "matchedPaperCorpusId": "235187330"
                },
                {
                    "start": 1551,
                    "end": 1576,
                    "matchedPaperCorpusId": "26698484"
                },
                {
                    "start": 1621,
                    "end": 1639,
                    "matchedPaperCorpusId": "216552978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86083984375
        },
        {
            "corpus_id": "271938110",
            "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
            "text": "\u2022 Leveraging data from the publicly available PubMed dataset, our research demonstrates consistent and favorable results compared to state-of-the-art heterogeneous graph structures used for both extractive and abstractive summarization tasks. The paper is organized as follows: Section II critically reviews existing abstractive summarization techniques, Topic models, and Neural Graph-based models. Section III presents the proposed model's methodology in detail. Section IV illustrates a comparative analysis of our model with stateof-the-art approaches. It also details the experimental setup, overall performance, and ablation studies of the proposed model. Finally, Section V concludes the paper and outlines potential avenues for future research.",
            "score": 0.8153727262387422,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 6673,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 752
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.921875
        },
        {
            "corpus_id": "250374734",
            "title": "Two-phase Multi-document Event Summarization on Core Event Graphs",
            "text": "Existing multi-document summarization methods mainly focus on sentence-level summarization and can be categorized into extractive and abstractive methods. Most extractive methods are operated over graph-based representations of sentences or passages with edge weights computed by tf-idf (Erkan et al., 2004), discourse relations (Christensen et al., 2013) or sentence embeddings (Yasunaga et al., 2017b) and then a specific algorithm is further adopted for ranking text units for inclusion in the final summary. More recently, some extractive summarization works also utilize graph convolutional networks for salient sentences estimation (Kipf et al., 2016) and sentence ordering (Yin et al., 2019). Abstractive models, especially neural abstractive ones, have achieved promising results on single-document summarization (See et al., 2017;Paulus et al., 2018;Lewis et al., 2019). However, the extension of sequence-to-sequence architectures to multi-document summarization is less straightforward due to the lack of sufficient training data and the computational challenge of processing multiple documents. Intuitively, graph-based extractive methods are suitable to identify relationship of different sentences or documents and extract salient information, while neural sequence-to-sequence architectures are effective in abstraction and content rewriting. Hence, abstractive models based on graphs gain much attention (Yasunaga et al., 2017b;Li et al., 2020). Our model, which also combines a graph-based extractive module and neural abstractive sequence-to-sequence architecture, is a higher level summarization and focuses on the core event summarization from multiple documents, which is more challenging.",
            "score": 0.7984654416181444,
            "section_title": "Multi-document Summarization",
            "char_start_offset": 5936,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1710
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 307,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 821,
                    "end": 839,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 839,
                    "end": 859,
                    "matchedPaperCorpusId": "21850704"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9775390625
        },
        {
            "corpus_id": "248571519",
            "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization",
            "text": "Abstractive multi document summarization is the task of writing a single summary of the key points and content in multiple related documents. This task has evolved from research in single document abstractive and extractive summarization; however, it faces unique challenges due to input documents having duplicate and conflicting content as well a larger body of text. (Radev et al., 2000). This task has evolved from early approaches using sequence to sequence (Seq2Seq) neural architectures to transformer based architectures with the introduction of large-scale datasets (Liu et al., 2018), (Fabbri et al., 2019). Beyond the introduction of approaches now commonly used for single document abstractive summarization, cross document attention and graphs that capture relations between text in various documents have further improved the state of the art for multi document summarization tasks. (Liu et al., 2018), (Li et al., 2014). These graphs aim to better represent the inter dependencies between articles by representing text spans as nodes in the graph and capturing the relations between these sentences as edge weights. \n\nDespite the advances made with these approaches, a significant challenge remains in multi document summarization with respect to how to deal with contradictory information present in the multiple source documents. It is critical to both learn the relationships between different documents as well as to extract salient information that is consistent with the output viewpoint. This is a situation often faced with summarizing multiple news articles where different viewpoints on an issue can significantly change the semantic structure of the content present in each article making it challenging for the abstractive summarization model to learn the relationships between inconsistent or conflicting information. In this work we define conflicting opinions as a combined measure of the polarity and sentiment of text. By this definition, two pieces of text on the same topic that have a differing sentiment and/ or polarity are determined to have different viewpoints. This definition is used throughout the paper. \n\nThis paper proposes ACM, attribute conditioned multi document summarization, a novel approach to multi document summarization that incorporates an attribute conditioning module with an abstractive multi document summarization model in order to condition for a particular attribute when generating the multi document summary.",
            "score": 0.7961666082314961,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2101
                },
                {
                    "start": 2102,
                    "end": 2147
                },
                {
                    "start": 2150,
                    "end": 2474
                }
            ],
            "ref_mentions": [
                {
                    "start": 370,
                    "end": 390,
                    "matchedPaperCorpusId": "1320"
                },
                {
                    "start": 917,
                    "end": 934,
                    "matchedPaperCorpusId": "10112929"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96435546875
        },
        {
            "corpus_id": "259203165",
            "title": "One model to rule them all: ranking Slovene summarizers",
            "text": "Early approaches to text summarization relied on statistical frequencies of words, sentence position, and sentences containing keywords [12]. These approaches aimed to extract important sentences or phrases from a text and generate a summary by concatenating them. Abstractive methods involved deleting less important words from the text to create a summary [6].\n\nGraph-based methods have been another popular approach to text summarization. In this approach, the document is represented as a graph, where sentences are nodes, and edges represent the relationships between them. The graph is then used to generate a summary by selecting the most important sentences. This method has been explored in several works [10], [3].\n\nWith the advent of neural networks, there has been an increasing interest in developing abstractive summarization techniques. Early neural abstractive systems used methods such as LSTM and other recurrent neural networks [14], [11]. However, transformer-based architectures have emerged as state-of-the-art models for abstractive text summarization [18], [9]. These models use self-attention mechanisms to selectively focus on important parts of the text and can generate more fluent and coherent summaries compared to earlier methods.\n\nWhile several approaches have been proposed for text summarization, many of them are designed to handle specific genres or types of text. In this work, our goal is to build a summarization system that can handle every type of text and genre with every possible property that can appear in the real world. This includes texts of varying lengths, topics, styles, and summaries that capture the most important information in the text. Achieving this goal requires developing a robust and adaptable model that can learn to summarize texts of diverse types and produce high-quality summaries.",
            "score": 0.7850071439596589,
            "section_title": "Related work",
            "char_start_offset": 4021,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 358,
                    "end": 361,
                    "matchedPaperCorpusId": "7793213"
                },
                {
                    "start": 714,
                    "end": 718,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 720,
                    "end": 723,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 947,
                    "end": 951,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 953,
                    "end": 957,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 1075,
                    "end": 1079,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 1081,
                    "end": 1084,
                    "matchedPaperCorpusId": "204960716"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92822265625
        },
        {
            "corpus_id": "271938110",
            "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
            "text": "This category includes models like Topic-GraphSum [12], SSN-DM [57] HeterGraphLongSum [14] and GTASum [3] that utilize graph structures to represent document information and relationships between sentences or topics, facilitating summarization. \n\nTable 2 in this study shows the Rouge F-scores for various models. The first segment of Table 2 demonstrates results for traditional summarization models and Oracle. The second segment presents the results of neural abstractive summarization models. The third segment reveal the results of Match-Sum, a sophisticated BERT-based summarizing model. The fourth segment illustrates the results achieved by recent advanced graph-based models for abstractive summarization. The last segment of Table 2 presents the results of our proposed model. \n\nOur model significantly outperformed conventional extractive summarization models on all ROUGE metrics, as shown in Table 2. This achievement extends to state-of-the-art neural abstractive and extractive models, where our approach also achieved higher ROUGE scores. This improvement highlights the benefit of integrating global semantic information with a dedicated graph layer. This layer facilitates the model's ability to generate summaries by considering the broader context within the document. \n\nAdditionally, traditional sequence-to-sequence (seq2seq) models with attention and pointer networks often struggle with lengthy scientific documents. This limitation arises from the challenges encoders face in capturing long-range dependencies within long texts [57]. Our model overcomes this limitation. Furthermore, our model surpasses the advanced BERT-based Match-Sum model, which experiences performance drops on the PubMed scientific dataset. Match-Sum's difficulty lies in grasping semantic and global information, hindering its ability to interpret the meaning of sentences and summaries. In contrast, our model can learn semantic information and leverage latent topics to focus on salient in long documents. \n\nOur model's performance is compared with highly advanced neural graph-based models for both extractive and abstractive summarization, which utilize rich semantic information. Our model achieved near-identical results in R-1, comparable results in R-2, and surpassed Topic-GraphSum in R-L.",
            "score": 0.7653193041829127,
            "section_title": "5) GRAPH BASED MODELS",
            "char_start_offset": 32157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 244
                },
                {
                    "start": 247,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 2007
                },
                {
                    "start": 2010,
                    "end": 2184
                },
                {
                    "start": 2185,
                    "end": 2298
                }
            ],
            "ref_mentions": [
                {
                    "start": 63,
                    "end": 67,
                    "matchedPaperCorpusId": "235097475"
                },
                {
                    "start": 86,
                    "end": 90,
                    "matchedPaperCorpusId": "252818946"
                },
                {
                    "start": 102,
                    "end": 105,
                    "matchedPaperCorpusId": "250012667"
                },
                {
                    "start": 1553,
                    "end": 1557,
                    "matchedPaperCorpusId": "235097475"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9658203125
        },
        {
            "corpus_id": "265607988",
            "title": "Generating Extractive and Abstractive Summaries in Parallel from Scientific Articles Incorporating Citing Statements",
            "text": "Another approach is to construct a sentence-level fully connected graph. Zhong et al. (Zhong et al., 2019) and Liu et al. (Liu and Lapata, 2019) used transformer (Vaswani et al., 2017) encoders to determine how sentences interact with each other. Wang et al. (Wang et al., 2020) introduced an heterogeneous graph neural network for extractive summarization which used additional semantic units (words) as intermediate nodes to construct relationships between sentences. \n\nAbstractive summarizers focus heavily on form, with the goal of producing a generalized summary, which tends to necessitate complex languagegenerating models. These models are typically based on sequence-to-sequence (seq2seq) architectures, in which a source document is seen as one sequence whereas its summary as another. The majority of previous research on neural abstractive summarization depended on large-scale, high-quality datasets of supervised document-summarization pairings (See et al., 2017). Recently, state-ofthe-art solutions on abstractive summarization are built upon the transformer (Vaswani et al., 2017) and BERT (Devlin et al., 2019) models. These attention-based abstractive models are being used in different fields like clinical note summariza-tion (Kanwal and Rizzo, 2022), scientific document summarization (Zhang et al., 2022), and layabstract generation (Yu et al., 2020). \n\nIn this paper, addressing the above-mentioned issues, we have built a standalone summarization model which can generate both extractive and abstractive summaries from scientific documents incorporating the citation network. Analyzing the citation network, citing statements from the citing articles are accumulated with the original text document to incorporate the research community's observation on that particular cited manuscript. These summaries are the abstracts of the original papers with additional information reflecting the research community's view. After that, we run the Long-Former (Beltagy et al., 2020) encoder to generate sentence and word representations and train extractive and abstractive summarizers together.",
            "score": 0.7531684553059691,
            "section_title": "Introduction",
            "char_start_offset": 4141,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 469
                },
                {
                    "start": 472,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1374
                },
                {
                    "start": 1377,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2110
                }
            ],
            "ref_mentions": [
                {
                    "start": 122,
                    "end": 144,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 162,
                    "end": 184,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 247,
                    "end": 278,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 959,
                    "end": 977,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1075,
                    "end": 1097,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1107,
                    "end": 1128,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1247,
                    "end": 1271,
                    "matchedPaperCorpusId": "233296914"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.962890625
        },
        {
            "corpus_id": "273784877",
            "title": "Cross-Document Distillation via Graph-Based Summarization of Extracted Essential Knowledge",
            "text": "Cross-Document Distillation via Graph-Based Summarization of Extracted Essential Knowledge Luca Ragazzi , Gianluca Moro , Lorenzo Valgimigli, and Riccardo Fiorani Abstract-Abstractive multi-document summarization aims to generate a comprehensive summary that encapsulates crucial content derived from multiple input documents. Despite the proficiency exhibited by language models in text summarization, challenges persist in capturing and aggregating salient information dispersed across a cluster of lengthy sources. To accommodate more input, existing solutions prioritize sparse attention mechanisms, relying on sequence truncation without incorporating graph-based modeling of multiple semantic units to locate essential facets. Furthermore, the limited availability of training examples adversely impacts performance, thereby compromising summarization quality in real-world few-shot scenarios. In this paper, we present G-SEEK-2, a graph-enhanced approach designed to distill multiple topic-related documents by pinpointing and processing solely the pertinent information. We use a heterogeneous graph to model the input cluster, interconnecting various encoded entities via informative semantic edges. Then, a graph neural network locates the most salient sentences that are provided to a language model to generate the summary. We extensively evaluate G-SEEK-2 across seven datasets spanning various domains-including news articles, lawsuits, government reports, and scientific texts-under few-shot settings with a limited training sample size of only 100 examples. The experimental findings demonstrate that our model consistently outperforms advanced summarization baselines, achieving improvements as measured by syntactic and semantic metrics. \n\nIndex Terms-Multi-document summarization, graphenhanced transformer, few-shot learning.",
            "score": 0.7189857944159705,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1755
                },
                {
                    "start": 1758,
                    "end": 1845
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.982421875
        },
        {
            "corpus_id": "255125107",
            "title": "GAE-ISUMM: Unsupervised Graph-based Summarization for Indian Languages",
            "text": "Document summarization aims to minimize the content in a text document and preserve the salient information. There are usually two categories of summarization techniques: Extractive [1] and Abstractive [2]. Extractive summarization extracts salient text from the document, whereas Abstractive summarization concisely paraphrases the information contained in the document. Understanding text's contextual and semantic representation is one of the main challenges for effective summaries. Traditional methods extract summaries based on hand-crafted features, including term Frequency [3], Sentence Position, and Length [4], and largely depend on the availability of natural language processing (NLP) tools.\n\nSignificant progress has been made in single document extractive summarization by using recent popular deep learning models such as recurrent neural networks (RNNs) [1], convolutional neural networks (CNNs) [5], attention-based models [6]. In particular, most of these models follow encoderdecoder-based approaches to generate summaries in a supervised or unsupervised setting [7]. However, these models lack in capturing the global context and the long-distance 1 https://github.com/scsmuhio/Summarization sentence relationships in a document. Modeling long-range inter-sentence relationships with transformer-based models is still challenging [8] and requires massive computation and memory. Graph-based approaches assist in overcoming these limitations by capturing the long-term dependencies and treating the document as a graph [9].\n\nIn NLP, the recently proposed graph neural networks (GNNs) have been used to capture the cross-sentence relationships for summarization [10]. Another work proposed a Graph-Clustering framework to summarize financial text documents [11]. In particular, graph convolutional networks (GCNs) have proven their dominance in several NLP tasks such as text classification [12], and semantic role labelling [13]. GCNs are capable of drawing syntactic information, exploiting long-range multi-word relations, and have been deployed on document-word relationships [14]. More recently, Kipf et al. [15] built a graph autoencoder (GA",
            "score": 0.7078278366411294,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 182,
                    "end": 185,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 582,
                    "end": 585,
                    "matchedPaperCorpusId": "15475171"
                },
                {
                    "start": 617,
                    "end": 620,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 871,
                    "end": 874,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 941,
                    "end": 944,
                    "matchedPaperCorpusId": "4500716"
                },
                {
                    "start": 1083,
                    "end": 1086,
                    "matchedPaperCorpusId": "223953446"
                },
                {
                    "start": 1681,
                    "end": 1685,
                    "matchedPaperCorpusId": "219036690"
                },
                {
                    "start": 1776,
                    "end": 1780,
                    "matchedPaperCorpusId": "246081124"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9208984375
        },
        {
            "corpus_id": "271903777",
            "title": "GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization",
            "text": "Multi-document summarization (MDS) aims to produce a summary from a document set containing a series of related topics. The generated summary needs to cover all important information in the document set, while remaining fluent and concise. Compared with single-document summarization, it's more challenging because an increasing number of input documents will make the source information more redundant and scattered. It also has practical significance, for example, key information of multiple news articles can be generated efficiently. Multi-document summarization approaches can also be applied in other scenarios, such as extracting opinions from social media. \n\nAs the application of MDS becomes more and more widespread, various approaches have been proposed. Non-neural approaches are primarily based on extracting key sentences (Erkan and Radev, 2004;Mihalcea and Tarau, 2004;Rossiello et al., 2017). These approaches assess sentence importance based on their relevance to each other or proximity to keywords, selecting sentences with high importance scores to form the summary. The main drawback of such approaches is that they retain only a subset of key sentences, which can lead to information loss and may not capture finegrained details. \n\nNeural approaches can generate more abstractive text and are recently widely used in multidocument summarization. Given the structural characteristics of the multi-document input, most approaches utilize attention mechanism to build hierarchical models (Fabbri et al., 2019;Mao et al., 2020;Jin et al., 2020), enabling the extraction of different-grained features and the selection of important information. Other methods employ graphs to model relationships and can leverage interaction features to enhance representation (Yasunaga et al., 2017;Yin et al., 2019;Li et al., 2020). Despite their capability in extracting abstract features, neural models are often resource-intensive and require large parallel training datasets. Moreover, recent studies (Pagnoni et al., 2021) have shown that they also suffer from factuality problems. \n\nPre-trained language models use large-scale cor- pora to optimize objective functions, allowing them to acquire more general feature representations. This capability enables them to require only a small amount of data to fine-tune downstream tasks and achieve impressive results.",
            "score": 0.7018824496476516,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 665
                },
                {
                    "start": 668,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1252
                },
                {
                    "start": 1255,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2089
                },
                {
                    "start": 2092,
                    "end": 2241
                },
                {
                    "start": 2242,
                    "end": 2371
                }
            ],
            "ref_mentions": [
                {
                    "start": 837,
                    "end": 860,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 860,
                    "end": 885,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 885,
                    "end": 908,
                    "matchedPaperCorpusId": "2346086"
                },
                {
                    "start": 1508,
                    "end": 1529,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 1529,
                    "end": 1546,
                    "matchedPaperCorpusId": "222090788"
                },
                {
                    "start": 1546,
                    "end": 1563,
                    "matchedPaperCorpusId": "220633461"
                },
                {
                    "start": 1778,
                    "end": 1801,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 1801,
                    "end": 1818,
                    "matchedPaperCorpusId": "199466313"
                },
                {
                    "start": 1818,
                    "end": 1834,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 2008,
                    "end": 2030,
                    "matchedPaperCorpusId": "233407441"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88671875
        },
        {
            "corpus_id": "254246336",
            "title": "NarraSum: A Large-Scale Dataset for Abstractive Narrative Summarization",
            "text": "We investigate the performance of several baselines and state-of-the-art neural summarization models on NARRASUM. We include both extractive and abstractive models. For extractive models, we use the following methods: RANDOM selects n sentences from the document randomly. LEAD selects the top-n sentences from the document to compose the summary. This is a strong baseline for news summarization. TEXTRANK (Mihalcea and Tarau, 2004) is a graphbased extractive summarization model based on PageRank (Brin and Page, 1998) in a graph representation of sentences. LEXRANK (Erkan and Radev, 2004) is another graph-based extractive summarization model based on eigenvector centrality . HSG (Wang et al., 2020) is a heterogeneous graphbased neural extractive summarization model that uses word co-occurrence to enhance sentence contextual representation. PRESUMM (Liu and Lapata, 2019) relies on a pretrained language model to enhance the sentence representation during text encoding and extractive summarization. We choose BERT (Devlin et al., 2019), ROBERTA (Liu et al., 2019), and LONG-FORMER (Beltagy et al., 2020) as the pre-trained models. BERT and RoBERTa limit the input length to be shorter than 512 tokens, while Longformer can accept up to 4,096 tokens. \n\nFor abstractive models, we use the following pre-trained sequence-to-sequence models: BART (Lewis et al., 2020), T5 (Raffel et al., 2020), PEGASUS (Zhang et al., 2020), and LED (Beltagy et al., 2020). The input length of the first three models is limited to 512 (base version) or 1,024 (large version). LED uses Longformer as the encoder and therefore can accept up to 4,096 tokens as input.",
            "score": 0.6818732439535895,
            "section_title": "Baseline Models",
            "char_start_offset": 22609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1258
                },
                {
                    "start": 1261,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1652
                }
            ],
            "ref_mentions": [
                {
                    "start": 407,
                    "end": 432,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 569,
                    "end": 591,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 685,
                    "end": 703,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 857,
                    "end": 879,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 1023,
                    "end": 1044,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1352,
                    "end": 1372,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1377,
                    "end": 1398,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1408,
                    "end": 1428,
                    "matchedPaperCorpusId": "209405420"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88330078125
        },
        {
            "corpus_id": "266362657",
            "title": "Shaping Political Discourse using multi-source News Summarization",
            "text": "There has been a lot of recent research interest in multi-document summarization, primarily owing to the success of deep learning in the single document case. In [10] the authors propose a document-level reconstruction framework named DocRebuild, which reconstructs the documents with summary sentences through a neural document model and selects summary sentences to minimize the reconstruction error. They also apply two strategies, sentence filtering and beamsearch, to improve the performance of their method. \n\n[19] proposes a neural multi-document summarization system that incorporates sentence relation graphs. They employ a Graph Convolutional Network (GCN) on the relation graphs, with sentence embeddings obtained from Recurrent Neural Networks as input node features. Through multiple layer-wise propagation, the GCN generates high-level hidden sentence features for salience estimation. The authors then use a greedy heuristic to extract salient sentences that avoid redundancy. \n\nMore recently, [8] uses a transformer based model to generate summaries, drawing from the recent success of transformers in natural language understanding and generation tasks. The authors use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, they introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder-decoder architectures used in sequence transduction. They show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. [7] extends methods from single document summarization to multiple documents. Even more recently, [4] converts the multiple source documents into a knowledge graph before feeding it into a summarization module. \n\nArgument mining [11,5] is an area within natural language understanding that focuses on mining arguments from opinionated text. It is an advanced form of human language understanding by the machine. When sufficient explicit discourse markers are present in the language utterances, the argumentation can be interpreted by the machine with an acceptable degree of accuracy.",
            "score": 0.6760399626357555,
            "section_title": "Related Work",
            "char_start_offset": 3310,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 513
                },
                {
                    "start": 516,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 991
                },
                {
                    "start": 994,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1841
                },
                {
                    "start": 1844,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2216
                }
            ],
            "ref_mentions": [
                {
                    "start": 162,
                    "end": 166,
                    "matchedPaperCorpusId": "5845797"
                },
                {
                    "start": 1860,
                    "end": 1864,
                    "matchedPaperCorpusId": "153313213"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95361328125
        },
        {
            "corpus_id": "277284940",
            "title": "Global and Local Context Fusion in Heterogeneous Graph Neural Network for Summarizing Lengthy Scientific Documents",
            "text": "Table 3 and Table 4 demonstrate the ROUGE F-scores for our proposed model and cutting-edge summarization systems on the arXiv and PubMed datasets respectively. The results are categorized into six distinct segments for clarity. Traditional extractive summarization models, including SumBasic, LexRank, LSA, and Oracle, form the first group. As expected, Oracle ranks the highest across all metrics due to its optimized approach of selecting sentences that have the maximum overlap with reference summaries. Neural abstractive summarization models form the second group, excelling at rephrasing content but often struggling with long documents due to difficulties in capturing document-wide semantics. Neural extractive summarization models, the third group, demonstrate improvements over traditional approaches but lack mechanisms to incorporate global contextual information effectively. The fourth segment consists of BERT-based extractive summarization models, including BERTSUMEXT as well as MatchSum models. However, these models have issues of fixed input length and quadratic computational complexity of BERT's self-attention mechanism, which are unsuitable for large scientific documents. The fifth segment includes neural graph-based models that comprise Topic-GraphSum and HeterGraphLongSum. The core idea behind this research is the use of heterogeneous graphs to capture the inter-sentence relations and inter-document semantics in order to achieve remarkable performance. \n\nLastly, the proposed Global and Local Context-aware Heterogeneous Graph Neural Network yields promising results on both the arXiv and PubMed datasets for the task of summarizing lengthy scientific documents. It efficiently fuses the global and local semantic knowledge within a graph-based structure augmented with word, sentence, and topic nodes. \n\nAdditionally, it utilizes relatively fewer resources while prioritizing scalability. \n\nOur model consistently outperforms traditional extractive models and achieves higher ROUGE scores compared to both neural abstractive and extractive models. This success is largely due to its potential to handle lengthy scientific documents beyond the input length restrictions. By leveraging GloVe embeddings, CNNs, and BiLSTMs, the model effectively captures the local dependencies as well as the overall context.",
            "score": 0.6745095568926462,
            "section_title": "1) PERFORMANCE INSIGHT ON ARXIV AND PUBMED",
            "char_start_offset": 43198,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1484
                },
                {
                    "start": 1487,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1834
                },
                {
                    "start": 1837,
                    "end": 1921
                },
                {
                    "start": 1924,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2202
                },
                {
                    "start": 2203,
                    "end": 2339
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83447265625
        },
        {
            "corpus_id": "264429084",
            "title": "EXABSUM: a new text summarization approach for generating extractive and abstractive summaries",
            "text": "They clustered akin sentences among pivotal documents and employed word-graphs to identify shortest paths. The ILP model facilitated the identification of sentences with maximal information and readability, effectively reducing redundancy. Nayeem et al. [34] formulated an unsupervised abstractive summarization system. Their innovation was a paraphrastic sentence fusion model amalgamating sentence fusion with paraphrasing at the sentence level through a skip-gram word embedding model. This model augmented information coverage and heightened the abstract nature of the generated phrases. Shang et al. [35] introduced a fully unsupervised graph-based architecture tailored for abstractive summarization of meeting speeches. Their unified framework amalgamated the strengths of six prevailing approaches across three distinct tasks (keyword extraction, multi-sentence compression, and summarization), effectively addressing their respective limitations. Their abstractive summarization approach underwent four key processes: preprocessing, community recognition, multi-sentence compression, and submodular maximization.",
            "score": 0.6723110334865255,
            "section_title": "Related works",
            "char_start_offset": 12280,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1121
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 258,
                    "matchedPaperCorpusId": "52011473"
                },
                {
                    "start": 605,
                    "end": 609,
                    "matchedPaperCorpusId": "44243501"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94921875
        },
        {
            "corpus_id": "258654556",
            "title": "A Multi-Granularity Heterogeneous Graph for Extractive Text Summarization",
            "text": "Extractive text summarization selects the most important sentences from a document, preserves their original meaning, and produces an objective and fact-based summary. It is faster and less computationally intensive than abstract summarization techniques. Learning cross-sentence relationships is crucial for extractive text summarization. However, most of the language models currently in use process text data sequentially, which makes it difficult to capture such inter-sentence relations, especially in long documents. This paper proposes an extractive summarization model based on the graph neural network (GNN) to address this problem. The model effectively represents cross-sentence relationships using a graph-structured document representation. In addition to sentence nodes, we introduce two nodes with different granularity in the graph structure, words and topics, which bring different levels of semantic information. The node representations are updated by the graph attention network (GAT). The final summary is obtained using the binary classification of the sentence nodes. Our text summarization method was demonstrated to be highly effective, as supported by the results of our experiments on the CNN/DM and NYT datasets. To be specific, our approach outperformed baseline models of the same type in terms of ROUGE scores on both datasets, indicating the potential of our proposed model for enhancing text summarization tasks.",
            "score": 0.669685781064558,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96435546875
        },
        {
            "corpus_id": "252185277",
            "title": "Multi-Document Scientific Summarization from a Knowledge Graph-Centric View",
            "text": "In this work, we propose a knowledge graph-centric Transformer-based model for MDSS. Our model is able to incorporate knowledge graph information into the paper encoding process with a graph updater and an entity-sentence updater, and introduce a two-stage decoder including a KGtext generator and a summary generator to guide the summary decoding with knowledge graph information. Experiments show that the proposed model significantly outperforms all strong baselines and achieves the best result on the Multi-Xscience dataset. \n\nIn the future, we will explore other more intuitive and effective methods to incorporate graph information in both the encoding and decoding phase of summary generation. 2020) is a heterogeneous graph-based extractive model with semantic nodes of different granularity. HiMAP (Fabbri et al., 2019) expands the pointergenerator network (See et al., 2017) into a hierarchical network and integrates an MMR module. HierSumm (Liu and Lapata, 2019a) is a Transformer based model with an attention mechanism to share information cross-document for abstractive multi-document summarization. MGSum (Jin et al., 2020) is a multi-granularity interaction network for abstractive multi-document summarization. We also consider evaluating on single document summarization models by concatenating multiple papers into a long sequence. GraphSum (Li et al., 2020) is a neural multi-document summarization model that leverages well-known graphs to produce abstractive summaries. We use TF-IDF graph as the input graph. PEGASUS (Zhang et al., 2020) is a sequence-to-sequence model with gapsentences generation as a pre-training objective tailored for abstractive summarization. Pointer-Generator (See et al., 2017) is an RNN based model with an attention mechanism and allows the system to copy words from the source via pointing for abstractive summarization. BertABS (Liu and Lapata, 2019b) uses a pretrained BERT (Devlin et al., 2019) as the encoder for abstractive summarization. We also report the performance of BertABS with an encoder (SciBertABS) pretrained on scientific articles.",
            "score": 0.66835169672834,
            "section_title": "Conclusion",
            "char_start_offset": 25793,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 529
                },
                {
                    "start": 532,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2103
                }
            ],
            "ref_mentions": [
                {
                    "start": 867,
                    "end": 885,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1122,
                    "end": 1140,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 1362,
                    "end": 1378,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 1542,
                    "end": 1562,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 1710,
                    "end": 1728,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1930,
                    "end": 1951,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98095703125
        },
        {
            "corpus_id": "273784877",
            "title": "Cross-Document Distillation via Graph-Based Summarization of Extracted Essential Knowledge",
            "text": "Abstractive multi-document summarization aims to generate a comprehensive summary that encapsulates crucial content derived from multiple input documents. Despite the proficiency exhibited by language models in text summarization, challenges persist in capturing and aggregating salient information dispersed across a cluster of lengthy sources. To accommodate more input, existing solutions prioritize sparse attention mechanisms, relying on sequence truncation without incorporating graph-based modeling of multiple semantic units to locate essential facets. Furthermore, the limited availability of training examples adversely impacts performance, thereby compromising summarization quality in real-world few-shot scenarios. In this paper, we present G-Seek-2, a graph-enhanced approach designed to distill multiple topic-related documents by pinpointing and processing solely the pertinent information. We use a heterogeneous graph to model the input cluster, interconnecting various encoded entities via informative semantic edges. Then, a graph neural network locates the most salient sentences that are provided to a language model to generate the summary. We extensively evaluate G-Seek-2 across seven datasets spanning various domains\u2014including news articles, lawsuits, government reports, and scientific texts\u2014under few-shot settings with a limited training sample size of only 100 examples. The experimental findings demonstrate that our model consistently outperforms advanced summarization baselines, achieving improvements as measured by syntactic and semantic metrics.",
            "score": 0.6647292555132034,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.986328125
        },
        {
            "corpus_id": "274896099",
            "title": "Advancements in Natural Language Processing for Automatic Text Summarization",
            "text": "Graphs are frequently employed for extractive summarization, where the graph is typically undirected and consists of texts as nodes connected by edges representing similarity. Here, the graph data structure is a unique technique where each node represents a word unit, and the directed edges depict the sentence structure. For this task to represent natural language text, the central concept is to represent it using a graph data structure known as Opinosis-Graph and to formulate this abstractive summarization problem as locating ideal paths within the graph. This formulation exhibits characteristics of abstractive summarization by incorporating fusion (the combination of extracted components) and compression, distinguishing it from traditional sentence level extractive summarization. \n\nTo accomplish the desired outcome, several sub routes in the graph are examined and evaluated according to the following scoring system: Assign a numerical rank to each of the paths, and then arrange their scores in a decreasing order. The ranking also encompasses the collapsed paths. Remove redundant (or highly similar) pathways by use of a similarity metric such as Jaccard. Choose the most prominent residual paths as the created summary, with the number of paths to be selected determined by a value that specifies the desired summary size [7] [31]. \n\nAnother suggested approach is the utilization of Gated Graph Neural Attention Networks (GGNANs) for abstractive summarization. The renowned Seq2seq and the proposed GGNANs unified graph neural network are utilized to encode graph-structured data more efficiently. The model comprises two components: a sentence encoder and a decoder. The encoding component consists of a word embedding layer, a two-layer Bi-LSTM, and a GGNN. The decoder consists of a unidirectional LSTM and a sequence attention mechanism. Formally, a directed graph is a mapping of a set of nodes and edges. The suggested approach establishes connections between each node based on the correlations of word cooccurrence in the entire text. To incorporate global graphstructured data, they utilize a sliding window of a predetermined size across the text to calculate the information on word cooccurrence. They further employ a technique called point-wise mutual information (PMI) to compute the importance between two words.",
            "score": 0.6590745719834944,
            "section_title": "B. Graph Based Methodologies",
            "char_start_offset": 27616,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 792
                },
                {
                    "start": 795,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1350
                },
                {
                    "start": 1353,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2226
                },
                {
                    "start": 2227,
                    "end": 2346
                }
            ],
            "ref_mentions": [
                {
                    "start": 1341,
                    "end": 1344,
                    "matchedPaperCorpusId": "224955327"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9462890625
        },
        {
            "corpus_id": "258999996",
            "title": "Contrastive Hierarchical Discourse Graph for Scientific Document Summarization",
            "text": "Thus, sparse Transformer models like BigBird (Zaheer et al., 2020) and Longformer (Beltagy et al., 2020) are proposed. \n\nRecently, researchers have also turned to graph neural networks (GNN) as an alternative approach. Graph neural networks have been demonstrated to be effective at tasks with rich relational structure and can preserve global structure information (Yao et al., 2019;Xu et al., 2019;Zhang and Zhang, 2020). By representing a document as a graph, GNNs update and learn sentence representations by message passing, and turn extractive summarization into a node classification problem. Among all attempts, one popular way is to construct crosssentence similarity graphs (Erkan and Radev, 2004;Zheng and Lapata, 2019), which uses sentence representation cosine similarity as edge weights to model cross-sentence semantic relations. Xu et al. (2019) proposed using Rhetorical Structure Theory (RST) trees and coreference mentions to capture cross-sentence discourse relations. Wang et al. (2020) proposed constructing a word-document het-erogeneous graph by using words as the intermediary between sentences. Despite their success, how to construct an effective graph to capture the hierarchical structure for academic papers remains an open question. \n\nTo address the above challenges, we propose CHANGES (Contrastive HierArchical Graph neural network for Extractive Summarization), a hierarchical graph neural network model to fully exploit the section structure of scientific papers. CHANGES first constructs a sentence-section hierarchical graph for a scientific paper, and then learns hierarchical sentence representations by dedicated designed information aggregation with iterative intra-section and inter-section message passing. Inspired by recent advances in contrastive learning (Liu and Liu, 2021;Chen et al., 2020), we also propose a graph contrastive learning module to learn global theme-aware sentence representations and provide fine-grained discriminative information. The local sentence and global section representations are then fused for salient sentence prediction. We validate CHANGES with extensive experiments and analyses on two scientific paper summarization datasets. Experimental results demonstrate the effectiveness of our proposed method. Our main contributions are as follows: \n\n\u2022 We propose a hierarchical graph-based model for long scientific paper extractive summarization.",
            "score": 0.6501439029980846,
            "section_title": "Introduction",
            "char_start_offset": 2113,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 121,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1263
                },
                {
                    "start": 1266,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2100
                },
                {
                    "start": 2101,
                    "end": 2208
                },
                {
                    "start": 2209,
                    "end": 2283
                },
                {
                    "start": 2284,
                    "end": 2322
                },
                {
                    "start": 2325,
                    "end": 2422
                }
            ],
            "ref_mentions": [
                {
                    "start": 45,
                    "end": 66,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 366,
                    "end": 384,
                    "matchedPaperCorpusId": "52284222"
                },
                {
                    "start": 400,
                    "end": 422,
                    "matchedPaperCorpusId": "226262336"
                },
                {
                    "start": 684,
                    "end": 707,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1821,
                    "end": 1839,
                    "matchedPaperCorpusId": "211096730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98486328125
        },
        {
            "corpus_id": "271525553",
            "title": "Abstractive text summarization: State of the art, challenges, and improvements",
            "text": "This method strengthens the model's capacity to generate coherent summaries and help tackle the issues of OOV words. On the Gigaword and DUC-2004 [52] datasets, the authors evaluated the performance of the model in comparison to a number of benchmark models. The results demonstrated that the suggested modifications improved the functionality of the basic Seq2Seq model, leading to more precise and insightful abstractive summaries. A graph-based attention mechanism for abstractive document summarization was introduced by Tan et al. [53] that takes into account the relationships between sentences in a document. Traditional Seq2Seq models with attention mechanisms frequently concentrate on the words within a sentence but are unable to recognize the inter-sentence dependencies. The authors aimed to overcome this restriction by embedding the structural information of the document into the attention mechanism. The suggested approach first creates a sentence graph that represents the document, where the nodes are the sentences and the edges are the relationships between them. The attention mechanism then works on this graph, permitting the model to focus on both local and global sentence-level information. The model consists of a bidirectional RNN encoder to capture a representation of the input sentence and a decoder with a graph-based attention mechanism for generating the summary. On the CNN/Daily Mail and DUC-2004 datasets, the authors assessed the performance of the model in comparison with various state-of-the-art abstrac-tive and extractive summarization models. The results showed that by precisely capturing the connections between sentences in a document, the graph-based attention mechanism improved the quality of generated summaries.",
            "score": 0.6492219533767885,
            "section_title": "Attention Mechanisms",
            "char_start_offset": 24259,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1764
                }
            ],
            "ref_mentions": [
                {
                    "start": 536,
                    "end": 540,
                    "matchedPaperCorpusId": "26698484"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.984375
        },
        {
            "corpus_id": "250012667",
            "title": "GATSum: Graph-Based Topic-Aware Abstract Text Summarization",
            "text": "In this study, we integrate the global semantic structure of the text by proposing a novel graph-based topic-aware abstract text summarization model GTA-Sum and investigate the abstract text summarization problem. In particular, neural topic models, BERT, and graph neural networks are combined for summarization. Extensive experiments are conducted on two real-world datasets to compare GTASum with several methods. It turns out that GTASum outperforms the vast majority of classical models, with performance approaching industry-leading methods. In future work, we will continue to explore incorporating more types of high-level semantic units into the model to improve the performance and robustness of the model.",
            "score": 0.6413846483905024,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 47429,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 716
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9765625
        },
        {
            "corpus_id": "259289652",
            "title": "Graph-Enhanced Biomedical Abstractive Summarization Via Factual Evidence Extraction",
            "text": "Human language is highly ambiguous, with multiple ways to express the same concept unit, where the underlying meaning is oftentimes altered by high-level linguistic constructs. Additionally, a single sentence may incorporate various predicate-argument structures. Despite these observations, current language models only consider the superficial organization of the text document, which is almost irrelevant to identifying its real and deeper semantic content [13]. Climbing towards natural language understanding, an increasing number of researchers argue that a model trained purely on the form will never learn the meaning, lacking signals to learn non-linguistic relations [33]. \n\nTo this end, structured representations allow different quality improvements (e.g., coherence, factuality, low redundancy, long-range dependencies, informativeness, consistency) depending on how they are constructed. In particular, semantic parsing graphs normalize lexical and syntactic variations, providing formal meaning representations capable of decoupling concept units (what to say) from language competencies (how to say it). \n\nGraph structures have long been used for extractive summarization. In this sense, early approaches, such as Tex-tRank [34], propose unsupervised keyword and sentence extraction methods exploiting graph-based ranking algorithms to determine each vertex's importance. Extensions have been devised to incorporate document-level information [35] or introduce graph-based attention into encoderdecoder architectures [36]. As for abstractive summaries, results are mostly built on the cross-cutting success of graph neural networks (GNNs), a famed class of deep learning methods designed to process graph-represented data without imposing linearization or hierarchical constraints. Fernandes et al. [37] combine sequence encoders with GNNs feed with weakly-structured data inferred by the text through off-theshell NLP tools, including named entity recognition and coreference resolution; the final model compares favorably with baselines using only the sequential or graphical structure. Structured summarization also relates to the graph verbalization trend [38][39][40], where inputs may originate from knowledge graphs, information extraction or semantic parsing techniques.",
            "score": 0.6410999633535948,
            "section_title": "Graph-Enhanced Summarization",
            "char_start_offset": 7392,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 682
                },
                {
                    "start": 685,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1119
                },
                {
                    "start": 1122,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 2104
                },
                {
                    "start": 2105,
                    "end": 2294
                }
            ],
            "ref_mentions": [
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "202539891"
                },
                {
                    "start": 677,
                    "end": 681,
                    "matchedPaperCorpusId": "211029226"
                },
                {
                    "start": 1459,
                    "end": 1463,
                    "matchedPaperCorpusId": "17446655"
                },
                {
                    "start": 1533,
                    "end": 1537,
                    "matchedPaperCorpusId": "26698484"
                },
                {
                    "start": 1815,
                    "end": 1819,
                    "matchedPaperCorpusId": "53216170"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90283203125
        },
        {
            "corpus_id": "250572619",
            "title": "Enhancing Biomedical Scientific Reviews Summarization with Graph-based Factual Evidence Extracted from Papers",
            "text": "Abstractive Document Summarization. Summarizing text implies compressing the input document into a shorter version, retaining salient information, and discarding redundant or unnecessary attributes. An abstractive summarizer is asked to generate new sentences, rather than simply selecting the core ones, thus imitating a paraphrasing process closer to human-like interpretation. \n\nNeural models have achieved unprecedented results in recent years, mainly thanks to encoderdecoder frameworks. In a nutshell, an encoder maps the source tokens into a sequence of continuous representations, while a decoder generates the summary step-by-step. Remarkably, transformer-based architectures and self-supervised pre-training techniques have been responsible for a profound impetus in abstractive summarization (Liu and Lapata, 2019;Dong et al., 2019;Rothe et al., 2020;Zhang et al., 2019;Qi et al., 2020;Lewis et al., 2020)-even in low-resource (Moro and Ragazzi, 2022) and multi-document settings (Moro et al., 2022), promoting the creation of large unlabeled corpora. \n\nHowever, according to large-scale human evaluations (Maynez et al., 2020), nowadays text generators are highly prone to hallucinate content that is unfaithful to the input document. For this reason, latest contributions (Pasunuru and Bansal, 2018;Arumae and Liu, 2019;Huang et al., 2020a) tend to include reinforcement learning modules to improve informativeness and consistency. \n\nGraph-enhanced Summarization. Graphs are one of the most effective forms for introducing external knowledge into summarization models, allowing different quality improvements (e.g., coherence, factuality, low redundancy, long-range dependencies, informativeness, semantic coverage) depending on how they are constructed. \n\nParticularly, graph structures have long been used for extractive summarization. In this sense, early approaches, such as TextRank (Mihalcea and Tarau, 2004), propose to build a connectivity network with inter-sentence cosine similarity and document-level relations (Wan, 2008).",
            "score": 0.6338391743052384,
            "section_title": "RELATED WORK",
            "char_start_offset": 4508,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 36,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 1062
                },
                {
                    "start": 1065,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1444
                },
                {
                    "start": 1447,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1767
                },
                {
                    "start": 1770,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 2048
                }
            ],
            "ref_mentions": [
                {
                    "start": 803,
                    "end": 825,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 825,
                    "end": 843,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 843,
                    "end": 862,
                    "matchedPaperCorpusId": "198967997"
                },
                {
                    "start": 881,
                    "end": 897,
                    "matchedPaperCorpusId": "210164665"
                },
                {
                    "start": 897,
                    "end": 916,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1117,
                    "end": 1138,
                    "matchedPaperCorpusId": "218487034"
                },
                {
                    "start": 1285,
                    "end": 1312,
                    "matchedPaperCorpusId": "4940548"
                },
                {
                    "start": 1333,
                    "end": 1353,
                    "matchedPaperCorpusId": "218487279"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93798828125
        },
        {
            "corpus_id": "259138479",
            "title": "Modeling Structural Similarities between Documents for Coherence Assessment with Graph Convolutional Networks",
            "text": "Inspired by human reading habits, Jeon and Strube (2020b) investigate a model to measure a document's coherence by incrementally interpreting sentences. Our work is in line with the above approaches to learning a coherence model based on neural networks. The main difference is that the above neural models focus on extracting features within a single document, whereas our graph-based approach aims to study the effectiveness of correlations between documents. Specifically, rooting on the linguistic definition of text coherence, we model the correlation from the perspective of structural similarities between documents. \n\nGNN-based Methods for NLP. Graph neural networks are a family of neural networks that operate naturally on graphs. Many NLP problems can be expressed with a graph structure, so there is a surge of interest in applying GNNs for NLP tasks. Marcheggiani and Titov (2017) present a Syntactic GCN to learn latent feature representations of words in a sentence over dependency trees for Semantic Role Labeling. Yasunaga et al. (2017) propose a GCN-based multi-document summarization system that exploits the sentence relation information encoded in graph representations of document \n\n+'()'($' ,-./0* +%1,-./023-'4%'($5'* \"#$%&'()6*%1,-./02,-./0 7892'($#:'-;-':5$)5#( \n\nFigure 2: Overview of the proposed approach. Our method identifies a document's graph structure, converts the graph into a subgraph set, constructs a corpus-level graph based on the shared subgraphs between structurally similar documents, and finally encodes those connections with a GCN. For simplicity, we only show three documents and five subgraphs and limit the number of sentences in a document. s u , d i , and g j denote the u-th sentence in a document, the i-th document in the training corpus, and the j-th defined subgraph, respectively. \n\nclusters.",
            "score": 0.6335100318770269,
            "section_title": "Related Work",
            "char_start_offset": 6214,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1202
                },
                {
                    "start": 1205,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1287
                },
                {
                    "start": 1290,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1838
                },
                {
                    "start": 1841,
                    "end": 1850
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 57,
                    "matchedPaperCorpusId": "227231546"
                },
                {
                    "start": 1031,
                    "end": 1053,
                    "matchedPaperCorpusId": "6532096"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9208984375
        },
        {
            "corpus_id": "259262067",
            "title": "Vietnamese multi-document summary using subgraph selection approach - VLSP 2022 AbMuSu Shared Task",
            "text": "In multi-document summarization (MDS), the input is a set of documents, and the output is a summary that describes important information in a coherent and non-redundant manner. It is a complex problem that has gained attention from the research community. In recent times, there have been significant improvements in MDS due to the availability of MDS datasets and advances in pretraining approaches. Extractive and abstractive summarizing are two well-known methods for multi-document summarization. Abstractive summarizing methods try to succinctly summarize the substance of the texts, whereas extractive summarization systems aim to extract prominent snippets, phrases, or sections from documents. \n\nGraphs capturing relationships between textual units are of great benefit to MDS, which can help create more concise, informative, and coherent summaries from multiple sources documents. Furthermore, graphs can be easily constructed by representing sentences or paragraph as graph nodes and edges. LexRank (Erkan and Radev, 2004) computes sentence importance based on a lexical similarity graph of sentences. Graph representations of documents such as discourse graph based on discourse relations (Christensen et al., 2013). \n\nIn the VLSP 2022 Evaluation Campaign, Vietnamese Abstractive multi-document summarization (AbMuSu) Shared Task (Tran et al., 2022) is proposed to promote the development of research on abstractive multi-document summarization for Vietnamese text. The Vietnamese abstractive multidocument summarization task's goal is to develop summarizing systems that could automatically generate abstractive summaries for a collection of documents on a given topic. The model produces an abstractive summary that is connected to the input, which is a collection of news items on the same topic. In this paper, we approach by switching back to the graph-based extractive problem, specifically, extracting summaries for multi-document by selecting sub-graphs based on the constructed graph. Inspired by study (Chen et al., 2021), we exploit more edge weights representing the relationship of sentences in the same document and train and improve the model for Vietnamese dataset.",
            "score": 0.6323143858667619,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 701
                },
                {
                    "start": 704,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1228
                },
                {
                    "start": 1231,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2193
                }
            ],
            "ref_mentions": [
                {
                    "start": 1010,
                    "end": 1032,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1201,
                    "end": 1227,
                    "matchedPaperCorpusId": "337730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91455078125
        },
        {
            "corpus_id": "252780923",
            "title": "HEGEL: Hypergraph Transformer for Long Document Summarization",
            "text": "in (Xu et al., 2019), approximate discourse graph in (Yasunaga et al., 2017), topicsentence graph in (Cui and Hu, 2021) and worddocument heterogeneous graph in (Wang et al., 2020). However, the usability of these approaches is limited by the following two aspects: (1) These methods only model the pairwise interaction between sentences, while sentence interactions could be triadic, tetradic, or of a higher-order in natural language (Ding et al., 2020). How to capture high-order cross-sentence relations for extractive summarization is still an open question.\n\n(2) These graph-based approaches rely on either semantic or discourses structure cross-sentence relation but are incapable of fusing sentence interactions from different perspectives. Sentences within a document could have various types of interactions, such as embedding similarity, keywords coreference, topical modeling from the semantic perspective, and section or rhetorical structure from the discourse perspective. Capturing multi-type crosssentence relations could benefit sentence representation learning and sentence salience modeling. Figure 1 is an illustration showing different types of sentence interactions provide different connectivity for document graph construction, which covers both local and global context information.\n\nTo address the above issues, we propose HEGEL (HypErGraph transformer for Extractive Long document summarization), a graph-based model designed for summarizing long documents with rich discourse information. To better model high-order cross-sentence relations, we represent a document as a hypergraph, a generalization of graph structure, in which an edge can join any number of vertices. We then introduce three types of hyperedges that model sentence relations from different perspectives, including section structure, latent topic, and keywords coreference, respectively. We also propose hypergraph transformer layers to update and learn effective sentence embeddings on hypergraphs. We validate HEGEL by conducting extensive experiments and analyses on two benchmark datasets, and experimental results demonstrate the effectiveness and efficiency of HEGEL. We highlight our contributions as follows:\n\n(i) We propose a hypergraph neural model, HEGEL, for long document summarization. To the best of our knowledge, we are the first to model high-order cross-sentence relations with hypergraphs",
            "score": 0.6314765615072985,
            "section_title": "Introduction",
            "char_start_offset": 2030,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 119,
                    "matchedPaperCorpusId": "239050558"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96484375
        },
        {
            "corpus_id": "257219819",
            "title": "Topic-Selective Graph Network for Topic-Focused Summarization",
            "text": "Graph neural network [20] has been valued in the field of deep learning for its excellent processing ability on unstructured data and node-centric information aggregation mode. With the advance of graph neural networks, there are many graph networks with special structures, e.g., GCN [21], GAT [27], HAN [30] and r-GCN [21]. Moreover, GNN is often used for downstream tasks such as text classification, information extraction, and text generation. In text summarization, Wang et al. [28] propose a heterogeneous graph-based neural network for extracting summaries, which contains semantic nodes of different granularity levels except sentences. These extra nodes act as \"intermediaries\" between sentences and enrich cross-sentence relations. The introduction of document nodes allows the graph structure to be flexibly extended from a single document setup to multiple documents. Another work [8] proposes a multiplex graph summary (Multi-GraS) model based on multiplex graph convolutional networks that can be used to extract text summaries. This model not only considers Various types of intersentential relations (such as semantic similarity and natural connection), and intra-sentential relations (such as semantic and syntactic relations between words) are also modeled.",
            "score": 0.6282457414339021,
            "section_title": "Graph Neural Network",
            "char_start_offset": 7591,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1276
                }
            ],
            "ref_mentions": [
                {
                    "start": 21,
                    "end": 25,
                    "matchedPaperCorpusId": "4675456"
                },
                {
                    "start": 285,
                    "end": 289,
                    "matchedPaperCorpusId": "5458500"
                },
                {
                    "start": 295,
                    "end": 299,
                    "matchedPaperCorpusId": "3292002"
                },
                {
                    "start": 305,
                    "end": 309,
                    "matchedPaperCorpusId": "6857205"
                },
                {
                    "start": 320,
                    "end": 324,
                    "matchedPaperCorpusId": "5458500"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97119140625
        },
        {
            "corpus_id": "250301954",
            "title": "Hierarchical Heterogeneous Graph Attention Network for Syntax-Aware Summarization",
            "text": "The task of summarization often requires a non-trivial understanding of the given text at the semantic level. In this work, we essentially incorporate the constituent structure into the single document summarization via the Graph Neural Networks to learn the semantic meaning of tokens. More specifically, we propose a novel hierarchical heterogeneous graph attention network over constituency-based parse trees for syntax-aware summarization. This approach reflects psychological findings that humans will pinpoint specific selection patterns to construct summaries hierarchically. Extensive experiments demonstrate that our model is effective for both the abstractive and extractive summarization tasks on five benchmark datasets from various domains. Moreover, further performance improvement can be obtained by virtue of state-of-the-art pre-trained models.",
            "score": 0.6235911521508446,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96875
        },
        {
            "corpus_id": "271791043",
            "title": "Enhanced Topic-Aware Summarization Using Statistical Graph Neural Networks",
            "text": "Nonetheless, recurrent models face difficulties in capturing long-range dependencies among sentences and effective handling of computational factors, primarily because they only process input and output sequences in a strictly sequential manner [11]. \n\nIn recent years, there has been a notable achievement in applying Graph Attention Networks (GATs) for the purpose to summarized documents [12]. This success stems from their proficiency in capturing complex inter-sentence relationships within documents [3,[13][14][15][16][17]. To be precise, GNN models demonstrate the capability to effectively model intricate structural data that comprises semantic units (nodes) and the relationships (edges) connecting them. Authors of [18] employed the Rhetorical Structure Theory (RST) graph, which relies on joint extraction and syntactic compression, to model cross-sentence associations in the context of SDS. Reference [19] proposes an unsupervised graph-based model that leverages discourse information for hierarchical ranking, which is specifically tailored for extensive scientific documents, utilizing both intra-connections and inter-connections among sentences. The authors in [20] incorporate three types of nodes, namely Sentence, Entity Division Unit (EDU), and Entity Nodes. They utilize RST discourse parsing to capture the relationships between the EDUs. The researchers of [21] devised a Heterogeneous Transformer based model tailored for extractive summarization of lengthy text, incorporating multi-granularity sparse attention and treating tokens, entities, and sentences as distinct types of nodes in the process. \n\nAnother vital aspect of summarization pertains to modeling global information (topical information), which plays a pivotal role in the selection of sentences [22]. They concurrently trained with latent topics to model global information, employing the Neural Topic Model (NTM) for this purpose. The study conducted by the researchers cited in [23] offers an expanded model that employs NTM for abstractive text summarization. Within the graph structure, topics are extracted using clustering methods like K-Means and Gaussian Mixture Models (GMM).",
            "score": 0.6218051325504148,
            "section_title": "Introduction",
            "char_start_offset": 2340,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 253,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1628
                },
                {
                    "start": 1631,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2056
                },
                {
                    "start": 2057,
                    "end": 2178
                }
            ],
            "ref_mentions": [
                {
                    "start": 245,
                    "end": 249,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 391,
                    "end": 395,
                    "matchedPaperCorpusId": "256921197"
                },
                {
                    "start": 506,
                    "end": 509,
                    "matchedPaperCorpusId": "252670538"
                },
                {
                    "start": 513,
                    "end": 517,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 517,
                    "end": 521,
                    "matchedPaperCorpusId": "252818946"
                },
                {
                    "start": 521,
                    "end": 525,
                    "matchedPaperCorpusId": "250301954"
                },
                {
                    "start": 727,
                    "end": 731,
                    "matchedPaperCorpusId": "268699293"
                },
                {
                    "start": 916,
                    "end": 920,
                    "matchedPaperCorpusId": "233189611"
                },
                {
                    "start": 1384,
                    "end": 1388,
                    "matchedPaperCorpusId": "269525843"
                },
                {
                    "start": 1789,
                    "end": 1793,
                    "matchedPaperCorpusId": "269700993"
                },
                {
                    "start": 1974,
                    "end": 1978,
                    "matchedPaperCorpusId": "213817744"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9423828125
        },
        {
            "corpus_id": "257496469",
            "title": "Compressed Heterogeneous Graph for Abstractive Multi-Document Summarization",
            "text": "Multi-document summarization (MDS) aims to automatically generate a concise and informative summary for a cluster of topically related source documents (Ma et al. 2020;Radev, Hovy, and McKeown 2002). It has a wide range of applications such as creating news digests (Fabbri et al. 2019), product review summaries (Gerani et al. 2014), and summaries for scientific literature (Moro et al. 2022;Otmakhova et al. 2022). Our work targets abstractive MDS, which generates summaries with words that do not necessarily come from the source documents, resembling the summarization process of human beings. \n\nState-of-the-art text summarization models use pretrained language models (PLMs) including both generalpurpose PLMs for text generation (Beltagy, Peters, and Cohan 2020;Lewis et al. 2020) and PLMs designed for text summarization (Zhang et al. 2020a;Xiao et al. 2022). When applied to the abstractive MDS task, these models take a flat concatenation of the (multiple) source documents, which may not capture cross-document relationships such as contradiction, redundancy, or complementary information very Figure 1: The structure of the heterogeneous graph given three documents in a document cluster: The orange triangles denote document nodes d, the blue quadrates denote sentence nodes s, the green circles denote word nodes w, and the line (or curve) segments between nodes denote edges. A detailed description of the graph is in the Preliminaries. \n\nwell (Radev 2000). Ma et al. (2020) argue that explicit modeling of cross-document relationships can potentially improve the quality of summaries. Following this, several recent studies (Li et al. 2020;Jin, Wang, and Wan 2020;Cui and Hu 2021) explore graphs to model source documents to improve abstractive MDS. However, these graphs are homogeneous in that the nodes or edges are not distinguished for different semantic units (e.g., words, sentences, and paragraphs) in the encoding process. This means these MDS models cannot capture the diverse cross-document relationships among different types of semantic units.",
            "score": 0.6212627246595182,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 597
                },
                {
                    "start": 600,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1451
                },
                {
                    "start": 1454,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2072
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 168,
                    "matchedPaperCorpusId": "248780330"
                },
                {
                    "start": 266,
                    "end": 286,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 313,
                    "end": 333,
                    "matchedPaperCorpusId": "2767900"
                },
                {
                    "start": 393,
                    "end": 415,
                    "matchedPaperCorpusId": "248780431"
                },
                {
                    "start": 829,
                    "end": 849,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 849,
                    "end": 866,
                    "matchedPaperCorpusId": "247519084"
                },
                {
                    "start": 1459,
                    "end": 1471,
                    "matchedPaperCorpusId": "10103200"
                },
                {
                    "start": 1473,
                    "end": 1489,
                    "matchedPaperCorpusId": "248780330"
                },
                {
                    "start": 1640,
                    "end": 1656,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 1656,
                    "end": 1680,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 1680,
                    "end": 1695,
                    "matchedPaperCorpusId": "239050558"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.916015625
        },
        {
            "corpus_id": "248571519",
            "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization",
            "text": "Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013). These papers primarily aim to address de-duplicating information and learning relationships between the different topics shared across documents however none of these architectures are built to deal with conflicting information.",
            "score": 0.6179287379772154,
            "section_title": "Multi Document Summarization",
            "char_start_offset": 8653,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1844
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 231,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 241,
                    "end": 264,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 491,
                    "end": 508,
                    "matchedPaperCorpusId": "10112929"
                },
                {
                    "start": 1345,
                    "end": 1362,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 1448,
                    "end": 1465,
                    "matchedPaperCorpusId": "218718706"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96142578125
        },
        {
            "corpus_id": "247292760",
            "title": "A Multi-Document Coverage Reward for RELAXed Multi-Document Summarization",
            "text": "Multi-document summarization (MDS) aims to consolidate salient points of information across a set of documents into a concise summary. The main requirement for the summary is that it adequately represent the document set, with low redundancy and high coverage across all documents, while at the same time being readable and fluent. Combined with this, is the need to develop techniques that can handle the significant memory complexity required to tackle MDS. Recently, the re-lease of dedicated datasets (Fabbri et al., 2019;Gholipour Ghalandari et al., 2020), and intelligently designed Transformer models (Liu et al., 2018;Liu and Lapata, 2019;Beltagy et al., 2020), have helped drive advancements in multi-document summarization, generally improving the accuracy and fluency of the predicted summaries. However, aspects such as the requirement to cover as much salient information from the input documents as possible, whilst still maintaining low repetition and low redundancy, have certainly been less explored to date (Nayeem et al., 2018;Mao et al., 2020).\n\nWithin the sphere of contemporary neural MDS models, two main lines of investigation can be identified: graph-based approaches (Li et al., 2020;Pasunuru et al., 2021), and concatenation approaches (Liu et al., 2018;Zhang et al., 2020a). The former are approaches that rely on the construction of graphs to capture the inter-and intra-document relations. While powerful, they need to elicit the relations explicitly. The latter instead assume that all the input documents within a document set can be simply concatenated, possibly with document separators and tags, such that the relations can be \"discovered\" by the model. Like ordinary summarization, also MDS comes in two remarkably different styles: extractive, where the generated summaries consist of verbatim sentences from the original input documents (Nallapati et al., 2017), and abstractive, where the model is instead encouraged to generate a paraphrased understanding of the input documents. The intrinsic appeal of abstractive summaries and the advent of sequenceto-sequence models have increasingly shifted the trend toward abstractive summar",
            "score": 0.6131806123885866,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8935546875
        },
        {
            "corpus_id": "252715398",
            "title": "Towards Improving Faithfulness in Abstractive Summarization",
            "text": "Abstractive Summarization. In recent years, the research on text generation has made impressive progress [14,15,16], which promotes the progress of abstractive summarization. The abstractive summarization task generates novel words and phrases not featured in the source text to capture the salient ideas of the source text [17]. Most works apply an encoder-decoder architecture to implicitly learn the summarization procedure [18,19]. More recently, applying pretrained language models as encoder [4,20] or pre-training the generation process by leveraging a large-scale of unlabeled corpus [21,22] brings significant improvements. Explicit structure modeling has also been shown to be effective in summarization tasks. For example, Jin et al. [23] incorporated semantic dependency graphs to help generate sentences with better semantic relevance, and Wu et al. [9] came up with a unified semantic graph to aggregate relevant disjoint context from the input. \n\nFact Consistency for Abstractive Summarization. Producing a summary that is entailed by the information presented in the source document is a key challenge in the summarization task, and less progress has been made on it. Pioneer works [24,25] incorporated fact descriptions or entailment knowledge to enhance faithfulness. More recently, Zhu et al. [26] modeled the facts in the source article with knowledge graphs based on a graph neural network. Cao et al. [10] proposed to leverage reference summaries as positive training data and erroneous summaries as negative data, to train summarization systems that are better at distinguishing between them. Aralikatte et al. [27] introduced focus attention mechanism to encourage decoders to proactively generate tokens that are similar or topical to the input document. On the contrary, other works post-edit the generated summaries. Different from previous works, we enhance the semantic understanding of the document with faithfulness evaluation as a direct signal and prevent the overconfidence of LM which is not addressed before. Multi-task Learning. Multi-task learning is a learning paradigm in machine learning and it aims to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks [28].",
            "score": 0.6130510506988149,
            "section_title": "Related Work",
            "char_start_offset": 4116,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 959
                },
                {
                    "start": 962,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2065
                },
                {
                    "start": 2066,
                    "end": 2277
                }
            ],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 112,
                    "matchedPaperCorpusId": "199466316"
                },
                {
                    "start": 112,
                    "end": 115,
                    "matchedPaperCorpusId": "236460044"
                },
                {
                    "start": 324,
                    "end": 328,
                    "matchedPaperCorpusId": "1412297"
                },
                {
                    "start": 427,
                    "end": 431,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 431,
                    "end": 434,
                    "matchedPaperCorpusId": "4406182"
                },
                {
                    "start": 498,
                    "end": 501,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 501,
                    "end": 504,
                    "matchedPaperCorpusId": "236478143"
                },
                {
                    "start": 592,
                    "end": 596,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 596,
                    "end": 599,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 745,
                    "end": 749,
                    "matchedPaperCorpusId": "214303164"
                },
                {
                    "start": 863,
                    "end": 866,
                    "matchedPaperCorpusId": "235187330"
                },
                {
                    "start": 1198,
                    "end": 1202,
                    "matchedPaperCorpusId": "19198109"
                },
                {
                    "start": 1202,
                    "end": 1205,
                    "matchedPaperCorpusId": "52012819"
                },
                {
                    "start": 1312,
                    "end": 1316,
                    "matchedPaperCorpusId": "235097229"
                },
                {
                    "start": 1423,
                    "end": 1427,
                    "matchedPaperCorpusId": "237571691"
                },
                {
                    "start": 1634,
                    "end": 1638,
                    "matchedPaperCorpusId": "235186832"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9287109375
        },
        {
            "corpus_id": "265607988",
            "title": "Generating Extractive and Abstractive Summaries in Parallel from Scientific Articles Incorporating Citing Statements",
            "text": "The investigated summarization model has two units: an extractive and an abstractive summarizer. The overall architecture of the model is portrayed in Figure 1. This section discusses the architecture and working principle of these two units. \n\nWhile designing the extractive summarizer, we have considered two issues: how the sentences are connected to each other and how semantic units like words affect the sentence level correlations. \n\nTo fulfill these purposes, we have utilized two different graph-based neural networks: an heterogeneous graph neural network (HeterSumGraph) (Wang et al., 2020) and a graph attention network (GAT) (Velickovic et al., 2018). \n\nFor any graph G = {V, E}, V denotes the nodes and E, the edges between them. HeterSumGraph defines V = V w \u222a V s , V w is the set of unique words and V s is the set of sentences in the document. For a document with n unique words and m sentences, E is the edge weight matrix, where e i,j represents word i in sentence j, (i \u2208 {1 : n}, j \u2208 {1 : m}) (Wang et al., 2020). The nodes that represent the sentences are initialized with LongFormer [CLS] tokens. Because LogFormer generates a contextualized word embedding for each occurrence of the word in the document, all of the word embeddings for a word are averaged to initialize that particular word-representing node in the graph. The edges between the words and sentences are initialized with the corresponding TF-IDF values. \n\nAfter the graph G is constructed, a graph attention network (GAT) is used to update the node feature values. Considering h i \u2208 R d h where i \u2208 {1 : (n + m)} as the hidden states of the word and sentence nodes, the GAT layer is designed as: \n\nwhere \u03c9 a , \u03c9 q , \u03c9 k and \u03c9 v are learnable weight matrices. N i denotes the list of the neighbor nodes. The attention value between h i and h j is denoted by \u03b1 i,j . The GAT with multi-head attention (considering K attention heads) is designed as:",
            "score": 0.6127039271730996,
            "section_title": "Model Overview",
            "char_start_offset": 13262,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 242
                },
                {
                    "start": 245,
                    "end": 438
                },
                {
                    "start": 441,
                    "end": 664
                },
                {
                    "start": 667,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1443
                },
                {
                    "start": 1446,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1685
                },
                {
                    "start": 1688,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1936
                }
            ],
            "ref_mentions": [
                {
                    "start": 582,
                    "end": 601,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 638,
                    "end": 663,
                    "matchedPaperCorpusId": "3292002"
                },
                {
                    "start": 1015,
                    "end": 1034,
                    "matchedPaperCorpusId": "216552978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.943359375
        },
        {
            "corpus_id": "269188314",
            "title": "Prompt-tuning for Clickbait Detection via Text Summarization",
            "text": "Text summarization is a prominent and challenging area in Natural Language Processing (NLP) [50].It involves the task of creating a concise and coherent summary of lengthy text documents, highlighting key points while maintaining the document's overall meaning.This process aims to produce summaries that efficiently capture the primary concepts, enabling a more efficient understanding of extensive textual information.\n\nTwo primary paradigms govern text summarization: extractive and abstractive.In the extractive approach, specific keywords from the input are chosen for output generation, aligning well with this model.Many extractive summarization methods treat the task as a sequence classification problem, utilizing diverse encoders such as recurrent neural networks [51] and pre-trained language models [52].Nallapati et al. introduced SummaRuNNer, utilizing a dual-layer bidirectional GRU-RNN for document extractive summarization [53].They introduced an abstract training method that leverages Rouge score optimization to transform abstract summaries into extractive labels, enabling training exclusively based on human-generated summaries.Xie et al. introduce KeBioSum, utilizing SciBERT to identify PICO elements and incorporating domain knowledge into base PLMs through generative and discriminative adapters [54].The method significantly improves the performance of extractive summarization in the biomedical domain.An alternative view considers extractive summarization as a node classification problem, utilizing graph neural networks to model inter-sentence dependencies [55].Cui et al. present Topic-GraphSum, an extractive text summarization approach that integrates graph neural networks and neural topic models [56].Document encoding utilizes BERT, and the neural topic model (NTM) learns document topics.The graph attention layer captures intersentence relationships, facilitating the efficient extraction of key information and addressing limitations in existing models regarding intersentence relationships and neglect of topic information.\n\nIn the realm of abstractive summarization, significant progress has been achieved through Seq2Seq models [51].These models build a neural network to establish a true relationship between input and output, refining the process with local attention mechanisms and advanced techniques like BERT.",
            "score": 0.612462745338357,
            "section_title": "Text Summarization",
            "char_start_offset": 18044,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 97,
                    "end": 261
                },
                {
                    "start": 261,
                    "end": 420
                },
                {
                    "start": 422,
                    "end": 498
                },
                {
                    "start": 498,
                    "end": 623
                },
                {
                    "start": 623,
                    "end": 817
                },
                {
                    "start": 817,
                    "end": 946
                },
                {
                    "start": 946,
                    "end": 1151
                },
                {
                    "start": 1151,
                    "end": 1328
                },
                {
                    "start": 1328,
                    "end": 1431
                },
                {
                    "start": 1431,
                    "end": 1594
                },
                {
                    "start": 1594,
                    "end": 1738
                },
                {
                    "start": 1738,
                    "end": 1827
                },
                {
                    "start": 1827,
                    "end": 2065
                },
                {
                    "start": 2067,
                    "end": 2177
                },
                {
                    "start": 2177,
                    "end": 2359
                }
            ],
            "ref_mentions": [
                {
                    "start": 92,
                    "end": 96,
                    "matchedPaperCorpusId": "219504970"
                },
                {
                    "start": 941,
                    "end": 945,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 1323,
                    "end": 1327,
                    "matchedPaperCorpusId": "250713599"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.890625
        },
        {
            "corpus_id": "249097545",
            "title": "Target-aware Abstractive Related Work Generation with Contrastive Learning",
            "text": "We evaluate our models against popular and state-of-the-art multidocument summarization baselines and related work generation techniques. We adopt the LEAD baseline which selects the first sentence of each reference as the summary as a baseline, and ORACLE as an upper bound of extractive summarization systems. \n\nOur extractive baselines include: LexRank [11]: a graph-based extractive model that computes sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. \n\nNES [37]: a neural data-driven summarizer that extracts sentences from reference papers based on target paper. A joint contextdriven attention mechanism is proposed to measure the contextual relevance within full texts. \n\nBertSumEXT [25]: an extractive summarization model with BERT encoder, which is able to express the semantics of a document and obtain representations for its sentences. \n\nAbstractive baseline models include: BertSumABS [25]: an abstractive summarization system built on BERT with a new fine-tuning schedule that adopts different optimizers for the encoder and the decoder. \n\nMGSum [20]: a multi-granularity interaction network for abstractive multi-document summarization. The multi granularities include document-level, sentence-level, and word-level. \n\nEMS [42]: an entity-aware model for abstractive multi-document summarization with BERT encoder. It augments the classical Transformer based encoder-decoder framework with a graph consisting of paragraph nodes and entity nodes. EMS uses the same keyphrase extraction tool as ours. \n\nRRG [5]: an abstractive related work generator based on reference papers. It augments the RNN encoder with a relation-aware graph between multiple references. The relation graph and the document representation are refined iteratively. \n\nThe essential difference between our model and baselines is that we explicitly model the relationship between the target work and the references. To make a fair comparison, for baselines that have relatively good performance, we give an additional with.Target (w.T ) version which takes references and target paper as input. Thus, the baseline models treat the references and the target paper as multiple inputs without distinguishing them.",
            "score": 0.611568779454541,
            "section_title": "Baselines",
            "char_start_offset": 23597,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 311
                },
                {
                    "start": 314,
                    "end": 513
                },
                {
                    "start": 516,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 735
                },
                {
                    "start": 738,
                    "end": 906
                },
                {
                    "start": 909,
                    "end": 1110
                },
                {
                    "start": 1113,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1290
                },
                {
                    "start": 1293,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1572
                },
                {
                    "start": 1575,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1809
                },
                {
                    "start": 1812,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2136
                },
                {
                    "start": 2137,
                    "end": 2252
                }
            ],
            "ref_mentions": [
                {
                    "start": 356,
                    "end": 360,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 520,
                    "end": 524,
                    "matchedPaperCorpusId": "53083244"
                },
                {
                    "start": 749,
                    "end": 753,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 957,
                    "end": 961,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 1119,
                    "end": 1123,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 1297,
                    "end": 1301,
                    "matchedPaperCorpusId": "236478143"
                },
                {
                    "start": 1579,
                    "end": 1582,
                    "matchedPaperCorpusId": "236460044"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.962890625
        },
        {
            "corpus_id": "250118028",
            "title": "An Empirical Survey on Long Document Summarization: Datasets, Models, and Metrics",
            "text": "The final summary is generated by extracting the top-k sentences ranked by  (  ). Importantly, while there are other classical architectures [38,112], the graph architecture is worth a separate mentioning here due to the fact that (a) it remains as a strong baseline against other advanced architectures, (b) it can effectively incorporate external knowledge as an inductive bias to the calculation of the importance of a sentence and (c) it achieves state-of-the-art result in long document unsupervised extractive summarization setting when integrated with current state-of-the-art pre-trained models [25,69]. Lastly, other than the multi-sentence compression approach [6,54,132] that may be extended to long document summarization tasks, there has been no applicable work on classical graph-based architecture for long document abstractive summarization.",
            "score": 0.6096254801021538,
            "section_title": "Graph Architecture:",
            "char_start_offset": 38367,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 857
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 145,
                    "matchedPaperCorpusId": "7003631"
                },
                {
                    "start": 145,
                    "end": 149,
                    "matchedPaperCorpusId": "10496513"
                },
                {
                    "start": 603,
                    "end": 607,
                    "matchedPaperCorpusId": "231592914"
                },
                {
                    "start": 607,
                    "end": 610,
                    "matchedPaperCorpusId": "236477774"
                },
                {
                    "start": 671,
                    "end": 674,
                    "matchedPaperCorpusId": "6545121"
                },
                {
                    "start": 674,
                    "end": 677,
                    "matchedPaperCorpusId": "226284006"
                },
                {
                    "start": 677,
                    "end": 681,
                    "matchedPaperCorpusId": "220633461"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83203125
        },
        {
            "corpus_id": "277284940",
            "title": "Global and Local Context Fusion in Heterogeneous Graph Neural Network for Summarizing Lengthy Scientific Documents",
            "text": "The The following contributions of the proposed model are listed below: \n\n\u2022 This work introduces a new HGNN architecture that preserves important information within three-scale structures consisting of sentence, word, and topic nodes, in order to cover both local and global relevant information in lengthy scientific texts. The model is able to cope with long-range dependencies and complex structures, which are essential for producing coherent and extensive summaries across different parts of the document. \n\n\u2022 This work presents a new combined approach, which utilizes the GloVe embeddings for fast and effective word representations, CNNs for local feature extraction, and BiLSTMs for the global features of the data. The combination also enables the model to accommodate large inputs besides the constraint of lengths of inputs, which makes the model very efficient in generating summaries of large scientific papers. \n\n\u2022 Our proposed model also addresses various limitations of prior BERT-based models, such as restricted input length and quadratic complexities. The proposed framework offers improved scalability, enabling efficient summarization of large datasets and reducing computational costs, making it suitable for real-world applications in scientific document summarization. \n\n\u2022 Leveraging data from publicly available arXiv and PubMed datasets, our research demonstrates consistent and favorable results compared to state-of-the-art heterogeneous graph structures used for both extractive and abstractive summarization tasks. The model showcases superior performance in terms of capturing intricate relationships and generating coherent summaries. \n\nThe rest of the paper is structured as follows: Section II critically reviews existing techniques in extractive summarization, topic modelling, neural graph-based models, and the summarization of long documents. In Section III, the proposed model is demonstrated. This includes the integration of LDA, CNN, BiLSTM, and GAT within a novel neural graph framework designed for extractive summarization. Section IV illustrates a comparison of our model with state-of-theart approaches, along with details of datasets, experimental setup and performance evaluations of the proposed model. Section V concludes with a summary of findings and highlights potential avenues for future research. It discusses the implications of our work and suggests directions for further enhancement of summarization techniques.",
            "score": 0.6081872548209131,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 6935,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 74,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 510
                },
                {
                    "start": 513,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 924
                },
                {
                    "start": 927,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1292
                },
                {
                    "start": 1295,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1666
                },
                {
                    "start": 1669,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2068
                },
                {
                    "start": 2069,
                    "end": 2252
                },
                {
                    "start": 2253,
                    "end": 2353
                },
                {
                    "start": 2354,
                    "end": 2472
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9384765625
        },
        {
            "corpus_id": "272253963",
            "title": "Whale-optimized LSTM networks for enhanced automatic text summarization",
            "text": "Over the past decade, text summarization has predominantly been a manual process, often time-consuming and subject to individual biases. The surge in the volume of information available online and from various sources has rendered manual summarization increasingly impractical. In the current era of information overload, automatic summarization has become essential for efficiently managing and distilling extensive text data. \n\nDocument summarization involves condensing text while retaining its crucial content, making it vital for effective information consumption. Whether extracting a summary from a single document or synthesizing multiple documents, summarization highlights key content, simplifying overall comprehension. Text summarization encompasses acquiring textual documents, processing the content, and delivering necessary information in a concise, userfriendly format. This essential process in modern information handling is broadly categorized into two types: abstractive and extractive summarization (Yang et al., 2019). \n\nAbstractive summarization leverages Natural Language Processing (NLP) techniques to parse, reduce words, and generate summaries that encapsulate the original text's core ideas in a new form. Conversely, extractive summarization, known for its flexibility and time efficiency, involves analyzing sentences in matrix forms and identifying key sentences through feature vectors-n-dimensional numerical representations of text objects (Mohan and Kumar, 2023). The objective of extractive summarization is to select sentences that align with specific requirements, effectively reducing text content while preserving its main message. Single document summarization focuses on condensing individual texts, whereas multi document summarization aims to synthesize and shorten a collection of similar documents into a cohesive summary. Despite their straightforward objectives, these tasks are complex and challenging to execute to the desired standards. \n\nThe advent of deep learning techniques has driven remarkable advancements in NLP. Text summarization, along with other tasks such as text translation and sentiment analysis, has significantly benefited from deep neural network models. These contemporary summarization approaches often utilize a sequence-to-sequence framework, typically an encoder-decoder model comprising neural networks trained on both input and output. Deep neural networks, leveraging large datasets, have demonstrated substantial improvements in summarization results.",
            "score": 0.5972590225520916,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 427
                },
                {
                    "start": 430,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1041
                },
                {
                    "start": 1044,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 1988
                },
                {
                    "start": 1991,
                    "end": 2072
                },
                {
                    "start": 2073,
                    "end": 2225
                },
                {
                    "start": 2226,
                    "end": 2413
                },
                {
                    "start": 2414,
                    "end": 2531
                }
            ],
            "ref_mentions": [
                {
                    "start": 1021,
                    "end": 1040,
                    "matchedPaperCorpusId": "209852777"
                },
                {
                    "start": 1475,
                    "end": 1498,
                    "matchedPaperCorpusId": "252963446"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7802734375
        },
        {
            "corpus_id": "254408844",
            "title": "Scientific Paper Extractive Summarization Enhanced by Citation Graphs",
            "text": "Text summarization is to automatically glean the most important concepts from an article, removing secondary or redundant concepts. Among various summarization tasks such as news summarization (Wang et al., 2020), dialog summarization (Zhang et al., 2021), and timeline summarization (Chen et al., 2019), scientific paper summarization remains a challenging task, since scientific papers are usually longer, and full of complex concepts Target paper abstract: ...we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization... We employ attention mechanisms to interact between different granularity of semantic representations... Our proposed model outperforms all strong baseline methods... Paper 1 abstract: ...We propose a unified model combining the strength of extractive and abstractive summarization... Paper 2 abstract: ...we propose HIBERT (HIerachical Bidirectional Encoder Representations from Transformers)... Paper 3 abstract: ... Our empirical analysis shows state-of-the-art performance on several multi document datasets. Paper 4 abstract: ...The Hidden Markov Model (HMM) for part-of-speech (POS) tagging is typically based on tag trigrams... Paper 5 abstract: ...Paraphrase recognition is used in a number of applications such as tutoring systems... ... and domain-specific items in specific fields (Cohan et al., 2020;An et al., 2021). \n\nExisting works on extractive scientific paper summarization mainly focus on utilizing intradocument relationships. For example, Xiao and Carenini (2019) divided papers into sections and incorporated the global and local context for extractive summarization. Dong et al. (2021b) proposed an unsupervised graph-based ranking model, which assumes a hierarchical graph representation of the scientific papers. However, the effectiveness of the citation graph in extractive summarization tasks is left to be explored. \n\nIn the citation graph domain, nodes are inherently linked and dependent on each other. Correspondingly, we assume that a good scientific paper abstract should be able to capture this structural information. Figure 1 demonstrates this intuition.",
            "score": 0.5966984961108858,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1405
                },
                {
                    "start": 1408,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1920
                },
                {
                    "start": 1923,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2129
                },
                {
                    "start": 2130,
                    "end": 2167
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 212,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 235,
                    "end": 255,
                    "matchedPaperCorpusId": "221702889"
                },
                {
                    "start": 284,
                    "end": 303,
                    "matchedPaperCorpusId": "199466316"
                },
                {
                    "start": 1368,
                    "end": 1388,
                    "matchedPaperCorpusId": "215768677"
                },
                {
                    "start": 1388,
                    "end": 1404,
                    "matchedPaperCorpusId": "233169121"
                },
                {
                    "start": 1536,
                    "end": 1560,
                    "matchedPaperCorpusId": "201697450"
                },
                {
                    "start": 1666,
                    "end": 1685,
                    "matchedPaperCorpusId": "231592914"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73583984375
        },
        {
            "corpus_id": "266599825",
            "title": "Summarization of Investment Reports Using Pre-trained Model",
            "text": "Related research on multi-document summarization includes the following papers. Moro et al. proposed the probabilistic method based on the combination of three language models to tackle multi-document summarization in the medical domain [6]. Liao et al. investigated the feasibility of utilizing Abstract Meaning Representation formalism for multidocument summarization [7]. Fabbri et al. constructed Multi-News, the large-scale multi-document news summarization dataset [8]. Xiao et al. introduced PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data [9]. Nayeem et al. designed an abstractive fusion generation model at the sentence level, which jointly performs sentence fusion and paraphrasing [10]. They applied their sentence-level model to implement an abstractive multi-document summarization system where documents usually contain a related set of sentences. Liu et al. developed the neural summarization model, which can effectively process multiple input documents and distill abstractive summaries [11]. Li et al. develop a neural abstractive multi-document summarization model which can leverage explicit graph representations of documents to more effectively process multiple input documents and distill abstractive summaries [12]. Jin et al. proposed the multigranularity interaction network to encode semantic representations for documents, sentences, and words [13]. Deyoung et al. released MS\u02c62 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature [14]. \n\nAs related work of extractive summarization, there is research by Cui et al. [15]. They proposed extractive summarization that can summarize long-form documents without content loss. Xu et al. proposed the neural network framework for extractive and compressive summarization [16]. \n\nAs related work of abstractive summarization, there is research by Nallapati et al. [17].",
            "score": 0.5948525043045738,
            "section_title": "VII. RELATED WORKS",
            "char_start_offset": 9491,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 1986
                },
                {
                    "start": 1989,
                    "end": 2078
                }
            ],
            "ref_mentions": [
                {
                    "start": 237,
                    "end": 240,
                    "matchedPaperCorpusId": "248780330"
                },
                {
                    "start": 370,
                    "end": 373,
                    "matchedPaperCorpusId": "49210924"
                },
                {
                    "start": 471,
                    "end": 474,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 841,
                    "end": 845,
                    "matchedPaperCorpusId": "52011473"
                },
                {
                    "start": 1153,
                    "end": 1157,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 1383,
                    "end": 1387,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 1521,
                    "end": 1525,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 1697,
                    "end": 1701,
                    "matchedPaperCorpusId": "233231380"
                },
                {
                    "start": 1782,
                    "end": 1786,
                    "matchedPaperCorpusId": "235097475"
                },
                {
                    "start": 1981,
                    "end": 1985,
                    "matchedPaperCorpusId": "59599804"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91455078125
        },
        {
            "corpus_id": "271791043",
            "title": "Enhanced Topic-Aware Summarization Using Statistical Graph Neural Networks",
            "text": "The rapid expansion of online content and big data has precipitated an urgent need for efficient summarization techniques to swiftly comprehend vast textual documents without compromising their original integrity. Current approaches in Extractive Text Summarization (ETS) leverage the modeling of inter-sentence relationships, a task of paramount importance in producing coherent summaries. This study introduces an innovative model that integrates Graph Attention Networks (GATs) with Transformer-based Bidirectional Encoder Representations from Transformers (BERT) and Latent Dirichlet Allocation (LDA), further enhanced by Term Frequency-Inverse Document Frequency (TF-IDF) values, to improve sentence selection by capturing comprehensive topical information. Our approach constructs a graph with nodes representing sentences, words, and topics, thereby elevating the interconnectivity and enabling a more refined understanding of text structures. This model is stretched to Multi-Document Summarization (MDS) from Single-Document Summarization, offering significant improvements over existing models such as THGS-GMM and Topic-GraphSum, as demonstrated by empirical evaluations on benchmark news datasets like Cable News Network (CNN)/Daily Mail (DM) and Multi-News. The results consistently demonstrate superior performance, showcasing the model\u2019s robustness in handling complex summarization tasks across single and multi-document contexts. This research not only advances the integration of BERT and LDA within a GATs but also emphasizes our model\u2019s capacity to effectively manage global information and adapt to diverse summarization challenges.",
            "score": 0.5926377154680638,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97802734375
        },
        {
            "corpus_id": "269762702",
            "title": "Leveraging Knowledge-aware Methodologies for Multi-document Summarization",
            "text": "Along with the prosperity of knowledge-aware research in the natural language processing field, more and more document summarization models attempted to incorporate knowledge graphs to enhance the quality of generated summaries.SDS with KG Gunel et al. [6]  MDS with KG Zhou et al. [26] presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP.Pasunuru et al. [18] presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News [5] and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model [13] with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. [18] utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features.",
            "score": 0.5926342385333716,
            "section_title": "STATE OF THE ART",
            "char_start_offset": 6022,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 228,
                    "end": 547
                },
                {
                    "start": 547,
                    "end": 740
                },
                {
                    "start": 740,
                    "end": 879
                },
                {
                    "start": 879,
                    "end": 1129
                },
                {
                    "start": 1129,
                    "end": 1335
                },
                {
                    "start": 1335,
                    "end": 1466
                },
                {
                    "start": 1466,
                    "end": 1729
                }
            ],
            "ref_mentions": [
                {
                    "start": 253,
                    "end": 256,
                    "matchedPaperCorpusId": "204735695"
                },
                {
                    "start": 282,
                    "end": 286,
                    "matchedPaperCorpusId": "236478143"
                },
                {
                    "start": 895,
                    "end": 899,
                    "matchedPaperCorpusId": "235097309"
                },
                {
                    "start": 1112,
                    "end": 1115,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 1210,
                    "end": 1214,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1525,
                    "end": 1529,
                    "matchedPaperCorpusId": "235097309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96484375
        },
        {
            "corpus_id": "249097545",
            "title": "Target-aware Abstractive Related Work Generation with Contrastive Learning",
            "text": "For example, Zhao et al. [41] converted the original documents to a sentence graph, taking both linguistic and deep representation into account, and Liu et al. [23] constructed sentence graph based on both the similarities and relative distances in the neighborhood of each sentence. Due to the success of neural abstractive models on single-document summarization, abstractive models are adapted to multi-document summarization. Jin et al. [20] proposed a multi-granularity interaction network for extractive and abstractive multi-document summarization, and Zhou et al. [42] augmented the classical Transformer [35] with a heterogeneous graph consisting of entity nodes. However, MDS aims at synthesizing the similar and removing the redundant information among multiple input documents, while RWG needs to find the specific contributions of individual papers even if their research directions are the same and arrange them in comparative discussion [19]. \n\nOur model is also related to contrastive learning, which has boosted unsupervised representation learning in recommendation [31,40], computer vision [21,30], and natural language processing [29,43]. In text generation, Logeswaran and Lee [27] proposed to learn better sentence representations by using a classifier to distinguish context sentences from other contrastive sentences. Bui et al. [3] introduced contrastive learning into code summarization, where the model is asked to recognize similar and dissimilar code snippets through contrastive learning objectives. In scientific paper related studies, Cohan et al. [8] explored contrastive learning and designed a loss function that trains the Transformer model to learn closer representations for papers when one cites the other. The contrastive learning mechanism in our paper is also based on citation graph relatedness but is employed differently for generating comparative related work discussion.",
            "score": 0.5924006559842446,
            "section_title": "RELATED WORK",
            "char_start_offset": 8465,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1917
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 29,
                    "matchedPaperCorpusId": "220633461"
                },
                {
                    "start": 160,
                    "end": 164,
                    "matchedPaperCorpusId": "235792514"
                },
                {
                    "start": 441,
                    "end": 445,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 572,
                    "end": 576,
                    "matchedPaperCorpusId": "236478143"
                },
                {
                    "start": 613,
                    "end": 617,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 952,
                    "end": 956,
                    "matchedPaperCorpusId": "1948673"
                },
                {
                    "start": 1084,
                    "end": 1088,
                    "matchedPaperCorpusId": "235792396"
                },
                {
                    "start": 1088,
                    "end": 1091,
                    "matchedPaperCorpusId": "235792358"
                },
                {
                    "start": 1113,
                    "end": 1116,
                    "matchedPaperCorpusId": "235490270"
                },
                {
                    "start": 1150,
                    "end": 1154,
                    "matchedPaperCorpusId": "248780405"
                },
                {
                    "start": 1154,
                    "end": 1157,
                    "matchedPaperCorpusId": "237277941"
                },
                {
                    "start": 1198,
                    "end": 1202,
                    "matchedPaperCorpusId": "3525802"
                },
                {
                    "start": 1580,
                    "end": 1583,
                    "matchedPaperCorpusId": "215768677"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96240234375
        },
        {
            "corpus_id": "246388617",
            "title": "COVIDSum: A linguistically enriched SciBERT-based summarization model for COVID-19 scientific papers",
            "text": "Then, we apply SciBERT [9] and a graph attention network (GAT) [10] based graph encoder to encode the sentences and word co-occurrence graphs, respectively. Finally, we fuse the above two encodings using highway networks [11], incorporating linguistic knowledge into the contextual embeddings of scientific papers, and generating an abstractive summary for each scientific paper. The main contributions of this paper are thus threefold: \n\n(1) Heuristic sentence extraction methods based on prior knowledge are developed, and word co-occurrence graphs are utilized as linguistic features of sentences. (2) A novel linguistically enhanced SciBERT-based summarization model is proposed, which utilizes pre-trained language model, graph neural networks and highway networks to incorporate linguistic knowledge into the contextual embeddings of scientific papers. (3) Thorough experimental studies are designed and conducted to verify the effectiveness of the proposed model. \n\nWe organize the remaining part of this paper as follows. Section 2 reviews related works. Section 3 introduces our proposed linguistically enriched SciBERT-based summarization model. Section 4 and Section 5 present the experimental settings and the evaluation results, respectively. Conclusions are presented in Section 6.",
            "score": 0.592266584173962,
            "section_title": "Introduction",
            "char_start_offset": 2124,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 436
                },
                {
                    "start": 439,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1295
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 26,
                    "matchedPaperCorpusId": "202558505"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.857421875
        },
        {
            "corpus_id": "258865347",
            "title": "Is Summary Useful or Not? An Extrinsic Human Evaluation of Text Summaries on Downstream Tasks",
            "text": "Summarization models can be broadly categorized into two groups: extractive and abstractive. Extractive models directly identify and extract the most important sentences or words from the source text as the summary. Non-neural models, such as graph-based models, fuzzy logic-based models, and latent semantic analysis have been proposed and investigated [7,18,25,26,33,42]. Additionally, researchers have also explored extractive summarization based on neural network models [22,27,29,44]. On the other hand, abstractive models generate a summary text that is not necessarily a direct extraction of the source text. In recent years, abstractive summarization models based on neural networks have been advancing and become dominant in the summarization field. A common paradigm is pre-training and fine-tuning [20,23,45]. Additionally, some prompt-based approaches have been proposed [3,39], enabling summarization models to learn from specific task instructions.",
            "score": 0.5919571199596645,
            "section_title": "Summarization Models",
            "char_start_offset": 7507,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 962
                }
            ],
            "ref_mentions": [
                {
                    "start": 354,
                    "end": 357,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 357,
                    "end": 360,
                    "matchedPaperCorpusId": "10238698"
                },
                {
                    "start": 360,
                    "end": 363,
                    "matchedPaperCorpusId": "22191584"
                },
                {
                    "start": 363,
                    "end": 366,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 366,
                    "end": 369,
                    "matchedPaperCorpusId": "36744189"
                },
                {
                    "start": 369,
                    "end": 372,
                    "matchedPaperCorpusId": "2971834"
                },
                {
                    "start": 479,
                    "end": 482,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 813,
                    "end": 816,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 816,
                    "end": 819,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 883,
                    "end": 886,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81689453125
        },
        {
            "corpus_id": "271938110",
            "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
            "text": "The development of abstractive summarization methods is a crucial task in Natural Language Processing (NLP) that presents challenges, which require the creation of intelligent systems that are capable of extracting the main idea from text effectively and generate coherent summary. Numerous existing abstractive approaches do not take into account the importance of the broader context or fail to capture the global semantics in identifying salient content for summary. Moreover, there is lack of studies that extensively evaluated abstractive summarization models for specific domains, such as medical scientific document summarization. With this motivation behind, this paper developed an integrated framework for abstractive summarization of medical scientific documents that integrates topic-aware Heterogeneous Graph Neural Network with a Transformer model. The suggested framework uses Latent Dirichlet Allocation (LDA) for topic modeling to uncover latent topics and global information, thus preserving document-level attributes important for creation of effective summaries. In addition to topic modeling, the framework utilized a Heterogeneous Graph Neural Network (HGNN), capable of capturing the relationship between sentences through graph-based document representation, and allows for the concurrent updating of both local and global information. Finally, the framework is integrated with a Transformer decoder, which greatly enhances the ability of model to produce accurate and informative abstractive summaries. The performance of proposed framework is evaluated on publicly available PubMed dataset related to medical scientific papers. Experimental results illustrate that the suggested framework for abstractive summarization showed superior performance as compared to the state-of-the-art models, achieving high F1-Scores: 46.03 for Rouge-1, 21.42 for Rouge-2, and 39.71 for Rouge-L. Our research makes a significant contribution to the field of natural language processing, particularly in the area of medical scientific document summarization. It demonstrates superior performance and provides a deeper understanding of document structure, and has the potential to impact various applications by offering efficient access to information.",
            "score": 0.5914987155932159,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.958984375
        },
        {
            "corpus_id": "270560254",
            "title": "A Systematic Survey of Text Summarization: From Statistical Methods to Large Language Models",
            "text": "Several surveys have been conducted to examine various aspects of summarization.Nevertheless, existing surveys mostly focus on investigating traditional statistical approaches and deep learningbased summarization approaches [16,48,54].The scarcity of comprehensive up-to-date survey studies and the lack of consensus continue to inhibit progress.With the significant paradigm shift initiated by pre-trained language models and, more recently, large language models, there is still a dearth of thorough investigations that comprehensively encompass the continuous progress in the field of summarization in this new era.\n\nFor example, early survey papers [54,151] provide a comprehensive survey of statistical and deep learning-based automatic text summarization models, discussing their detailed taxonomy and applications.Following this, Cajueiro et al. [16] provide a comprehensive literature review of the methods, data, evaluation, and coding of ATS up to the advent of pre-trained language models.\n\nResearchers have also produced surveys with a more specific focus.Survey paper [98] focuses on the datasets, methods, and evaluation metrics of long documents such as academic articles and business reports.Another work [181] focuses on graph neural network-based (GNN-based) approaches to the task of automatic text summarization.Cao [17] conducts a survey on neural abstractive summarization methods and explores the factual consistency of the abstractive summarization systems.\n\nConsidering the rapid advancement of summarization approaches and the disruptive change introduced by LLMs, we believe it is imperative to review the details of representative methods both before and during the LLM era, analyze the uniqueness of each method, and discuss open challenges and prospective directions to facilitate the further advancement of the field.",
            "score": 0.5912561573577889,
            "section_title": "Major Differences",
            "char_start_offset": 1804,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 80,
                    "end": 235
                },
                {
                    "start": 235,
                    "end": 346
                },
                {
                    "start": 346,
                    "end": 618
                },
                {
                    "start": 620,
                    "end": 821
                },
                {
                    "start": 821,
                    "end": 1000
                },
                {
                    "start": 1002,
                    "end": 1068
                },
                {
                    "start": 1068,
                    "end": 1208
                },
                {
                    "start": 1208,
                    "end": 1332
                },
                {
                    "start": 1332,
                    "end": 1481
                },
                {
                    "start": 1483,
                    "end": 1848
                }
            ],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 234,
                    "matchedPaperCorpusId": "224955327"
                },
                {
                    "start": 653,
                    "end": 657,
                    "matchedPaperCorpusId": "224955327"
                },
                {
                    "start": 657,
                    "end": 661,
                    "matchedPaperCorpusId": "244510269"
                },
                {
                    "start": 1081,
                    "end": 1085,
                    "matchedPaperCorpusId": "250118028"
                },
                {
                    "start": 1221,
                    "end": 1226,
                    "matchedPaperCorpusId": "252819333"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56005859375
        },
        {
            "corpus_id": "248266537",
            "title": "A Survey on Neural Abstractive Summarization Methods and Factual Consistency of Summarization",
            "text": "Automatic summarization is the process of shortening a set of textual data computationally, to create a subset (a summary) that represents the most important pieces of information in the original text. Existing summarization methods can be roughly divided into two types: extractive and abstractive. An extractive summarizer explicitly selects text snippets (words, phrases, sentences, etc.) from the source document, while an abstractive summarizer generates novel text snippets to convey the most salient concepts prevalent in the source. \n\nThe purpose of this review is to provide a thorough survey of state-of-the-art abstractive summarization approaches and discuss some of the challenges these methods face. We focus on the abstractive summarization task because it is computationally more challenging than extraction methods and is closer to the way humans write summaries. There are two parts in this survey. In the first part, we will explore some classic as well as recent summarization methods. The focus will be on neural network-based abstractive summarization methods. Firstly, we will briefly review some non-neural abstractive summarization methods from the pre-neural network era. Most of these methods use either deletion-based [Knight and Marcu, 2002] or statistical models [Banko et al., 2000]. Secondly, we examine in detail five neural-based abstractive text summarization models [Rush et al., 2015, Chopra et al., 2016, Nallapati et al., 2016, See et al., 2017, Gehrmann et al., 2018, Paulus et al., 2018]. These methods all adapt recurrent neural network (RNN) or convolutional neural network (CNN) architecture for sentence representation. With the help of the representational capabilities of neural networks and large training data sets, these models are largely superior to previous non-neural summarization methods. Then, we will move on to the most recent work on abstractive summarization using large pre-trained language models [Liu and Lapata, 2019, Lewis et al., 2020, Zhang et al., 2020, Stiennon et al., 2020].",
            "score": 0.5895373455049617,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 540
                },
                {
                    "start": 543,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 1246,
                    "end": 1270,
                    "matchedPaperCorpusId": "7793213"
                },
                {
                    "start": 1293,
                    "end": 1313,
                    "matchedPaperCorpusId": "133195"
                },
                {
                    "start": 1402,
                    "end": 1420,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 1420,
                    "end": 1441,
                    "matchedPaperCorpusId": "7147309"
                },
                {
                    "start": 1441,
                    "end": 1465,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 1465,
                    "end": 1483,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1483,
                    "end": 1506,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 1506,
                    "end": 1528,
                    "matchedPaperCorpusId": "21850704"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87744140625
        },
        {
            "corpus_id": "269363075",
            "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization",
            "text": "In recent years, however, such distinctions between summarization tasks that are abstractive versus extractive, generic versus query-focused, and single-document versus multi-document, have become less relevant.While early applications of the transformer architecture showed substantial improvements on the state-of-the-art for all such summarization tasks (Goodwin et al., 2020;Laskar et al., 2022;Liu and Lapata, 2019), these tasks are now trivialized by modern LLMs, including the GPT (Achiam et al., 2023;Brown et al., 2020), Llama (Touvron et al., 2023), and Gemini (Anil et al., 2023) series, all of which can use in-context learning to summarize any content provided in their context window.\n\nThe challenge remains, however, for query-focused abstractive summarization over an entire corpus.Such volumes of text can greatly exceed the limits of LLM context windows, and the expansion of such windows may not be enough given that information can be \"lost in the middle\" of longer contexts (Kuratov et al., 2024;Liu et al., 2023).In addition, although the direct retrieval of text chunks in na\u00efve RAG is likely inadequate for QFS tasks, it is possible that an alternative form of pre-indexing could support a new RAG approach specifically targeting global summarization.\n\nIn this paper, we present a Graph RAG approach based on global summarization of an LLM-derived knowledge graph (Figure 1).In contrast with related work that exploits the structured retrieval and traversal affordances of graph indexes (subsection 4.2), we focus on a previously unexplored quality of graphs in this context: their inherent modularity (Newman, 2006) and the ability of community detection algorithms to partition graphs into modular communities of closely-related nodes (e.g., Louvain, Blondel et al., 2008;Leiden, Traag et al., 2019).LLM-generated summaries of these  (Yang et al., 2018) varies with chunk size and gleanings for our generic entity extraction prompt with gpt-4-turbo.",
            "score": 0.58661486210074,
            "section_title": "Indexing Time Query Time Pipeline Stage",
            "char_start_offset": 2199,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 211,
                    "end": 698
                },
                {
                    "start": 700,
                    "end": 798
                },
                {
                    "start": 798,
                    "end": 1035
                },
                {
                    "start": 1035,
                    "end": 1275
                },
                {
                    "start": 1277,
                    "end": 1399
                },
                {
                    "start": 1399,
                    "end": 1826
                },
                {
                    "start": 1826,
                    "end": 1975
                }
            ],
            "ref_mentions": [
                {
                    "start": 357,
                    "end": 379,
                    "matchedPaperCorpusId": "227231483"
                },
                {
                    "start": 379,
                    "end": 399,
                    "matchedPaperCorpusId": "245385261"
                },
                {
                    "start": 509,
                    "end": 528,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1626,
                    "end": 1640,
                    "matchedPaperCorpusId": "2774707"
                },
                {
                    "start": 1777,
                    "end": 1798,
                    "matchedPaperCorpusId": "334423"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8310546875
        },
        {
            "corpus_id": "250012667",
            "title": "GATSum: Graph-Based Topic-Aware Abstract Text Summarization",
            "text": "The purpose of text summarization is to compress a text document into a summary containing key information. abstract approaches are challenging tasks, it is necessary to design a mechanism to effectively extract salient information from the source text, and then generate a summary. However, most of the existing abstract approaches are difficult to capture global semantics, ignoring the impact of global information on obtaining important content. To solve this problem, this paper proposes a Graph-Based Topic Aware abstract Text Summarization (GTASum) framework. Specifically, GTASum seamlessly incorporates a neural topic model to discover potential topic information, which can provide document-level features for generating summaries. In addition, the model integrates the graph neural network which can effectively capture the relationship between sentences through the document representation of graph structure, and simultaneously update the local and global information. The further discussion showed that latent topics can help the model capture salient content. We conducted experiments on two datasets, and the result shows that GTASum is superior to many extractive and abstract approaches in terms of ROUGE measurement. The result of the ablation study proves that the model has the ability to capture the original subject and the correct information and improve the factual accuracy of the summarization.",
            "score": 0.5860706532355354,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9697265625
        },
        {
            "corpus_id": "268247594",
            "title": "A Second Look on BASS - Boosting Abstractive Summarization with Unified Semantic Graphs - A Replication Study",
            "text": "The goal of automatic text summarization is to generate a fluent, concise, informative, and faithful summary of source documents [10]. Extractive summarization systems select salient phrases from the source document and concatenate them to form the summary. In contrast, abstractive summarization systems freely generate text conditioned on an intermediate representation of the source document [10]. Consequently, the capabilities of abstractive summarization systems depend on the richness of this intermediate representation. \n\nMany state-of-the-art abstractive summarization systems are based on Pretrained Language Models (PLM), such as BERT [4], PEGASUS [34], or T5 [25]. And the success of transformers [27] across many domains shows that they are capable of generating rich representations for a wide range of signals, including vision [6], audio [5] and graphs [33]. The Graphormer [33] is one of many Attentive Graph Neural Networks [28,3,33], which have been successfully adapted for transformers to leverage graphs in abstractive summarization systems [20,24,36,15,7,32,16,17,11], with the aim to complement or guide the rich representation of transformers with explicitly structured data to improve accuracy and faithfulness of the generated summaries. \n\nOne of the graph-enhanced transformer models is BASS [30], which is of specific interest because i) it introduces a compressed dependency graph structure based on the idea of semantic units and ii) the authors report competitive performance in abstractive summarization while being only half the size (201M parameters for BASS vs. 406M parameters for BART [19] and PEGASUS). \n\nBecause the original paper [30] is not accompanied by source code, we conduct a replication study of the BASS framework. Our results contribute to the broader discourse surrounding reproducibility concerns of Machine Learning [12,14] and in particular NLP research, sometimes even referred to as the \"reproducibility crisis\" [1,2]. Belz et. al report that fewer than 15% scores of their study were reproducible, and that \"worryingly small differences in code have been found to result in big differences in performance\" [1].",
            "score": 0.5829081008026331,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1265
                },
                {
                    "start": 1268,
                    "end": 1642
                },
                {
                    "start": 1645,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2169
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 133,
                    "matchedPaperCorpusId": "224955327"
                },
                {
                    "start": 395,
                    "end": 399,
                    "matchedPaperCorpusId": "224955327"
                },
                {
                    "start": 647,
                    "end": 650,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 660,
                    "end": 664,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 672,
                    "end": 676,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 710,
                    "end": 714,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 855,
                    "end": 858,
                    "matchedPaperCorpusId": "52287921"
                },
                {
                    "start": 870,
                    "end": 874,
                    "matchedPaperCorpusId": "235376852"
                },
                {
                    "start": 891,
                    "end": 895,
                    "matchedPaperCorpusId": "235376852"
                },
                {
                    "start": 949,
                    "end": 952,
                    "matchedPaperCorpusId": "235376852"
                },
                {
                    "start": 1064,
                    "end": 1068,
                    "matchedPaperCorpusId": "260854142"
                },
                {
                    "start": 1068,
                    "end": 1071,
                    "matchedPaperCorpusId": "248922534"
                },
                {
                    "start": 1071,
                    "end": 1074,
                    "matchedPaperCorpusId": "235097229"
                },
                {
                    "start": 1074,
                    "end": 1077,
                    "matchedPaperCorpusId": "236477655"
                },
                {
                    "start": 1077,
                    "end": 1079,
                    "matchedPaperCorpusId": "223953416"
                },
                {
                    "start": 1079,
                    "end": 1082,
                    "matchedPaperCorpusId": "219036690"
                },
                {
                    "start": 1082,
                    "end": 1085,
                    "matchedPaperCorpusId": "218487279"
                },
                {
                    "start": 1085,
                    "end": 1088,
                    "matchedPaperCorpusId": "214303164"
                },
                {
                    "start": 1088,
                    "end": 1091,
                    "matchedPaperCorpusId": "202785778"
                },
                {
                    "start": 1321,
                    "end": 1325,
                    "matchedPaperCorpusId": "235187330"
                },
                {
                    "start": 1672,
                    "end": 1676,
                    "matchedPaperCorpusId": "235187330"
                },
                {
                    "start": 1871,
                    "end": 1875,
                    "matchedPaperCorpusId": "251102207"
                },
                {
                    "start": 1970,
                    "end": 1973,
                    "matchedPaperCorpusId": "232232827"
                },
                {
                    "start": 1973,
                    "end": 1975,
                    "matchedPaperCorpusId": "259859021"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9375
        },
        {
            "corpus_id": "254408844",
            "title": "Scientific Paper Extractive Summarization Enhanced by Citation Graphs",
            "text": "Figure 1 demonstrates this intuition. A good paper abstract of the query paper is more rel-evant to the directly cited paper 1, 2, and indirectly cited paper 3 by sharing domain-specific terms, and not relevant to paper 4 and 5 that are far from it in the citation graph. \n\nBased on this observation, we first propose a Multi-granularity Unsupervised Summarization (MUS) as a light solution to the task without the requirement of rich supervision information. MUS first finetunes a pre-trained encoder model on the citation graph to obtain better sentence and document representations by solving a link prediction task. Then, MUS transforms the citation graph into a multi-granularity graph. Sentences are then extracted considering the multi-granularity positionaware centrality. Experimental results demonstrate that citation graph information can enhance the summarization even by this simple unsupervised framework. \n\nMotivated by this, we further propose a Graphbased Supervised Summarization model (GSS) to achieve more accurate results for the scenario where large-scale labeled data is feasible. Firstly, a graph neural network encoder based on a pretrained language model is employed to obtain sentence representations for the target document and document representations for related papers in the citation graph. Secondly, we propose a gated sentence encoder that polishes the sentence representations based on their relatedness to the document gist. Then, a graph information fusion module is utilized to incorporate the information from reference papers to the polished sentence representations. Finally, a multi-task framework is applied to the model, which jointly assigns selection weights to extract abstracts and predicts whether there exists an edge between two nodes. Under the supervision setting, the graph information gives stronger guidance in two ways. In one way, we employ the link prediction task on the graph to obtain better document representation, and in the other way, the document representation is used to polish sentence representations. Results show that our model significantly surpasses the prior state-of-the-art model by on public benchmark dataset. \n\nOur contributions can be summarized as follows: \n\n\u2022 Our work demonstrates the effectiveness of citation graph modeling in scientific paper extractive summarization.",
            "score": 0.5810116389486473,
            "section_title": "Introduction",
            "char_start_offset": 2145,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 271
                },
                {
                    "start": 274,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 919
                },
                {
                    "start": 922,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2072
                },
                {
                    "start": 2073,
                    "end": 2189
                },
                {
                    "start": 2192,
                    "end": 2239
                },
                {
                    "start": 2242,
                    "end": 2356
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94970703125
        },
        {
            "corpus_id": "271938110",
            "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
            "text": "This section illustrates the methodology of our proposed Heterogeneous Graph Neural Network framework, enriched with topical information, for abstractive summarization as shown in Fig. 1. The proposed framework consists of four core trainable modules: a BERT Document Encoder, an LDA Topic Extractor, a Graph Attention Layer, and a Transformer Decoder. Given a corpus of scientific medical document with n documents, D = [D 1 , D 2 , . . . . . . ., D n ], and the corresponding 'M' human generated gold summaries Y = [Y 1 , Y 2 , . . . . . ., Y m ], our model aims to create abstractive summaries of the documents (D) that capture the essential meaning of the documents using their own phrasing, achieving quality comparable to human-written summaries (Y). The proposed framework utilizes a multi-step process to generate abstractive summaries for scientific medical documents. First, the BERT Document Encoder processes each document to generate contextualized embeddings for each sentence, capturing its semantic meaning. These embeddings become the foundation for sentence and word nodes within the document graph. Next, the LDA model identifies a set of latent topics that represent the underlying thematic structure of each document. These topics are also incorporated as nodes in the document graph. A heterogeneous document graph is then constructed, where nodes represent sentences, words, and topics and the edges capture the relationships between these nodes, and are represented by TF-IDF weights. \n\nThe Graph Attention Layer then operates on heterogeneous document graph. It Utilizes TF-IDF weights on the edges to emphasize connections between important words and sentences within the document. It leverages an attention mechanism to selectively focus on crucial relationships within the graph, refining the representation of each node. Finally, the refined document graph is fed into a Transformer decoder. The decoder utilizes these refined node representations to generate an abstractive summary that captures the essential meaning of the document using its own phrasing. The details behind each module are discussed below as follows. \n\nA. DOCUMENT ENCODER BERT's encoder leverages an attention-based architecture, offering significant advantages compared to standard language models.",
            "score": 0.5796693513754119,
            "section_title": "III. METHODOLOGY",
            "char_start_offset": 15890,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1508
                },
                {
                    "start": 1511,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2087
                },
                {
                    "start": 2088,
                    "end": 2150
                },
                {
                    "start": 2153,
                    "end": 2300
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97119140625
        },
        {
            "corpus_id": "252604304",
            "title": "Abstractive Summarization of Broadcast News Stories for Estonian",
            "text": "The growth of online multimedia, such as talks, presentations, lectures and news, has created a significant need to provide easy access to these resources (Furui et al., 2001). Although speech is the most natural and effective method of communication between human beings, it is not easy to quickly review, retrieve and reuse speech documents if they are simply recorded as an audio signals (Furui, 2003). Automatic summarization has the potential to efficiently generate concise and fluent summaries while preserving critical information from the original media. \n\nThere are two major approaches for automatic text summarization: extractive and abstractive. The extractive summarization approach produces summaries by choosing a subset of sentences from the original text. One of many extractive techniques is LexRank, which analytically computes the relative importance of words and sentences to produce the summary (Erkan and Radev, 2011). Abstractive summarization, on the other hand, can generate novel sentences by either rephrasing or using the new words, instead of simply extracting the important sentences (Rachabathuni, 2018). The method better approximates human summaries, however abstractive summarization is an exceedingly non-trivial and challenging task (Allahyari et al., 2017). \n\nAs news broadcasts primarily contain spoken-word content, summarization can be performed in the text domain on the transcript of an episode, as shown with Pod-Summ (Vartakavi and Garg, 2020). Deep learning-based neural summarization performs well when applied to abstract text summarization (Salakhutdinov, 2014) compared to structure-based and semantic-based abstractive summarization approaches. In general, neural summarization is solved using an encoder-decoder architecture with recurrent neural networks or self-attention (Sutskever et al., 2014). There is an inherent limitation to natural language processing tasks, such as text summarization for resourcepoor and morphologically complex languages, owing to a shortage of quality linguistic data available (Kurniawan and Louvan, 2019). The state-of-the-art neural abstractive summarization models are trained with annotated datasets of hundreds of thousands or millions of data points.",
            "score": 0.5777160461737894,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 563
                },
                {
                    "start": 566,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1296
                },
                {
                    "start": 1299,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2242
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 175,
                    "matchedPaperCorpusId": "18919544"
                },
                {
                    "start": 391,
                    "end": 404,
                    "matchedPaperCorpusId": "14353922"
                },
                {
                    "start": 918,
                    "end": 941,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1116,
                    "end": 1136,
                    "matchedPaperCorpusId": "44090206"
                },
                {
                    "start": 1827,
                    "end": 1851,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 2063,
                    "end": 2091,
                    "matchedPaperCorpusId": "52978189"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.434326171875
        },
        {
            "corpus_id": "254408844",
            "title": "Scientific Paper Extractive Summarization Enhanced by Citation Graphs",
            "text": "Extractive summarization aims to generate a summary by integrating the salient sentences in the document. Traditional extractive summarization methods are mostly unsupervised, which directly select sentences based on explicit features or graphbased methods. Unsupervised methods can save a lot of manual work and expenses since it does not demand labeled data. More recently, benefiting from the success of neural sequence models and large-scale datasets, neural network-based supervised summarization models have been proposed (Gao et al., 2019(Gao et al., , 2020)). More recetly, Liu and Lapata (2019) and Xu et al. (2020) showcased how pre-trained models can be applied in extractive text summarization tasks. These models often achieve better performance since labeled data is provided. \n\nResearch on summarizing scientific articles has been studied for decades (Nenkova and McKeown, 2011). In the unsupervised domain, Cohan and Goharian (2015) proposed to extract citationcontexts from citing articles, which does not help draft an abstract when the paper has not been cited yet. Dong et al. (2021b) came up with an unsupervised graph-based ranking model for extractive summarization. As for supervised methods, Subramanian et al. (2019) performed an extractive step before generating a summary, which is then used as guidance for abstractive summarization. Xiao and Carenini (2019) incorporated both the global context and local context to summarize papers. \n\nEarly approaches for extractive summarization, such as LexRank and TextRank (Mihalcea and Tarau, 2004), have taken advantage of graph structures with inter-sentence cosine similarity. As for the neural-based approaches, Koncel-Kedziorski et al. (2019) designed a graph attention-based transformer encoder to generate a summary with the help of knowledge graphs extracted from scientific texts. Graph modeling is also explored in abstractive summarization. For example, An et al. (2021) proposed a citation graph-based summarization model which incorporates inter-document information of the source paper and its references.",
            "score": 0.5773333874903603,
            "section_title": "Related Work",
            "char_start_offset": 4850,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 790
                },
                {
                    "start": 793,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1463
                },
                {
                    "start": 1466,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2089
                }
            ],
            "ref_mentions": [
                {
                    "start": 528,
                    "end": 545,
                    "matchedPaperCorpusId": "202676734"
                },
                {
                    "start": 545,
                    "end": 566,
                    "matchedPaperCorpusId": "218581875"
                },
                {
                    "start": 582,
                    "end": 603,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 608,
                    "end": 624,
                    "matchedPaperCorpusId": "219036690"
                },
                {
                    "start": 923,
                    "end": 948,
                    "matchedPaperCorpusId": "5523604"
                },
                {
                    "start": 1085,
                    "end": 1104,
                    "matchedPaperCorpusId": "231592914"
                },
                {
                    "start": 1363,
                    "end": 1387,
                    "matchedPaperCorpusId": "201697450"
                },
                {
                    "start": 1542,
                    "end": 1568,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 1686,
                    "end": 1717,
                    "matchedPaperCorpusId": "102354588"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9130859375
        },
        {
            "corpus_id": "259129368",
            "title": "AaKOS: Aspect-adaptive Knowledge-based Opinion Summarization",
            "text": "Similar to general text summarization, opinion summarization can be categorized into abstractive and extractive methods. Mirroring the developmental trajectory of general summarization, extractive methods initially gained widespread use in this domain due to the high cost of creating golden summaries for datasets, particularly for review datasets where such summaries are not mandatory [22]. Early works in this field predominantly treated the task as a sentence or phrase selection problem, employing either ranking or classification approaches. For instance, Wei et al. prioritize sentences that closely corresponded to the query [23], while Erkan et al. employed a stochastic graph-based method to rank sentences by calculating their importance based on eigenvector centrality within a graph representation of the sentences [24]. Presently, extractive methods remain popular for opinion summarization [1,11,25,26]. \n\nAbstractive methods for opinion summarization began to emerge around 2010, with Ganesan et al. introducing a graph-based algorithm for generating abstractive summaries [27]. Although this approach produced abstractive summaries, it selected words, phrases, or sentences from the original text, rendering it more akin to an extractive method. In recent years, a growing number of studies have begun to leverage machine learning and deep learning techniques for this task. Notably, Chu et al. presented an unsupervised neural model for multi-document summarization, proposing an end-to-end architecture featuring an auto-encoder. This approach decoded the mean of the input reviews' representations into a coherent summary review without relying on any review-specific attributes [22].",
            "score": 0.5744551136763414,
            "section_title": "Opinion Summarization",
            "char_start_offset": 9591,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 919
                },
                {
                    "start": 922,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1705
                }
            ],
            "ref_mentions": [
                {
                    "start": 388,
                    "end": 392,
                    "matchedPaperCorpusId": "59413781"
                },
                {
                    "start": 634,
                    "end": 638,
                    "matchedPaperCorpusId": "2814261"
                },
                {
                    "start": 829,
                    "end": 833,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 906,
                    "end": 909,
                    "matchedPaperCorpusId": "237434243"
                },
                {
                    "start": 909,
                    "end": 912,
                    "matchedPaperCorpusId": "33846824"
                },
                {
                    "start": 912,
                    "end": 915,
                    "matchedPaperCorpusId": "230102949"
                },
                {
                    "start": 915,
                    "end": 918,
                    "matchedPaperCorpusId": "233306955"
                },
                {
                    "start": 1090,
                    "end": 1094,
                    "matchedPaperCorpusId": "988010"
                },
                {
                    "start": 1700,
                    "end": 1704,
                    "matchedPaperCorpusId": "59413781"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91796875
        },
        {
            "corpus_id": "259289652",
            "title": "Graph-Enhanced Biomedical Abstractive Summarization Via Factual Evidence Extraction",
            "text": "We introduced EASumm, the first abstractive summarization model augmenting source documents with explicit, structured medical evidence extracted from them, thereby concretizing a tandem text-graph architecture. We demonstrated the significant positive influence of biomedical event extraction for summarization, allowing a model to better distinguish semantics and lexical surface. Indeed, we showed improvements in ROUGE and readability scores, observing a strong connection between the summer quality and (1) the number of events extracted from the input document, (2) the enhanced node features initialization considering both domain-specific pretrained language models and entity types. Contrary to expectations, event-graph representation learning does not benefit from the awareness of the relation type. The motivation is to be found in the task-driven nature of event extraction and in the poor capacity of current graph neural networks in managing multi-relational graphs. \n\nAlthough the numerous newly introduced graph-LSTM models combined with structured knowledge, we establish that these architectures are far from being competitive with generative transformer-based solutions like BART.",
            "score": 0.5743713889642212,
            "section_title": "Conclusion",
            "char_start_offset": 37264,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 981
                },
                {
                    "start": 984,
                    "end": 1200
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87744140625
        },
        {
            "corpus_id": "265561377",
            "title": "Automatic Extractive Summarization using GAN Boosted by DistilBERT Word Embedding and Transductive Learning",
            "text": "Abstractive summarization methods, a notable strategy in NLP, aim to produce summaries that don't merely pick and reorganize existing sentences or phrases [16]. These techniques endeavor to grasp the essence of the text and formulate new, succinct, and cohesive statements that reflect the main ideas of the original content [17]. Abstractive summarization seeks to produce summaries with a human-like touch, capturing the heart of the source material without restricting itself to direct extractions. By discerning the core semantics, connections, and subtleties of the document, abstractive methods can potentially craft summaries that are richer, more concise, and linguistically smooth. To realize this, such techniques frequently utilize advanced tools like neural networks and natural language generation models [18,19]. These models employ methods such as sequence-to-sequence frameworks, attention systems, and reinforcement learning to craft summaries that hold semantic significance and flow smoothly. By grasping the underlying context and essence of the text, abstractive summarization models can reword and restructure the original material, introducing fresh phrases, reshaping statements, and even creating unique expressions to highlight the primary details. This capability to transcend basic extraction allows abstractive summarization to deliver shorter summaries that still encapsulate the primary intent of the original text. Yet, this approach comes with its set of challenges. The crafted summaries must walk the fine line of being brief yet informative, ensuring logical flow and upholding the truthfulness of the source. Moreover, abstractive techniques often demand vast training data and intricate models to effectively decipher the subtleties and variances in natural language [20]. \n\nNumerous extractive summarization methods, spanning graph-based to deep learning techniques, have been explored [21,22]. LeClair et al. [23] delved into code summarization advancements via Graph Neural Network (GNN) application, enhancing summary insightfulness. Zhong et al. [24] derived word features from documents and then determined sentence scores based on word scores. Yousefi et al. [25] scored sentences using the cosine similarity between them and their topics.",
            "score": 0.5734437619519365,
            "section_title": "II. RELATED WORKS",
            "char_start_offset": 5579,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1810
                },
                {
                    "start": 1813,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2284
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 159,
                    "matchedPaperCorpusId": "244711230"
                },
                {
                    "start": 325,
                    "end": 329,
                    "matchedPaperCorpusId": "253562909"
                },
                {
                    "start": 818,
                    "end": 822,
                    "matchedPaperCorpusId": "258970618"
                },
                {
                    "start": 1805,
                    "end": 1809,
                    "matchedPaperCorpusId": "257413656"
                },
                {
                    "start": 1925,
                    "end": 1929,
                    "matchedPaperCorpusId": "247249484"
                },
                {
                    "start": 1949,
                    "end": 1953,
                    "matchedPaperCorpusId": "214802082"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8388671875
        },
        {
            "corpus_id": "254408844",
            "title": "Scientific Paper Extractive Summarization Enhanced by Citation Graphs",
            "text": "(1) BertSumEXT (Liu and Lapata, 2019): the extractive summarization model with BERT as the encoder. (2) MGSum-ext (Jin et al., 2020a): the extractive multi-document summarization baseline, which extracts sentences from both the document and the abstracts of the reference papers. We incorporate this baseline to see if sentences from the reference papers are useful. (3) EMSum (Zhou et al., 2021): the abstractive multi-document summarization model, which takes the paper with references as multiple input documents. (4) CGSum (An et al., 2021): the abstractive summarization baseline with citation graph as input, which is also the paper that proposes the SSN dataest. ( 5) HSG (Wang et al., 2020): the heterogeneous graph-based neural network for extractive summarization.",
            "score": 0.5733410449791243,
            "section_title": "Supervised summarization baselines:",
            "char_start_offset": 17397,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 774
                }
            ],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 133,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 377,
                    "end": 396,
                    "matchedPaperCorpusId": "236478143"
                },
                {
                    "start": 527,
                    "end": 544,
                    "matchedPaperCorpusId": "233169121"
                },
                {
                    "start": 679,
                    "end": 698,
                    "matchedPaperCorpusId": "216552978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94091796875
        },
        {
            "corpus_id": "257496469",
            "title": "Compressed Heterogeneous Graph for Abstractive Multi-Document Summarization",
            "text": "Abstractive Multi-Document Summarization PLM-Based Models Recent PLM-based models have shown strong performance for abstractive text summarization tasks. These models follow a Transformerbased (Vaswani et al. 2017) encoder-decoder architecture. For example, general-purpose PLMs such as T5 (Raffel et al. 2020), BART (Lewis et al. 2020), and LED (Beltagy, Peters, and Cohan 2020) can be fine-tuned for abstractive text summarization. PEGASUS (Zhang et al. 2020a) is a strong PLM-based model pre-trained with an objective that predicts gap sentences as a pseudo summary. These models can be used for MDS by concatenating the source documents into a single document. PRIMERA (Xiao et al. 2022) has the same architecture as LED, but is designed for MDS specifically in that it is pre-trained to generate pseudo summaries -text spans that are automatically extracted based on the entity salience. Although these models show impressive performances and can even handle zero-shot cases, they use a flat concatenation of the input documents, which limits their capability in learning the cross-document relationships among different semantic units. \n\nGraph-Based Models Although graphs are commonly used to boost text summarization (Wu et al. 2021b;You et al. 2022;Song and King 2022), there are only a handful of models which have been proposed to use graphs to encode the documents in abstractive MDS (Li et al. 2020;Jin, Wang, and Wan 2020;Li and Zhuge 2021;Cui and Hu 2021). Most of these models only leverage homogeneous graphs as they do not consider different edge types of graphs. For example, MGSum (Jin, Wang, and Wan 2020) constructs a threelevel (i.e., document, sentence, and word levels) hierarchical graph and learns semantics with a multi-level interaction network. GraphSum (Li et al. 2020) constructs a similarity graph over the paragraphs. It learns a graph representation for the paragraphs and uses a hierarchical graph attention mechanism to guide the summary generation process.",
            "score": 0.5715745543620938,
            "section_title": "Related Work",
            "char_start_offset": 23964,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1141
                },
                {
                    "start": 1144,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 1994
                }
            ],
            "ref_mentions": [
                {
                    "start": 290,
                    "end": 310,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 442,
                    "end": 461,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 673,
                    "end": 690,
                    "matchedPaperCorpusId": "247519084"
                },
                {
                    "start": 1225,
                    "end": 1242,
                    "matchedPaperCorpusId": "235187330"
                },
                {
                    "start": 1242,
                    "end": 1258,
                    "matchedPaperCorpusId": "250390945"
                },
                {
                    "start": 1258,
                    "end": 1277,
                    "matchedPaperCorpusId": "250301954"
                },
                {
                    "start": 1396,
                    "end": 1412,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 1412,
                    "end": 1436,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 1436,
                    "end": 1454,
                    "matchedPaperCorpusId": "196170254"
                },
                {
                    "start": 1454,
                    "end": 1470,
                    "matchedPaperCorpusId": "239050558"
                },
                {
                    "start": 1601,
                    "end": 1626,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 1784,
                    "end": 1799,
                    "matchedPaperCorpusId": "218718706"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97021484375
        },
        {
            "corpus_id": "273613861",
            "title": "Text Summarization: A Bibliometric Study and Systematic Literature Review",
            "text": "Extractive text summarization technology has made substantial progress since its debut. Preliminary research conducted by Nazari and Mahdavi [6] mostly examined conventional extraction techniques that relied on term frequency and sentence position. In contrast, Kirmani et al. [7] incorporated word embedding models to improve the precision of sentence scoring. Yadav and Chatterjee [8] enhanced extractive summarization methods by utilising graph-based strategies to optimize the representation of connections between sentences. Utilizing deep learning technologies, Suleiman et al. [9] employed these methods for extractive summarization, resulting in a notable enhancement in summarization effectiveness. On the other hand, research has also explored certain domains. Kanapala et al. [10] specifically examined domain-specific summarization strategies for legal documents, whereas Dutta et al. [11] assessed and contrasted several extraction algorithms employed in microblog summarization. In their study, Jacquenet et al. [12] investigated abstractive deep-learning techniques and discussed the difficulties related to summarization. Mahajani et al. [13] have conducted reviews on both extractive and abstractive summarization approaches. Together, these works have enhanced the theoretical comprehension of extractive text summarization and expanded its practical applications and efficacy. \n\nIn the field of abstractive summarization technology, initial research primarily focused on models driven by neural networks, as demonstrated by the studies of Lin and Ng [14] and Tandel et al. [15]. More recent inquiries have further explored abstractive summarization techniques utilizing deep learning. The works of Suleiman and Awajan [16], Zhao et al. [17], and Dong et al. [18] are notable in this field. Additionally, several studies, such as those by Jacquenet et al. [12] and El-Kassas et al. [5], delve into deep learning-based abstractive summarization techniques and explore the challenges associated with them. \n\nIn recent years, numerous comprehensive surveys have evaluated traditional abstractive and extractive methods. Notable examples include the works of Gupta and Gupta [19], and Wibawa and Kurniawan [20].",
            "score": 0.5714984918441044,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2421,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1395
                },
                {
                    "start": 1398,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 2021
                },
                {
                    "start": 2024,
                    "end": 2134
                },
                {
                    "start": 2135,
                    "end": 2225
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 144,
                    "matchedPaperCorpusId": "148028"
                },
                {
                    "start": 277,
                    "end": 280,
                    "matchedPaperCorpusId": "57036582"
                },
                {
                    "start": 383,
                    "end": 386,
                    "matchedPaperCorpusId": "37471938"
                },
                {
                    "start": 584,
                    "end": 587,
                    "matchedPaperCorpusId": "208879754"
                },
                {
                    "start": 787,
                    "end": 791,
                    "matchedPaperCorpusId": "26448969"
                },
                {
                    "start": 897,
                    "end": 901,
                    "matchedPaperCorpusId": "70133915"
                },
                {
                    "start": 1026,
                    "end": 1030,
                    "matchedPaperCorpusId": "164530776"
                },
                {
                    "start": 1154,
                    "end": 1158,
                    "matchedPaperCorpusId": "133465268"
                },
                {
                    "start": 1569,
                    "end": 1573,
                    "matchedPaperCorpusId": "198189946"
                },
                {
                    "start": 1592,
                    "end": 1596,
                    "matchedPaperCorpusId": "212704515"
                },
                {
                    "start": 1755,
                    "end": 1759,
                    "matchedPaperCorpusId": "220633461"
                },
                {
                    "start": 1777,
                    "end": 1781,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 1874,
                    "end": 1878,
                    "matchedPaperCorpusId": "164530776"
                },
                {
                    "start": 1900,
                    "end": 1903,
                    "matchedPaperCorpusId": "224955327"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87451171875
        },
        {
            "corpus_id": "250012667",
            "title": "GATSum: Graph-Based Topic-Aware Abstract Text Summarization",
            "text": "The purpose of text summarization is to aid people in quickly grasping the key data of the text, and it is a valuable job in NLP (Natural Language Process-ing) tasks. Currently, summarization approaches can separate into two types: extractive and abstract. Extractive models mainly copy important information Information Technology and Control 2022/2/51 346 from the original text and aggregate them into text [10,17]. This can usually retain the salient information of the original text and has the correct grammar, but it is easy to generate a lot of redundant messages. In contrast, neural-based abstract models usually use the sequence-to-sequence framework, which can understand text content and generate words that are not appearance in the source document. Abstract models are closer to the essence of summarization and have the potential to generate high-quality summarization. Therefore, the research in this paper is biased towards abstract approaches. \n\nA key point of summarization is topic information modeling. Although current Transformer-based abstract summarization models have achieved great success which because they can effectively capture contextual features and obtain local semantic information between sentences and paragraphs, they ignore higher-level semantic information. To better capture the global semantics of input documents, researchers try to introduce topic information to conduct the process of making a summary. Topic models such as LDA [3] (Latent Dirichlet Allocation), PFA [41] (Poisson Factor Analysis), NVDM (Neural Variational Document Model) [22], and NTM [7] (Neural Topic Model) can provide additional information for document understanding. The distribution of all tokens in the vocabulary is described by taking topic information as a global variable. For the text summarization domain, by integrating document-level characteristics into the summarization model, we trust it will enhance the representation of the model. However, despite the extensive literature incorporating topic modeling into the text summarization task [31,39], we found that quite a few previous studies used topic models as a single source of information rather than unified and jointly improved the text summarization task and topic models. This attracted our attention, and GNN (Graph Neural Networks) was considered to solve this problem.",
            "score": 0.5696501497693944,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2264
                },
                {
                    "start": 2265,
                    "end": 2364
                }
            ],
            "ref_mentions": [
                {
                    "start": 414,
                    "end": 417,
                    "matchedPaperCorpusId": "2010014"
                },
                {
                    "start": 1475,
                    "end": 1478,
                    "matchedPaperCorpusId": "3177797"
                },
                {
                    "start": 1601,
                    "end": 1604,
                    "matchedPaperCorpusId": "52182945"
                },
                {
                    "start": 2078,
                    "end": 2081,
                    "matchedPaperCorpusId": "224803505"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "corpus_id": "259129368",
            "title": "AaKOS: Aspect-adaptive Knowledge-based Opinion Summarization",
            "text": "\u2022 LexRank [24] is an unsupervised graph-based summarization method. It employs a ranking algorithm to determine node centrality. In LexRank, sentences are treated as nodes to form a graph with weighted edges calculated using tf-idf. In our work, following the settings from [31], we also use BERT [30] and SentiNeuron [34] vectors to calculate the adjacency matrices. \u2022 Opinosis [27] is a graph-based summarization framework that generates concise abstractive summaries of highly redundant opinions. It assumes no domain knowledge and leverages mostly the word order in the existing text. \u2022 Meansum [22] is an unsupervised neural model for multi-document summarization. \n\nIt proposes an end-to-end architecture with an auto-encoder, where the mean of input review representations decodes into a reasonable summary review without relying on any review-specific features. \u2022 Copycat [13] is a summarization model based on the pointer-generator mechanism [5]. It follows the intuition of controlling the \"amount of novelty\" during summary generation. With this intuition, they define a hierarchical variational autoencoder model to produce summaries that reflect common opinions. \u2022 QT [31] enhances the ability to control the summarization process by leveraging the properties of quantized space to generate aspect-specific summaries.",
            "score": 0.5688438081614253,
            "section_title": "Baselines",
            "char_start_offset": 24227,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 669
                },
                {
                    "start": 672,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1330
                }
            ],
            "ref_mentions": [
                {
                    "start": 10,
                    "end": 14,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 274,
                    "end": 278,
                    "matchedPaperCorpusId": "227745131"
                },
                {
                    "start": 297,
                    "end": 301,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 379,
                    "end": 383,
                    "matchedPaperCorpusId": "988010"
                },
                {
                    "start": 599,
                    "end": 603,
                    "matchedPaperCorpusId": "59413781"
                },
                {
                    "start": 880,
                    "end": 884,
                    "matchedPaperCorpusId": "218883429"
                },
                {
                    "start": 951,
                    "end": 954,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1181,
                    "end": 1185,
                    "matchedPaperCorpusId": "227745131"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.947265625
        },
        {
            "corpus_id": "264146402",
            "title": "Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive Review",
            "text": "Although Graph Neural Networks (GNN) [62,114,145,158] are primarily used in domains where data naturally exhibits graph structures, such as social networks, molecular structures, and knowledge graphs, they can also be adapted for NLP tasks. In NLP, GNNs are typically used to model relationships between words, sentences, or documents by representing them as nodes in a graph, with edges representing the relationships between these nodes. A GNN model learns to propagate information through the graph by iteratively updating the node representations based on the information from their neighbors. The core building blocks of GNNs are graph convolutional layers, which are designed to aggregate information from neighboring nodes and update the node features. \n\nJing et al. [75] present a novel Multiplex Graph Convolutional Network (Multi-GCN) approach for extractive summarization. Multi-GCN learns node embedding of different relations among sentences and words separately and combines them to produce a final embedding. This approach helps to mitigate the over-smoothing and vanishing gradient problems of the original GCN. \n\nA heterogeneous GNN, HETERSUMGRAPH [179] is introduced for extractive document summarization. This network includes nodes of different granularity levels apart from sentences, which act as intermediaries and enrich cross-sentence relations. This approach allows different sentences to interact considering overlapping word information. Moreover, the graph network can accommodate additional node types, such as document nodes for multi-document summarization. Doan et al. [40] propose a method for long document summarization by applying Heterogeneous Graph Neural Networks (HeterGNN) and introducing a homogeneous graph structure (HomoGNN). The HomoGNN focuses on sentence-level nodes to create a graph structure, enriching inter-sentence connections. Simultaneously, the HeterGNN explores the complex relationships between words and sentences, tackling intra-sentence connections. Both networks are constructed and updated using a Graph Attention Network model. In the HomoGNN, a BERT model is used for the initial encoding of sentences, while the HeterGNN uses a combination of CNN and BiLSTM for node feature extraction.",
            "score": 0.5684432941095042,
            "section_title": "Graph neural networks",
            "char_start_offset": 38428,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1127
                },
                {
                    "start": 1130,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2093
                },
                {
                    "start": 2094,
                    "end": 2254
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "207238980"
                },
                {
                    "start": 41,
                    "end": 45,
                    "matchedPaperCorpusId": "17486263"
                },
                {
                    "start": 45,
                    "end": 49,
                    "matchedPaperCorpusId": "3051291"
                },
                {
                    "start": 49,
                    "end": 53,
                    "matchedPaperCorpusId": "206756462"
                },
                {
                    "start": 774,
                    "end": 778,
                    "matchedPaperCorpusId": "237353135"
                },
                {
                    "start": 1165,
                    "end": 1170,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 1602,
                    "end": 1606,
                    "matchedPaperCorpusId": "252819335"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.974609375
        },
        {
            "corpus_id": "271791043",
            "title": "Enhanced Topic-Aware Summarization Using Statistical Graph Neural Networks",
            "text": "This research evaluation strategy is focused on rigorously testing the proposed model's effectiveness in extractive summarization against contemporary state-of-the-art models. The performance assessment utilizes the CNN/DM and Multi-News datasets, which are standards in the field for SDS and MDS tasks, respectively. This deliberate choice confirms a broad and challenging test environment, reflecting real-world applications and the complexity of integrating multiple document contexts. \n\nThrough systematic comparisons, proposed model is tested against well-known benchmarks and recent advances in neural graph-based models. These assessments are not just limited to performance metrics like ROUGE scores but also extend to an examination of how well each model assimilates and processes linguistic and thematic information across different document types. This research examines the performance in relation to various node configurations and graph structures used in competing models, providing a comprehensive view of the proposed model's abilities. Furthermore, the assessment comprises a detailed review of the implementation settings, such as vocabulary size, tokenization standards, and hyperparameters like learning rates and dropout settings. These particulars underscore the tailored approach of proposed model in handling the refinements of extractive summarization, highlighting its innovative integration of advanced neural network techniques and its adaptability to both SDS and MDS summarization challenges. This laborious evaluation framework ensures conclusions are well-founded and validate clear advancements over existing methodologies.",
            "score": 0.5682286825042536,
            "section_title": "Performance Evaluation",
            "char_start_offset": 29267,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 488
                },
                {
                    "start": 491,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1658
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9765625
        },
        {
            "corpus_id": "248367513",
            "title": "AmpSum: Adaptive Multiple-Product Summarization towards Improving Recommendation Captions",
            "text": "In this section, we review the two groups of literature most related to our problem, namely: text summarization (particularly relating to online product summarization) and explainable recommendation. \n\nProduct/Document Summarization. The vast majority of document summarization works are categorized into extractive and abstractive approaches. Various extractive methods are based on graph [7], topic model [2,36], and supervise learning of summary/nosummary binary classification [27,48]. Recent advances in abstractive summarization witness a success of applying neural attention network [30]. The core idea of leveraging the encoder-decoder architecture inspires [6,26] applying recurrent network and [31] introducing pointer network to the decoders. Further improvements with neural abstractive summarization have been achieved with Transformer-based models [19,24,29]. Concurrently, multidocument summarization is also extensively studied with methods largely inherited from single-document summarization [2,11,22,37,41]. One direction is topic-based approach trying to identify topic representations of the document sets, which in turns are used to extract relevant sentences to form the summaries [12,35]. Another direction is cluster-based approach where the clustering is done on the sentence graph [46] or on the sentence affinity matrix [37]. \n\nThese approaches can only generate a summary as a combination of selected sentences. Such output format is undesirable and concise enough for our problem setting. As a competitive method for document summarization, BART [19] is included as a baseline. We also compare with different variants of the Pointer-Generator Networks [31] for completeness. \n\nThe line of works focusing on product summarization share some similarities to the general setting of text-based summarization while still owning its specific nature. Most works try to produce summaries based on the input of product descriptions [42] or user reviews [25,28,32]. Liu et al. [23] propose a neural network for review summarization leveraging both user and product information.",
            "score": 0.5654426022418255,
            "section_title": "RELATED WORK",
            "char_start_offset": 28107,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 202,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1353
                },
                {
                    "start": 1356,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1704
                },
                {
                    "start": 1707,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2097
                }
            ],
            "ref_mentions": [
                {
                    "start": 407,
                    "end": 410,
                    "matchedPaperCorpusId": "5833592"
                },
                {
                    "start": 410,
                    "end": 413,
                    "matchedPaperCorpusId": "189209"
                },
                {
                    "start": 481,
                    "end": 485,
                    "matchedPaperCorpusId": "36267198"
                },
                {
                    "start": 485,
                    "end": 488,
                    "matchedPaperCorpusId": "6459227"
                },
                {
                    "start": 590,
                    "end": 594,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 666,
                    "end": 669,
                    "matchedPaperCorpusId": "133195"
                },
                {
                    "start": 669,
                    "end": 672,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 704,
                    "end": 708,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 866,
                    "end": 869,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 869,
                    "end": 872,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1010,
                    "end": 1013,
                    "matchedPaperCorpusId": "5833592"
                },
                {
                    "start": 1013,
                    "end": 1016,
                    "matchedPaperCorpusId": "8294822"
                },
                {
                    "start": 1016,
                    "end": 1019,
                    "matchedPaperCorpusId": "8448956"
                },
                {
                    "start": 1019,
                    "end": 1022,
                    "matchedPaperCorpusId": "226262300"
                },
                {
                    "start": 1022,
                    "end": 1025,
                    "matchedPaperCorpusId": "226262229"
                },
                {
                    "start": 1204,
                    "end": 1208,
                    "matchedPaperCorpusId": "14336759"
                },
                {
                    "start": 1208,
                    "end": 1211,
                    "matchedPaperCorpusId": "9849366"
                },
                {
                    "start": 1308,
                    "end": 1312,
                    "matchedPaperCorpusId": "220633461"
                },
                {
                    "start": 1348,
                    "end": 1352,
                    "matchedPaperCorpusId": "226262300"
                },
                {
                    "start": 1682,
                    "end": 1686,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1953,
                    "end": 1957,
                    "matchedPaperCorpusId": "227231848"
                },
                {
                    "start": 1974,
                    "end": 1978,
                    "matchedPaperCorpusId": "14455932"
                },
                {
                    "start": 1978,
                    "end": 1981,
                    "matchedPaperCorpusId": "220730183"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.892578125
        },
        {
            "corpus_id": "259064158",
            "title": "Hybrid Long Document Summarization using C2F-FAR and ChatGPT: A Practical Study",
            "text": "Existing approaches for extractive summarization are mainly based on Transformer [31,32,33,34], graph [31,35,36], and reinforcement learning [35,37] models, as well as their combinations [31,35]. Although these models show promising results in summarizing relatively long documents (e.g., 3,000 to 8,000 words), they need to be trained and fine-tuned based on the given new datasets, which is time-consuming and hinders the direct application of these models in practice. For example, Zhang et al. [31] proposed HEGEL, which stands for HypErGraph transformer for Extractive Long document summarization, and used the so-called hyperedges to model high-order cross-sentence relations in different perspectives, such as section structure, latent topic, and keyword coreference. Since the proposed hyperedge can connect an arbitrary number of vertices, the model is able to effectively learn sentence embeddings and capture their interdependence using the designed hypergraph transformation layers. The test of HEGEL on two benchmarking datasets (arXiv and PubMed [38]) showed almost always the best results in terms of ROUGE [20] scores compared to other recent models. A more recent study by Bian et al. [35] proposed GoSum, which aims to train the agent's action based on its state in a reinforcement learning environment to evaluate and select sentences and produce an extractive summary. Specifically, the states of the sentences were encoded using graph neural networks (GNNs), which capture the hierarchical structure of the document via a heterogeneous graph that treats sentences and sections as nodes. This enables GoSum to intelligently select sentences and create summaries that take into account local, global, and historical sentence information. The performance of GoSum on the arXiv and PubMed datasets [38], based on the measurement of ROUGE [20] scores, showed the best results compared to the other state-of-the-art extractive and abstractive summarization models.",
            "score": 0.5647843398573653,
            "section_title": "Extractive Summarization",
            "char_start_offset": 6680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1979
                }
            ],
            "ref_mentions": [
                {
                    "start": 1122,
                    "end": 1126,
                    "matchedPaperCorpusId": "964287"
                },
                {
                    "start": 1855,
                    "end": 1859,
                    "matchedPaperCorpusId": "964287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94775390625
        },
        {
            "corpus_id": "250572619",
            "title": "Enhancing Biomedical Scientific Reviews Summarization with Graph-based Factual Evidence Extracted from Papers",
            "text": "We introduced EASUMM, an abstractive lay summarization model with a text-graph tandem architecture utilizing biomedical event graphs. Our work demonstrates the importance of event extraction for document summarization, allowing a model to better separate semantics and lexical surface. By achieving competitive results in terms of ROUGE and readability on CDSR, we observe a strong link between the summary quality and (i) a high number of events recognized in the source document, (ii) node features initialization via domain-specific pre-trained language models, (iii) the consideration of entity and event types. Despite being a popular solution characterized by reduced inference time, we show that graph-LSTMs struggle to compete with large pre-trained language models such as BART, suggesting the need for architectural improvements in future research. \n\nFuture Directions. Based on our findings, we recognize nine promising future research directions: \n\n1. use of large pre-trained encoder-decoder transformers to replace the most common graph-LSTMs architectures; 2. increase in the number and size of the events, with summarization datasets accompanied by event annotations (e.g., using metric learning techniques like in (Moro and Valgimigli, 2021)); 3. end-to-end event extraction and document summarization; 4. discovering of new connections between nodes useful for increasing summarization performance (i.e., dynamic event graph construction), with techniques such as random perturbation (Domeniconi et al., 2014a) and iterative deep graph learning (Chen et al., 2020); 5. node relevance scoring supported by term weighting (Domeniconi et al., 2015) and/or perplexity metrics (Yasunaga et al., 2021); 6. transfer-learning methods (Domeniconi et al., 2014b;Moro et al., 2018) across multiple biomedical fields; 7. exploitation of continuous edge features within the graph neural network; \n\n8. additional loss functions based on reinforcement learning and semantic-driven rewards; 9. interaction and mutual influence between graph and text encoders.",
            "score": 0.5645589745063547,
            "section_title": "CONCLUSION",
            "char_start_offset": 24097,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 858
                },
                {
                    "start": 861,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 958
                },
                {
                    "start": 961,
                    "end": 1900
                },
                {
                    "start": 1903,
                    "end": 2061
                }
            ],
            "ref_mentions": [
                {
                    "start": 1231,
                    "end": 1257,
                    "matchedPaperCorpusId": "238639683"
                },
                {
                    "start": 1563,
                    "end": 1582,
                    "matchedPaperCorpusId": "214003631"
                },
                {
                    "start": 1638,
                    "end": 1663,
                    "matchedPaperCorpusId": "51873494"
                },
                {
                    "start": 1770,
                    "end": 1788,
                    "matchedPaperCorpusId": "56482680"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94580078125
        },
        {
            "corpus_id": "250012667",
            "title": "GATSum: Graph-Based Topic-Aware Abstract Text Summarization",
            "text": "This attracted our attention, and GNN (Graph Neural Networks) was considered to solve this problem. Lately, GNN has been universally used for cross-sentence relation modeling for summarization tasks [15]. Several studies [35,37] set up document graphs according to discourse analysis. But, this method relies on external tools, which possibly bring out semantically fragmented outputs [20]. To sum up, GAT (Graph Attention Network) is constructed using sentence context representation and topic information, and the docu-ment context vector and topic information are updated simultaneously. This can not only jointly update local semantics and global semantics, but also reduce the problem of semantic fragmentation. \n\nIn this work, we suggest a novel GTASum model (Graph-Based Topic-Aware abstract text summarization). First, the document is encoded with BERT [6] (Bidirectional Encoder Representation from Transformers) to obtain contextual sentence representations; meanwhile, NTM [7] is used to learn the potential topic of the document. Second, construct a heterogeneous document graph that contains sentence representation nodes and potential topic nodes, then revise them using a adapted GAT [30]. Finally, the sentence representations containing topic information are fed into a Transformer-based decoder to generate summaries. Overall, the primary contributions of us are as follows: \n\n1 A novel Graph-Based Topic-Aware abstract text summarization model is proposed, which helps to capture global semantic information and provides guidance in the procedure of making a summary. This solves the problem of text summarization lacking global semantic information. \n\n2 A heterogeneous document graph is designed to jointly update local semantics and global semantics and reduce the problem of semantic fragmentation. \n\n3 GTASum is evaluated on two standard datasets (CNN/DailyMail and XSum) and outperforms many existing extractive and abstract models in terms of ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measure.",
            "score": 0.5642473185802919,
            "section_title": "Introduction",
            "char_start_offset": 2280,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 716
                },
                {
                    "start": 719,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1392
                },
                {
                    "start": 1395,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1669
                },
                {
                    "start": 1672,
                    "end": 1821
                },
                {
                    "start": 1824,
                    "end": 2035
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 203,
                    "matchedPaperCorpusId": "102354588"
                },
                {
                    "start": 221,
                    "end": 225,
                    "matchedPaperCorpusId": "59599804"
                },
                {
                    "start": 385,
                    "end": 389,
                    "matchedPaperCorpusId": "208331728"
                },
                {
                    "start": 861,
                    "end": 864,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 984,
                    "end": 987,
                    "matchedPaperCorpusId": "52182945"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95703125
        },
        {
            "corpus_id": "247446769",
            "title": "Pruned Graph Neural Network for Short Story Ordering",
            "text": "Graph neural networks (GNN) have shown to be effective in NLP applications, including syntactic dependency trees (Marcheggiani and Titov, 2017), neural machine translation (Beck et al., 2018), knowledge graphs (Wang et al., 2018), semantic graphs (Song et al., 2018), sequence-to-graph learning (Johnson, 2017), graph-to-sequence learning (Beck et al., 2018), sentence ordering (Yin et al., 2019), and multi-document summarization (Christensen et al., 2013;Yasunaga et al., 2017). \n\nIn particular, text classification is a common application of GNNs in natural language processing. A GNN infers document labels based on the relationships among documents or words (Hamilton et al., 2017). Christensen et al. (2013) used a GNN in multi-document summarization. They create multi-document graphs which determine pairwise ordering constraints of sentences based on the discourse relationship between them. Kipf and Welling (2017) proposed Graph Convolutional Networks (GCN), which is used in Yasunaga et al. (2017) to generate sentence relation graphs. The final sentence embeddings indicate the graph representation and are utilized as inputs to achieve satisfactory results on multi-document summarization. \n\nAnother method is presented in Marcheggiani and Titov (2017) where a syntactic GCN is developed with a CNN/RNN as sentence encoder. The GCN indicates syntactic relations between words in a sentence. In a more recent work, Yin et al. (2019) proposed a graph-based neural network for sentence ordering, in which paragraphs are modeled as graphs where sentences and entities are the nodes. The method showed improvement in evaluation metrics for sentence ordering task. In this work, we explore the use of GRN for NLP tasks, es-pecially to perform sentence-ordering on a corpus of short stories.",
            "score": 0.5642473185802919,
            "section_title": "Graph Neural Networks in NLP",
            "char_start_offset": 7547,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 480
                },
                {
                    "start": 483,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1203
                },
                {
                    "start": 1206,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1798
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 143,
                    "matchedPaperCorpusId": "16839291"
                },
                {
                    "start": 172,
                    "end": 191,
                    "matchedPaperCorpusId": "49430686"
                },
                {
                    "start": 210,
                    "end": 229,
                    "matchedPaperCorpusId": "53082628"
                },
                {
                    "start": 247,
                    "end": 266,
                    "matchedPaperCorpusId": "52115592"
                },
                {
                    "start": 295,
                    "end": 310,
                    "matchedPaperCorpusId": "29504454"
                },
                {
                    "start": 339,
                    "end": 358,
                    "matchedPaperCorpusId": "49430686"
                },
                {
                    "start": 378,
                    "end": 396,
                    "matchedPaperCorpusId": "199466313"
                },
                {
                    "start": 457,
                    "end": 479,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 663,
                    "end": 686,
                    "matchedPaperCorpusId": "4755450"
                },
                {
                    "start": 987,
                    "end": 1009,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 1237,
                    "end": 1266,
                    "matchedPaperCorpusId": "16839291"
                },
                {
                    "start": 1428,
                    "end": 1445,
                    "matchedPaperCorpusId": "199466313"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97265625
        },
        {
            "corpus_id": "258999996",
            "title": "Contrastive Hierarchical Discourse Graph for Scientific Document Summarization",
            "text": "Graph models have been widely applied to extractive summarization due to the capability of modeling cross-sentence relations within a document. The sparsity nature of graph structure also brings scalability and flexibility, making it a good fit for long documents. Graph neural networks' memory costs are generally linear with regard to the input size compared to the quadratic self-attention mechanism. \n\nResearchers have explored supervised graph neural network methods for summarization (Cui and Hu, 2021;Jia et al., 2020;Huang and Kurohashi, 2021;Xie et al., 2022;Phan et al., 2022). Yasunaga et al. (2017) first proposed to use Graph Convolutional Network (GCN) on the approximate discourse graph. Xu et al. (2019) then applied GCN on structural discourse graphs based on RST trees and coreference mentions. Recently, Wang et al. (2020) proposed constructing a word-document heterogeneous graph by using words as the intermediary Figure 1: The overall model architecture of CHANGES. We first construct a hierarchical graph for an input document, and then learn representations with a graph contrastive module and hierarchical graph layers. The concatenation representations of sentence node and its section node will be fused for summary sentence selection. between sentences. Zhang et al. (2022a) proposed to use hypergraph to capture the high-order sentence relations within the document. Our paper follows the series of work but incorporates hierarchical graphs for scientific paper discourse structure modeling and graph contrastive learning for theme-aware sentence representation learning.",
            "score": 0.5642473185802919,
            "section_title": "Graph-based Summarization",
            "char_start_offset": 6565,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 403
                },
                {
                    "start": 406,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1600
                }
            ],
            "ref_mentions": [
                {
                    "start": 508,
                    "end": 525,
                    "matchedPaperCorpusId": "226262332"
                },
                {
                    "start": 525,
                    "end": 551,
                    "matchedPaperCorpusId": "233189611"
                },
                {
                    "start": 568,
                    "end": 586,
                    "matchedPaperCorpusId": "252818946"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9736328125
        },
        {
            "corpus_id": "250301954",
            "title": "Hierarchical Heterogeneous Graph Attention Network for Syntax-Aware Summarization",
            "text": "We evaluate the quality of the summarization based on ROUGE (Lin 2004). We report unigram and bigram overlap (ROUGE-1 and ROUGE-2) between generated or extracted summaries and gold summaries to assess informativeness. The longest common subsequence (ROUGE-L) on sentencelevel is reported for evaluating fluency. \n\nEvaluation w/o pre-training In our original model, no pre-trained models are involved so we remove any pretrained models in the baseline models for fair comparison in this section. We show the results of our proposed model against recently released summarization models on three news datasets in Table 3. We classify all the baselines into two groups: non-graph-based models and graph-based models. From the results, we can see that graph-based methods generally show comparable performance with the nongraph-based ones when GNN models are incorporated, as the case for Gated GNN and SemSUM. Furthermore, Synap-Sum outperforms the existing popular graph-based models and the listed non-graph-based models without pre-training in abstractive and extractive settings. SynapSum achieves the improvement by converting each sentence into a hierarchical graph with finer granularity and generating representations for words, constituents, and sentences simultaneously with rich syntactic and semantic meanings. However, the graphs used in other models tend only to capture the relationship between words and sentences, which leads to the loss of semantic meanings of the phrases as a whole. Therefore, our model can better mimic the human way of conducting summarization hierarchically, from words to phrases to sentences and documents, finally. \n\nEvaluation w/ pre-training The most recent summarization models are typically instantiated with some contextualized pre-trained models like BERT (Devlin et al. 2019). Consequently, it is essential to demonstrate the potential performance improvement of our proposed model with the help of pre-trained models. We use two popular recent pre-trained models: BART (Lewis et al. 2020) and MatchSum (Zhong et al. 2020) for abstractive and extensive summarization tasks respectively. More specifically, we firstly pre-train the model, following the setting used in PEGASUS (Zhang et al. 2020).",
            "score": 0.5637849684746766,
            "section_title": "Automatic Evaluation",
            "char_start_offset": 18075,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 311
                },
                {
                    "start": 314,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1653
                },
                {
                    "start": 1656,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2242
                }
            ],
            "ref_mentions": [
                {
                    "start": 60,
                    "end": 69,
                    "matchedPaperCorpusId": "964287"
                },
                {
                    "start": 1801,
                    "end": 1820,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 2049,
                    "end": 2068,
                    "matchedPaperCorpusId": "215828313"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.890625
        },
        {
            "corpus_id": "268444581",
            "title": "End to End Urdu Abstractive Text Summarization With Dataset and Improvement in Evaluation Metric",
            "text": "In addition, comprehensive comparisons with several state-of-the-art unsupervised summarizing algorithms demonstrate the effectiveness of our RNNLMbased techniques. This technique is unsupervised and their results are not significant as compared to supervised methods. While Nallapati et al. proposed neural encoder-decoder and attention models for abstractive text summarization [41]. As input, they employ words, morphemes, and embeddings. Bidirectional RNNs and LSTMs are employed. Their work builds on the framework established but adds new models that solve fundamental issues in abstractive summarization. The fundamental model is the encoder-decoder model with attention and a broad vocabulary trick. A featurerich encoder is used to handle the difficulty of capturing essential concepts in a topic. A switching generator/pointer system is modeled to cope with Out-of-vocabulary words. In the case of extensive texts, significant sentences must be recorded in addition to keywords. Hierarchical attention is used to accomplish this. English data sets were used for this architecture. According to See et al., existing abstractive summarization algorithms, according to experts, have two flaws: they present factual facts incorrectly, and they occasionally create repetitious output [109]. An abstractive summarization architecture is proposed in this paper, which combines a hybrid pointer-generator and coverage with the classic attention-based seq2seq approach. The hybrid pointergenerator may copy words from the input text by pointing, resulting in accurate information replication while allowing the generator to produce new words. Although this gives precise data, it is still repetitious. Using a coverage method that retains a record of material that has already been summarized eliminates repetition. This work is also done for the English language. And transformers enhanced the results of this architecture. In [110], Harvard researchers extended the [109] by adding the attention techniques in bottom-up way. The content selector here predetermines the terms included in the summary and then uses this content to limit the neural model to generate an abstractive summary. This model performs better on the CNN/Daily Mail and New York Times corpora.",
            "score": 0.5636257436098152,
            "section_title": "II. LITERATURE REVIEW",
            "char_start_offset": 14804,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2190
                },
                {
                    "start": 2191,
                    "end": 2267
                }
            ],
            "ref_mentions": [
                {
                    "start": 380,
                    "end": 384,
                    "matchedPaperCorpusId": "8928715"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.783203125
        },
        {
            "corpus_id": "250572619",
            "title": "Enhancing Biomedical Scientific Reviews Summarization with Graph-based Factual Evidence Extracted from Papers",
            "text": "We propose EASUMM, the first model employing event extraction for abstractive single-document summarization, using a tandem architecture to integrate traditional text encoders with graph representations learned by a graph neural network (GNN). By experimenting on the CDSR dataset (Guo et al., 2021), we demonstrate biomedical event extraction graphs can indeed help the model to preserve the essential global context and keep the connection between the most relevant entities, thus generating a higher quality summary (Figure 1). \n\nThe rest of the paper is organized as follows. First, in Section 2, we examine related work. Then, Section 3 describes our event-based strategy for deriving semantic graphs from text. Next, Section 4 details our model, from the architecture to the training process. \n\nSection 5 presents our experimental setup, while Section 6 exhibits the results obtained. Finally, Section 7 closes the discussion and points out future directions.",
            "score": 0.5631789850064325,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3526,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 530
                },
                {
                    "start": 533,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 798
                },
                {
                    "start": 801,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 965
                }
            ],
            "ref_mentions": [
                {
                    "start": 281,
                    "end": 299,
                    "matchedPaperCorpusId": "229363446"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9375
        },
        {
            "corpus_id": "265607988",
            "title": "Generating Extractive and Abstractive Summaries in Parallel from Scientific Articles Incorporating Citing Statements",
            "text": "Summarization of scientific articles often overlooks insights from citing papers, focusing solely on the document\u2019s content. To incorporate citation contexts, we develop a model to summarize a scientific document using the information in the source and citing documents. It concurrently generates abstractive and extractive summaries, each enhancing the other. The extractive summarizer utilizes a blend of heterogeneous graph-based neural networks and graph attention networks, while the abstractive summarizer employs an autoregressive decoder. These modules exchange control signals through the loss function, ensuring the creation of high-quality summaries in both styles.",
            "score": 0.5615562408443329,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8994140625
        },
        {
            "corpus_id": "265607988",
            "title": "Generating Extractive and Abstractive Summaries in Parallel from Scientific Articles Incorporating Citing Statements",
            "text": "(Bergsma, 2005) For the gender task that we study in our experiments, we acquire class instances by filtering the dataset of nouns and their genders created by Bergsma and Lin (2006). (Bergsma and Van Durme, 2013) Jaidka et al. (Jaidka et al., 2016(Jaidka et al., , 2019) ) have identified this missing aspect in scientific document summarization and addressed it by introducing a shared task. This task aims to create summaries that take into account not only the information in the body of the documents but also the research community's overview of the documents over time. The work described here continues in this direction. \n\nWith the advancement of neural networks, there have been a few prominent research works in recent years for generating extractive (Yasunaga et al., 2019) and abstractive (Yu et al., 2020;Zhang et al., 2019) summaries from scientific documents (Cohan et al., 2018;Zhang et al., 2022). Extractive summarization recognizes key sentences from the source document as the summary but lack the flow of information, whereas the abstractive summarization technique generates new phrases using language models while preserving the semantics of the input document but may miss some important aspects of the text. This is a motivation for designing a model to generate both summaries in parallel and help the counterpart to achieve a performance boost with additional guidance. \n\nA key step in extracting brief synopsis sentences from a manuscript is to map the cross-sentence correlations. A lot of recent prominent works (Nallapati et al., 2017) have tried to do so using recurrent neural networks (RNNs). However, because of using RNNs, these models fail to capture longdistance sentence-level dependencies. Another approach to preserve sentence-level dependencies from long documents is using graph-based neural networks. A few recent works (e.g., (Cohan and Goharian, 2018;Yasunaga et al., 2017)) have utilized discourse information in the article along with inter-sentence correlations for constructing graphs and summarizing document. Another approach is to construct a sentence-level fully connected graph.",
            "score": 0.5605727840152064,
            "section_title": "Introduction",
            "char_start_offset": 2079,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 629
                },
                {
                    "start": 632,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1397
                },
                {
                    "start": 1400,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2134
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 213,
                    "matchedPaperCorpusId": "5132275"
                },
                {
                    "start": 228,
                    "end": 248,
                    "matchedPaperCorpusId": "5668000"
                },
                {
                    "start": 762,
                    "end": 785,
                    "matchedPaperCorpusId": "58053521"
                },
                {
                    "start": 819,
                    "end": 838,
                    "matchedPaperCorpusId": "155100086"
                },
                {
                    "start": 1543,
                    "end": 1567,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 1898,
                    "end": 1920,
                    "matchedPaperCorpusId": "6532096"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.828125
        },
        {
            "corpus_id": "266176619",
            "title": "Towards a Unified Framework for Reference Retrieval and Related Work Generation",
            "text": "We mainly compare our UR 3 WG with three types of baselines: multi-document summarization methods, related work generation methods, and large language models. The multi-document summarization baselines include: LEAD, which concatenates the first sentence of each reference; LexRank (Erkan and Radev, 2004), which extract summarization based on the graph representation of sentences; NES (Wang et al., 2019), a extractive method which measures the relevance via bibliography graph; MGSum (Jin et al., 2020), an abstractive method based on multigranularity interaction network; BertSum (Liu and Lapata, 2019), a summarization system built on a pre-trained BERT (Devlin et al., 2019); EMS (Zhou et al., 2021), which augments the encoder-decoder framework with a heterogeneous graph. \n\nThe related work generation methods include: RRG (Chen et al., 2021) , which enhanced by a iteratively refined relation-aware graph between references; TAG (Chen et al., 2022a), a transformerbased model with a target-aware graph encoder. \n\nThe large language models (LLMs) include flan-t5 (Chung et al., 2022), Llama-7B (Touvron et al., 2023), ChatGLM (Du et al., 2022), Vicuna5 , claude6 , Davinci-text-003, and ChatGPT7 , where the instruction I GEN is taken as input. \n\nTable 5: Results on the TAS2 and TAD dataset. We abbreviate ROUGE as R We underline the best results in each category of baseline methods. w/ T denotes the method which uses both references and abstract as input. All ROUGE scores have a 95% confidence interval as reported by the official R script.",
            "score": 0.5590390812683206,
            "section_title": "A.3 Details for baselines",
            "char_start_offset": 29667,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 779
                },
                {
                    "start": 782,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1252
                },
                {
                    "start": 1255,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1553
                }
            ],
            "ref_mentions": [
                {
                    "start": 282,
                    "end": 305,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 387,
                    "end": 406,
                    "matchedPaperCorpusId": "53083244"
                },
                {
                    "start": 487,
                    "end": 505,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 584,
                    "end": 606,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 659,
                    "end": 680,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 686,
                    "end": 705,
                    "matchedPaperCorpusId": "236478143"
                },
                {
                    "start": 831,
                    "end": 850,
                    "matchedPaperCorpusId": "236460044"
                },
                {
                    "start": 938,
                    "end": 958,
                    "matchedPaperCorpusId": "249097545"
                },
                {
                    "start": 1134,
                    "end": 1151,
                    "matchedPaperCorpusId": "247519241"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7978515625
        },
        {
            "corpus_id": "252780923",
            "title": "HEGEL: Hypergraph Transformer for Long Document Summarization",
            "text": "Extractive summarization aims to generate a shorter version of a document while preserving the most salient information by directly extracting relevant sentences from the original document. With recent advances in neural networks and large pretrained language models (Devlin et al., 2018;Lewis et al., 2019), researchers have achieved promising results in news summarization (around 650 words/document) (Nallapati et al., 2016a;Cheng and Lapata, 2016;See et al., 2017;Zhang et al., 2022;Narayan et al., 2018;Liu and Lapata, 2019). However, these models struggle when applied to long documents like scientific papers. The input length of a scientific paper can range from 2000 to 7, 000 words, and the expected summary (abstract) is more than 200 words compared to 40 words in news headlines.\n\nScientific paper extractive summarization is highly challenging due to the long structured input. The extended context hinders sequential models like RNN from capturing sentence-level longdistance dependency and cross-sentence relations, Figure 1: An illustration of modeling cross-sentence relations from section structure, latent topic, and keyword coreference perspectives. which are essential for extractive summarization. In addition, the quadratic computation complexity of attention with respect to the input tokens length makes Transformer (Vaswani et al., 2017) based models not applicable. Moreover, long documents typically cover diverse topics and have richer structural information than short news, which is difficult for sequential models to capture.\n\nAs a result, researchers have turned to graph neural network (GNN) approaches to model crosssentence relations. They generally represent a document with a sentence-level graph and turn extractive summarization into a node classification problem. These work construct graph from document in different manners, such as inter-sentence cosine similarity graph in (Erkan and Radev, 2004;Dong et al., 2020), Rhetorical Structure Theory (RST) tree relation graph in (Xu et al., 2019), approximate discourse graph in (Yasunaga et al., 2017), topicsentence graph in (Cui and Hu, 2021) and worddocument heterogeneous graph in (Wang et al., 2020). However, the usability of",
            "score": 0.555676005587228,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 468,
                    "end": 487,
                    "matchedPaperCorpusId": "250311140"
                },
                {
                    "start": 1341,
                    "end": 1363,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1918,
                    "end": 1941,
                    "matchedPaperCorpusId": "506350"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8662109375
        },
        {
            "corpus_id": "271903777",
            "title": "GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization",
            "text": "Pre-trained language models are increasingly being used in multi-document summarization tasks. However, these models need large-scale corpora for pre-training and are domain-dependent. Other non-neural unsupervised summarization approaches mostly rely on key sentence extraction, which can lead to information loss. To address these challenges, we propose a lightweight yet effective unsupervised approach called GLIMMER: a Graph and LexIcal features based unsupervised Multi-docuMEnt summaRization approach. It first constructs a sentence graph from the source documents, then automatically identifies semantic clusters by mining low-level features from raw texts, thereby improving intra-cluster correlation and the fluency of generated sentences. Finally, it summarizes clusters into natural sentences. Experiments conducted on Multi-News, Multi-XScience and DUC-2004 demonstrate that our approach outperforms existing unsupervised approaches. Furthermore, it surpasses state-of-the-art pre-trained multi-document summarization models (e.g. PEGASUS and PRIMERA) under zero-shot settings in terms of ROUGE scores. Additionally, human evaluations indicate that summaries generated by GLIMMER achieve high readability and informativeness scores. Our code is available at https://github.com/Oswald1997/GLIMMER.",
            "score": 0.555562007542781,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71240234375
        },
        {
            "corpus_id": "247237482",
            "title": "Follow the Timeline! Generating Abstractive and Extractive Timeline Summary in Chronological Order",
            "text": "Despite the focus on abstractive summarization, extractive summarization remains an attractive method. In extractive summarization, Kobayashi et al. [30] propose a summarization method using document-level similarity based on word embeddings. Meanwhile, Filippova et al. [20] use an RNN to delete words in a document for sentence compression. Yan \n\nand Wan [76] propose more meaningful and informative units named frequent deep dependency sub-structure and a topic-sensitive multi-task learning model for multi-doc summarization. Cheng and Lapata [13] propose a general framework for single-document text summarization using a hierarchical article encoder composed with an attentionbased extractor. Following this, Nallapati et al. [47] propose a simple RNN-based sequence classifier that outperforms or matches the state-of-art models at the time. Chen et al. [11] introduce a model which iteratively polishes the document representation on many passes through the document, so as to produce a better summary. In another approach, Narayan et al. [49] use a reinforcement learning method to optimize the ROUGE evaluation metric for text summarization. Ren et al. [52] study the use of sentence relations, e.g., contextual sentence relations, title sentence relations, and query sentence relations, so as to improve the performance of extractive summarization. \n\nManuscript submitted to ACM Recently, pre-trained language models are also applied in summarization for contextual word representations [39,83]. \n\nAnother intuitive structure for extractive summarization is the graph, which can better utilize the statistical or linguistic information between sentences. Early works focus on document graphs constructed with the content similarity among sentences, like LexRank [19] and TextRank [43]. Some recent works aim to incorporate a relational prior into the encoder by graph neural networks (GNNs) [78].",
            "score": 0.5550024647046525,
            "section_title": "Extractive Summarization",
            "char_start_offset": 13510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 346
                },
                {
                    "start": 349,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1359
                },
                {
                    "start": 1362,
                    "end": 1506
                },
                {
                    "start": 1509,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1907
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 153,
                    "matchedPaperCorpusId": "15448388"
                },
                {
                    "start": 271,
                    "end": 275,
                    "matchedPaperCorpusId": "1992250"
                },
                {
                    "start": 357,
                    "end": 361,
                    "matchedPaperCorpusId": "1786930"
                },
                {
                    "start": 732,
                    "end": 736,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 861,
                    "end": 865,
                    "matchedPaperCorpusId": "52879916"
                },
                {
                    "start": 1047,
                    "end": 1051,
                    "matchedPaperCorpusId": "3510042"
                },
                {
                    "start": 1163,
                    "end": 1167,
                    "matchedPaperCorpusId": "14001298"
                },
                {
                    "start": 1773,
                    "end": 1777,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1791,
                    "end": 1795,
                    "matchedPaperCorpusId": "577937"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89453125
        },
        {
            "corpus_id": "271938110",
            "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
            "text": "Our research includes three key areas: 1. A review of fundamental methods used for abstractive summarization. \n\n2. An examination of topic-aware models that combine topic modeling with other techniques to improve summarization. \n\n3. An exploration of how neural graph-based networks are used for both extractive and abstractive summarization tasks.",
            "score": 0.5528985147593463,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 7446,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 42,
                    "end": 109
                },
                {
                    "start": 112,
                    "end": 227
                },
                {
                    "start": 230,
                    "end": 348
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97119140625
        },
        {
            "corpus_id": "271938110",
            "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
            "text": "In recent years, there have been notable achievements in applying Graph Neural Networks (GNNs) or Graph Attention Networks (GAT) [11] to summarize documents [4], [12], [13], [14], [15], [16], [17]. This success arises from their ability to capture complex inter-sentence relationships within documents. Specifically, GNN models demonstrate proficiency in representing complex structural data by encapsulating semantic units (nodes) and their interconnections (edges). Due to these capabilities, there is growing interest in leveraging the GAT framework alongside topic modeling for abstractive summarization tasks. The researchers in [12] proposed a heterogeneous neural graph structure using BERT to obtain contextual sentence representations. They simultaneously trained with latent topics using the Neural Topic Model (NTM) to model global information. The study of [18] proposed an expanded model that employed NTM for abstractive text summarization. \n\nThe authors of [15] introduced a graph structure enriched with latent topical information. They employed K-Means and Gaussian Mixture Models (GMM) for topic extraction. The study of [3] proposed an innovative Graph-Based Topic-Aware abstractive text summarization model. Initially, documents were encoded with BERT to generate sentence representations. Concurrently, NTM identified potential topics, while GAT refined these sentences and topic nodes. Finally, the sentence embeddings, enriched with topic information, were fed into a Transformer-driven decoder to generate summaries. \n\nAnother study by [19] introduced a heterogeneous graph that incorporated topical information for abstractive summarization of Chinese complaint reports. To achieve this, GAT was formulated using sentence context embeddings and latent topics. Both the document context embeddings and topic data were updated concurrently, thereby addressing the issue of semantic fragmentation [3]. \n\nDespite the effectiveness of existing methods, there remains a need for a comprehensive, topic-aware neural graph model that integrates global semantic information and is coupled with an advanced decoder designed specifically for abstractive summarization of medical scientific documents. \n\nThis study introduces an innovative Topic-aware Heterogenous Graph Neural Network (HGNN) model aimed at summarizing medical documents.",
            "score": 0.5528985147593463,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 954
                },
                {
                    "start": 957,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1540
                },
                {
                    "start": 1543,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1923
                },
                {
                    "start": 1926,
                    "end": 2214
                },
                {
                    "start": 2217,
                    "end": 2351
                }
            ],
            "ref_mentions": [
                {
                    "start": 168,
                    "end": 172,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 174,
                    "end": 178,
                    "matchedPaperCorpusId": "252818946"
                },
                {
                    "start": 180,
                    "end": 184,
                    "matchedPaperCorpusId": "252670538"
                },
                {
                    "start": 186,
                    "end": 190,
                    "matchedPaperCorpusId": "250301954"
                },
                {
                    "start": 192,
                    "end": 196,
                    "matchedPaperCorpusId": "266285531"
                },
                {
                    "start": 972,
                    "end": 976,
                    "matchedPaperCorpusId": "252670538"
                },
                {
                    "start": 1139,
                    "end": 1142,
                    "matchedPaperCorpusId": "250012667"
                },
                {
                    "start": 1560,
                    "end": 1564,
                    "matchedPaperCorpusId": "264064249"
                },
                {
                    "start": 1919,
                    "end": 1922,
                    "matchedPaperCorpusId": "250012667"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9892578125
        },
        {
            "corpus_id": "259138479",
            "title": "Modeling Structural Similarities between Documents for Coherence Assessment with Graph Convolutional Networks",
            "text": "Those models learn a document's representation from word embeddings or pre-trained language models, giving significantly better performance than previous statistical methods. However, one drawback of existing neural-based methods is that they solely focus on extracting features within a single document, ignoring the underlying correlations between documents. Coherence describes how sentences of a text connect to each other (Reinhart, 1980;Foltz et al., 1998;Schwarz, 2001). Theoretically, documents with similar connection structures should tend to have similar degrees of coherence, which can be useful prior knowledge for coherence modeling. For example, a model is more likely to accurately assess a new document's coherence if it can refer to the labels of known documents with a similar organizational structure (see Appendix E.1 for an example). \n\nTo fill this gap, we investigate a graph-based approach to model the correlation between docu-ments from the perspective of structural similarity. The main idea is to connect structurally similar documents through a graph and capture those connectivity relationships using Graph Convolutional Networks (GCN) (Kipf and Welling, 2017). In particular, inspired by Guinaudeau and Strube (2013), we first represent a document as a sentence graph, where nodes are sentences and two nodes will be connected if they contain semantically related nouns. Our method further converts each sentence graph into a subgraph set as it proves to be an efficient approach for measuring the topological similarity between graphs (Shervashidze et al., 2009;Kondor et al., 2009). Then, we construct a heterogeneous graph for the training corpus, containing document and subgraph nodes, based on subgraphs shared between documents. In this way, structurally-similar documents are explicitly linked through the subgraphs (shown in Figure 1). Finally, a GCN is applied to the heterogeneous graph to learn the representation of document nodes while considering the connections between them. \n\nWe evaluate our method on two benchmark tasks1 : assessing discourse coherence and automatic essay scoring. Experimental results show that our method significantly outperforms a baseline model that does not consider structural similarities between documents, achieving a new state-ofthe-art performance on both tasks.",
            "score": 0.552002251087319,
            "section_title": "Introduction",
            "char_start_offset": 1687,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2022
                },
                {
                    "start": 2025,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2342
                }
            ],
            "ref_mentions": [
                {
                    "start": 427,
                    "end": 443,
                    "matchedPaperCorpusId": "170424753"
                },
                {
                    "start": 443,
                    "end": 462,
                    "matchedPaperCorpusId": "62729021"
                },
                {
                    "start": 1219,
                    "end": 1247,
                    "matchedPaperCorpusId": "1851389"
                },
                {
                    "start": 1567,
                    "end": 1594,
                    "matchedPaperCorpusId": "17557614"
                },
                {
                    "start": 1594,
                    "end": 1614,
                    "matchedPaperCorpusId": "8479149"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78369140625
        },
        {
            "corpus_id": "258060196",
            "title": "LBMT team at VLSP2022-Abmusu: Hybrid method with text correlation and generative models for Vietnamese multi-document summarization",
            "text": "Multi-document summarization is an arduous problem due to some reasons including high redundancy as well as contradictory information inside the doc-arXiv:2304.05205v1 [cs.CL] 11 Apr 2023 uments, sophisticated relation between documents, diverse input document types, etc. For tackling this problem, the model often is separated into two phases: the extractive phase for reducing redundancies and collecting important information, abstractive phase for generating natural-language summarise. In (Yasunaga et al., 2017), the authors built a sentence-based graph where each node is a sentence and each edge is a relation between two sentences. Then they used a GRU model for extracting vector representation for each sentence and fed the graph and the representation vector to a GCN for aggregating the final representation vector. After the sentence embedding stage, a secondlevel GRU is employed for extracting the entire cluster's vector representation which will be employed with a sentence-level representation vector for estimating the salience of each sentence. For paying attention to the relation between words, sentences, and documents, the work in (Wang et al., 2020) has proposed HeterDoc-SUM Graph which represents each of these elements (word, sentence, and document) by a node. The sentences node and documents node are connected by the containing words node, which means word nodes have a role as the bridge between sentence and document node and enhance the relations that exist in the data including sentence-sentence, sentence-document, and document-document relation. The built graph is fed into a graph attention network. \n\nDue to the success of transformer-based models in recent years, this class of model that utilizes the attention-based mechanism has been employed for getting more abstract information about the context. The authors of (Jin et al., 2020) proposed a multigranularity interaction network MGSUM based on the Transformer model for capturing the semantic relationships. Three levels of granularity include Word, Sentence, and Document. The sentence level is used for extractive summarization and the word level is utilized for abstractive summarising tasks.",
            "score": 0.5514985297602023,
            "section_title": "Related Work",
            "char_start_offset": 2573,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1640
                },
                {
                    "start": 1643,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2072
                },
                {
                    "start": 2073,
                    "end": 2194
                }
            ],
            "ref_mentions": [
                {
                    "start": 495,
                    "end": 518,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 1157,
                    "end": 1176,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 1861,
                    "end": 1879,
                    "matchedPaperCorpusId": "220045815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89990234375
        },
        {
            "corpus_id": "277284940",
            "title": "Global and Local Context Fusion in Heterogeneous Graph Neural Network for Summarizing Lengthy Scientific Documents",
            "text": "Long document summarization has recently gained popularity due to the increasing complexity of modeling large documents [30]. Currently, the most common approach to addressing this issue involves truncating documents into smaller, fixed-length sequences and using sliding window techniques to process the document incrementally [31]. However, this approach results in significant information loss, making it unsuitable for the Extractive Document Summarization (EDS) task. EDS requires an understanding of the informational relationships among the extracted sentences [32]. In response to this challenge, several promising approaches have been developed specifically for long document summarization. \n\nThe study conducted by [33] proposed a hierarchical encoder designed to extract the syntactic structure of the input document, coupled with a discourse aware decoding module for abstractive summarization. The research team in [13] advances the task of long text summarization by using distributed representations to capture the global context of the entire document and the local context of specific sections or topics. In the domain of graph-based approaches, [3] implement a modified version of the GAT designed to identify inter-sentence relationships. To enhance the summarization of lengthy documents, they implemented Neural Topic Modeling (NTM) to identify latent topics. This approach introduced NTM as a new node type, which was then integrated with existing sentence nodes to improve the overall structure and accuracy of the summarization process. \n\nThe researchers of [7] integrated passage nodes into the heterogeneous graph structure alongside the existing word and sentence nodes. In the study [34], the authors attempted to redefine the task of review formalization, focusing on scientific paper summarization using citation graphs. The CGSUM model included information from both the source paper and its references, and enhanced the contextual relevance of summarization. The authors also built the Semantic Scholar Network (SSN) from 141K research papers and 661K citation relations with a broad view of scientific domains. The researchers of [35] explored the effect of pre-training on a BERT-based extractive summarization system created specifically for scientific documents. They gained significant performance improvements by adding an intermediate pre-training phase that makes use of pre-existing summarization datasets.",
            "score": 0.5495629685562042,
            "section_title": "C. LONG DOCUMENT EXTRACTIVE SUMMARIZATION",
            "char_start_offset": 14209,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 699
                },
                {
                    "start": 702,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1560
                },
                {
                    "start": 1563,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2143
                },
                {
                    "start": 2144,
                    "end": 2298
                },
                {
                    "start": 2299,
                    "end": 2447
                }
            ],
            "ref_mentions": [
                {
                    "start": 120,
                    "end": 124,
                    "matchedPaperCorpusId": "196184038"
                },
                {
                    "start": 328,
                    "end": 332,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1582,
                    "end": 1585,
                    "matchedPaperCorpusId": "252818946"
                },
                {
                    "start": 1711,
                    "end": 1715,
                    "matchedPaperCorpusId": "233169121"
                },
                {
                    "start": 2163,
                    "end": 2167,
                    "matchedPaperCorpusId": "235097196"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9658203125
        },
        {
            "corpus_id": "248780330",
            "title": "Discriminative Marginalized Probabilistic Neural Method for Multi-Document Summarization of Medical Literature",
            "text": "The task of multi-document summarization aims to generate a compact and informative summary from a cluster of topic-related documents, which represents a very challenging natural language processing (NLP) application due to the presence of redundant and sometimes conflicting information among documents (Radev, 2000). In the medical domain, in which machine learning plays an increasingly significant role (Domeniconi et al., 2014a;di Lena et al., 2015), multi-document summarization finds application in the generation of systematic literature reviews, a biomedical paper that summarizes results across many studies (Khan et al., 2003). DeYoung et al. (2021) are the first that address this task, showing the related issues. \n\nState-of-the-art approaches leverage two leading solutions: hierarchical networks that capture crossdocument relations via graph encodings (Wan and Yang, 2006;Liao et al., 2018;Li et al., 2020;Pasunuru et al., 2021) or hidden states aggregation (Fabbri et al., 2019;Liu and Lapata, 2019a;Jin et al., 2020), and long-range neural models that apply multi-input concatenation (Xiao et al., 2021). While effective, these solutions struggle to process clusters of many topic-related documents in low computational resource scenarios (Moro and Ragazzi, 2022) because they need to truncate the inputs. Moreover, pre-trained state-of-the-art Transformers are not leveraged despite showing strong performance when fine-tuned in downstream tasks such as single-document summarization (Liu and Lapata, 2019b;Lewis et al., 2020a;Raffel et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020). \n\nMulti-document summarization requires models to have more robust capabilities for analyzing the cluster to discriminate the correct information from noise and merge it consistently. In this work, we propose a discriminative marginalized probabilistic neural method (DAMEN) that selects worthy documents in the cluster with respect to a shared background and generates the summary via token probability marginalization.",
            "score": 0.549562547963085,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 726
                },
                {
                    "start": 729,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1608
                },
                {
                    "start": 1611,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 2029
                }
            ],
            "ref_mentions": [
                {
                    "start": 304,
                    "end": 317,
                    "matchedPaperCorpusId": "10103200"
                },
                {
                    "start": 407,
                    "end": 433,
                    "matchedPaperCorpusId": "33320463"
                },
                {
                    "start": 433,
                    "end": 454,
                    "matchedPaperCorpusId": "6271496"
                },
                {
                    "start": 618,
                    "end": 637,
                    "matchedPaperCorpusId": "22459429"
                },
                {
                    "start": 639,
                    "end": 660,
                    "matchedPaperCorpusId": "233231380"
                },
                {
                    "start": 868,
                    "end": 888,
                    "matchedPaperCorpusId": "5457260"
                },
                {
                    "start": 888,
                    "end": 906,
                    "matchedPaperCorpusId": "49210924"
                },
                {
                    "start": 906,
                    "end": 922,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 922,
                    "end": 944,
                    "matchedPaperCorpusId": "235097309"
                },
                {
                    "start": 974,
                    "end": 995,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 995,
                    "end": 1017,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 1017,
                    "end": 1034,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 1257,
                    "end": 1281,
                    "matchedPaperCorpusId": "250304727"
                },
                {
                    "start": 1503,
                    "end": 1526,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 1526,
                    "end": 1546,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1546,
                    "end": 1566,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1587,
                    "end": 1607,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7548828125
        },
        {
            "corpus_id": "250118028",
            "title": "An Empirical Survey on Long Document Summarization: Datasets, Models, and Metrics",
            "text": "Transformer-based model is ubiquitously state-of-the-art across a wide range of tasks in the NLP domain. In line with this development, recent works in the long document summarization models often involve using the same Transformer base architecture but with different proposing mechanisms. These Transformer-based models involve implementing novel mechanisms with long document adaptations to ensure the task of summarizing a document with significantly longer input sequence texts can be effectively addressed. The mechanisms used by extractive, abstractive and hybrid Transformer-based summarization models are described in the following with an overview of mechanisms used by abstractive and hybrid summarization models shown in Figure 3. A. Extractive Transformer. As Transformer and its pre-trained models are optimized for short document settings, they may not reason well over long text sequences if not properly fine-tuned. To this end, Cui et al. [22] proposed combining neural topic modeling together with BERT in learning a topic-enhanced, inter-sentence relationship across the entire document. Nonetheless, the issues of memory complexity and input token length limits were not resolved and significant source text is truncated under this research setting. Recently, Cui and Hu [21] proposed a memory network that incorporates graph attention networks and gated recurrent units to dynamically select important sentences through sliding a window along the entire source document. This approach can effectively integrate the pre-trained BERT model for long document summarization task by limiting its usage within each window, where the window size is set to be lower than or equal to 512 tokens.",
            "score": 0.549372818421166,
            "section_title": "Mechanisms of Transformer-based Architectures",
            "char_start_offset": 43687,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1708
                }
            ],
            "ref_mentions": [
                {
                    "start": 957,
                    "end": 961,
                    "matchedPaperCorpusId": "222310577"
                },
                {
                    "start": 1292,
                    "end": 1296,
                    "matchedPaperCorpusId": "235097475"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.923828125
        },
        {
            "corpus_id": "250572619",
            "title": "Enhancing Biomedical Scientific Reviews Summarization with Graph-based Factual Evidence Extracted from Papers",
            "text": "In this sense, early approaches, such as TextRank (Mihalcea and Tarau, 2004), propose to build a connectivity network with inter-sentence cosine similarity and document-level relations (Wan, 2008). Alternative neural systems design graph-based attention to identify important sentences (Tan et al., 2017). \n\nAs for abstractive summaries, results are based on the cross-cutting success of GNNs, which allow applying deep learning to highly structured data without imposing linearization or hierarchical constraints. Fernandes et al. (2019) extend standard sequence encoders with GNNs to leverage named entities and entity coreferences inferred by existing NLP tools, surpassing models that use only the sequential structure or graph structure. This also relates to the recent graph verbalization trend (Song et al., 2018;Koncel-Kedziorski et al., 2019;Agarwal et al., 2021), where inputs may originate from both knowledge graphs and information extraction or semantic parsing techniques (e.g., abstract meaning representation, AMR). Instead of directly generating text from a graph in a data-to-text scenario, An et al. (2021) redefine the task of scientific papers summarization by utilizing a graph-enhanced encoder on top of a citation network. Following a similar text-graph complementary view-where graphs are used in addition to document encoder-several researchers have tried to automatically build and incorporate a straightforward and machine-readable knowledge representation of the underlying text (Fan et al., 2019;Huang et al., 2020b;Zhu et al., 2021), also considering different level of granularities (Ji and Zhao, 2021). To this end, OpenIE (Angeli et al., 2015) and Stanford CoreNLP (Manning et al., 2014) are by far the two most popular libraries, focusing on triplets and coreference resolution, respectively.",
            "score": 0.5491585218634679,
            "section_title": "RELATED WORK",
            "char_start_offset": 6359,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 305
                },
                {
                    "start": 308,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1827
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 196,
                    "matchedPaperCorpusId": "17446655"
                },
                {
                    "start": 286,
                    "end": 304,
                    "matchedPaperCorpusId": "26698484"
                },
                {
                    "start": 515,
                    "end": 538,
                    "matchedPaperCorpusId": "172138081"
                },
                {
                    "start": 801,
                    "end": 820,
                    "matchedPaperCorpusId": "25111673"
                },
                {
                    "start": 820,
                    "end": 851,
                    "matchedPaperCorpusId": "102354588"
                },
                {
                    "start": 851,
                    "end": 872,
                    "matchedPaperCorpusId": "263864711"
                },
                {
                    "start": 1109,
                    "end": 1125,
                    "matchedPaperCorpusId": "233169121"
                },
                {
                    "start": 1508,
                    "end": 1526,
                    "matchedPaperCorpusId": "202785778"
                },
                {
                    "start": 1526,
                    "end": 1546,
                    "matchedPaperCorpusId": "218487279"
                },
                {
                    "start": 1546,
                    "end": 1563,
                    "matchedPaperCorpusId": "235097229"
                },
                {
                    "start": 1615,
                    "end": 1634,
                    "matchedPaperCorpusId": "237598718"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87109375
        },
        {
            "corpus_id": "248218467",
            "title": "Summarization with Graphical Elements",
            "text": "The field of automatic summarization can classically be divided into abstractive (e.g., Gehrmann et al., 2018;Lewis et al., 2020) and extractive summarization (e.g., Narayan et al., 2018b;Ju et al., 2021). In our work the summaries cannot be generated by simply copying from the input document, classifying our task as a form of abstractive summarization. \n\nModeling. Many recent automatic summarization methods rely on some variety of neural sequence to sequence modeling (e.g., Cheng and Lapata, 2016;Vaswani et al., 2017;Lewis et al., 2020;Xue et al., 2021). Summaries produced by recent approaches such as BART (Lewis et al., 2020) and T5 (Xue et al., 2021) are of very high quality in terms of fluency and grammaticality, yet they struggle with factual consistency (e.g., Cao et al., 2020;Maynez et al., 2020). Hence, there has been a surge in work that focuses on improving the factuality of generated summaries. These works focus either on the evaluation of summarization (e.g., Wang et al., 2020;Durmus et al., 2020), or on the modeling procedures themselves, for example by explicitly incorporating graph-based meaning representations in the modeling process (Ribeiro et al., 2022). In this work we use BART and T5 as the summarization backbone of our baselines, but we are excited to explore graph-based methods in future work.",
            "score": 0.5480147304468967,
            "section_title": "Related Work",
            "char_start_offset": 5082,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 355
                },
                {
                    "start": 358,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1337
                }
            ],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 110,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 110,
                    "end": 129,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 166,
                    "end": 188,
                    "matchedPaperCorpusId": "3510042"
                },
                {
                    "start": 188,
                    "end": 204,
                    "matchedPaperCorpusId": "238259960"
                },
                {
                    "start": 524,
                    "end": 543,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 615,
                    "end": 635,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 777,
                    "end": 794,
                    "matchedPaperCorpusId": "224706057"
                },
                {
                    "start": 794,
                    "end": 814,
                    "matchedPaperCorpusId": "218487034"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.810546875
        },
        {
            "corpus_id": "269791107",
            "title": "Hierarchical Attention Graph for Scientific Document Summarization in Global and Local Level",
            "text": "Graph neural networks have been widely used for extractive summarization due to their flexibility and scalability.Dong et al. (2020) proposed an unsupervised graph-based model that combines both sentence similarity and hierarchical discourse structure to rank sentences.Cui et al. (2020) injected latent topic information into graph neural networks to further improve performance.Wang et al. (2020) constructed a word-document heterogeneous graph using word nodes as intermediate to connect sentences.Zhang et al. (2022) proposed a hypergraph transformer to model long-distance dependency while emphasizing the importance of high-order inter-sentence relations in extraction summarization.Our paper follows this line of work, but the main difference is that our approach combines both intra-sentence relations and high-order crosssentence relations and efficiently leverages the hierarchical discourse structure of scientific documents to learn sentence representations that incorporate both local and global information.",
            "score": 0.5473260482125347,
            "section_title": "Graph based Summarization",
            "char_start_offset": 7172,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 114,
                    "end": 270
                },
                {
                    "start": 270,
                    "end": 380
                },
                {
                    "start": 380,
                    "end": 501
                },
                {
                    "start": 501,
                    "end": 689
                },
                {
                    "start": 689,
                    "end": 1021
                }
            ],
            "ref_mentions": [
                {
                    "start": 380,
                    "end": 398,
                    "matchedPaperCorpusId": "216552978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97802734375
        },
        {
            "corpus_id": "266244733",
            "title": "Multi-granularity adaptive extractive document summarization with heterogeneous graph neural networks",
            "text": "The research introduces a multi-granularity adaptive automatic summarization model predicated on a heterogeneous graph neural network. This approach innovatively incorporates topic semantic units into the heterogeneous graph, thereby enriching the complexity of inter-sentence relations. Attention is also paid to the relative constancy of attention coefficients and the significance of edge features. Adaptive techniques are employed to update nodal characteristics across varying granularities, both in terms of breadth and depth. The proposed model further allows for effortless inclusion of document-level nodes into a single-document framework, thereby simplifying the construction of multi-document summarization models. The designed graph structureincorporating sentences, topic words, and individual words-can be seamlessly integrated as a module within existing generative models. This integrated module efficiently encodes sentence-level information and channels it into broader linguistic and thematic structures through the graph architecture. Our model, serving as an encoding module, can be applied to other datasets by simply extracting sentences, topics, and words from the dataset and incorporating them into the corresponding graph structure nodes. Performance assessments on the CNN/DailyMail and Multi-News datasets reveal that the model delivers commendable results, particularly when benchmarked against models not leveraging BERT.",
            "score": 0.5473260482125347,
            "section_title": "CONCLUSION",
            "char_start_offset": 35331,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1453
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97314453125
        },
        {
            "corpus_id": "253155963",
            "title": "Abstractive Summarization Guided by Latent Hierarchical Document Structure",
            "text": "Sequential abstractive neural summarizers often do not use the underlying structure in the input article or dependencies between the input sentences. This structure is essential to integrate and consolidate information from different parts of the text. To address this shortcoming, we propose a hierarchy-aware graph neural network (HierGNN) which captures such dependencies through three main steps: 1) learning a hierarchical document structure through a latent structure tree learned by a sparse matrix-tree computation; 2) propagating sentence information over this structure using a novel message-passing node propagation mechanism to identify salient information; 3) using graph-level attention to concentrate the decoder on salient information. Experiments confirm HierGNN improves strong sequence models such as BART, with a 0.55 and 0.75 margin in average ROUGE-1/2/L for CNN/DM and XSum. Further human evaluation demonstrates that summaries produced by our model are more relevant and less redundant than the baselines, into which HierGNN is incorporated. We also find HierGNN synthesizes summaries by fusing multiple source sentences more, rather than compressing a single source sentence, and that it processes long inputs more effectively.",
            "score": 0.5473260482125347,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97607421875
        },
        {
            "corpus_id": "271227654",
            "title": "Comprehensive Survey of Abstractive Text Summarization Techniques",
            "text": "Text summarization using pre-trained encoders has become a crucial technique for efficiently managing large volumes of text data. The rise of automatic summarization systems addresses the need to process ever-increasing data while meeting user-specific requirements. Recent scientific research highlights significant advancements in abstractive summarization, with a particular focus on neural network-based methods. A detailed review of various neural network models for abstractive summarization identifies five key components essential to their design: encoder-decoder architecture, mechanisms, training strategies and optimization algorithms, dataset selection, and evaluation metrics. Each of these elements is pivotal in enhancing the summarization process. This study aims to provide a thorough understanding of the latest developments in neural network-based abstractive summarization models, offering insights into the evolving field and underscoring the associated challenges. Qualitative analysis using a concept matrix reveals common design trends in contemporary neural abstractive summarization systems. Notably, BERT-based encoder-decoder models have emerged as leading innovations, representing the most recent progress in the field. Based on the insights from this review, the study recommends integrating pre-trained language models with neural network techniques to achieve optimal performance in abstractive summarization tasks. As the volume of online information continues to surge, the field of automatic text summarization has garnered significant attention within the Natural Language Processing (NLP) community. Spanning over five decades, researchers have approached this problem from diverse angles, exploring various domains and employing a multitude of paradigms. This survey aims to delve into some of the most pertinent methodologies, focusing on both single-document and multiple-document summarization techniques, with a particular emphasis on empirical methods and extractive approaches. Additionally, the survey explores promising strategies that target specific intricacies of the summarization task. Notably, considerable attention is dedicated to the automatic evaluation of summarization systems, recognizing its pivotal role in guiding future research endeavors.",
            "score": 0.5447615101655048,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92919921875
        },
        {
            "corpus_id": "251081212",
            "title": "Attention based Recurrent Neural Network for Nepali Text Summarization",
            "text": "This approach is for generating a query-based summary. A centroid-based text summarization technique works for single as well as multi-document summarization (Radev et al., 2004). A popular extractive text summarization technique called LexRank is an unsupervised approach to text summarization based on graph-based centrality scoring of sentences (Erkan & Radev, 2004). \n\nBesides extractive text summarization, there is increasing attention from researchers on abstractive text summarization in the recent past (Yeasmin et al., 2017), especially after the success of deep neural networks in various applications such as computer vision and image processing (Mishra & Shahi, 2021). Initially, the text summarization task was investigated to generate news headline generation, abstract of research papers, and so on (Edmundson, 1969;DeJong, 1977;Carbonell & Goldstein, 1998). A text summarization method inspired by statistical machine translation was proposed by Banko et al. (2000) using a news corpus of headline-article pairs. They generated the headlines for the news article even shorter than one sentence using statistical term selection and term ordering jointly which their model learns directly from the training corpus. Similarly, a neural network-based approach with a larger dataset of headline-article pairs was implemented by Rush et al., 2015). They achieve state-of-the-art performance on both DUC-2004 and Gigaword datasets which have two sentence level summaries. A further performance boost was reported in Nallapati et al. ( 2016) using a sequence to a sequence-based attentional encoder-decoder neural network. This work was based on an attentional recurrent neural network implemented for a machine translation task (Bahdanau et al., 2014). Furthermore, an LSTM network with attention was implemented for news headline generation (Lopyrev, 2015) A few works on text summarization using the generative adversarial network are also reported in the literature. \n\nFor instance, a text summarization using GAN is proposed by Lin (2004). In this work, they have designed and trained both a generator and discriminator in an endto-end fashion.",
            "score": 0.5440807646966125,
            "section_title": "RELATED WORK",
            "char_start_offset": 7000,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 370
                },
                {
                    "start": 373,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1979
                },
                {
                    "start": 1982,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2158
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 178,
                    "matchedPaperCorpusId": "10151424"
                },
                {
                    "start": 348,
                    "end": 369,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 512,
                    "end": 534,
                    "matchedPaperCorpusId": "212449070"
                },
                {
                    "start": 658,
                    "end": 680,
                    "matchedPaperCorpusId": "238219952"
                },
                {
                    "start": 815,
                    "end": 832,
                    "matchedPaperCorpusId": "1177942"
                },
                {
                    "start": 845,
                    "end": 873,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 963,
                    "end": 982,
                    "matchedPaperCorpusId": "9952653"
                },
                {
                    "start": 1340,
                    "end": 1358,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 2042,
                    "end": 2052,
                    "matchedPaperCorpusId": "964287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76904296875
        },
        {
            "corpus_id": "252780923",
            "title": "HEGEL: Hypergraph Transformer for Long Document Summarization",
            "text": "The performance of HEGEL and baseline methods on arXiv and Pubmed datasets are shown in Table 2. The first block lists the extractive ground truth ORACLE and the unsupervised methods. The second block includes recent extractive summarization models, and the third contains state-of-the-art abstractive methods.\n\nThe LEAD method has limited performance on scientific paper summarization compared to  its strong performance on short news summarization like CNN/Daily Mail (Hermann et al., 2015) and New York Times (Sandhaus, 2008). The phenomenon indicates that academic paper has less positional bias than news articles, and the ground truth sentence distributes more evenly. For graph-based unsupervised baselines, HIPORANK (Dong et al., 2020) achieves state-of-the-art performance that could even compete with some supervised methods. This demonstrates the significance of incorporating discourse structural information when modeling cross-sentence relations for long documents. In general, neural extractive methods perform better than abstractive methods due to the extended context. Among extractive baselines, transformerbased methods like SentPTR and HiStruct+ show substantial performance gain, demonstrating the effectiveness of the attention mechanism. HiStruct+ achieves strong performance by injecting inherent hierarchical structures into large pre-trained language models Longformer. In contrast, our model HEGEL only relies on hypergraph transformer layers for sentence representation learning and requires no pre-trained knowledge.\n\nAs shown in Table 2, HEGEL outperforms stateof-the-art extractive and abstractive baselines on both datasets. The supreme performance of HEGEL shows hypergraphs' capability of modeling highorder cross-sentence relations and the importance of fusing both semantic and structural information. We conduct an extensive ablation study and performance analysis next.",
            "score": 0.5438585485932829,
            "section_title": "Experiment Results",
            "char_start_offset": 18651,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 470,
                    "end": 492,
                    "matchedPaperCorpusId": "6203757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77099609375
        },
        {
            "corpus_id": "250301954",
            "title": "Hierarchical Heterogeneous Graph Attention Network for Syntax-Aware Summarization",
            "text": "Text summarization has always been a fundamental task in natural language processing (NLP) to condense a complex input to a concise expression by retaining the core information at the same time. The relevant techniques can be categorized as either extractive ones, which only need to identify salient sentences from the original text, or abstractive ones, which may generate novel words and sentences. \n\nGraph-based methods for summarization are becoming heated topics with the rise of Graph Neural Networks (GNN) thanks to its powerful capability to model the underlying useful relationships in the text graph. However, the output summary by existing graph-based methods tends to suffer from semantic deviation from the input text as the graphs constructed in these models are mostly at the statistical level, like word-sentence graph in HSG (Wang et al. 2020). In this case, the nodes tend to be text units and edges are connected by simple statistical scores like co-occurrence, pointwise mutual information (PMI), ignoring rich syntactic and semantic information for summarization. Other latest works (Jin, Wang, and Wan 2020;Wu et al. 2021) extend them with the aid of semantic graphs directly but suffer from relatively higher computational costs due to the complex graph construction step. \n\nFigure 1: The constituency parsing tree for sentence Eight British nationals died in the plane crash. Red dotted line denotes the syntactic dependency path from died to in for the relationship prep. The path from died to in reflects the relation edge \"died in\" in the corresponding semantic graph. Each constituent can store the semantic meaning hierarchically for the final summary. Previous studies (Li et al. 2014;Xu and Durrett 2019) have shown that syntactic structure is beneficial for generating compressed yet informative summaries because its hierarchical structure facilitates the removal of insignificant parts and pays more attention to more salient ones (Figure 1). This also mimics the human way of generating summaries: fusing the semantic meaning by extracting the most significant information level by level, from words to phrases and finally to sentences. Therefore, it is natural to guide the neural summarization system with a tree-like text graph that embodies syntactic information so that it can identify summary-worthy content and compose summaries that preserve the vital meaning of the source texts.",
            "score": 0.5418199205531851,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 401
                },
                {
                    "start": 404,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1296
                },
                {
                    "start": 1299,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2424
                }
            ],
            "ref_mentions": [
                {
                    "start": 843,
                    "end": 861,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 1105,
                    "end": 1130,
                    "matchedPaperCorpusId": "214303164"
                },
                {
                    "start": 1130,
                    "end": 1144,
                    "matchedPaperCorpusId": "235187330"
                },
                {
                    "start": 1700,
                    "end": 1716,
                    "matchedPaperCorpusId": "10112929"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88232421875
        },
        {
            "corpus_id": "271938110",
            "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
            "text": "This study introduces an innovative Topic-aware Heterogenous Graph Neural Network (HGNN) model aimed at summarizing medical documents. Instead of focusing solely on creating a sentence-level graph, the model integrates additional semantic components such as words and latent topic nodes by incorporating them as auxiliary elements within the graph. The initial phase involves using a BERT-Encoder (Bidirectional Encoder Representations) [20] to encode the entire document, thereby generating sentence and word nodes. Following this, latent topic nodes are identified using a well-established generative statistical topic modeling technique known as Latent Dirichlet Allocation (LDA) [21]. \n\nIn the next phase, a heterogeneous document graph is constructed, comprising sentences, words, and global topic nodes linked by edge features quantified as TF-IDF values. The representations of these nodes are then refined and updated using GAT. Ultimately, the sentence representations, enriched with topic information, are fed into a Transformer-based decoder to generate abstractive summaries. Our approach differs from previous works. For instance, the research by [3] relies on the Neural Topic Model (NTM) for topical information, which requires complex training setups [15]. Similarly, our method differs from work conducted by [19], which focused on constructing a heterogeneous graph solely for word and topic nodes, specifically for summarizing Chinese complaint reports. \n\nThe key contributions of our model are outlined as follows: \n\n\u2022 This study introduces a novel Neural Graph structure enriched with topical information for abstractive summarization of medical scientific documents. This approach integrates the LDA Topic Extractor, BERT-Encoder, Graph Attention Network, and Transformer model into a cohesive framework. \n\n113856 VOLUME 12, 2024 Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply. \n\n\u2022 The framework effectively captures global semantic information, guiding the summary generation process and addressing the challenge of limited semantic context in traditional summarization methods. \n\n\u2022 Leveraging data from the publicly available PubMed dataset, our research demonstrates consistent and favorable results compared to state-of-the-art heterogeneous graph structures used for both extractive and abstractive summarization tasks.",
            "score": 0.541749456549423,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 4505,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 688
                },
                {
                    "start": 691,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1472
                },
                {
                    "start": 1475,
                    "end": 1534
                },
                {
                    "start": 1537,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1826
                },
                {
                    "start": 1829,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 1963
                },
                {
                    "start": 1966,
                    "end": 2165
                },
                {
                    "start": 2168,
                    "end": 2410
                }
            ],
            "ref_mentions": [
                {
                    "start": 683,
                    "end": 687,
                    "matchedPaperCorpusId": "753304"
                },
                {
                    "start": 1160,
                    "end": 1163,
                    "matchedPaperCorpusId": "250012667"
                },
                {
                    "start": 1267,
                    "end": 1271,
                    "matchedPaperCorpusId": "252670538"
                },
                {
                    "start": 1326,
                    "end": 1330,
                    "matchedPaperCorpusId": "264064249"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.958984375
        },
        {
            "corpus_id": "248496597",
            "title": "Large-Scale Multi-Document Summarization with Information Extraction and Compression",
            "text": "We develop an abstractive summarization framework independent of labeled data for multiple heterogeneous documents. Unlike existing multi-document summarization methods, our framework processes documents telling different stories instead of documents on the same topic. We also enhance an existing sentence fusion method with a uni-directional language model to prioritize fused sentences with higher sentence probability with the goal of increasing readability. Lastly, we construct a total of twelve dataset variations based on CNN/Daily Mail and the NewsRoom datasets, where each document group contains a large and diverse collection of documents to evaluate the performance of our model in comparison with other baseline systems. Our experiments demonstrate that our framework outperforms current state-of-the-art methods in this more generic setting.",
            "score": 0.5399799603965584,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91162109375
        },
        {
            "corpus_id": "256461119",
            "title": "SEM-F1: an Automatic Way for Semantic Evaluation of Multi-Narrative Overlap Summaries at Scale",
            "text": "As SOS can be viewed as a multi-document summarization task with additional commonality constraint, text summarization literature is the most relevant to our work. Over the years, many paradigms for document summarization have been explored (Zhong et al., 2019). The two most popular among them are extractive approaches (Cao et al., 2018;Narayan et al., 2018;Wu and Hu, 2018;Zhong et al., 2020) and abstractive approaches (Bae et al., 2019;Hsu et al., 2018;Liu et al., 2017;Nallapati et al., 2016). Some researchers have also tried combining extractive and abstractive approaches (Chen and Bansal, 2018;Hsu et al., 2018;Zhang et al., 2019). \n\nRecently, encoder-decoder-based neural models have become really popular for abstractive summarization (Rush et al., 2015;Chopra et al., 2016;Zhou et al., 2017;Paulus et al., 2017). It has become prevalent to train a general language model on a huge corpus of data and then transfer/fine-tune it for the summarization task (Radford et al., 2019;Devlin et al., 2019;Lewis et al., 2019;Xiao et al., 2020;Yan et al., 2020;Zhang et al., 2019;Raffel et al., 2019). Summary length control for abstractive summarization has also been studied (Kikuchi et al., 2016;Fan et al., 2017;Liu et al., 2018;Fevry and Phang, 2018;Schumann, 2018;Makino et al., 2019). In general, multiple document summarization (Goldstein et al., 2000;Yasunaga et al., 2017;Zhao et al., 2020;Ma et al., 2020;Meena et al., 2014) is more challenging than single document summarization. However, the SOS task is different from traditional multi-document summarization tasks in that the goal here is to summarize content with an overlap constraint, i.e., the output should only contain the common information from both input narratives.",
            "score": 0.5389717526039692,
            "section_title": "Related Works",
            "char_start_offset": 4904,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 641
                },
                {
                    "start": 644,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1742
                }
            ],
            "ref_mentions": [
                {
                    "start": 321,
                    "end": 339,
                    "matchedPaperCorpusId": "51878811"
                },
                {
                    "start": 766,
                    "end": 786,
                    "matchedPaperCorpusId": "133195"
                },
                {
                    "start": 989,
                    "end": 1009,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1218,
                    "end": 1235,
                    "matchedPaperCorpusId": "53079938"
                },
                {
                    "start": 1272,
                    "end": 1292,
                    "matchedPaperCorpusId": "196180065"
                },
                {
                    "start": 1338,
                    "end": 1362,
                    "matchedPaperCorpusId": "8294822"
                },
                {
                    "start": 1384,
                    "end": 1402,
                    "matchedPaperCorpusId": "220633461"
                },
                {
                    "start": 1418,
                    "end": 1437,
                    "matchedPaperCorpusId": "39340997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.802734375
        },
        {
            "corpus_id": "247348977",
            "title": "Template-Based Headline Generator for Multiple Documents",
            "text": "In this paper, we develop a neural multi-document summarization model, named MuD2H (refers to Multi-Document to Headline) to generate an attractive and customized headline from a set of product descriptions. To the best of our knowledge, no one has used a technique for multi-document summarization to generate headlines in the past. Therefore, multi-document headline generation can be considered new problem setting. Our model implements a two-stage architecture, including an extractive stage and an abstractive stage. The extractive stage is a graph-based model that identified salient sentences, whereas the abstractive stage uses existing summaries as soft templates to guild the seq2seq model. A series of experiments are conducted by using KKday dataset. Experimental results show that the proposed method outperforms the others in terms of quantitative and qualitative aspects.",
            "score": 0.536695936017188,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91015625
        },
        {
            "corpus_id": "265607988",
            "title": "Generating Extractive and Abstractive Summaries in Parallel from Scientific Articles Incorporating Citing Statements",
            "text": "After that, we run the Long-Former (Beltagy et al., 2020) encoder to generate sentence and word representations and train extractive and abstractive summarizers together. For the extractive summarizer, an heterogeneous graph neural network (Wang et al., 2020) is used as it has the ability to preserve sentence-level dependencies utilizing additional semantic units as intermediate nodes in the graph representation. Abstractive summaries are generated by the autoregressive decoder. The loss function is defined in such a way that both summarizers can achieve better ROUGE and ME-TEOR scores. Furthermore, we have developed a corpus containing 10K research articles along with their corresponding citation statements and is a subset of the Semantic Scholar Network (SSN) corpus. The citation statements are collected utilizing the citation graph used in the SSN corpus. In short, the contributions of this work are: \n\n\u2022 We have built a stand-alone summarizer model which can produce both extractive and abstractive summaries and each counterpart helps the other to generate better summaries.",
            "score": 0.5363789541679107,
            "section_title": "Introduction",
            "char_start_offset": 6081,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 916
                },
                {
                    "start": 919,
                    "end": 1092
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 259,
                    "matchedPaperCorpusId": "216552978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90234375
        },
        {
            "corpus_id": "253708345",
            "title": "GoSum: Extractive Summarization of Long Documents by Reinforcement Learning and Graph Organized discourse state",
            "text": "Document summarization refers to generating short and conductive summaries over given texts, which can help readers rapidly acquire essential knowledge from documents. There are two main categories of approaches to summarization: the extractive approach and the abstractive approach. The extractive approaches score and filter out the sentences of a given document to ensure the semantic and grammatical correctness of the selected sentences in the summary. Abstractive approaches mostly read an input text, comprehend it, and output its summary within the seq2seq framework. This procedure is similar to humans' summarising articles. The theoretical upper bound on the performance of the seq2seq model is higher than what extractive approaches can achieve. However, abstractive approaches have the drawback of producing some meaningless and unfaithful summaries (Kry\u015bci\u0144ski et al., 2020). The generated summaries read smoothly with a high ROUGE score, but there is a significant gap in semantic information between them and the gold summaries. \n\nIn this paper, we focus on the use of extractive models for summarizing scientific literature. Extractive summarization (Zhong et al., 2020;Zhou et al., 2018a) has been extensively studied in short summarization datasets such as CNN/DailyMail (Hermann et al., 2015). However, studies on long texts have lagged relatively behind because long document summarization is more challenging due to the following two reasons: 1) An increase in the input length expands the memory cost of the model; and 2) The complex discourse structural information about long-form documents should be taken into account. Reading a long text, especially scientific literature, one usually glances at the discourse structure of the whole text. Once reading a section title, one roughly should know on which this section focuses. Using this structural information of a text, one can better understand the meanings of its sentences. From the perspective of extractive summarization, it would be better to use this information for encoding sentences. The previous method encodes sentences and sections separately, making it difficult to capture the hier-archical structure of the document. In this paper, we thereby propose to use a graph neural network (GNN) to well represent the structure information of documents.",
            "score": 0.5363789541679107,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1044
                },
                {
                    "start": 1047,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2209
                },
                {
                    "start": 2210,
                    "end": 2337
                }
            ],
            "ref_mentions": [
                {
                    "start": 863,
                    "end": 888,
                    "matchedPaperCorpusId": "204976362"
                },
                {
                    "start": 1167,
                    "end": 1187,
                    "matchedPaperCorpusId": "215828313"
                },
                {
                    "start": 1187,
                    "end": 1206,
                    "matchedPaperCorpusId": "49656757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83740234375
        },
        {
            "corpus_id": "252280335",
            "title": "Graph-to-Text Generation with Dynamic Structure Pruning",
            "text": "Graph-to-text is a challenging task which aims at generating a descriptive text from the structured knowledge, such Knowledge Graph (KG), and Abstract Meaning Representation (AMR) graphs. It is helpful for interpretability of KGs in general (Schmitt et al., 2020) and knowledge-based question answering (Hui et al., 2022;Wang et al., 2022;Fu et al., 2020;Qin et al., 2022). \n\nIn recent years, most graph-to-text methods have been built based on the encoder-decoder architecture. This kind of method usually consists of a structural encoder and a decoder. The structural encoder aims to model the structure information into the representation of the input graph. Song et al. (2018) first propose the graph recurrent networks (GRNs) to encode the AMR node directly. And then, some works (Shi et al., 2020;Chen et al., 2020) introduce the Graph Neural Networks (GNNs) as the structural encoder, which updates the representations of nodes based on their immediate neighbors. To integrate both local and non-local features and learn a better structural representation of a graph, Guo et al. (2019) introduce the dense connection, allowing deeper GCNs. Unlike the local information aggregation scheme, Zhu et al. (2019); Cai and Lam (2020) propose the Graph Transformer that uses explicit relation encoding and allows direct communication between two distant nodes. \n\nA recently proposed neural abstractive Multi-Document Summarization (MDS) model, Graph-Summ (Li et al., 2020), also considers the input graph structure during decoding. The biggest difference between Graphsum and our proposed SACA is that the former only introduces one graph attention layer in each decoder layer. SACA, on the other hand, injects graph structure into decoding by re-encoding the input graph. Specifically, it re-computes the input graph representation by conditioning it on the newly generated text at each decoding step. \n\nRecent approaches try to apply the Pre-trained Language Models (PLMs) (Kenton and Toutanova, 2019;Raffel et al., 2019) into the graph-to-text generation.",
            "score": 0.5360579305721715,
            "section_title": "Related Works",
            "char_start_offset": 5273,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 373
                },
                {
                    "start": 376,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1359
                },
                {
                    "start": 1362,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1901
                },
                {
                    "start": 1904,
                    "end": 2057
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 263,
                    "matchedPaperCorpusId": "218470233"
                },
                {
                    "start": 303,
                    "end": 321,
                    "matchedPaperCorpusId": "265038300"
                },
                {
                    "start": 321,
                    "end": 339,
                    "matchedPaperCorpusId": "250089255"
                },
                {
                    "start": 662,
                    "end": 680,
                    "matchedPaperCorpusId": "25111673"
                },
                {
                    "start": 785,
                    "end": 803,
                    "matchedPaperCorpusId": "220730154"
                },
                {
                    "start": 1196,
                    "end": 1213,
                    "matchedPaperCorpusId": "202540640"
                },
                {
                    "start": 1215,
                    "end": 1233,
                    "matchedPaperCorpusId": "208138227"
                },
                {
                    "start": 1454,
                    "end": 1471,
                    "matchedPaperCorpusId": "218718706"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9609375
        },
        {
            "corpus_id": "258461571",
            "title": "DiffuSum: Generation Enhanced Extractive Summarization with Diffusion",
            "text": "Recent advances in deep neural networks have dramatically boosted the progress in extractive summarization systems. Existing extractive summarization systems span an extensive range of approaches. Most works formulate the task as a sequence classification problem and use sequential neural models with different encoders like recurrent neural networks (Cheng and Lapata, 2016;Nallapati et al., 2016) and pre-trained language models (Egonmwan and Chali, 2019;Liu and Lapata, 2019;Zhang et al., 2023). Another group of work formulates extractive summarization as a node classification problem and applies graph neural networks to model inter-sentence dependencies (Xu et al., 2019;Zhang and Zhang, 2020;Wang et al., 2020;Zhang et al., 2022a). These formulations are sentence-level methods that make individual predictions for each sentence. Recently, Zhong et al. (2020) observed that a summary consisting of sentences with the highest scores is not necessarily the best. As a result, summary-level formulation like text matching (Zhong et al., 2020;An et al., 2023) and reinforcement learning (Narayan et al., 2018b;Bae et al., 2019) are proposed. Our proposed framework DiffuSum is also a novel summary-level extractive system with generation augmentation. Instead of sequentially labeling sentences, DiffuSum directly generates the desired summary sentence representations with diffusion models and extracts sentences by representation matching.",
            "score": 0.5360508657590006,
            "section_title": "Extractive Summarization",
            "char_start_offset": 4659,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1446
                }
            ],
            "ref_mentions": [
                {
                    "start": 432,
                    "end": 458,
                    "matchedPaperCorpusId": "208246040"
                },
                {
                    "start": 679,
                    "end": 701,
                    "matchedPaperCorpusId": "226262336"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9384765625
        },
        {
            "corpus_id": "271791043",
            "title": "Enhanced Topic-Aware Summarization Using Statistical Graph Neural Networks",
            "text": "Contemporary researchers often utilize a heterogeneous network, incorporating multiple nodes that undergo updates, instead of a homogeneous graph with uniform, unmodified nodes. Research [46] delves into the exploration of syntactic Graph Convolutional Networks (GCNs) to effectively model non-Euclidean document structures. Additionally, they incorporate an attention information gate to selectively highlight salient information for the purpose of generating text summarization. The findings by [47] introduce a HG network designed to effectively capture various levels of information, encompassing both words and sentences. The model emphasizes redundancy dependencies between sentences, iteratively refining sentence representations through a redundancy-aware graph. In the study presented by [48], a novel Multiplex Graph Convolutional Network (MGCN) is introduced to collectively model various types of relationships between sentences and words. Reference [49] primarily employs the Text Graph Multi-Headed Attention Network (TGA) to proficiently acquire sentence representations across various types of text graphs at different levels. In [50], they employ advanced techniques, such as hypergraph transformer layers, to iteratively update and learn robust sentence representations. This approach strategically fuses various types of sentence dependencies, including latent topics, keywords, coreference, and section structure. The detailed methodology of the novel proposed model is discussed in the following section. \n\nThe study situates itself uniquely among a spectrum of contemporary research in extractive summarization for text but video summarization is also a promising field that identifies key segments in untrimmed videos leverage dual streams and a context-semantics interaction layer to enhance content detection [51,52]. In the realm of sequence-to-sequence and transformer-based models, current methodologies for text summarization frequently utilize complex neural architectures like BERT and attention mechanisms to enhance the extractive process. However, proposed model diverges by not only employing these advanced techniques but also innovatively integrating GATs and LDA. This combination permits for a refined handling of inter-sentence relationships and topic coherence, which is less stressed in traditional models.",
            "score": 0.5353446226303076,
            "section_title": "Neural Graph-Based Summarization",
            "char_start_offset": 13649,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1525
                },
                {
                    "start": 1528,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 2072
                },
                {
                    "start": 2073,
                    "end": 2201
                },
                {
                    "start": 2202,
                    "end": 2348
                }
            ],
            "ref_mentions": [
                {
                    "start": 187,
                    "end": 191,
                    "matchedPaperCorpusId": "212747858"
                },
                {
                    "start": 497,
                    "end": 501,
                    "matchedPaperCorpusId": "258654556"
                },
                {
                    "start": 962,
                    "end": 966,
                    "matchedPaperCorpusId": "247249484"
                },
                {
                    "start": 1146,
                    "end": 1150,
                    "matchedPaperCorpusId": "270211573"
                },
                {
                    "start": 1834,
                    "end": 1838,
                    "matchedPaperCorpusId": "268154033"
                },
                {
                    "start": 1838,
                    "end": 1841,
                    "matchedPaperCorpusId": "261573135"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.955078125
        },
        {
            "corpus_id": "253708345",
            "title": "GoSum: Extractive Summarization of Long Documents by Reinforcement Learning and Graph Organized discourse state",
            "text": "In this paper, we thereby propose to use a graph neural network (GNN) to well represent the structure information of documents. The additional benefit is that the computational complexity of GNNs is linear for long inputs. \n\nUnlike abstractive approaches that are trained by using available gold summaries directly, the training labels of an extractive model need to be obtained by using a search algorithm (typically greedy search) based on the gold summary provided. This kind of label is not optimal and deterministic, i.e., the algorithm yields a single extracted label for each pair of document-abstract. In fact, there may be many valid labels that are very similar to these suboptimal labels. Insufficient such positive pairs may cause under-fitting (Narayan et al., 2018). These problems can be alleviated by increasing the number of samples and giving each training sample a reward from reinforcement learning (RL). \n\nTo address the above problems, we propose a novel model called GoSum that is trained by using reinforcement learning. Based on a given input and previously extracted sentences, GoSum generates the sentences of a summary sequentially. The process of scoring and selecting a sentence is regarded as an action in reinforcement learning. This action is taken after the agent (the GoSum model) takes the sentence state as input. For encoding sentence states, we leverage the structure of a document. Specifically, we use a graph neural network to encode the hierarchical structure of a document. In more detail, we treat each sentence and section as a node of a heterogeneous graph. A state contains 1) a local representation of a sentence with discourse awareness, 2) the global context of a sentence within the document, and 3) information about the extraction history. As such, we seamlessly integrate RL with GNN in GoSum. To summarize, our main contributions of this paper are: 1) We propose an approach called GoSum 1 as a novel graph-based discourse-aware extractive summarization model. GoSum can generate a concise and informative summary operating on a subsentential discourse unit level. 2) We effectively integrate reinforcement learning with GNN under GoSum.",
            "score": 0.5349203193189873,
            "section_title": "Introduction",
            "char_start_offset": 2225,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 222
                },
                {
                    "start": 225,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 924
                },
                {
                    "start": 927,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2193
                }
            ],
            "ref_mentions": [
                {
                    "start": 757,
                    "end": 779,
                    "matchedPaperCorpusId": "3510042"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94287109375
        },
        {
            "corpus_id": "262592258",
            "title": "A hybrid approach for text summarization using semantic latent Dirichlet allocation and sentence concept mapping with transformer",
            "text": "The author used three methods: the first used a graph and a supervised model separately and then combined them to assign a score to the sentence. The second method evaluated the importance of sentences by using the graph model as an independent feature of the supervised model. The third model used a priori value to the graph model to score the sentences using a supervised approach. \n\nMulti-document summarization using deep learning architecture as a hybrid model [19] generates comprehensive summaries from news articles on specific topics. The architecture performed better than the traditional extractive model when evaluated using DUC 2004 data. Extracting the gist of documents is possible by using information such as titles, image captions, and side headings [20]. The author has proposed a single-document summarizer framework with a hierarchical document encoder with attention to side information. The extractive summarization framework with side information generates a better summary with fluency. Another framework matches extracted summary with the original document in semantic space [21] and models sentence relationships. It also provides a deep analysis of the gap between summary-level and sentence-level extractors based on the features of a dataset. \n\nOne of the main driving forces in recent development in abstractive text summarization is the availability of new neural architectures and new strategies in training. However, there is a need to address issues such as a proper model and data analysis tool and understanding the failure model of summarization. SummVis [22], an open-source tool, allows us to visualize, generate a summary, and analyze the summarization models and the evaluation metrics used. Topic modeling has been recently used in text summarization to identify hidden topics in the document [23]. Latent Dirichlet allocation (LDA) performs better than latent semantic analysis (LSA) if the number of features increases in the sentences. A hybrid approach for text summarization [24] proposed a novel sentence scoring method for extractive summarization. The sentence scoring parameter significantly improves the performance of the model. The researchers presented a single-document text summarization technique based on sentence similarity and document context [25]. Their approach utilized undirected graph-based scoring to evaluate sentences and determine which ones should be included in the summary.",
            "score": 0.5316617814473902,
            "section_title": "LITERATURE REVIEW",
            "char_start_offset": 8520,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 384
                },
                {
                    "start": 387,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1273
                },
                {
                    "start": 1276,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2099
                },
                {
                    "start": 2100,
                    "end": 2183
                },
                {
                    "start": 2184,
                    "end": 2312
                },
                {
                    "start": 2313,
                    "end": 2449
                }
            ],
            "ref_mentions": [
                {
                    "start": 1102,
                    "end": 1106,
                    "matchedPaperCorpusId": "215828313"
                },
                {
                    "start": 1594,
                    "end": 1598,
                    "matchedPaperCorpusId": "233241131"
                },
                {
                    "start": 2024,
                    "end": 2028,
                    "matchedPaperCorpusId": "237974591"
                },
                {
                    "start": 2307,
                    "end": 2311,
                    "matchedPaperCorpusId": "23433138"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94091796875
        },
        {
            "corpus_id": "269909180",
            "title": "Graph Based Abstractive Text Summarization of YouTube Comments",
            "text": "Extractive Summarization takes place by combining several sentences and from these several results are generated for this processes are used as fusion, compression, and suppression [1].Further these sentences are classified into the structured base and semantic based approach.Structure approach encodes essential information without losing original meaning of information.Predefined structures used in this approach are template based, tree, ontology, graph based structures.In semantic based approach Document is fetched to NLP system and here noun and verbs over which focus is taken.Generating abstractive summary is necessary in large digital world and from this data concise and short summary is generated.\n\nThe authors of [2] identify the key phrases in the document.The original document is not modified in this process.A novel graph-based approach for abstractive text summarization that incorporates both syntactic and semantic features is proposed in [5].They used the CNN/Daily Mail dataset to evaluate their method and showed that their approach outperformed several state-of-the-art methods in terms of ROUGE metrics.Their main contribution was the development of a new syntactic and semantic feature extraction method that improved the performance of graphbased summarization.The proposed approach is a hybrid approach for text summarization of YouTube comments that combines both extractive and abstractive methods using convolutional neural networks (CNNs).They evaluated their method using F1 score, BLEU score, and ROUGE metrics on a YouTube comments dataset and showed that their approach outperformed several existing methods.The main contribution in [6], [7] was the development of a hybrid summarization method that leverages both extractive and abstractive methods for improved performance.\n\nIn [8], [9], [10] authors have explored the combination of graph-based and transformer-based methods for abstractive summarization of YouTube comments.They evaluated their method using ROUGE metrics and human evaluation and showed that the combination of graph-based and transformer-based methods outperformed either method alone.Their main contribution was demonstrating the effectiveness of combining these two approaches for improved summarization performance.In [10] the method using ROUGE metrics on a YouTube comments dataset is evaluated and showed that this approach outperformed several existing methods.",
            "score": 0.5311945045759059,
            "section_title": "Literature Survey",
            "char_start_offset": 3182,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 185,
                    "end": 277
                },
                {
                    "start": 277,
                    "end": 373
                },
                {
                    "start": 373,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 587
                },
                {
                    "start": 587,
                    "end": 712
                },
                {
                    "start": 714,
                    "end": 774
                },
                {
                    "start": 774,
                    "end": 828
                },
                {
                    "start": 828,
                    "end": 966
                },
                {
                    "start": 966,
                    "end": 1131
                },
                {
                    "start": 1131,
                    "end": 1291
                },
                {
                    "start": 1291,
                    "end": 1474
                },
                {
                    "start": 1474,
                    "end": 1647
                },
                {
                    "start": 1647,
                    "end": 1814
                },
                {
                    "start": 1816,
                    "end": 1967
                },
                {
                    "start": 1967,
                    "end": 2146
                },
                {
                    "start": 2146,
                    "end": 2279
                },
                {
                    "start": 2279,
                    "end": 2429
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 184,
                    "matchedPaperCorpusId": "249614940"
                },
                {
                    "start": 729,
                    "end": 732,
                    "matchedPaperCorpusId": "52898893"
                },
                {
                    "start": 962,
                    "end": 965,
                    "matchedPaperCorpusId": "3342465"
                },
                {
                    "start": 1672,
                    "end": 1675,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 1677,
                    "end": 1680,
                    "matchedPaperCorpusId": "11212020"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9072265625
        },
        {
            "corpus_id": "277284940",
            "title": "Global and Local Context Fusion in Heterogeneous Graph Neural Network for Summarizing Lengthy Scientific Documents",
            "text": "Text summarization is a key area of natural language processing (NLP) that allows users to quickly find relevant information in large datasets [1]. There are two main types: extractive and abstractive summarization [2]. Extractive summarization selects key phrases and sentences from the original text, while abstractive summarization reformulates the content, which may lead to less fluent results. Extractive methods are favored for their factual accuracy and ease of implementation, making them popular among researchers [3], [4]. Moreover, Traditional methods for Extractive Text Summarization (ETS), such as TextRank and LexRank, focus primarily on calculating sentence similarity. The acquisition and modeling of cross-sentence relationships represent a critical phase in the process of generating a summary that encapsulates significantly meaningful sentences from the input content. Over the past few years, neural sequence-to-sequence encoder-decoder frameworks have shown exceptional performance in the domain of extractive summarization. The methodologies introduced by [1] and [2] predominantly utilized the sequence-to-sequence architecture with an encoder-decoder framework. In this setup, each sentence is encoded using various neural components in distinct ways. However, the empirical research showed that when this paradigm was used to define inter-sentence relations, it did not help to improve the summarization performance [3]. \n\nNumerous studies, as indicated by various references, have utilized Recurrent Neural Networks (RNNs) to capture and structure the relationships between sentences. However, recurrent models face troubles in effectively controlling computational factors and capturing long-range connections among words, mainly due to their processing of text in a strictly sequential fashion. \n\nIn recent developments, Graph Neural Networks (GNNs) [4] have drawn significant attention due to their capacity to efficiently model cross-sentence interactions in summarization tasks [3], [5], [6], [7], [8], [9]. It is noted by the researchers of [3] that a crucial step within this framework involves constructing an effective document graph. The study presented by [10] investigates an unsupervised, discourseaware hierarchical and ranking graph-based model, meticulously tailored for comprehensive scientific documents.",
            "score": 0.5310019991518748,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1448
                },
                {
                    "start": 1451,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1825
                },
                {
                    "start": 1828,
                    "end": 2041
                },
                {
                    "start": 2042,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2351
                }
            ],
            "ref_mentions": [
                {
                    "start": 143,
                    "end": 146,
                    "matchedPaperCorpusId": "52221845"
                },
                {
                    "start": 1081,
                    "end": 1084,
                    "matchedPaperCorpusId": "52221845"
                },
                {
                    "start": 2017,
                    "end": 2020,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 2027,
                    "end": 2030,
                    "matchedPaperCorpusId": "252818946"
                },
                {
                    "start": 2032,
                    "end": 2035,
                    "matchedPaperCorpusId": "252670538"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91845703125
        },
        {
            "corpus_id": "252780923",
            "title": "HEGEL: Hypergraph Transformer for Long Document Summarization",
            "text": "Graph-based models have been exploited for extractive summarization to capture cross-sentence dependencies. Unsupervised graph summarization methods rely on graph connectivity to score and rank sentences Zheng and Lapata, 2019;Dong et al., 2020). Researchers also explore supervised graph neural networks for summarization. Yasunaga et al. (2017) applied Graph Convolutional Network (GCN) on the approximate discourse graph. Xu et al. (2019) proposed to apply GCN on structural discourse graphs based on RST trees and coreference mentions. Cui et al. (2020) leveraged topical information by building topicsentence graphs. Recently, Wang et al. (2020) proposed to construct word-document heterogeneous graphs and use word nodes as the intermediary between sentences. Jing et al. (2021) proposed to use multiplex graph to consider different sentence relations. Our paper follows this line of work on developing novel graph neural networks for single document extractive summarization. The main difference is that we construct a hypergraph from a document that could capture high-order crosssentence relations instead of pairwise relations, and fuse different types of sentence dependencies, including section structure, latent topics, and keywords coreference.",
            "score": 0.5310019991518748,
            "section_title": "Graph based summarization",
            "char_start_offset": 5898,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97119140625
        },
        {
            "corpus_id": "254823405",
            "title": "Meeting Summarization: A Survey of the State of the Art",
            "text": "We can see a paradigm shift in the abstractive summarization approaches after the evolution of neural networks. A categorization of deep learning-based summarization approaches is shown in Fig. 4. Query Based: According to (Zhong et al., 2021b), meeting participants would like to get the summary of the meetings according to their area of interest. To serve this purpose the authors have proposed a summarization task based on a query. For this task they have also created a multi-domain summarization dataset with query-summary pairs. The summarization was done in a pipeline of two steps, locating the relevant span of meeting transcripts according to the query, and summarizing it with the help of state-of-art abstractive models like Pointer-Generator Network (See et al., 2017), BART (Lewis et al., 2020), HMNet (Zhu et al., 2020). The authors used two methods for locator, a Pointer Network (Vinyals et al., 2015) and a hierarchical ranking based model. Vig et al. conducted an exploratory study of various approaches on Query Focused Summarization. The authors also proposed two end-to-end neural network-based approaches which are composed of extractor models followed by abstractor models.",
            "score": 0.5292440401846041,
            "section_title": "Deep Learning Based Summarization",
            "char_start_offset": 18099,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1199
                }
            ],
            "ref_mentions": [
                {
                    "start": 790,
                    "end": 810,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 898,
                    "end": 920,
                    "matchedPaperCorpusId": "7608649"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "271710592",
            "title": "Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization",
            "text": "We focus on brief literature survey on generic summarization, which encompasses a broad approach to summarizing text without focusing on specific aspects, queries, or goals, using abstractive or extractive approaches. Among abstractive approaches, Chopra et al. (2016) introduced an abstractive summarization model using attentive recurrent neural networks and discuss the challenges of generating coherent and informative summaries while avoiding redundancy. Based on pointer-generator network framework, See et al. (2017) presents a model that combines extractive and abstractive techniques for summarization by effectively incorporating source information into the generated summaries. On the other hand, among purely extractive approaches, earlier researchers used graph-based approaches like TextRank (Mihalcea & Tarau (2004)) and LexRank (Erkan & Radev (2004)). Hayashi et al. (2021) employed a method for aspect-based summarization focusing on multiple domains while (Coavoux et al. (2019)) focused on aspect-based multi-document abstractive summarization with an unsupervised approach. Few works have also explored domain-specific aspect-based summarization such as (Mukherjee et al. (2020)) that focus on data from tourist review domain and (Akhtar et al. (2017)) that focus on dataset of hotel reviews. (Tang et al. (2016)) developed a deep memory network for aspect-level sentiment classification, emphasizing the extraction of aspects within a document and these are relevant for aspect-based summarization. Again, (Wang et al. (2016)) proposed an attention-based LSTM model which helps identify and emphasize important aspects and these are used for aspect-based summarization.",
            "score": 0.5292232293686118,
            "section_title": "Generic Summarization",
            "char_start_offset": 4951,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1690
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 268,
                    "matchedPaperCorpusId": "133195"
                },
                {
                    "start": 806,
                    "end": 830,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 844,
                    "end": 865,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 868,
                    "end": 889,
                    "matchedPaperCorpusId": "226965071"
                },
                {
                    "start": 974,
                    "end": 996,
                    "matchedPaperCorpusId": "207969843"
                },
                {
                    "start": 1174,
                    "end": 1198,
                    "matchedPaperCorpusId": "219531507"
                },
                {
                    "start": 1250,
                    "end": 1271,
                    "matchedPaperCorpusId": "67264987"
                },
                {
                    "start": 1527,
                    "end": 1546,
                    "matchedPaperCorpusId": "18993998"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86865234375
        },
        {
            "corpus_id": "267069444",
            "title": "Synergizing Machine Learning & Symbolic Methods: A Survey on Hybrid Approaches to Natural Language Processing",
            "text": "Researching in this direction, Narayan et al. (2018) attempted to enforce the generation of topic-aware summaries by integrating the topic models with neural approaches. The authors first applied the topic model to the source document and input the topic distribution as an additional input of an attention-based convolutional encoder-decoder model. This enabled the model to associate each word in the document with key topics and condition the output words on the topic distribution of the document. Here, the Latent Dirichlet Allocation (LDA) model (Blei et al., 2003) was used to extract the topics from input documents. \n\nSimilar to abstractive summarization, extractive summarization techniques have also adopted hybrid approaches to effectively model cross-sentence relations prior to the selection of summary-worthy sentences from the source. Wang et al. (2020a) proposed a heterogeneous graph-based neural network to model the inter-sentence relationships. The authors constructed a heterogeneous graph by modeling words and sentences as nodes in the graph. Semantic features of the nodes and edges were modeled using various techniques, including Convolutional Neural Network (CNN) and Bidirectional Long Short-Term Memory (BiLSTM) based sentence representation and TF-IDF-based edge weights. Graph attention networks (GAN) combined with transformers were used to obtain the final representation of nodes. Finally, the authors chose sentence nodes in the heterogeneous graph for summary generation via node classification. Different from this approach, Xu et al. (2020) modeled the source document as a Rhetorical Structure Theory (RST) graph and a coreference graph, to capture long-term dependencies among the discourse units in the input document. Here, the coreference graph was constructed using the entities and their coreferences. Both the document and graph were encoded using a BERT model and GCN respectively, and the encoded information was used to predict whether the input sentence should appear in the summary or not.",
            "score": 0.52919033484907,
            "section_title": "Text Summarization",
            "char_start_offset": 37544,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 624
                },
                {
                    "start": 627,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 2041
                }
            ],
            "ref_mentions": [
                {
                    "start": 31,
                    "end": 52,
                    "matchedPaperCorpusId": "215768182"
                },
                {
                    "start": 552,
                    "end": 571,
                    "matchedPaperCorpusId": "3177797"
                },
                {
                    "start": 851,
                    "end": 870,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 1563,
                    "end": 1579,
                    "matchedPaperCorpusId": "219036690"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9521484375
        },
        {
            "corpus_id": "277954872",
            "title": "Automatic Text Summarization (ATS) for Research Documents in Sorani Kurdish",
            "text": "Nikolov et al. (2018) set forward a direction for text summarization using scientific articles both as a data basis for multi-sentence training and an inspiration source for document structure. They argue that the scientific paper format is structurally already containing summaries, in the form of the title and the abstract. They introduce two new datasets: title-gen, containing title-abstract pairs from biomed papers, and abstract-gen, containing abstract-body pairs from across domains. They evaluated a number of the extraction and abstraction neural compiled summarization approaches on these datasets. On the extraction side are the tfidf-emb and rwmd-rank approaches; on the abstraction side, there are the Long Short-Term Memory (LSTM), Convolutional Encoder-Decoder models (FConv), and Char-Level Encoder-Decoder models (C2C). The results indicate that the extractive approaches perform quite well, but the abstractive models, specifically fconv, perform best in the ability to generate coherent and relevant summaries.",
            "score": 0.5272835837233556,
            "section_title": "ATS in Sorani Kurdish",
            "char_start_offset": 8613,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1031
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6748046875
        },
        {
            "corpus_id": "270674365",
            "title": "Summarizing Business News: Evaluating BART, T5, and PEGASUS for Effective Information Extraction",
            "text": "The era of abstractive summarization began with this revolution, in which models were able to produce summaries that resembled those of humans by rephrasing and paraphrasing the original text.Abstractive methods, as opposed to extractive summarization, concentrated on comprehending the context and meaning of the content, enabling a more nuanced representation in the summary.This was a big step forward, as models were now able to understand the underlying semantics and produce more concise but still coherent versions of the original content, going beyond simple sentence selection.\n\nThe development of text summarization techniques over time can be summarized as a trip from rule-based simplicity to statistical analysis and finally to the revolutionary power of deep learning.The transition from extractive to abstractive summarization is indicative of an ongoing effort to capture the subtle meaning of textual content, which has led to the development of advanced methodologies used in modern NLP.Table 1 shows the summarized literature reviewof related works.\n\nAbstractive summarization: Abstractive summarization is the process of generating more human-like, concise summaries using NLP [9].Text summarization tasks have been very famous due to the fastmoving life in the 21st century.Let's look at the most used approaches:\n\n(1) Transformer-Based Techniques: Transformer-based techniques are the techniques we have focused on in this paper.These techniques have gained popularity because of their ability to produce human-like, accurate, and concise summaries.\n\n(2) Pointer Generator Networks: These networks are known to combine abstractive and extractive techniques to address the difficulties in content selection and fluency.These, however, have a few drawbacks, which include limited semantics understanding cannot handle rare words or new words well.\n\n(3) Graph-Based Approaches: Graph based Neural Networks (GNNs) are used when the document structure and relationships between the sentences are very important.\n\n(4) Reinforcement Learning approaches: These models are re-trained to generate more human-like summaries until we achieve a desired output.\n\nExtractive summarization:\n\nUsing key phrases or sentences that are extracted straight from the original content, extractive summarizing techniques create summaries.Below is a quick summary of the extractive summarization techniques that were discussed:",
            "score": 0.5267838304032779,
            "section_title": "RELATED WORK",
            "char_start_offset": 6873,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 192,
                    "end": 377
                },
                {
                    "start": 377,
                    "end": 586
                },
                {
                    "start": 588,
                    "end": 782
                },
                {
                    "start": 782,
                    "end": 1005
                },
                {
                    "start": 1005,
                    "end": 1068
                },
                {
                    "start": 1070,
                    "end": 1201
                },
                {
                    "start": 1201,
                    "end": 1295
                },
                {
                    "start": 1295,
                    "end": 1334
                },
                {
                    "start": 1336,
                    "end": 1451
                },
                {
                    "start": 1451,
                    "end": 1571
                },
                {
                    "start": 1573,
                    "end": 1740
                },
                {
                    "start": 1740,
                    "end": 1867
                },
                {
                    "start": 1869,
                    "end": 2028
                },
                {
                    "start": 2030,
                    "end": 2169
                },
                {
                    "start": 2171,
                    "end": 2196
                },
                {
                    "start": 2198,
                    "end": 2335
                },
                {
                    "start": 2335,
                    "end": 2423
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5947265625
        },
        {
            "corpus_id": "246388617",
            "title": "COVIDSum: A linguistically enriched SciBERT-based summarization model for COVID-19 scientific papers",
            "text": "A word in the sequence can connect more other words when increasing window size. To verify whether the word co-occurrence graph with more edges would lead to better abstractive summarization performance, we conduct experiments on word co-occurrence graphs with different window sizes. We construct the word cooccurrence graph using a strategy in which the same words in different positions of the sentence share their neighbors. Though this  scheme allows diverse information to propagate to identical words and alleviate the polysemy problem to some extent, the number of edges surges as the window size increases because their neighbors are shared. \n\nTo this end, we restrict the number of neighbors for each node in the word co-occurrence graph to five times of the corresponding window size at most. We set the window size of the word co-occurrence graph to 2, 3, 5, and 10, respectively, while settings of the other components remain default as represented in Table 2. We run our proposed abstractive framework with Heuristic1 sentence extraction method, SciBERT encoder, and highway networks as the feature fusion module on the CORD-19 test set. Table 8 shows experimental results. \n\nAs shown in Table 8, the word co-occurrence graph with a window size of 3 achieves the best performance of abstractive summarization. We observe that the ROUGE scores increase a little when the window size has not reached 3, but the ROUGE scores drop when the window size is above 3. We attribute the above results to the over-smoothing issue, that the node representations become indistinguishable when the graph neural networks go deeper, we deem that increasing number of connections in word co-occurrence graphs might hurt the summarization performance. Thus, we can conclude that the window size of the word co-occurrence graph indirectly affects the performance of our abstractive summarization framework. The underlying insight behind this observation is that the number of connections is associated with the over-smoothing issue for graph attention networks.",
            "score": 0.5256879324985996,
            "section_title": "Performance with different window size of word co-occurrence graph",
            "char_start_offset": 42885,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 650
                },
                {
                    "start": 653,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1187
                },
                {
                    "start": 1190,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2056
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7314453125
        },
        {
            "corpus_id": "277284940",
            "title": "Global and Local Context Fusion in Heterogeneous Graph Neural Network for Summarizing Lengthy Scientific Documents",
            "text": "The proposed Global and Local Context Fusion in Heterogeneous Graph Neural Networks introduces an innovative approach to extractive summarization of lengthy scientific documents. By leveraging GloVe embeddings, CNN, BiL-STM encoders, and an advanced graph-based architecture, the model effectively integrates both global and local semantic contexts. This method outperforms current state-of-the-art models on benchmark datasets such as arXiv and PubMed. Its ability to process long documents with complex structures while maintaining relatively low computational complexity makes it a promising tool for future applications in fields like medical record analysis and scientific research. \n\nThe proposed approach has several limitations that require further investigation. While LDA is a relatively simple and effective topic modeling technique with low complexity, it may not be well-suited for handling flexible, dynamic, and diverse document structures. Additionally, there is significant room for improvement in terms of scalability, particularly for very large datasets, as well as in generalizing the approach to work effectively across multiple languages, including low-resource languages. Furthermore, although the model incorporates both local and global semantics of the contexts, it occasionally prioritizes more prominent information over diverse content, which can reduce the representativeness of the generated summaries. \n\nThe future direction aims to build upon these discoveries and address the limitations mentioned earlier. To enhance the performance and flexibility of topic modeling, we plan to extend existing approaches based on neural topic models, such as the Neural Variational Document Model (NVDM). The model's effectiveness could be further improved for multilingual summarization by incorporating language-specific embeddings or leveraging pre-trained multilingual transformers like XLM-R. Additionally, exploring dynamic graph architectures or hierarchical attention mechanisms-such as optimizing for real-time processing through techniques like model pruning and quantization-could yield significant advancements. \n\nIn conclusion, integrating the model with abstractive summarization capabilities and evaluating it across diverse datasets would strengthen the robustness of the results. This would also demonstrate that the proposed model is a versatile and advanced solution for summarizing long documents. \n\nMUHAMMAD UMAIR , photograph and biography not available at the time of publication. \n\nATIF KHAN , photograph and biography not available at the time of publication.",
            "score": 0.5246068791433227,
            "section_title": "V. CONCLUSION AND FUTURE DIRECTIONS",
            "char_start_offset": 56989,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 687
                },
                {
                    "start": 690,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1434
                },
                {
                    "start": 1437,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2144
                },
                {
                    "start": 2147,
                    "end": 2317
                },
                {
                    "start": 2318,
                    "end": 2438
                },
                {
                    "start": 2441,
                    "end": 2524
                },
                {
                    "start": 2527,
                    "end": 2605
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.958984375
        },
        {
            "corpus_id": "258571171",
            "title": "Adapting Static and Contextual Representations for Policy Gradient-Based Summarization",
            "text": "Instances with a higher score are then extracted and rewritten with the leverage of the Pointer-Generator Network [27]. \n\nRL aims at training an agent to interact with an environment for the purpose of generating a feasible policy which could perform a proper action in a given state in order to maximize rewards [28]. It has been broadly and increasingly adopted in many applications such as autonomous driving, robotics and video games. The research area of text sum-marization is of no exception, where many related approaches have been proposed and have yielded state-of-the-art results [29]. An extractive technique based on a graph and reinforcement neural network, GoSum, is proposed to build a heterogeneous graph for the source document and then apply a policy gradient to optimize the reward scores for the summarization of long papers [30]. As coherence is the key factor to produce a high-quality summary, efforts to develop learning models which could take both in-sentence dependencies and cross-sentence semantics into consideration are necessary. The Reinforced Neural Extractive Summarization model [31] learns a neural extractive summarizer based on the REINFORCE algorithm, where the reward score is mainly provided by another neural model, ARC-II, which is used to measure the degree of coherence between sentences [32]. Unlike traditional abstractive summarization approaches which assume a deterministic target distribution and are trained by maximum likelihood estimation, the BRIO model applies non-deterministic distribution to assign probability mass for candidate summaries based on their quality [33]. To imitate the human procedure when writing a summary, FusionSum [34] devises a module to combine salient sentences into different groups and another module to rewrite the summary sentences for each group. The training mechanism between the two modules is based on an end-to-end cooperative reinforcement learning. \n\nBased on summarization approaches, extractive methods only extract information from the original text, which could be more factually accurate but may also introduce redundant or uninformative text. Abstractive approaches can produce more concise and readable summaries, but the text generated by the model may contain errors or inaccuracies. Based on learning methods, supervised summarizers can achieve more reliable results but need training data with human input.",
            "score": 0.520626756354771,
            "section_title": "Related Work",
            "char_start_offset": 9472,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 122,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1944
                },
                {
                    "start": 1947,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2288
                },
                {
                    "start": 2289,
                    "end": 2413
                }
            ],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 118,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 591,
                    "end": 595,
                    "matchedPaperCorpusId": "238208308"
                },
                {
                    "start": 1116,
                    "end": 1120,
                    "matchedPaperCorpusId": "4999752"
                },
                {
                    "start": 1335,
                    "end": 1339,
                    "matchedPaperCorpusId": "4497054"
                },
                {
                    "start": 1695,
                    "end": 1699,
                    "matchedPaperCorpusId": "247275013"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92578125
        },
        {
            "corpus_id": "253708345",
            "title": "GoSum: Extractive Summarization of Long Documents by Reinforcement Learning and Graph Organized discourse state",
            "text": "Early summarization solutions are graph-based unsupervised methods (Erkan and Radev, 2004), relying on explicit surface features. They construct a similarity graph between sentences and formulate extractive summarization as a task of ranking nodes. Recently, researchers use graph neural network on supervised summarization. HSG (Wang et al., 2020) was the first to construct a heterogeneous graph neural network for extractive document summarization. HahSum (Jia et al., 2020) considers inter-sentence redundancy in graph construction. HEROS (Zhu et al., 2021) applies graph-based to the long text field and uses the information about input article discourse. All these methods treat sentences and words as nodes in a graph. Based on the RST tree, DiscoSum (Xu et al., 2020)  a graph to capture the long-range dependencies among discourse units, with Elementary Discourse Units as the nodes in a graph. To some extent, the graph-based approach solves the quadratic computational and memory complexities encoded using Transformer and works well with the structural information of the input. Therefore, we choose to use GNNs for GoSum.",
            "score": 0.5204356573180369,
            "section_title": "Graph-based Extractive Summarization",
            "char_start_offset": 6161,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1134
                }
            ],
            "ref_mentions": [
                {
                    "start": 329,
                    "end": 348,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 459,
                    "end": 477,
                    "matchedPaperCorpusId": "226262332"
                },
                {
                    "start": 543,
                    "end": 561,
                    "matchedPaperCorpusId": "240230741"
                },
                {
                    "start": 758,
                    "end": 775,
                    "matchedPaperCorpusId": "219036690"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95556640625
        },
        {
            "corpus_id": "273662136",
            "title": "GraphLSS: Integrating Lexical, Structural, and Semantic Features for Long Document Extractive Summarization",
            "text": "Heterogeneous graph neural networks have recently gained attention for long document summarization, modeling the extraction as a node classification task. Although effective, these models often require external tools or additional machine learning models to define graph components, producing highly complex and less intuitive structures. We present GraphLSS, a heterogeneous graph construction for long document extractive summarization, incorporating Lexical, Structural, and Semantic features. It defines two levels of information (words and sentences) and four types of edges (sentence semantic similarity, sentence occurrence order, word in sentence, and word semantic similarity) without any need for auxiliary learning models. Experiments on two benchmark datasets show that GraphLSS is competitive with top-performing graph-based methods, outperforming recent non-graph models. We release our code on GitHub.",
            "score": 0.5204356573180369,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9599609375
        },
        {
            "corpus_id": "277284940",
            "title": "Global and Local Context Fusion in Heterogeneous Graph Neural Network for Summarizing Lengthy Scientific Documents",
            "text": "The study presented by [10] investigates an unsupervised, discourseaware hierarchical and ranking graph-based model, meticulously tailored for comprehensive scientific documents. It takes advantage of the internal relations and the relations between different sentences to improve document comprehension. Nonetheless, this methodology's dependence on external tools can precipitate complications, including the production of outputs with semantic fragmentation [11]. \n\nThe research conducted by [9] established a word-sentence document graph grounded on word appearance. Nonetheless, this statistical method of graph construction is limited in its ability to encapsulate relationships at the semantic level. The research conducted by [6] presented a Heterogeneous Transformer-based model designed specifically for extractive summarization of large texts that contains novel concepts of multi-scale sparse attention with explicit categorization of tokens (words), entities, and sentences as separate nodes. The authors of [7] augmented the heterogeneous graph structure with the introduction of passage nodes, alongside the existing word and sentence nodes. This enhancement aimed to improve the overall sentence representation for long document summarization. The researchers in [12] focused on the extractive summarization of scholarly articles by proposing an innovative approach. This approach synergizes the strengths of BERT with BiGRU (Bidirectional Gated Recurrent Units), a recurrent neural network adept at capturing sequential dependencies within the text to extract salient information effectively. \n\nAnother pivotal aspect of summarization involves the modeling of global information, which significantly influences the selection of sentences [13]. The authors of [3], employed BERT for contextual sentence representations and simultaneously conducted training with latent topics to capture global context, making use of Neural Topic Model (NTM) for this objective. It should be noted that the pre-trained model boosts the summarization process but suffers from limitations such as a fixed maximum input length and the computational overhead associated with the quadratic complexity of their self-attention mechanism, making them less scalable and inefficient for processing extensive scientific texts. The authors in [8] presented a graph structure augmented with latent topical information, where topics are delineated utilizing clustering techniques such as K-means and Gaussian Mixture Models (GMM).",
            "score": 0.5204356573180369,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2191,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 466
                },
                {
                    "start": 469,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1609
                },
                {
                    "start": 1612,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2314
                },
                {
                    "start": 2315,
                    "end": 2515
                }
            ],
            "ref_mentions": [
                {
                    "start": 1021,
                    "end": 1024,
                    "matchedPaperCorpusId": "252818946"
                },
                {
                    "start": 1279,
                    "end": 1283,
                    "matchedPaperCorpusId": "261471547"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9365234375
        },
        {
            "corpus_id": "273784877",
            "title": "Cross-Document Distillation via Graph-Based Summarization of Extracted Essential Knowledge",
            "text": "We face this task by using a semantic graph to represent the documents. This approach intuitively aggregates all source information, facilitating the identification and extraction of summary-worthy sentences. Previous contributions have used graph representations for text summarization [21]. Yet, they exhibit the following limitations: (i) they are mainly proposed for extractive summarization [22], [23], [24]; (ii) they are tailored for short texts [25], [26], which diverge highly from MDS settings; (iii) they do not leverage state-of-the-art generative pre-trained language models (PLMs) [27]. \n\nIn light of this, we present G-SEEK-2 (Fig. 1), 3 a graph-based summarization of extracted essential knowledge. Our approach selects the most relevant sentences from a cluster of related documents and feeds them to a PLM to generate the summary. Technically, we model documents with a heterogeneous graph composed of multiple semantic edges and nodes of different granularities (i.e., keywords and sentences). Then, a graph neural network (GNN) is trained to select the salient sentence nodes-soft labeled with a heuristic-which are thereby provided to a PLM to produce the summary. This approach bypasses the limitation of feeding models only until their maximum input size, which otherwise prevents them from fully processing the entire source, leading to performance drop [28]. Furthermore, by modeling a heterogeneous graph, we effectively capture cross-document relationships, a crucial aspect in handling multiple inputs [8]. Experimentally, we benchmark our solution in a realistic low-resource setting where 3 https://disi-unibo-nlp.github.io/publications/ a limited number of labeled training instances are available. This scenario is motivated by two key factors: (i) PLMs exhibit enhanced performance in generating summaries when trained with highly correlated source-target samples [18]; (ii) The limited number of trainable parameters of our learnable module (4M) allows G-SEEK-2 to avoid overfitting with a small number of examples.",
            "score": 0.5199879890026928,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 6284,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 600
                },
                {
                    "start": 603,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 2049
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 291,
                    "matchedPaperCorpusId": "252819333"
                },
                {
                    "start": 396,
                    "end": 400,
                    "matchedPaperCorpusId": "226262332"
                },
                {
                    "start": 402,
                    "end": 406,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "239768911"
                },
                {
                    "start": 453,
                    "end": 457,
                    "matchedPaperCorpusId": "237598718"
                },
                {
                    "start": 459,
                    "end": 463,
                    "matchedPaperCorpusId": "253155963"
                },
                {
                    "start": 595,
                    "end": 599,
                    "matchedPaperCorpusId": "235187330"
                },
                {
                    "start": 1378,
                    "end": 1382,
                    "matchedPaperCorpusId": "259055192"
                },
                {
                    "start": 1530,
                    "end": 1533,
                    "matchedPaperCorpusId": "253428366"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94921875
        },
        {
            "corpus_id": "263256651",
            "title": "A Two-Stage Long Text Summarization Method Based on Discourse Structure",
            "text": "SummaRuNNer is a Recurrent Neural Network (RNN) based sequence model for extractive summarization of documents. It proposes a new training mechanism that uses a generative summary pattern to train extracted tasks. Refresh is a system based on reinforcement learning, trained through global optimization using ROUGE indicators. \n\nSumo is an end-to-end extractive model. It defines the abstract problem as a tree induction problem, utilizing structured attention and iterative structure improvement methods to learn document representation. \n\nNeusum is a novel end-to-end neural network framework for extractive document summarization by jointly learning to score and select sentences. \n\nBanditsum is a novel method for training neural networks to perform single-document extractive summarization without heuristically generated extractive labels. \n\nDiscobert is a discourse-aware neural summarization model that uses a discourse unit as the minimal selection basis to reduce summarization redundancy and leverages two types of discourse graphs as inductive bias to capture long-range dependencies among discourse units.",
            "score": 0.5197815388966314,
            "section_title": "Extractive Text Summarization Models",
            "char_start_offset": 25920,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 326
                },
                {
                    "start": 329,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 538
                },
                {
                    "start": 541,
                    "end": 683
                },
                {
                    "start": 686,
                    "end": 845
                },
                {
                    "start": 848,
                    "end": 1118
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83837890625
        },
        {
            "corpus_id": "269158067",
            "title": "Biomedical semantic text summarizer",
            "text": "Deep learning has gained popularity due to recent technological advancements and decreased memory costs. When appropriate training data is available, neural network summarizers perform better than traditional automatic summarizers with minimal human intervention. In their paper, [25] presents an overview of all the neural network algorithms that are used as state-of-art for summarising text. Researchers have used neural networks in various forms to develop text summarization systems. For example, [26] utilized continuous vectors based on neural networks to create an extractive summarizer and achieved better results. The first abstractive summarizer using CNNs was introduced by [27]. [28] built on this work by creating an abstractive summarizer using CNNs and other neural networks. An RNN with an attentional encoder-decoder was used by [29] to generate an abstractive summary. COPYNET, a sequence-to-sequence learning technique that copies text segments at specific intervals, was introduced by [30]. The off-vocabulary problem was tackled by [31] using a neural network and a pointer-generator approach. The Chinese corpus was used by [32] to generate a summary. [33] described a neural network that uses a distraction strategy to allow users to focus on different input parts. A semantic relevance-based neural network was used by [34] to create semantically important summaries of Chinese data. Finally, [35] used a bidirectional LSTM encoder to generate an extracted summary. [36] Graph-based techniques used supervised and unsupervised learning schemes to create extractive single-document summaries. The goal is to find relevant sentences by extracting statistical aspects from these two approaches and then use graphs to determine sentence coherence. [37] computed word similarity adjacency networks to arbitrate authorship. They used text as a graph to identify the author. [38] employs multi-layer graph approaches for summarising several documents, where nodes represent sentences and edges represent the coherence between the two sentences. In their paper [39] achieved good results on multi-lingual datasets.",
            "score": 0.5196990001781154,
            "section_title": "Neural networks based approaches",
            "char_start_offset": 16335,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2062
                },
                {
                    "start": 2063,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 280,
                    "end": 284,
                    "matchedPaperCorpusId": "251257816"
                },
                {
                    "start": 502,
                    "end": 506,
                    "matchedPaperCorpusId": "17394382"
                },
                {
                    "start": 692,
                    "end": 696,
                    "matchedPaperCorpusId": "133195"
                },
                {
                    "start": 1344,
                    "end": 1348,
                    "matchedPaperCorpusId": "3708647"
                },
                {
                    "start": 1491,
                    "end": 1495,
                    "matchedPaperCorpusId": "181972670"
                },
                {
                    "start": 1769,
                    "end": 1773,
                    "matchedPaperCorpusId": "1114638"
                },
                {
                    "start": 1893,
                    "end": 1897,
                    "matchedPaperCorpusId": "4360899"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85791015625
        },
        {
            "corpus_id": "271855030",
            "title": "MovieSum: An Abstractive Summarization Dataset for Movie Screenplays",
            "text": "We evaluate the MovieSum dataset using several baselines and state-of-the-art neural abstractive summarization models. We first report the Lead-N baseline, which simply outputs the first N tokens of the movie script as the movie summary. We varied the value N to understand the impact of summary length on performance and report results for Lead-512, Lead-768, and Lead-1024. For the extractive baseline, we used TextRank (Mihalcea and Tarau, 2004), a graph-based unsupervised extractive summarization method. For instructiontuned large language models, we used Vicuna 1.5 13B 16K (Zheng et al., 2023), built on Llama-2 (Touvron et al., 2023), and FLAN-UL2 (Tay et al., 2023;Wei et al., 2022) in a zero-shot setting. For fine-tuned models with long inputs, we utilized LongT5 (Guo et al., 2022), PEGASUS-X (Phang et al., 2023), and the Longformer Encoder-Decoder (LED) model (Beltagy et al., 2020). We fully finetuned these models and report results on the test set. The implementation details of the models are mentioned in Appendix A.",
            "score": 0.5193813287331865,
            "section_title": "Experiments",
            "char_start_offset": 9323,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1036
                }
            ],
            "ref_mentions": [
                {
                    "start": 422,
                    "end": 447,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 657,
                    "end": 675,
                    "matchedPaperCorpusId": "252780443"
                },
                {
                    "start": 675,
                    "end": 692,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 776,
                    "end": 794,
                    "matchedPaperCorpusId": "245144820"
                },
                {
                    "start": 806,
                    "end": 826,
                    "matchedPaperCorpusId": "251442728"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7548828125
        },
        {
            "corpus_id": "259108204",
            "title": "Absformer: Transformer-based Model for Unsupervised Multi-Document Abstractive Summarization",
            "text": "Multi-document summarization (MDS) refers to the task of summarizing the text in multiple documents into a concise summary. The generated summary can save the time of reading many documents by providing the important content in the form of a few sentences. Abstractive MDS aims to generate a coherent and fluent summary for multiple documents using natural language generation techniques. In this paper, we consider the unsupervised abstractive MDS setting where there are only documents with no groundtruh summaries provided, and we propose Absformer, a new Transformer-based method for unsupervised abstractive summary generation. Our method consists of a first step where we pretrain a Transformer-based encoder using the masked language modeling (MLM) objective as the pretraining task in order to cluster the documents into semantically similar groups; and a second step where we train a Transformer-based decoder to generate abstractive summaries for the clusters of documents. To our knowledge, we are the first to successfully incorporate a Transformer-based model to solve the unsupervised abstractive MDS task. We evaluate our approach using three real-world datasets from different domains, and we demonstrate both substantial improvements in terms of evaluation metrics over state-of-the-art abstractive-based methods, and generalization to datasets from different domains.",
            "score": 0.5191936449715108,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9365234375
        },
        {
            "corpus_id": "248406121",
            "title": "An End-to-End Dialogue Summarization System for Sales Calls",
            "text": ";Barzilay et al., 1999). Advancements in capabilities of deep neural models led to works such as Rush et al. (2015) where a seq-2-seq attention-based model is used for abstractive summarization. See et al. (2017) overcomes some of the former work's limitations by introducing a pointer generator network that has the ability to copy words from the source document. A major advancement in the field of deep neural models was the introduction of Transformer architecture (Vaswani et al., 2017), which is the basis for current state-of-the-art summarization approaches. Recently, several powerful Transformer-based models have been developed and showed remarkable results on various benchmark summarization tasks (Lewis et al., 2020;Zhang et al., 2019a;Raffel et al., 2020) .\n\nDialogue Summarization The task of dialogue summarization has been witnessing many commonalities as document summarization as well as new techniques for handling unique structures of various dialogue types. Early works in the domain suggested tackling the problem using extractive methods (Murray et al., 2005;Riedhammer et al., 2008). Shang et al. (2018) used a pure unsupervised graph-based method for keyword extraction and sentence compression. Goo and Chen (2018) proposed to explicitly model relationships between dialogue acts using attention-based sentence-gated mechanism. Chen and Yang (2020) extracted Transformer-based representations for different views of dialogues, conditioned on view representations, to generate summaries using a second Transformer. Zhu et al. (2020) presented a hierarchical Transformer architecture to encompass the structure of dialogues.",
            "score": 0.5189766987051262,
            "section_title": "Introduction",
            "char_start_offset": 4116,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1,
                    "end": 23,
                    "matchedPaperCorpusId": "7031344"
                },
                {
                    "start": 97,
                    "end": 115,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 195,
                    "end": 212,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 469,
                    "end": 491,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 710,
                    "end": 730,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 750,
                    "end": 770,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1063,
                    "end": 1084,
                    "matchedPaperCorpusId": "5776046"
                },
                {
                    "start": 1084,
                    "end": 1108,
                    "matchedPaperCorpusId": "8200487"
                },
                {
                    "start": 1110,
                    "end": 1129,
                    "matchedPaperCorpusId": "44243501"
                },
                {
                    "start": 1542,
                    "end": 1559,
                    "matchedPaperCorpusId": "221749138"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92041015625
        },
        {
            "corpus_id": "256460931",
            "title": "Semantic Simplification for Sentiment Classification",
            "text": "Intuitively, such issues can be alleviated by having a structural representation of semantic information, which treats concepts as nodes and builds structural relations between nodes, making it easy to find the important and sentiment-driven content. \n\nExplicit structures are more interpretable compared to neural representations and have been shown to be useful in many applications (Liao et al., 2018;Song et al., 2019;Xu et al., 2020;Bai et al., 2021). \n\nIn this study, we employ Abstract Meaning Representation (AMR) (Banarescu et al., 2013) for simplified clause generation in order to better exploit the semantic representation of the original text. As shown in Figure 1(b), AMR-based simplified semantic graph models the original text using rooted directed acyclic graph, which highlights its main concepts and semantic relations while abstracting away function words. It can thus potentially offer core concepts and explicit structures needed for aggregating the meaning of the original text. \n\nExisting work on AMR parsing focuses on the sentence level. However, as shown in the right green box in Figure 2, the semantic structure of an original text contains rich cross-sentence coreference links, and lots of duplicated and irrelevant information. To this end, we propose a simplified graph extraction algorithm to automatically derive a document-level simplified semantic graph from sentence-level AMRs, by merging co-reference links and pruning duplicate and irrelevant structures. \n\nIn summary, we firstly use a sequence-tostructure network to generate the AMR-based semantic graphs from sentences in original text. We then use a simplified graph extraction model to merge the sentence-level semantic graphs and extract a document-level simplified semantic graph. Thirdly, we employ a structure-to-sequence model to generate the simplified clause from the simplified semantic graph. Afterward, we integrate the simplified clause and original review text for sentiment classification. \n\nDetailed evaluation shows that our model significantly advances the state-of-the-art performance on several benchmark datasets. The results also show that the simplified clause is very useful for sentiment classification, and indicates AMR is beneficial for simplified clause generation.",
            "score": 0.5183280883444035,
            "section_title": "Introduction",
            "char_start_offset": 1928,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 253,
                    "end": 456
                },
                {
                    "start": 459,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1001
                },
                {
                    "start": 1004,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1495
                },
                {
                    "start": 1498,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 1998
                },
                {
                    "start": 2001,
                    "end": 2128
                },
                {
                    "start": 2129,
                    "end": 2288
                }
            ],
            "ref_mentions": [
                {
                    "start": 385,
                    "end": 404,
                    "matchedPaperCorpusId": "49210924"
                },
                {
                    "start": 404,
                    "end": 422,
                    "matchedPaperCorpusId": "67769427"
                },
                {
                    "start": 422,
                    "end": 438,
                    "matchedPaperCorpusId": "222133032"
                },
                {
                    "start": 438,
                    "end": 455,
                    "matchedPaperCorpusId": "235125766"
                },
                {
                    "start": 522,
                    "end": 546,
                    "matchedPaperCorpusId": "7771402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32470703125
        },
        {
            "corpus_id": "271525553",
            "title": "Abstractive text summarization: State of the art, challenges, and improvements",
            "text": "Previous research on automatic summarization has oftentimes focused on specific topics, for example, abstractive summarization utilizing neural approaches, deep learning-based models, sequence-to-sequence based models, and extractive summariza-tion. Some have included the challenges and limitations of abstractive summarization, while others have focused on the best way to improve and evaluate it. \n\nNonetheless, there is a need for a more comprehensive analysis that covers state-of-the-art methods, challenges, solutions, comparisons, limitations, and future improvements -providing a structured overview to advance abstractive summarization research. \n\nTable 1 gives an overall comparison of our review with various survey papers that are available in the literature. Unlike prior studies, our work examines state-of-the-art methods, challenges, solutions, comparisons, limitations and charts out future improvements -providing researchers an extensive overview to advance abstractive summarization research. The following are the main contributions of this study: \n\n\u2022 Overview of the state-of-the-art techniques in abstractive text summarization: This study incorporates information from a plethora of relevant studies to give an overview of the ongoing methodologies utilized in abstractive summarization. This can help researchers and practitioners familiarize themselves quickly with the most recent developments and patterns. \n\n\u2022 Comparative Analysis of Models: This study includes a comparative analysis of models in abstractive summarization, focusing on scales, training time, and resource consumption, among other categories. This unique dimension offers practical insights, aiding in the selection of efficient models and enriching the field with valuable, often overlooked information. \n\n\u2022 Identification of challenges in abstractive summarization: This study presents current issues and challenges in abstractive summarization by consolidating information from various research papers, for instance, the challenge of generating coherent and grammatically accurate summaries. \n\nOur work can help specialists focus on these areas and develop innovative solutions by highlighting such issues. \n\n\u2022 Discussion of potential improvements in abstractive summarization: This study explores strategies to enhance abstractive summarization, for example, incorporating knowledge and various techniques to generate factually accurate and coherent summaries. This can aid researchers in finding better approaches to generate high-quality abstractive summarization frameworks.",
            "score": 0.5180623019051809,
            "section_title": "Prior Surveys on Abstractive Summarization",
            "char_start_offset": 8346,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 655
                },
                {
                    "start": 658,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1069
                },
                {
                    "start": 1072,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1435
                },
                {
                    "start": 1438,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1801
                },
                {
                    "start": 1804,
                    "end": 2091
                },
                {
                    "start": 2094,
                    "end": 2206
                },
                {
                    "start": 2209,
                    "end": 2461
                },
                {
                    "start": 2462,
                    "end": 2578
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52880859375
        },
        {
            "corpus_id": "245827982",
            "title": "An Unsupervised Masking Objective for Abstractive Multi-Document News Summarization",
            "text": "While any neural multi-document abstractive summarization model could be used (e.g., Liu and Lapata (2019); Liu et al. (2018)), we use the Hierarchical MMR-Attention Pointer-generator network (Hi-MAP) model (Fabbri et al., 2019) because it was developed on the Multi-News dataset. 4 As the name implies, the Hi-MAP model is composed of a hierarchical pointer-generator network (See et al., 2017) that has an additional attention mechanism over MMR-ranked (Carbonell and Goldstein, 1998) input sentences. Thus, sentence-level extractive summarization scores balancing relevancy and redundancy can be effectively combined with a commonly used single document neural summarization model capable of learning when to produce new output or copy from the source vocabulary. We refer the reader to Fabbri et al. (2019) for hyperparameter settings and implementation details.  (Fabbri et al., 2019). The outputs from each model were truncated to meet the length requirement. The distribution of output lengths before truncation for each model is shown in the right-hand column. Results are divided into unsupervised baselines, supervised baselines, and our proposed unsupevised masked objective. The best unsupervised result in each column is bolded. Models with '\u2265 3' in their name indicate that they were only trained on document collections that contained at least three input documents. 'Bigram Overlap Src Doc' is a baseline that simply uses the truncated masking candidate as the output document. 'Random Src Doc' selects a random input document. (p 0.05) and not significantly different from the best supervised model on ROUGE-1 (p = 0.1). We attribute MeanSum's poor performance to it losing supervision from its review rating classifier and the subtle task differences between its original domain (Yelp reviews) and the news domain. \n\nHuman Evaluation We ask two native English speaking human annotators to evaluate 50 randomly sampled test document clusters and their summaries shuffled between our best bigram method and MeanSum, and our best method and the best supervised Hi-MAP.",
            "score": 0.5177053158680087,
            "section_title": "Neural Abstractive Summ. Model",
            "char_start_offset": 9876,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1832
                },
                {
                    "start": 1835,
                    "end": 2083
                }
            ],
            "ref_mentions": [
                {
                    "start": 85,
                    "end": 106,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 207,
                    "end": 228,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 377,
                    "end": 395,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 790,
                    "end": 810,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 868,
                    "end": 889,
                    "matchedPaperCorpusId": "174799390"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69873046875
        },
        {
            "corpus_id": "255125461",
            "title": "STRUDEL: Structured Dialogue Summarization for Dialogue Comprehension",
            "text": "Abstractive summarization aims to generate a concise summary of a text by producing a paraphrasing of the main contents using different vocabulary, rather than simply extracting the important sentences, which is referred to as extractive summarization. A popular approach to produce abstractive summaries of long documents is via neural abstractive summarization by using a singular extractive step to condition the transformer language model before generating a summary (Zhang and Zhao, 2021). Some other methods also take the structure of the dialogues into consideration when generating a single free-form abstractive summarization. For example, Wu et al. (2021) presented BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph and a graph-based encoder-decoder model to improve summary generation process by leveraging the graph structure. Villmow et al. (2021) improved source code summarization tasks using self-attention with relative position representations to consider structural relationships between nodes which can encode movements between any pair of nodes in the tree. \n\nAbstractive summarization has also been applied to solve NLP-related tasks such as text classification, news summarization, and headline generation. Furthermore, the generation of summaries can be integrated into these systems as an intermediate stage to reduce the length of documents. Mahalakshmi and Fatima (2022) presented a new text summarization model to retrieve information with deep learning methods. Du and Gao (2021) migrated the large-scale generic summarization datasets into query-focused datasets and proposed a model called SQAS, which can extract the rea-soning information by understanding the source document via the question-answering model.",
            "score": 0.5174241233746257,
            "section_title": "Abstractive Summarization",
            "char_start_offset": 2129,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1125
                },
                {
                    "start": 1128,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1789
                }
            ],
            "ref_mentions": [
                {
                    "start": 886,
                    "end": 907,
                    "matchedPaperCorpusId": "219964346"
                },
                {
                    "start": 1415,
                    "end": 1444,
                    "matchedPaperCorpusId": "246742430"
                },
                {
                    "start": 1538,
                    "end": 1555,
                    "matchedPaperCorpusId": "245978993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "256826757",
            "title": "PDSum: Prototype-driven Continuous Summarization of Evolving Multi-document Sets Stream",
            "text": "For the new task EMDS, we prepared strong baselines by adopting a centroid-based model [13,38] with a pretrained language model: DocCent and SentCent with document-and sentence-based centers, respectively, and their incremental versions IncDocCent, and IncSentCent. We also compared three popular unsupervised algorithms for multi-document summarization: the graph-based model Lexrank [9], the state-of-the-art extractive model Summpip [57], and the state-of-the-art abstractive model PRIMERA [47]. We fed each document set in a context to them so that they can only infer the temporally correlated documents to update their set summaries. See Appendix A.2 for details.",
            "score": 0.5172465834263585,
            "section_title": "Compared Algorithms.",
            "char_start_offset": 22425,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 669
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 91,
                    "matchedPaperCorpusId": "1564849"
                },
                {
                    "start": 91,
                    "end": 94,
                    "matchedPaperCorpusId": "2346086"
                },
                {
                    "start": 385,
                    "end": 388,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 436,
                    "end": 440,
                    "matchedPaperCorpusId": "220633461"
                },
                {
                    "start": 493,
                    "end": 497,
                    "matchedPaperCorpusId": "247519084"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6318359375
        },
        {
            "corpus_id": "250118028",
            "title": "An Empirical Survey on Long Document Summarization: Datasets, Models, and Metrics",
            "text": "However, given the effectiveness of sequence-to-sequence neural models, one limitation of this work is that it only utilizes a decoder framework to generate the final summary rather than an encoder-decoder framework that most subsequent works on abstractive and hybrid summarization approach do. Consequently, LoBART [77] proposes a hybrid summarization system that completes a summary generation in two separate steps, (i) content selection: using a multi-task RNN, select salient content from the original source document until the total text output reaches the limit of the sequenceto-sequence pre-trained BART model and (ii) abstractive summarization: summarize the carefully selected subset using a pre-trained BART model with efficient transformer mechanism. SEAL [134] presents a generalized encoder-decoder framework for transformer-based long document summarization and proposed an abstractive summarization system that selects salient content and dynamically choose segments of the selected content for the decoder to attend and summarize in an end-to-end manner. The architecture, however, did not attempt in exploiting the large-scale pre-trained models that were used in most summarization research works. Lastly, facing a similar issue, development in the open-domain question-answering and knowledge-intensive language tasks reflect an interesting parallel with the progress in the long document summarization domain [67,91].",
            "score": 0.5160404095453812,
            "section_title": "6) Discourse Bias",
            "char_start_offset": 53521,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1440
                }
            ],
            "ref_mentions": [
                {
                    "start": 317,
                    "end": 321,
                    "matchedPaperCorpusId": "235262720"
                },
                {
                    "start": 1436,
                    "end": 1439,
                    "matchedPaperCorpusId": "221507798"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69921875
        },
        {
            "corpus_id": "258865905",
            "title": "Neural Summarization of Electronic Health Records",
            "text": "In this section, we briefly review extractive and abstractive approaches that have been used in non-medical documents. It includes several models central to this paper. Recent works on text summarization algorithms can be broadly classified based on the type of summary generated -extractive and abstractive. Extractive summarization involves taking a subset of phrases and sentences from the input documents and concatenating them to form a summary. On the other hand, abstractive summarization algorithms produce summaries based on their own vocabulary and the concepts they associate with the input document, not necessarily using the exact words of a source article. One of the models in the extractive summarization category is the Luhn summarizer [10]. It selects sentences based on the maximum number of significant words present in a particular sentence. The significance of words is determined through Term Frequency -Inverse Document Frequency, also known as TF-IDF [11]. A common baseline model for extractive summarization is LEAD-3, which is another extractive summarization solution that takes the first three sentences of the input document and sets them as the document's summary. Another sub-category of extractive summary algorithms is topic-based approaches. For example, the model designed by Harabagiu et al. [12] represents topic themes based on events that frequently occur over a set of documentation. They illustrate five ways of determining such frequencies -topic signatures, enhanced topic signatures, thematic signatures, modeling documents' content structure, and templates. There are also graph-based [13,14] approaches that use text representation in a graph where words or sentences are represented as nodes and semantically-related text elements are connected through edges. Finally, discourse-based approaches [15] integrate linguistic knowledge to represent the connections within and between sentences. \n\nA state-of-the-art model for abstractive text summarization is the Bidirectional and Auto-Regressive Transformer (BART) [16]. As the name suggests, BART employs a standard Transformer-based neural machine translation architecture with a bidirectional encoder and a left-to-right decoder.",
            "score": 0.5158805207053547,
            "section_title": "General Text Summarization",
            "char_start_offset": 3099,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1939
                },
                {
                    "start": 1942,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2229
                }
            ],
            "ref_mentions": [
                {
                    "start": 753,
                    "end": 757,
                    "matchedPaperCorpusId": "15475171"
                },
                {
                    "start": 976,
                    "end": 980,
                    "matchedPaperCorpusId": "45793141"
                },
                {
                    "start": 1330,
                    "end": 1334,
                    "matchedPaperCorpusId": "14336759"
                },
                {
                    "start": 1632,
                    "end": 1636,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1636,
                    "end": 1639,
                    "matchedPaperCorpusId": "15709889"
                },
                {
                    "start": 1845,
                    "end": 1849,
                    "matchedPaperCorpusId": "60514661"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8701171875
        },
        {
            "corpus_id": "276294063",
            "title": "Ontology-based prompt tuning for news article summarization",
            "text": "While the model demonstrates promising results in generating meaningful summaries, it faces limitations in handling highly diverse or noisy data from multiple sources, which may affect summary coherence and relevance (Patel et al., 2020). \n\nAnother study developed a neural attention-based model for abstractive summarization, leveraging recent advances in neural machine translation by integrating a probabilistic framework with a generation algorithm, resulting in accurate and contextually relevant abstractive summaries. Despite these advancements, the current model faces challenges in achieving consistently high grammatical quality and effectively handling longer texts (Rush et al., 2015b).",
            "score": 0.5156613809706743,
            "section_title": "Abstractive summarization",
            "char_start_offset": 11197,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 241,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 698
                }
            ],
            "ref_mentions": [
                {
                    "start": 217,
                    "end": 237,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 677,
                    "end": 697,
                    "matchedPaperCorpusId": "65027557"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70849609375
        },
        {
            "corpus_id": "266244733",
            "title": "Multi-granularity adaptive extractive document summarization with heterogeneous graph neural networks",
            "text": "Yet another set of studies, such as those employing ExpandRank, utilize information from adjacent documents to enhance the sentence graph of a target document (Yang et al., 2018).The current approach maximizes the utility of both local document information and external data for evaluating the significance of sentences (Wan & Xiao, 2010). The method also incorporates topic information to bolster summarization efficacy. A distinguishing feature of this methodology is its incorporation of adaptive depth and breadth, facilitating the handling of long-distance dependencies across multiple layers. Additionally, the model adjusts the level of connectivity for each graph neural network (GNN) layer according to the local neighborhood size of individual nodes. Moreover, topic probability distribution values are employed as edge features between topic nodes and sentence nodes, thereby enabling the latter to aggregate more pertinent topics. Separate research has introduced innovative graph models for text summarization (Ferreira et al., 2013), constructing graphs based on a four-dimensional framework: similarity, semantic similarity, co-citations, and textual information. With regard to modifications in prior values, diverse strategies exist for assessing the preliminary importance of nodes within the graph. For example, a biased TextRank algorithm has been proposed to extract focused content by modifying the random restart probability based on node relevance to the targeted task (Kazemi, P\u00e9rez-Rosas & Mihalcea, 2020). Such a modification skews the algorithm toward selecting nodes of higher relevance. This particular methodology begins by scoring sentences using a pretrained model, employing these scores to establish the preliminary importance of the sentences within the graph. \n\nIn recent years, the advent of distributed representations in natural language processing (Mikolov et al., 2013;Devlin et al., 2018) has led to a proliferation of deep learning-based techniques for automatic summarization (An et al., 2021;Liu, 2019). These methods have made strides in enhancing the efficacy of automatic document summarization, yet the task of managing long-distance dependencies continues to pose a significant challenge, and the training procedures are often resource-intensive. Notwithstanding, textual semantic information can be efficiently mapped as nodes within a graph-based framework. This configuration augments the model's information flow and elevates the coherence of the generated summaries.",
            "score": 0.5152441020817581,
            "section_title": "Heterogeneous graph for NLP",
            "char_start_offset": 7442,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1796
                },
                {
                    "start": 1799,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2297
                },
                {
                    "start": 2298,
                    "end": 2410
                },
                {
                    "start": 2411,
                    "end": 2522
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 178,
                    "matchedPaperCorpusId": "53088171"
                },
                {
                    "start": 320,
                    "end": 338,
                    "matchedPaperCorpusId": "207178257"
                },
                {
                    "start": 1023,
                    "end": 1046,
                    "matchedPaperCorpusId": "11394330"
                },
                {
                    "start": 2021,
                    "end": 2038,
                    "matchedPaperCorpusId": "233169121"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.912109375
        },
        {
            "corpus_id": "269188314",
            "title": "Prompt-tuning for Clickbait Detection via Text Summarization",
            "text": "These models build a neural network to establish a true relationship between input and output, refining the process with local attention mechanisms and advanced techniques like BERT.Architectures combining neural convolutional networks (CNN) with bidirectional LSTM showcase advancements in abstractive summarization, outperforming traditional methods.For example, Song et al. introduce the Abstractive Text Summarization (ATS) framework, which encompasses the extraction of semantic phrases from source text utilizing a phrase-based LSTM-CNN model [57].Subsequently, it generates summaries through a sequence-to-sequence model.Featuring a dual-decoder mechanism involving generate and copy modes, the model predicts the next phrase by encoding annotations in the source text.This approach notably improves the model's capability to construct new sentences, addressing both semantic and syntactic aspects.Recent advancements in abstractive summarization highlight the efficacy of diverse abstract generation, re-ranking strategies, and innovative architectures for second-stage summarization in enhancing overall summary quality and relevance [58].Addressing the \"exposure bias\" associated with standard maximum likelihood teacher forcing is crucial for improving summary quality.Liu et al. introduce a second-stage summarization framework to tackle train-test distribution mismatch [59].Transformer-based models like GSum and BERTSum, independently trained on varied data percentages, generate multiple candidates through diverse beam search during inference.A binary classifier, trained on GSum's outputs, selects the optimal summary, achieving state-of-the-art ROUGE scores on CNN/DailyMail.Furthermore, Liu et al. adopt a contrastive learning approach for second-stage summarization, demonstrating state-ofthe-art performance on CNN/DailyMail through re-ranking BART-generated summaries [60].Ravaut et al. present SummaReranker, an innovative multitask mixture-of-experts architecture for re-ranking [58].Surpassing strong baseline models across diverse datasets, including CNN/DailyMail, XSum, and Reddit TIFU, SummaReranker concatenates source documents and summary candidates.",
            "score": 0.5136628422146337,
            "section_title": "Text Summarization",
            "char_start_offset": 20221,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 182,
                    "end": 352
                },
                {
                    "start": 352,
                    "end": 554
                },
                {
                    "start": 554,
                    "end": 628
                },
                {
                    "start": 628,
                    "end": 776
                },
                {
                    "start": 776,
                    "end": 905
                },
                {
                    "start": 905,
                    "end": 1148
                },
                {
                    "start": 1148,
                    "end": 1280
                },
                {
                    "start": 1280,
                    "end": 1388
                },
                {
                    "start": 1388,
                    "end": 1560
                },
                {
                    "start": 1560,
                    "end": 1694
                },
                {
                    "start": 1694,
                    "end": 1896
                },
                {
                    "start": 1896,
                    "end": 2009
                },
                {
                    "start": 2009,
                    "end": 2183
                }
            ],
            "ref_mentions": [
                {
                    "start": 549,
                    "end": 553,
                    "matchedPaperCorpusId": "3342465"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87841796875
        },
        {
            "corpus_id": "269157041",
            "title": "Disentangling Instructive Information from Ranked Multiple Candidates for Multi-Document Scientific Summarization",
            "text": "Multi-Document Scientific Summarization (MDSS) involves consolidating scattered information from multiple papers.According to the paradigm adopted, previous studies can be categorized into graph-based [5,36], flat-based [23,31], and hierarchical-based methods [6,30].Concretely, graph-based methods construct external graphs (e.g., entity-relation graph, AMR, TF-IDF) to assist document representation and cross-document relation modeling.In this regard, Chen et al. [5] leverage graphs to encode source articles, constructing keyphrase graphs and using a multi-level contrastive learning strategy to improve the accuracy of generated summaries.\n\nWang et al. [36] incorporate knowledge graphs into document encoding and decoding, generating the summary from a knowledge graph template.Flat-based methods simply concatenate multiple documents, thus they can leverage state-of-the-art pre-trained summarization models.Moro et al. [23] select top  documents with a dense retriever and use a BART model [13] to produce the summary.Shi et al. [31] leverage Large Language Model (LLM) to expand the query for reference retrieval and adopt an instruction-tuning method to guide LLM to generate summary.Compared with flatbased methods, hierarchical-based methods take the hierarchical structure of document clusters into account, thus preserving crossdocument relations and obtaining semantic-rich representations.Chen et al. [6] apply word representations and cross-document relationships to model document-level representations, and design a hierarchical decoding strategy.Shen et al. [30] modify the BART model by incorporating document-level representations in the encoder and decoder to facilitate multi-document interactions.However, we argue that all the above works fail to explore external guidance information for better global semantic control and summary decoding.",
            "score": 0.5133344502579544,
            "section_title": "RELATED WORK 2.1 Multi-Document Scientific Summarization",
            "char_start_offset": 6345,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 113,
                    "end": 267
                },
                {
                    "start": 267,
                    "end": 439
                },
                {
                    "start": 439,
                    "end": 645
                },
                {
                    "start": 647,
                    "end": 785
                },
                {
                    "start": 785,
                    "end": 916
                },
                {
                    "start": 916,
                    "end": 1027
                },
                {
                    "start": 1027,
                    "end": 1195
                },
                {
                    "start": 1195,
                    "end": 1406
                },
                {
                    "start": 1406,
                    "end": 1567
                },
                {
                    "start": 1567,
                    "end": 1723
                },
                {
                    "start": 1723,
                    "end": 1868
                }
            ],
            "ref_mentions": [
                {
                    "start": 201,
                    "end": 204,
                    "matchedPaperCorpusId": "249097545"
                },
                {
                    "start": 204,
                    "end": 207,
                    "matchedPaperCorpusId": "252185277"
                },
                {
                    "start": 220,
                    "end": 224,
                    "matchedPaperCorpusId": "248780330"
                },
                {
                    "start": 224,
                    "end": 227,
                    "matchedPaperCorpusId": "266176619"
                },
                {
                    "start": 260,
                    "end": 263,
                    "matchedPaperCorpusId": "236460044"
                },
                {
                    "start": 467,
                    "end": 470,
                    "matchedPaperCorpusId": "249097545"
                },
                {
                    "start": 659,
                    "end": 663,
                    "matchedPaperCorpusId": "252185277"
                },
                {
                    "start": 928,
                    "end": 932,
                    "matchedPaperCorpusId": "248780330"
                },
                {
                    "start": 999,
                    "end": 1003,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1038,
                    "end": 1042,
                    "matchedPaperCorpusId": "266176619"
                },
                {
                    "start": 1418,
                    "end": 1421,
                    "matchedPaperCorpusId": "236460044"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82666015625
        },
        {
            "corpus_id": "259292631",
            "title": "Cogito Ergo Summ: Abstractive Summarization of Biomedical Papers via Semantic Parsing Graphs and Consistency Rewards",
            "text": "Since these representations can be defined with a panoply of formalisms having different objectives and properties, we underline the importance of bridging the complementary strengths of two influential semantic parsing tasks: closed-domain Event Extraction (EE) and Abstract Meaning Representation (AMR). EE is task-driven and aims to derive n-ary and potentially nested interactions between participants having a specific semantic role, where event schemas (i.e., target event, entity, and role types) are pre-established conforming to a reference ontology; its history is very intertwined with health informatics (Frisoni, Moro, and Carbonaro 2021). AMR is linguistically-grounded and is conceived to graphically capture the general meaning of any sentence as high-level semantic relations between abstract concepts (Banarescu et al. 2013). Fig. 1 depicts and compares their expressive power. \n\nA growing body of research in natural language generation (NLG) calls attention to incorporating explicit semantic structures into the summarization process (Yu et al. 2020), thereby unlocking a higher level of abstraction than bags of sentences and more accurate emulation of human interpretation, rewriting, and paraphrasing. However, existing graphaugmented approaches have at least one of the following weaknesses: (i) they have not been designed for or evaluated in the biomedical domain; (ii) they employ graph-LSTMs architectures that struggle to compete with transformers; (iii) they are based on open-domain triplet-based extractions that are notoriously not adequate to represent the complete biological meaning of a document; (iv) they do not include a module to ensure document-summary consistency. \n\nWe present COGITOERGOSUMM 1 , the first semanticsaware transformer-based model for single-document abstractive summarization in the biomedical domain. Concretely, we condense source documents into sets of unambiguous EE and AMR graphs, using multi-relational graph neural networks (GNNs) to yield their dense embeddings without imposing linearization. From this foundation, we explore a fine-tuning recipe for merging predicted symbolic representations into pre-trained encoder-decoder language models.",
            "score": 0.5120761969515866,
            "section_title": "Introduction",
            "char_start_offset": 2108,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 895
                },
                {
                    "start": 898,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1708
                },
                {
                    "start": 1711,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 2062
                },
                {
                    "start": 2063,
                    "end": 2213
                }
            ],
            "ref_mentions": [
                {
                    "start": 616,
                    "end": 651,
                    "matchedPaperCorpusId": "52111191"
                },
                {
                    "start": 819,
                    "end": 842,
                    "matchedPaperCorpusId": "262580630"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82861328125
        },
        {
            "corpus_id": "253157611",
            "title": "Improving abstractive summarization with energy-based re-ranking",
            "text": "However, the human evaluation results suggest that re-ranking according to these metrics, while competitive, may yield lower quality summaries than those obtained by state-of-the-art abstractive systems trained with augmented data and contrastive learning. \n\nThe remainder of the paper is organized as follows: in Section 2, we discuss the related work; in Section 3, we do a brief high-level description of neural abstractive summarization systems and how different candidate summaries can be generated from them; in Section 4, we describe our methodology in detail, as well as the summarization metrics that we shall use to train our re-ranking model; Section 5 presents the experimental results of our model and baselines, which include both automatic and human evaluation; in Section 6, we discuss the limitations of our approach and point some directions for future work, and we conclude this work with some final remarks in Section 7.",
            "score": 0.5117994768170744,
            "section_title": "Introduction",
            "char_start_offset": 3814,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 256
                },
                {
                    "start": 259,
                    "end": 940
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6669921875
        },
        {
            "corpus_id": "277284940",
            "title": "Global and Local Context Fusion in Heterogeneous Graph Neural Network for Summarizing Lengthy Scientific Documents",
            "text": "The evaluation strategy of this research is designed to rigorously assess the effectiveness of the proposed model for extractive summarization of long documents, by comparing its performance against state-of-the-art techniques. \n\nTo achieve this, two widely used public datasets-arXiv and PubMed-are used for comprehensive and challenging evaluation. These datasets not only capture the complexities inherent in scientific documents but also demonstrate the model's capability for real-world applications, highlighting its versatility and robustness. For a thorough evaluation, the effectiveness of the proposed model is compared and contrasted with both previous methods and recent advancements in neural graph-based deep learning models. This comprehensive analysis includes an in-depth comparison of the models' capabilities in addressing linguistic and topical aspects across various document types, using standard performance metrics such as ROUGE scores. Furthermore, the performance analysis includes a detailed evaluation of implementation-specific parameters, such as vocabulary size, tokenization methods, and key hyperparameters like learning rates and dropout rates. These components highlight that the proposed model is specifically tailored to process lengthy and complex scientific documents, leveraging the latest advancements in neural network techniques. \n\nThis comprehensive evaluation not only confirms the novelty of the model's contributions but also highlights its superior performance in addressing the complex requirements of extractive summarization tasks. These results emphasize the significant advancements it offers over existing methodologies.",
            "score": 0.5117799628238389,
            "section_title": "IV. PERFORMANCE EVALUATION",
            "char_start_offset": 35073,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 230,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1674
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94970703125
        },
        {
            "corpus_id": "253708345",
            "title": "GoSum: Extractive Summarization of Long Documents by Reinforcement Learning and Graph Organized discourse state",
            "text": "In this paper, we have presented a novel approach called GoSum for extracting summaries from long documents. It effectively integrates reinforcement learning with a graph neural network. In particular, we have shown how graph-organized discourse information can be applied in reinforcement learning-based extractive summarization. Experimental results on the arXiv and PubMed datasets have demonstrated that GoSum achieves state-ofthe-art performance. The ablation experiments examine the effect of discourse information on Go-Sum. The results show that the performance of GoSum comes from the use of the hierarchical attribution of sentences and the semantic information about section titles of documents. With achieving satisfactory results in scientific literature, GoSum requires hierarchical discourse information about long texts as its inputs. In the future, we will attempt to automatically generate discourse information from documents.",
            "score": 0.5114910977249096,
            "section_title": "Conclusion",
            "char_start_offset": 21927,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 945
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96875
        },
        {
            "corpus_id": "271938110",
            "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
            "text": "This section presents a systematic evaluation of our proposed model. We compare its performance against state-of-the-art extractive and abstractive summarization models, including neural graph-based models that utilize various nodes. We also provide details of the long scientific medical documents dataset used in this research, along with the hyperparameters, implementation settings, and the impact of latent topics (global information) on the system's summaries.",
            "score": 0.511253164490331,
            "section_title": "IV. PERFORMANCE EVALUATION",
            "char_start_offset": 27929,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 466
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90234375
        },
        {
            "corpus_id": "271525553",
            "title": "Abstractive text summarization: State of the art, challenges, and improvements",
            "text": "Several prior surveys have explored the developments in automatic summarization methods. These survey papers offer vital insights into the methods, limitations, and potential future research directions in automatic summarization. A significant portion of these surveys, such as those conducted by Nazari et al. [28], and Moratanch et al. [29], primarily focused on extractive summarization methods. This focus can be attributed to the complexity inherent in abstractive summarization. In recent years, a growing body of work has concentrated on the state of the art in abstractive summarization. For instance, Suleiman et al. [30], Zhang et al. [31], and Gupta et al. [32] have exclusively focused on abstractive text summarization. These studies delve into deep learning-based abstractive summarization methods and compare performance on widely used datasets. Lin et al. [33] explored existing neural approaches to abstractive summarization, while Gupta et al. [32] characterized abstractive summarization strategies, highlighting the difficulties, tools, benefits, and drawbacks of various approaches. Syed et al. [34] evaluated various abstractive summarization strategies, including encoder-decoder, transformer-based, and hybrid models, and also discussed the challenges and future research prospects in the field. \n\nThere are studies that cover both extractive and abstractive methods, providing a more comprehensive view of the field. Examples of such works include Gupta et al. [35] and Mahajani et al. [36]. These studies offer a comparative examination of the effectiveness of both extractive and abstractive techniques as well as giving an overview of both. Ermakova et al. [37] presented a study on the evaluation techniques utilized in summarization, which is fundamental for comprehending the viability and potential improvements in both extractive and abstractive summarization methods. These works go about as an extension between the two summarization approaches, displaying their individual benefits and possible cooperative synergies. \n\nPrevious research on automatic summarization has oftentimes focused on specific topics, for example, abstractive summarization utilizing neural approaches, deep learning-based models, sequence-to-sequence based models, and extractive summariza-tion.",
            "score": 0.5112503008616469,
            "section_title": "Prior Surveys on Abstractive Summarization",
            "char_start_offset": 6290,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1319
                },
                {
                    "start": 1322,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2053
                },
                {
                    "start": 2056,
                    "end": 2305
                }
            ],
            "ref_mentions": [
                {
                    "start": 311,
                    "end": 315,
                    "matchedPaperCorpusId": "148028"
                },
                {
                    "start": 338,
                    "end": 342,
                    "matchedPaperCorpusId": "29980788"
                },
                {
                    "start": 645,
                    "end": 649,
                    "matchedPaperCorpusId": "251270540"
                },
                {
                    "start": 668,
                    "end": 672,
                    "matchedPaperCorpusId": "57992117"
                },
                {
                    "start": 872,
                    "end": 876,
                    "matchedPaperCorpusId": "198189946"
                },
                {
                    "start": 962,
                    "end": 966,
                    "matchedPaperCorpusId": "57992117"
                },
                {
                    "start": 1116,
                    "end": 1120,
                    "matchedPaperCorpusId": "231715075"
                },
                {
                    "start": 1486,
                    "end": 1490,
                    "matchedPaperCorpusId": "70036186"
                },
                {
                    "start": 1685,
                    "end": 1689,
                    "matchedPaperCorpusId": "145911543"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.681640625
        },
        {
            "corpus_id": "259705358",
            "title": "Enhancing Abstractive Summarization with Extracted Knowledge Graphs and Multi-Source Transformers",
            "text": "As the popularity of large language models (LLMs) has risen over the course of the last year, led by GPT-3/4 and especially its productization as ChatGPT, we have witnessed the extensive application of LLMs to text summarization. However, LLMs do not intrinsically have the power to verify the correctness of the information they supply and generate. This research introduces a novel approach to abstractive summarization, aiming to address the limitations of LLMs in that they struggle to understand the truth. The proposed method leverages extracted knowledge graph information and structured semantics as a guide for summarization. Building upon BART, one of the state-of-the-art sequence-to-sequence pre-trained LLMs, multi-source transformer modules are developed as an encoder, which are capable of processing textual and graphical inputs. Decoding is performed based on this enriched encoding to enhance the summary quality. The Wiki-Sum dataset, derived from Wikipedia text dumps, is introduced for evaluation purposes. Comparative experiments with baseline models demonstrate the strengths of the proposed approach in generating informative and relevant summaries. We conclude by presenting our insights into utilizing LLMs with graph external information, which will become a powerful aid towards the goal of factually correct and verified LLMs.",
            "score": 0.5105591825905618,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90771484375
        },
        {
            "corpus_id": "258436793",
            "title": "Summarizing Multiple Documents with Conversational Structure for Meta-Review Generation",
            "text": "Transformer-based pre-trained language models (PLMs) (Lewis et al., 2020;Zhang et al., 2020a;Guo et al., 2022;Phang et al., 2022) are the predominant approach in abstractive text summarization. However, it is challenging to incorporate structural information into the input as Transformer is designed to process flat text sequences. As such, most studies for MDS treat the input documents as a long flat string (via concatenation) without any explicit inter-document relationships (Xiao et al., 2022;Guo et al., 2022;Phang et al., 2022). To take into account the structural information, most work uses graph neural networks (Li et al., 2020;Jin et al., 2020;Cui and Hu, 2021;Li et al., 2023) but it is difficult to construct effective graphs to represent multiple documents and they introduce additional parameters to the pre-trained language models. Attention manipulation is one approach to introduce structural inductive bias without increasing the model size substantially. Studies that take this direction, however, by and large focus on incorporating syntax structure of sentences or internal structure of single documents (Bai et al., 2021;Cao and Wang, 2022) rather than higher level inter-document discourse structure. RAMMER is inspired by these works, and the novelty is that it uses attention manipulation to capture broader inter-document relationships. \n\n3 The PEERSUM Dataset",
            "score": 0.5104871316590572,
            "section_title": "Structural Inductive Bias for Summarization",
            "char_start_offset": 5953,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1366
                },
                {
                    "start": 1369,
                    "end": 1390
                }
            ],
            "ref_mentions": [
                {
                    "start": 53,
                    "end": 73,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 73,
                    "end": 93,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 93,
                    "end": 110,
                    "matchedPaperCorpusId": "245144820"
                },
                {
                    "start": 481,
                    "end": 500,
                    "matchedPaperCorpusId": "247519084"
                },
                {
                    "start": 500,
                    "end": 517,
                    "matchedPaperCorpusId": "245144820"
                },
                {
                    "start": 624,
                    "end": 641,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 641,
                    "end": 658,
                    "matchedPaperCorpusId": "220045815"
                },
                {
                    "start": 658,
                    "end": 675,
                    "matchedPaperCorpusId": "239050558"
                },
                {
                    "start": 675,
                    "end": 691,
                    "matchedPaperCorpusId": "257496469"
                },
                {
                    "start": 1129,
                    "end": 1147,
                    "matchedPaperCorpusId": "232147874"
                },
                {
                    "start": 1147,
                    "end": 1165,
                    "matchedPaperCorpusId": "247595303"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88671875
        },
        {
            "corpus_id": "259251722",
            "title": "Fusing Multimodal Signals on Hyper-complex Space for Extreme Abstractive Text Summarization (TL;DR) of Scientific Contents",
            "text": "The development and utilization of abstractive text summarization systems involve the formulation of textual summaries through the integration of two or more auxiliary signals. These signals, beyond the traditional text, may encompass video [12], images [51], and audio [19]. The integration of additional modalities, as compared to text-only systems, offers a plethora of opportunities to enhance the contextual richness and knowledge base of the generated summaries. Several recent studies [1,45] demonstrated that the integration of multimodal signals such as video and audio can significantly improve the contextual accuracy and informativeness of the summaries generated by unimodal systems. \n\nUnimodal text summarization: Text summarization is classified into two categories -extractive and abstractive. Extractive systems extract the most relevant sentences from the source document to form the summary, while abstractive systems paraphrase important sentences to generate a new summary. Conventional extractive summarization approaches either construct a graph representation of the source document [13,32,38] or pose the summarization task as a binary classification with ranking [8,35,40,62]. On the other hand, abstractive summarization has significantly benefited from the advent of deep neural networks. Early works [41,46] utilized CNN/Dailymail dataset [21] to explore abstractive summarization on a large scale. Later, Pointer Generators (PG) [46] were extended to capture the latent structures of documents [14,50]. The use of Transformers [54] and attention mechanisms [2] further improved the encoding of long sequential data. These improvements include leveraging Transformers [20] and repurposing attention heads as copy pointers [15] to enhance the quantitative performance. Large language models [11,44,59] have demonstrated impressive performance on multiple datasets [14,21]. Models proposed in [11] and [59] are pre-trained using token and phrase masking techniques, respectively, while Raffel et al. [44] approached all downstream tasks as a text-to-text problem and pre-train using a single loss function.",
            "score": 0.5104120761195186,
            "section_title": "RELATED WORK",
            "char_start_offset": 5146,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 696
                },
                {
                    "start": 699,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2133
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 245,
                    "matchedPaperCorpusId": "85499348"
                },
                {
                    "start": 254,
                    "end": 258,
                    "matchedPaperCorpusId": "251067146"
                },
                {
                    "start": 492,
                    "end": 495,
                    "matchedPaperCorpusId": "234790213"
                },
                {
                    "start": 495,
                    "end": 498,
                    "matchedPaperCorpusId": "53186236"
                },
                {
                    "start": 1107,
                    "end": 1111,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1111,
                    "end": 1114,
                    "matchedPaperCorpusId": "235792514"
                },
                {
                    "start": 1114,
                    "end": 1117,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 1189,
                    "end": 1192,
                    "matchedPaperCorpusId": "1499080"
                },
                {
                    "start": 1195,
                    "end": 1198,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 1329,
                    "end": 1333,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 1333,
                    "end": 1336,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1368,
                    "end": 1372,
                    "matchedPaperCorpusId": "6203757"
                },
                {
                    "start": 1459,
                    "end": 1463,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1524,
                    "end": 1528,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 1528,
                    "end": 1531,
                    "matchedPaperCorpusId": "46936631"
                },
                {
                    "start": 1557,
                    "end": 1561,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1587,
                    "end": 1590,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 1697,
                    "end": 1701,
                    "matchedPaperCorpusId": "236772854"
                },
                {
                    "start": 1751,
                    "end": 1755,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 1819,
                    "end": 1823,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1823,
                    "end": 1826,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1826,
                    "end": 1828,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 1892,
                    "end": 1896,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 1896,
                    "end": 1899,
                    "matchedPaperCorpusId": "6203757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81591796875
        },
        {
            "corpus_id": "273662136",
            "title": "GraphLSS: Integrating Lexical, Structural, and Semantic Features for Long Document Extractive Summarization",
            "text": "We introduced GraphLSS, a heterogeneous graph for long document summarization incorporating lexical, structural, and semantic features. Experiments on PubMed and arXiv highlight the impact of extractive labels due to their inherent imbalance. GraphLSS proves competitive with top-performing graph-based methods and outperforms recent nongraph models by using a greedy labeling strategy and adaptive weights during training. Future work will explore integrating an abstractive summarizer.",
            "score": 0.5101122198941723,
            "section_title": "Conclusions",
            "char_start_offset": 13636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 487
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94873046875
        },
        {
            "corpus_id": "266244733",
            "title": "Multi-granularity adaptive extractive document summarization with heterogeneous graph neural networks",
            "text": "The critical component of extractive summarization involves an in-depth analysis of the source text to produce condensed, semantically clear sentences that serve as a summary of the document. Prevailing models often utilize an encoder-decoder framework based on attention mechanisms and yield satisfactory outcomes in the realm of short text summarization. Recurrent neural networks (RNNs) (Cheng & Lapata, 2016;Nallapati, Zhai & Zhou, 2017;Zhou et al., 2018) commonly serve the purpose of elucidating crosssentence relationships. Nonetheless, these RNN-based frameworks often fall short in addressing long-distance dependencies, particularly when handling extensive or multidocument texts. An alternative avenue worth exploring is the utilization of graph structures for modeling inter-sentence connections. Yet, the effective design of these graph structures for automating document summarization remains an area warranting further research. \n\nDocuments inherently comprise a multitude of sentences, and sentence-level models frequently fail to consider the relationships between distantly-placed sentences, resulting in the omission of significant information in the summary. Moreover, information within documents tends to be distributed sparsely, challenging the efficacy of sentence-level models. To address these shortcomings, a fused multi-granularity node graph model is proposed. This model integrates a topic model to isolate keyword features, and leverages both TF-IDF and topic probabilities for the generation of edge features, thus strengthening the interconnections among nodes of varying granularities. Through iterative node information propagation, links are established between sentences, topics, and words, facilitating enhanced information transfer to both proximal and distant nodes. Additionally, an adaptive mechanism for adjusting the depth and breadth of the graph is incorporated, progressively improving the model's capacity to handle long-distance dependencies. \n\nSemantic units in the model are composed of words in conjunction with topics. Each sentence is interconnected with the words and topics it encompasses, yet subjects and words lack direct edges between them. Edge features are incorporated into the calculations of the graph attention layer, facilitating information transfer between nodes. Given the varying requirements for propagation width and depth across nodes of different granularities, adaptability in both dimensions is introduced to enhance the model's performance.",
            "score": 0.5101122198941723,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 943
                },
                {
                    "start": 946,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1991
                },
                {
                    "start": 1994,
                    "end": 2071
                },
                {
                    "start": 2072,
                    "end": 2200
                },
                {
                    "start": 2201,
                    "end": 2332
                },
                {
                    "start": 2333,
                    "end": 2518
                }
            ],
            "ref_mentions": [
                {
                    "start": 412,
                    "end": 441,
                    "matchedPaperCorpusId": "6405271"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95947265625
        },
        {
            "corpus_id": "270808749",
            "title": "Abstractive Summarization Model for Summarizing Scientific Article",
            "text": "However, GATs [33] have not achieved as many efficient results as GTN.In this paper, a graph-based abstractive summarization (GBAS) approach consisting of three stages was proposed, inspired by the SciBERT and graph transformer, to generate a summary from the text.",
            "score": 0.5101122198941723,
            "section_title": "C. GRAPH TRANSFORMER NETWORK (GTN)",
            "char_start_offset": 12076,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 70,
                    "end": 265
                }
            ],
            "ref_mentions": [
                {
                    "start": 14,
                    "end": 18,
                    "matchedPaperCorpusId": "220047360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.693359375
        },
        {
            "corpus_id": "252846544",
            "title": "Improving Graph-Based Text Representations with Character and Word Level N-grams",
            "text": "Graph-based text representation focuses on how text documents are represented as graphs for exploiting dependency information between tokens and documents within a corpus. Despite the increasing interest in graph representation learning, there is limited research in exploring new ways for graph-based text representation, which is important in downstream natural language processing tasks. In this paper, we first propose a new heterogeneous word-character text graph that combines word and character n-gram nodes together with document nodes, allowing us to better learn dependencies among these entities. Additionally, we propose two new graph-based neural models, WCTextGCN and WCTextGAT, for modeling our proposed text graph. Extensive experiments in text classification and automatic text summarization benchmarks demonstrate that our proposed models consistently outperform competitive baselines and state-of-the-art graph-based models.",
            "score": 0.5097833732456275,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94189453125
        },
        {
            "corpus_id": "258959349",
            "title": "MeetingBank: A Benchmark Dataset for Meeting Summarization",
            "text": "We evaluate state-of-the-art summarization systems on city council meetings, focusing on segments of the meetings rather than entire transcripts due to the length constraint imposed by abstractive summarizers. We split our dataset into train, validation and test sets, containing 5169, 861, 862 instances respectively. Each summarizer is given the transcript of a meeting segment and tasked with generating a concise summary. The results are reported for the test set of our meeting dataset. Extractive. Our extractive methods include the Oracle, LEAD, LexRank and TextRank (Erkan and Radev, 2004;Mihalcea and Tarau, 2004). The Extractive Oracle 8 selects the highest-scoring sentences from the input transcript, adding one sentence at a time until the combined R1 and R2 score can no longer be improved. The LEAD-N baseline selects the first N sentences of the input. LexRank and TextRank, both graph-based methods, determine the importance of sentences by analyzing their centrality in the graph structure. Both methods are set to extract two sentences from a transcript segment, which is the average number of sentences in the reference summaries. Abstractive with fine-tuning. We investigate five best-performing neural abstractive summarizers. These include BART-large (Lewis et al., 2020), a denoising autoencoder that is trained to reconstruct original text from corrupted input, Pegasus (Zhang et al., 2020a) Table 3: Evaluation of the BART summarizer using a series of ablations. LEFT: we remove all the training instances from a single city and fine-tune the model with the remaining instances, denoted by #Inst. We find that although the City of Seattle only contributes a moderate number of training instances, removing them has led to a substantial decrease in summarization performance. RIGHT: we evaluate the performance of the BART summarizer on a city-by-city basis. We show the variance in R-2 F-scores for each test city when training instances from the same city are included vs. when they are excluded. \u2193 indicates a performance drop and \u2191 a performance gain.\n\nthrough windowed attention, DialogLM (Zhong et al., 2022), a summarizer developed for",
            "score": 0.5087766135939517,
            "section_title": "Performance of Existing Systems",
            "char_start_offset": 17877,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 574,
                    "end": 597,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 597,
                    "end": 622,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 1274,
                    "end": 1294,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1395,
                    "end": 1416,
                    "matchedPaperCorpusId": "209405420"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.720703125
        },
        {
            "corpus_id": "271938110",
            "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
            "text": "Our model achieved near-identical results in R-1, comparable results in R-2, and surpassed Topic-GraphSum in R-L. This demonstrates the effectiveness of our model, which combined a GAT layer with a Transformer model, for abstractive summarization task. Topic-GraphSum, a state-ofthe-art model for extractive summarization of long scientific documents, employed an NTM model for topic modeling along with BERT and GAT networks. GraphSum utilizing an NTM model contributes to its higher R-1 score compared to our proposed model. This is because NTM can be jointly optimized with the document encoder and graph networks. However, NTM training configurations are more complex [15], and aligning it with a graph neural network is more challenging compared to the simpler and easier-to-train LDA model. It is important to note that extractive summarization focuses on selecting the most important sentences from a document, which often leads to higher ROUGE scores compared to abstractive methods [50]. \n\nOur analysis also compares our model with two other advanced models: HeterGraphLongSum and GTASum. \n\nHeterGraphLongSum: This graph-based model focuses on extracting summaries from long scientific papers. It leverages three semantic units -words, sentences, and passage nodes -within its graph structure. Notably, the inclusion of passage nodes as high-level semantic units contributes to HeterGraphLongSum's stronger performance on ROUGE scores compared to our proposed abstractive model. Another reason for this performance difference is that extractive summarization, by design, selects the most important sentences from a document, which often leads to higher ROUGE scores compared to abstractive methods. \n\nGTASum: This Graph-Based Topic-Aware abstractive summarization model is a direct competitor. GTASum employs a combination of techniques, including a BERT encoder, NTM for topic modeling, GAT, and a Transformer decoder. Our model surpassed GTASum in R-1 score, while achieving comparable results in R-2 and R-L scores. \n\nOur model differs from GTASum in its use of semantic units within the graph structure.",
            "score": 0.5080688508903602,
            "section_title": "5) GRAPH BASED MODELS",
            "char_start_offset": 34342,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 996
                },
                {
                    "start": 999,
                    "end": 1097
                },
                {
                    "start": 1100,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1707
                },
                {
                    "start": 1710,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2027
                },
                {
                    "start": 2030,
                    "end": 2116
                }
            ],
            "ref_mentions": [
                {
                    "start": 672,
                    "end": 676,
                    "matchedPaperCorpusId": "252670538"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.970703125
        },
        {
            "corpus_id": "264904532",
            "title": "Abstractive summarization with deep reinforcement learning using semantic similarity rewards",
            "text": "State-of-the-art abstractive summarization models are based on neural sequence-to-sequence models (Sutskever, Vinyals, and Le 2014). Initial work by Rush et al. (2015) introduced neural sequence-to-sequence abstractive sentence summarization model with an attention-based encoder and a feed-forward neural network language model decoder. Chopra, Auli, and Rush (2016) presented another abstractive summarization model with a convolutional attention-based encoder and a recurrent neural network (RNN) decoder. Nallapati et al. (2016) further explored RNNs for both encoder and decoder, with a novel switching generator pointer approach to address out-of-vocabulary words. In a later work, multi-sentence abstractive text summarization was addressed by See, Liu, and Manning (2017) through a hybrid pointer generator network and a coverage mechanism (Tu et al. 2016). Subsequent work by Gehrmann, Deng, and Rush (2018) was built upon the pointer generator summarization model by introducing content selection for relevant document portions and a bottom-up copy attention mechanism. \n\nRecent advances in pre-trained language models have significantly enhanced text summarization as well. Liu and Lapata (2019) introduced BERTSum exploring the use of BERT in text summarization framework, using both extractive and abstractive approaches. Meanwhile, Dong et al. (2019) introduced the Unified Pre-trained Language Model with unidirectional, bidirectional, and sequence-to-sequence language modeling objectives. In a prominent work, Raffel et al. (2020) presented the text-to-text-transfer-transformer (T5) pre-trained on the new open-source dataset called the Colossal Clean Crawled Corpus (C4) they introduced. In T5, every text processing problem is considered in a text-to-text framework, and a single model can be trained with the same loss function and decoding process on different NLP tasks. Additionally, Lewis et al. (2020) proposed BART, a denoising autoencoder for pre-training sequence-to-sequence models.",
            "score": 0.5080149158687273,
            "section_title": "Supervised learning based approaches",
            "char_start_offset": 6784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1079
                },
                {
                    "start": 1082,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2012
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 131,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 338,
                    "end": 367,
                    "matchedPaperCorpusId": "133195"
                },
                {
                    "start": 509,
                    "end": 532,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 751,
                    "end": 779,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 848,
                    "end": 864,
                    "matchedPaperCorpusId": "146843"
                },
                {
                    "start": 885,
                    "end": 916,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 1185,
                    "end": 1206,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 1346,
                    "end": 1364,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 1527,
                    "end": 1547,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9423828125
        },
        {
            "corpus_id": "245827982",
            "title": "An Unsupervised Masking Objective for Abstractive Multi-Document News Summarization",
            "text": "Multi-document summarization (MDS) aims to condense and combine information from topically related groups of documents into a single concise yet comprehensive short text. The news domain, where input documents are article clusters of varying size covering the same event, is a common challenge setting for MDS systems (Paul and James, 2004;Owczarzak and Dang, 2011). While most early approaches to the problem focused on extractive methods that select and copy relevant sections from the input documents (Radev et al., 2004;Erkan and Radev, 2004;Mihalcea and Tarau, 2004), advances in neural text generation have increased both performance of and, subsequently, interest in abstractive methods, which generate a summary from scratch conditioned on a combined representation of the encoded source documents. \n\nWhile large neural systems have seen success on single-document summarization, where human summaries are relatively easy to obtain at scale (Napoles et al., 2012;Hermann et al., 2015), their application to the multi-document task has been restricted by the difficulty of collecting such datareading and then summarizing a collection of documents is a daunting annotation task. We focus on the task of unsupervised abstractive MDS where the system is only allowed access to input document collections at training time (Chu and Liu, 2019). Similar to older work on unsupervised extractive summarization (Radev et al., 2004;Erkan and Radev, 2004;Mihalcea and Tarau, 2004), we propose an approach that leverages lexical centrality of the document clusters to select a document to mask out and then predict. This allows for any, black box discriminative neural abstractive summarization model to be trained without ground-truth summaries by maximizing the likelihood of the masked out document, drawing inspiration from masked language modeling (e.g., BERT (Devlin et al., 2018)). Some recent works have automatically built large-scale, supervised MDS datasets for news (Fabbri et al., 2019) and Wikipedia (Liu et al., 2018).",
            "score": 0.5077623592773739,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 806
                },
                {
                    "start": 809,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2029
                }
            ],
            "ref_mentions": [
                {
                    "start": 524,
                    "end": 546,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 546,
                    "end": 571,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 949,
                    "end": 971,
                    "matchedPaperCorpusId": "9586240"
                },
                {
                    "start": 971,
                    "end": 992,
                    "matchedPaperCorpusId": "6203757"
                },
                {
                    "start": 1326,
                    "end": 1345,
                    "matchedPaperCorpusId": "59413781"
                },
                {
                    "start": 1430,
                    "end": 1452,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1452,
                    "end": 1477,
                    "matchedPaperCorpusId": "577937"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8955078125
        },
        {
            "corpus_id": "266176619",
            "title": "Towards a Unified Framework for Reference Retrieval and Related Work Generation",
            "text": "Similar to multi-document summarization, the task of related work generation (RWG) which usually takes multiple reference papers as input and generate a related work section for a target paper (Chen et al., 2022a;Hoang and Kan, 2010) via summarizes the related information in a logical order. Existing RWG methods can be divided into extractive and abstractive methods. Specifically, extractive methods select a subset of words or sentences most relevant to the input abstract to form the final related work (Hoang and Kan, 2010;Hu and Wan, 2014;Deng et al., 2021) . With the emergence of neural-based models (Lewis et al., 2019;Raffel et al., 2019;Touvron et al., 2023), more abstractive methods are utilized to solve the RWG task (Zhao et al., 2020a;Chen et al., 2022b). For example, Chen et al. (2022a) leverage the information of the target paper and propose a target-aware graph encoder to model the relationship between reference papers. Although these methods have shown exemplary performance, they rely on the human-labeled references in the target paper, which still requires a lot of manual retrieval work.",
            "score": 0.5059611081561246,
            "section_title": "Related work generation",
            "char_start_offset": 4157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1116
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 213,
                    "matchedPaperCorpusId": "249097545"
                },
                {
                    "start": 213,
                    "end": 233,
                    "matchedPaperCorpusId": "12079994"
                },
                {
                    "start": 508,
                    "end": 529,
                    "matchedPaperCorpusId": "12079994"
                },
                {
                    "start": 529,
                    "end": 546,
                    "matchedPaperCorpusId": "1948673"
                },
                {
                    "start": 546,
                    "end": 564,
                    "matchedPaperCorpusId": "235259156"
                },
                {
                    "start": 609,
                    "end": 629,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 629,
                    "end": 649,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 732,
                    "end": 752,
                    "matchedPaperCorpusId": "220633461"
                },
                {
                    "start": 786,
                    "end": 805,
                    "matchedPaperCorpusId": "249097545"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.810546875
        },
        {
            "corpus_id": "267547632",
            "title": "Graph Neural Network and NER-Based Text Summarization",
            "text": "Graph Neural Networks (GNNs) are leveraged to capture the complex relational information among entities and contextual words in the text, effectively modeling the data as a graph. The process is visualized in Figure 3, which illustrates the transition from isolated nodes to a fully connected graph. \u2022 GNN Model Selection: Our selected GNN architecture is specifically designed to process text-based graph structures. It is designed to understand and make use of the relational dynamics between words and entities, which are essential for identifying crucial information in the text summarization process. The architecture is chosen for its ability to handle the heterogeneity of the graph's nodes and the complexity of their interrelations. \u2022 Graph Construction: The graph is constructed in a twostep process. Initially, sentences and identified entities are treated as separate nodes (as shown in parts 1 and 2 of Figure 3). These nodes are then interconnected through edges that represent their relationships, which are determined by co-occurrence within sentences, syntactical dependencies, and semantic relatedness (part 3 of Figure 3). This results in a rich graph where the nodes are text elements and the edges represent the contextual and relational nuances of the original document. \u2022 Edge Formation: Edges are not uniformly created; instead, their formation is informed by the depth of the relationship between nodes. Factors such as the frequency of co-occurrence and the strength of dependency links play a role in defining the edge weights, which are crucial for the GNN as it processes the graph to identify which nodes (text segments) are pivotal for the summary. \n\nThe incorporation of GNNs enables the system to move beyond linear text analysis, instead treating the text as a complex network, somewhat similar to human cognitive processes that interpret information. This networked approach allows for a more detailed understanding and generation of summaries, which are both comprehensive and contextually coherent.",
            "score": 0.5050389877887302,
            "section_title": "D. Incorporation of Graph Neural Networks (GNN)",
            "char_start_offset": 15552,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1679
                },
                {
                    "start": 1682,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2035
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9384765625
        },
        {
            "corpus_id": "266374529",
            "title": "Survey on Multi-Document Summarization: Systematic Literature Review",
            "text": "The goal of an abstractive summary is to generate a precise and comprehensive summary of the collection of documents. \n\nThe researcher proposed a plausibility-promoting reproductive adversarial network for abstractive summarization with multi-task constraints. This method is used to advance the performance of text summarization. This method is used to generate informative and grammatical error-free summaries. For the experiment, CNN/Daily Mail and Giga word are used. ROUGE is used for calculating performance. This method improves performance, generates high-quality, and informative summaries. The aim of the paper [15] is to propose an abstractive text summarization system for multiple documents. The object of the paper is to generate an accurate summary that contains all the main contents of all documents. For handling the redundancy issue research develop a hybrid method that used clustering, word graphs, and neuronal networks. In clustering, all data from multiple documents is alienated into all clusters equally on basis of content. In a word graph, the shortest path detection minimizes the text. For generating abstractive text summarization, the sequence-to-sequence method, and supervised recurrent natural network (RNN) are used. \n\nFor the experiment DUC2004 dataset is used and check performance by ROUGE and BLEU metrics.",
            "score": 0.5050389877887302,
            "section_title": "Abstractive Summarization:",
            "char_start_offset": 32653,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 120,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1252
                },
                {
                    "start": 1255,
                    "end": 1346
                }
            ],
            "ref_mentions": [
                {
                    "start": 621,
                    "end": 625,
                    "matchedPaperCorpusId": "215816696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87353515625
        },
        {
            "corpus_id": "262592258",
            "title": "A hybrid approach for text summarization using semantic latent Dirichlet allocation and sentence concept mapping with transformer",
            "text": "Their approach utilized undirected graph-based scoring to evaluate sentences and determine which ones should be included in the summary. Extractive text summarization [26] considers sentence length, position, cue phrases, and cohesion when selecting sentences for summarization. In recent years, the use of neural networks for text summarization has become widespread, as these models can learn sentence patterns. \n\nMostly used deep learning method is the recursive neural network (RNN) [27]. Long short-term memory (LSTM), gated recurrent units (GRU), and transformers were other approaches for solving gradient disappearance. The extractive method has given results since it can easily combine many techniques and improve performance. Using content attention, two-phase multi-document summarization [28] extracts subtopics from documents. The summary was formulated using different sub-topics as an optimization problem of minimizing the sentence distance. Huang et al. [29] employed Hepos, a novel encoder-decoder attention, to extract features from original documents. They also introduced a new dataset called GovReport, which includes lengthy documents and their corresponding summaries. The evaluation model for text summarization has its shortcoming in using neural networks [30]. The author has tried to overcome the shortcomings of evaluation metrics of text summarization in five dimensions. He re-evaluated the metrics using neural networks and benchmarked metrics using recent text summarization models. Sentence regression [31] identifies essential features that represent the sentences. The sentence relation such as contextual sentence relations, query sentence relations, and title sentence relations are used to extract basic sentences from the documents. \n\nTraining a neural network for text summarization has some difficulty processing large documents. We can replace phrases with general terms in semantic content generalization. Some used the pointer generator network [32], copying the original content and combining the semantic content generalization. There are many attention-based mechanisms to generate an article summary; one such method is attentive encoder-based summarization (AES) combined with unidirectional recurrent neural network (Uni-AES) [33].",
            "score": 0.5047864502650407,
            "section_title": "LITERATURE REVIEW",
            "char_start_offset": 10833,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1773
                },
                {
                    "start": 1776,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2283
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 171,
                    "matchedPaperCorpusId": "57036582"
                },
                {
                    "start": 487,
                    "end": 491,
                    "matchedPaperCorpusId": "238027281"
                },
                {
                    "start": 801,
                    "end": 805,
                    "matchedPaperCorpusId": "243638031"
                },
                {
                    "start": 972,
                    "end": 976,
                    "matchedPaperCorpusId": "233033613"
                },
                {
                    "start": 1283,
                    "end": 1287,
                    "matchedPaperCorpusId": "220768873"
                },
                {
                    "start": 1537,
                    "end": 1541,
                    "matchedPaperCorpusId": "14001298"
                },
                {
                    "start": 1991,
                    "end": 1995,
                    "matchedPaperCorpusId": "231713675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76025390625
        },
        {
            "corpus_id": "245827982",
            "title": "An Unsupervised Masking Objective for Abstractive Multi-Document News Summarization",
            "text": "We show that a simple unsupervised masking objective can approach near supervised performance on abstractive multi-document news summarization. Our method trains a state-of-the-art neural summarization model to predict the masked out source document with highest lexical centrality relative to the multi-document group. In experiments on the Multi-News dataset, our masked training objective yields a system that outperforms past unsupervised methods and, in human evaluation, surpasses the best supervised method without requiring access to any ground-truth summaries. Further, we evaluate how different measures of lexical centrality, inspired by past work on extractive summarization, affect final performance.",
            "score": 0.504157679583892,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86083984375
        },
        {
            "corpus_id": "246652372",
            "title": "Survey of Hallucination in Natural Language Generation",
            "text": "7.3.1 Architecture Methods. Researchers have made modifications to the architecture design of the models to reduce hallucinated content in the summaries.\n\nEncoder. Zhu et al. [170] use an explicit Graph Neural Network (GNN) to encode the fact tuples extracted from source documents. In addition to an explicit graph encoder, Huang et al. [55] further design a multiple-choice cloze test reward to encourage the model to better understand entity interactions. Moreover, Gunel et al. [49] use external knowledge from Wikipedia to make knowledge embeddings, and the results show improved factual consistency.\n\nDecoder. Song et al. [123] incorporate a sequential decoder and a tree-based decoder to generate a summary sentence and its syntactic parse. Aralikatte et al. [2] introduce the Focus Attention Mechanism, which encourages decoders to generate tokens similar or topical to the source documents. The results show the efficiency of these methods to generate more faithful summaries.\n\nEncoder-Decoder. Cao et al. [16] extract fact descriptions from the source text and apply a dualattention seq2seq framework to force the summaries to be conditioned on both source documents and the extracted fact descriptions. Li et al. [78] propose an entailment-aware encoder and decoder with multi-task learning that incorporates the entailment knowledge into abstractive summarization models.",
            "score": 0.5033430393447705,
            "section_title": "Hallucination Mitigation in Abstractive Summarization",
            "char_start_offset": 47573,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 175,
                    "end": 180,
                    "matchedPaperCorpusId": "235097229"
                },
                {
                    "start": 338,
                    "end": 342,
                    "matchedPaperCorpusId": "218487279"
                },
                {
                    "start": 628,
                    "end": 633,
                    "matchedPaperCorpusId": "208267908"
                },
                {
                    "start": 766,
                    "end": 769,
                    "matchedPaperCorpusId": "235186832"
                },
                {
                    "start": 1015,
                    "end": 1019,
                    "matchedPaperCorpusId": "19198109"
                },
                {
                    "start": 1224,
                    "end": 1228,
                    "matchedPaperCorpusId": "52012819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94384765625
        },
        {
            "corpus_id": "249282538",
            "title": "TSTR: Too Short to Represent, Summarize with Details! Intro-Guided Extended Summary Generation",
            "text": "We compare our model with two strong non-neural systems, and four state-of-the-art neural summarizers. We use all of these baselines for the purpose of extended summary generation whose documents hold different characteristics in length, writing style, and discourse structure as compared to documents in the other domains of summarization.\n\n-LSA (Steinberger and Jezek, 2004): an extractive vector-based model that utilizes Singular Value Decomposition (SVD) to find the semantically important sentences.\n\n-LEXRANK (Erkan and Radev, 2004): a widely adopted extractive summarization baseline that utilizes a graph-based approach based on eigenvector centrality to identify the most salient sentences.\n\n-BERTSUMEXT (Liu and Lapata, 2019): a contextualized summarizer fine-tuned for summarization task, which encodes input sentence representations, and then processes them through a multi-layer Transformers encoder to obtain document-level sentence representation. Finally, a linear output layer with Sigmoid activation function outputs a probability distribution over each input sentence, denoting the extent to which they are probable to be extracted.\n\n-BERTSUMEXT-INTRO (Liu and Lapata, 2019): a BERTSUMEXT model that only runs on the introductory sentences as the input, and extracts the salient introductory sentences as the summary.\n\n-BERTSUMEXTMULTI (Sotudeh et al., 2021): an extension of the BERTSUMEXT model that incorporates an additional linear layer with Sigmoid classifier to output a probability distribution over a fixed number of pre-defined sections that an input sentence might belong to. The additional network is expected to predict a single section for an input sentence and is trained jointly with BERTSUMEXT module (i.e., sentence extractor).\n\n-BART (Lewis et al., 2020): a state-of-the-art abstractive summarization model that makes use of pretrained encoder and decoder. BART can be thought of as an extension of BERTSUM in which merely encoder is pre-trained, but decoder is trained from scratch. While our model is an extractive one, at the same time, we find it of value to measure the abstractive model performance in the extended summary generation task",
            "score": 0.5025396605923278,
            "section_title": "Baselines",
            "char_start_offset": 14481,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 347,
                    "end": 376,
                    "matchedPaperCorpusId": "14961089"
                },
                {
                    "start": 516,
                    "end": 538,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1356,
                    "end": 1378,
                    "matchedPaperCorpusId": "229679767"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86865234375
        },
        {
            "corpus_id": "250953851",
            "title": "Improving extractive document summarization with sentence centrality",
            "text": "Automatic document summarization aims to produce a concise summary of a document while preserving its crucial information. Existing summarization methods can be divided into two categories: abstractive and extractive methods. Abstractive methods generate a summary word by word from scratch, and can introduce new words that do not appear in the document [1]. Extractive methods, on the other hand, form a summary by selecting text fragments from the original document. Compared with abstractive methods, extractive methods are inclined to generate semantically and grammatically correct sentences [2,3]. \n\nIn recent years, extractive document summarization (EDS) based on neural networks has achieved great success [4][5][6]. However, it faces a challenge in modeling the sentence-document hierarchical structure. Previous approaches to solve this problem can be divided into two categories: (1) constructing hierarchical structures to represent documents and sentences separately; (2) using certain sentence-document information to enhance the representation of sentences. \n\nThere is much excellent work based on the first approach. For example, Zhang et al. [7] proposed a hierarchical transformer [8] called HIBERT to strengthen the relationship between sentences and documents. Xu et al. [9] applied the self-attention scores in the sentence-level Transformer to measure the importance of sentences. Jia et al. [10] employed the hierarchical attention mechanism to establish intersentence relations. Although the hierarchical models effectively capture the sentence-document relationship, complex model architectures and huge computing power requirements limit their actual use scenarios. Another approach usually uses the position information of the sentence in the document to enhance the sentence representation. This method is simple and effective but will cause the sentence-leading bias problem, which means that the extractive summarizer tends to select the leading sentences in the document. Sentence-leading bias will cause the model to rely excessively on sentence position information rather than semantic information when selecting sentences [11,12]. In this paper, we replace the sentence position information with the sentence centrality information. \n\nThe sentence centrality is usually based on undirected graphs and is widely used in unsupervised extractive summarization tasks to identify salient sentences in a document [13,14].",
            "score": 0.5023777277545868,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1074
                },
                {
                    "start": 1077,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2167
                },
                {
                    "start": 2168,
                    "end": 2269
                },
                {
                    "start": 2272,
                    "end": 2452
                }
            ],
            "ref_mentions": [
                {
                    "start": 355,
                    "end": 358,
                    "matchedPaperCorpusId": "235489885"
                },
                {
                    "start": 598,
                    "end": 601,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 601,
                    "end": 603,
                    "matchedPaperCorpusId": "52843977"
                },
                {
                    "start": 716,
                    "end": 719,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 719,
                    "end": 722,
                    "matchedPaperCorpusId": "236459968"
                },
                {
                    "start": 722,
                    "end": 725,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 893,
                    "end": 896,
                    "matchedPaperCorpusId": "235489885"
                },
                {
                    "start": 1161,
                    "end": 1164,
                    "matchedPaperCorpusId": "155100086"
                },
                {
                    "start": 1201,
                    "end": 1204,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1293,
                    "end": 1296,
                    "matchedPaperCorpusId": "223953446"
                },
                {
                    "start": 2159,
                    "end": 2163,
                    "matchedPaperCorpusId": "203593998"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8251953125
        },
        {
            "corpus_id": "247348977",
            "title": "Template-Based Headline Generator for Multiple Documents",
            "text": "General abstractive summarization approaches have recently shown promising results with sequence-to-sequence neural network architectures [18], which encode documents and then decode the learned representations into an abstractive summary. Rush et al. [19] first applied an attention-based sequence-to-sequence model for abstractive summarization. Nallapati et al. [20] futher changed the sequence-to-sequence model to a fully RNN-based model and achieved outstanding performance. The use of the RNN-based encoder-decoder structure has been used from time to time until now. For example, DRGD [21] uses a recurrent generative decoder to learn latent information of the text. On the other hand, Cao et al. [4] considered that seq2seq models tend to copy source words in order, so they proposed a soft template-based summarization Re 3 Sum. In traditional template-based approaches [1] [22], a template using the manually defined rules is an incomplete sentence which can be filled with the keywords. Because templates are manually defined, it is very time-consuming and also requires a great deal of manual effort. Re 3 Sum [4] proposed a novel soft template-based architecture, which uses existing summaries as templates to guide the seq2seq model. BiSET [23], the state-of-the-art template-based abstractive summarization method, follows the previous architecture. To improve expression for output, BiSET uses a bidirectional selective layer with two gates to select key information. TemPEST [3] proposes a personalized subject generation model, which adds a user-aware sequence encoder to generate user-specific article representation, and assists machine generating user-specific subjects. Most of the abstraction-based summary method are seq2seq model, therefore a toolkit NATS [24] collect these methods and conduct on CNN/Daily Mail dataset. Usually, abstraction-based methods are only suitable for single document summarization. Recent publish method BASS [25] applies semantic graph to connect words in input documents. The method BASS connect inputs can connect words be-tween different documents, therefore it also works on multidocument summarization.",
            "score": 0.501549655932769,
            "section_title": "B. ABSTRACTIVE SUMMARIZATION",
            "char_start_offset": 5303,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2162
                }
            ],
            "ref_mentions": [
                {
                    "start": 138,
                    "end": 142,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 252,
                    "end": 256,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 365,
                    "end": 369,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 593,
                    "end": 597,
                    "matchedPaperCorpusId": "1508909"
                },
                {
                    "start": 705,
                    "end": 708,
                    "matchedPaperCorpusId": "51878811"
                },
                {
                    "start": 880,
                    "end": 883,
                    "matchedPaperCorpusId": "958927"
                },
                {
                    "start": 1123,
                    "end": 1126,
                    "matchedPaperCorpusId": "51878811"
                },
                {
                    "start": 1255,
                    "end": 1259,
                    "matchedPaperCorpusId": "186206777"
                },
                {
                    "start": 1493,
                    "end": 1496,
                    "matchedPaperCorpusId": "214423113"
                },
                {
                    "start": 1782,
                    "end": 1786,
                    "matchedPaperCorpusId": "54448559"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91455078125
        },
        {
            "corpus_id": "269663378",
            "title": "Keyword Acquisition for Language Composition Based on TextRank Automatic Summarization Approach",
            "text": "The model utilized pre-trained language model reinforcement sequences and structural encoders to extract key information through a network of structural graphs and graph transformers to effectively guide the decoding process.Tests on a legal opinion news corpus revealed that the model outperforms other baselines in terms of ROUGE and BERT scores, and its effectiveness was proven by manual evaluation [8]. A. Zagar and M. Robnik-Sikonja presented a cross-language AS approach to summarizing Slovenian news articles using a pre-trained English summary model.To address decoder limitations, additional language models were introduced for target language text evaluation.The cross-language model was demonstrated to be qualitatively similar to the target language-specific model through automatic and manual evaluation, but occasionally misleading or absurd content appeared [9].E. Inan proposed an entity-based text summarization method that recognizes named entities and constructs dependency graphs from a pre-trained language model.A reconciliation centrality algorithm was applied to summarize the entity ordering, outperforming the unsupervised learning baseline and approaching the state-of-the-art end-to-end model [10].\n\nIn summary, the recent literature in the field of automatic text processing, especially in keyword extraction and summary generation, has demonstrated several notable advances.Researchers have developed different approaches in order to accommodate multiple languages and text formats.For example, Dewi and Widiastuti developed a model containing multiple summarization techniques specifically for Indonesian text to accommodate long texts and complex documents.In the widely studied TextRank algorithm, Qiu and Zheng enhanced its performance in keyword extraction through tolerance rough sets, while Hernawan et al. improved the accuracy of the algorithm in sentence importance assessment using BM25.Huang and Xie improved the accuracy of keyword extraction for patented text by combining the TextRank algorithm with a priori knowledge networks.Further, Aliakbarpour et al. combined a convolutional neural network and a long and short-term memory network while incorporating an attention mechanism to enhance the quality of text summarization.The elemental graph augmented abstract summarization model proposed by Huang et al. on the other hand, demonstrates superiority in handling legal opinion news.",
            "score": 0.500574530634223,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 4369,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 225,
                    "end": 559
                },
                {
                    "start": 559,
                    "end": 670
                },
                {
                    "start": 670,
                    "end": 878
                },
                {
                    "start": 878,
                    "end": 1035
                },
                {
                    "start": 1035,
                    "end": 1227
                },
                {
                    "start": 1229,
                    "end": 1405
                },
                {
                    "start": 1405,
                    "end": 1513
                },
                {
                    "start": 1513,
                    "end": 1690
                },
                {
                    "start": 1690,
                    "end": 1929
                },
                {
                    "start": 1929,
                    "end": 2074
                },
                {
                    "start": 2074,
                    "end": 2272
                },
                {
                    "start": 2272,
                    "end": 2431
                }
            ],
            "ref_mentions": [
                {
                    "start": 403,
                    "end": 406,
                    "matchedPaperCorpusId": "237621017"
                },
                {
                    "start": 874,
                    "end": 877,
                    "matchedPaperCorpusId": "237396364"
                },
                {
                    "start": 1222,
                    "end": 1226,
                    "matchedPaperCorpusId": "225187980"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79345703125
        },
        {
            "corpus_id": "269922001",
            "title": "Leveraging Discourse Structure for Extractive Meeting Summarization",
            "text": "Meeting summarization. Most work focuses on abstractive summarization and tends to be organized in three interconnected streams (Rennard et al., 2023b). The first focuses on enhancing transcripts with additional information-such as annotations for dialogue acts (Goo and Chen, 2018), discourse structure (Feng et al., 2020) or visual cues (Liu et al., 2019)-that is assumed to be relevant for abstracting important content from a transcript. While adding information does show improvement, few papers actually quantify how much information is gained from adding linguistic features. Feng et al. (2020) used a global node to represent the discourse graph, aggregating all features into a single representative vector sent to an LSTM-based pointer decoder network for summarization; such an approach is not ideal for precisely evaluating the role of discourse structure in enhancing abstractive summaries. \n\nThe second stream aims to transform and compress meeting transcripts in order to produce cleaner and more condensed intermediate documents that can then be passed to downstream generation modules (Krishna et al., 2021;Oya et al., 2014). The third stream is dedicated to the development of multi-task systems, such as DialogLM (Zhong et al., 2022), a language model that is specifically designed to handle dialogue content. While we focus on extractive summarization, in exploiting discourse structure, our approach overlaps with work in the first stream above and lays groundwork for the second stream by providing a means of compressing meeting transcripts. \n\nTurning to extractive approaches to meeting summarization, Murray et al. (2005) evaluated the effectiveness of latent semantic analysis, TF-IDF, and MMR algorithms for this task, providing benchmarks for all three methodologies. More recently, Tixier et al. (2017) developed a technique to construct a graph from key words in a text and then apply submodular optimization to summarize content effectively. \n\nWe note that while significant strides have been made in developing sophisticated algorithms and techniques for meeting summarization, a problem that persists, for both extractive and abstractive summarization, is that of evaluation.",
            "score": 0.5002295802280837,
            "section_title": "Related work",
            "char_start_offset": 5876,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 23,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 903
                },
                {
                    "start": 906,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1564
                },
                {
                    "start": 1567,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1972
                },
                {
                    "start": 1975,
                    "end": 2208
                }
            ],
            "ref_mentions": [
                {
                    "start": 128,
                    "end": 151,
                    "matchedPaperCorpusId": "251402499"
                },
                {
                    "start": 304,
                    "end": 323,
                    "matchedPaperCorpusId": "234778273"
                },
                {
                    "start": 583,
                    "end": 601,
                    "matchedPaperCorpusId": "234778273"
                },
                {
                    "start": 1102,
                    "end": 1124,
                    "matchedPaperCorpusId": "235303695"
                },
                {
                    "start": 1124,
                    "end": 1141,
                    "matchedPaperCorpusId": "12299544"
                },
                {
                    "start": 1811,
                    "end": 1831,
                    "matchedPaperCorpusId": "42957184"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.703125
        },
        {
            "corpus_id": "252918339",
            "title": "Towards Summary Candidates Fusion",
            "text": "Second-stage methods have recently enabled strong progress in the state-of-the-art of abstractive summarization research. GSum (Dou et al., 2021) uses additional discrete guidance signal such as salient sentences predicted by an extractive model to better guide the abstractive system. While abstractive summarization models are trained to maximize MLE at the token-level, second-stage methods usually work at the sequence-level. ConSum (Sun and Li, 2021) and SeqCo (Xu et al., 2021) fine-tune the model with a different, contrastive loss to assign more confidence to higher-quality summary candidates. RefSum (Liu et al., 2021) uses metalearning to re-rank summary candidates produced by several base systems. SummaReranker (Ravaut et al., 2022) and SimCLS (Liu and Liu, 2021) train a RoBERTa to re-rank candidates, the former with multi-label binary cross-entopy, the latter with con-trastive learning and a ranking loss. BRIO (Liu et al., 2022b) re-uses the base model for a secondround of fine-tuning with both the cross-entropy loss and a candidate-level ranking loss. Existing fusion work in summarization focuses on sentence fusion. Fusing several sentences for the purpose of summarization was first proposed by Barzilay and McKeown (2005), paving the way for more abstractive summaries. Weiss et al. (2021) later proposed a much larger dataset for sentence fusion in multi-document abstractive summarization, driving up model performance. Through a thorough human evaluation, Lebanoff et al. (2019) ask annotators to label which type of fusion of sentences is taking place while also rating the sentence properties for sentences generated by several abstractive systems. In a follow-up work (Lebanoff et al., 2020b), the authors build a Transformer model (Vaswani et al., 2017) enriched with sentence structure information for the explicit goal of fusing sentences, and evaluate the model on a dataset dedicated to sentence fusion.",
            "score": 0.5000234060481228,
            "section_title": "Related Work",
            "char_start_offset": 5744,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1940
                }
            ],
            "ref_mentions": [
                {
                    "start": 127,
                    "end": 145,
                    "matchedPaperCorpusId": "223953416"
                },
                {
                    "start": 466,
                    "end": 483,
                    "matchedPaperCorpusId": "209832298"
                },
                {
                    "start": 725,
                    "end": 746,
                    "matchedPaperCorpusId": "247447662"
                },
                {
                    "start": 929,
                    "end": 948,
                    "matchedPaperCorpusId": "247839752"
                },
                {
                    "start": 1220,
                    "end": 1247,
                    "matchedPaperCorpusId": "16188305"
                },
                {
                    "start": 1485,
                    "end": 1507,
                    "matchedPaperCorpusId": "203610250"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8857421875
        },
        {
            "corpus_id": "250374734",
            "title": "Two-phase Multi-document Event Summarization on Core Event Graphs",
            "text": "To evaluate the effectiveness of the proposed graph dominant event detection and eventaware pointer generator, we implement three types of baselines: concat-based methods with all articles concatenated as input, traditional and neural extract-based methods with extracted sentence as the input, the ruled-based summarization model based on the extracted event graph. \n\n\u2022 S2S-att-concat / S2S-att-extract: Attention-equipped sequence-to-sequence is the basic neural network for abstractive summarization that contains a two-layer BiLSTM encoder and a one-layer LSTM decoder equipped with attention (Nallapati et al., 2016). \n\n\u2022 Ptr-Net-concat/ Ptr-extract: Pointer network directly uses the attention mechanism as a pointer to select tokens from the input as the output (Vinyals et al., 2015). \n\n\u2022 Ptr-gen-concat/ Pg-extract: Pointer generator is a hybrid model combing Seq2seqatt with pointer network (See et al., 2017). \n\n\u2022 mBART-concat/ mBART-extract: mBART (Liu et al., 2020) is a sequenceto-sequence denoising auto-encoder pretrained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019) and utilized for various generation tasks including summarization. \n\n\u2022 Trunc.: Truncation-based method is a simple traditional baseline where words are kept in the original order until the length limit is reached. \n\n\u2022 ILP: ILP-based method is an unsupervised method that relies on the preprocessing (i.e., NER, term weighting) results of input sequences (Clarke et al., 2008), which is a strong baseline for traditional sentence compression. \n\n\u2022 Graph-gen is the graph-based summarization baseline utilizing ILP maximization and links of core event semantic graph to select salient information and generate event sequence following some specific templates. \n\n\u2022 Event-Pg is our event-aware pointer network that integrates event semantic graph information for summarization.",
            "score": 0.499640188760187,
            "section_title": "Baselines",
            "char_start_offset": 25247,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 366
                },
                {
                    "start": 369,
                    "end": 622
                },
                {
                    "start": 625,
                    "end": 792
                },
                {
                    "start": 795,
                    "end": 920
                },
                {
                    "start": 923,
                    "end": 1203
                },
                {
                    "start": 1206,
                    "end": 1350
                },
                {
                    "start": 1353,
                    "end": 1578
                },
                {
                    "start": 1581,
                    "end": 1793
                },
                {
                    "start": 1796,
                    "end": 1909
                }
            ],
            "ref_mentions": [
                {
                    "start": 769,
                    "end": 791,
                    "matchedPaperCorpusId": "5692837"
                },
                {
                    "start": 901,
                    "end": 919,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1491,
                    "end": 1512,
                    "matchedPaperCorpusId": "3004447"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9375
        },
        {
            "corpus_id": "260829737",
            "title": "Cross-lingual extreme summarization of scholarly documents",
            "text": "Summarization is a longstanding task in NLP [33,67]. While early efforts focused mostly on extractive summarization [55], e.g., using an unsupervised graph-based approach [63], abstractive summarization has gained ever more traction in recent years starting with work using sequence-to-sequence models [75]. Just like in virtually all areas of NLP research, most successful current approaches to summarization rely on neural techniques using supervision from labeled data. This includes neural models to summarize documents in general domains such as news articles [56,81], including cross-and multilingual models and datasets [80,87], as well as specialized ones e.g., the biomedical domain [64]. Work on cross-lingual summarization has historically received little attention until recent years [90], arguably to due to the availability of new resources (Sect. 2.1) as well as neural multilingual summarizers. \n\nSummarization of scientific documents. In recent years, there has been much work on the problem of summarizing scientific publications and community-driven evaluation campaigns such as the CL-SciSumm shared tasks [12,40]. Previous work on summarization has focused on specific features of scientific documents such as using citation contexts [13,97] or document structure [15,19]. Complementary to these efforts is a recent line of work on automatically generating visual summaries or graphical abstracts [93,94]. In our work, we build upon recent contributions on using multilingual pre-trained language models for cross-lingual summarization [46] and extreme summarization for English [10] and bring these two lines of research together to propose the new task of cross-lingual extreme summarization of scientific documents. \n\nKnowledge distillation for summarization models. While massively large pretrained language models achieve strong results on various summarization tasks, the enormous sizes hinder their deployment in real-world applications. Knowledge distillation [36] offers a chance to reduce the model size by transferring knowledge of the original teacher model to a smaller student without large performance drops.",
            "score": 0.4992277180419923,
            "section_title": "Automated text summarization.",
            "char_start_offset": 12514,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 53,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 910
                },
                {
                    "start": 913,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1739
                },
                {
                    "start": 1742,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2144
                }
            ],
            "ref_mentions": [
                {
                    "start": 44,
                    "end": 48,
                    "matchedPaperCorpusId": "206411929"
                },
                {
                    "start": 48,
                    "end": 51,
                    "matchedPaperCorpusId": "47179013"
                },
                {
                    "start": 116,
                    "end": 120,
                    "matchedPaperCorpusId": "198189946"
                },
                {
                    "start": 171,
                    "end": 175,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 302,
                    "end": 306,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 565,
                    "end": 569,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 569,
                    "end": 572,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 627,
                    "end": 631,
                    "matchedPaperCorpusId": "216868259"
                },
                {
                    "start": 631,
                    "end": 634,
                    "matchedPaperCorpusId": "243865257"
                },
                {
                    "start": 692,
                    "end": 696,
                    "matchedPaperCorpusId": "22257046"
                },
                {
                    "start": 1255,
                    "end": 1259,
                    "matchedPaperCorpusId": "36804119"
                },
                {
                    "start": 1259,
                    "end": 1262,
                    "matchedPaperCorpusId": "218531461"
                },
                {
                    "start": 1285,
                    "end": 1289,
                    "matchedPaperCorpusId": "4894594"
                },
                {
                    "start": 1289,
                    "end": 1292,
                    "matchedPaperCorpusId": "13321076"
                },
                {
                    "start": 1418,
                    "end": 1422,
                    "matchedPaperCorpusId": "237218696"
                },
                {
                    "start": 1422,
                    "end": 1425,
                    "matchedPaperCorpusId": "211026676"
                },
                {
                    "start": 1557,
                    "end": 1561,
                    "matchedPaperCorpusId": "222177239"
                },
                {
                    "start": 1600,
                    "end": 1604,
                    "matchedPaperCorpusId": "216867622"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.861328125
        },
        {
            "corpus_id": "271525553",
            "title": "Abstractive text summarization: State of the art, challenges, and improvements",
            "text": "Reinforcement Learning (RL) approaches [12,13] also play a significant role, with models learning to optimize summary quality based on human-like preferences. Hierarchical approaches [14,15], on the other hand, focus on exploiting the inherent structure of input texts to generate more coherent and informative summaries. Finally, Multi-modal Summarization [16,17] methods combine different data modalities, such as text and images, to generate comprehensive and context-rich summaries. By categorizing the state-of-the-art techniques in this way, we hope to provide a clear and structured overview of the current research landscape in abstractive summarization. \n\nThe integration of structured knowledge bases has significantly improved the accuracy and coherence of content generated by natural language models [18,19]. Despite this, chal-lenges in capturing intricate meanings remain, leading to innovations like attention mechanisms [20] and advanced knowledge representation [21]. Traditional metrics often miss the semantic depth and have encouraged the adoption of BERTScore [22] and MoverScore [23]. Summarizing long documents is addressed using hierarchical models [15] and memory-augmented networks [24]. Emphasis on factual consistency has spurred strategies like knowledge integration [25] and reinforcement learning [26]. The emergence of models like CTRL [27] highlights the trend toward controllable summarization. With the digital realm diversifying, there is a push for multimodal summarization. As AI becomes more influential, transparency and interpretability are paramount. Overall, abstractive text summarization is continuously evolving, meeting challenges with innovative solutions.",
            "score": 0.49846210719410294,
            "section_title": "Introduction",
            "char_start_offset": 4538,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 662
                },
                {
                    "start": 665,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1705
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 187,
                    "matchedPaperCorpusId": "6857205"
                },
                {
                    "start": 357,
                    "end": 361,
                    "matchedPaperCorpusId": "237492095"
                },
                {
                    "start": 361,
                    "end": 364,
                    "matchedPaperCorpusId": "237490408"
                },
                {
                    "start": 813,
                    "end": 817,
                    "matchedPaperCorpusId": "59940"
                },
                {
                    "start": 817,
                    "end": 820,
                    "matchedPaperCorpusId": "715463"
                },
                {
                    "start": 1209,
                    "end": 1213,
                    "matchedPaperCorpusId": "6466088"
                },
                {
                    "start": 1297,
                    "end": 1301,
                    "matchedPaperCorpusId": "11918447"
                },
                {
                    "start": 1329,
                    "end": 1333,
                    "matchedPaperCorpusId": "207794190"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80126953125
        },
        {
            "corpus_id": "272831675",
            "title": "GLARE: Guided LexRank for Advanced Retrieval in Legal Analysis",
            "text": "LexRank (Erkan and Radev, 2004), an unsupervised graph-based method, is initially applied to the document to reduce it to a length corresponding to 35% of the original size. The second step uses the PEGASUS neural language model (Zhang et al., 2020) for text summarization. \n\nPEGASUS is based on the transformer (Vaswani et al., 2023) architecture. It follows the encoder-decoder pattern, where the encoder processes the original text to extract contextual representations while the decoder generates the summary based on these representations. The encoder comprises several layers of encoding, while the decoder also has several layers to generate a token-by-token summary. A linear layer is applied to map the decoder outputs to the vocabulary, allowing the prediction of the following words in the summary. In the method proposed by Dalal et al. (2023), the extractive summary generated by the LexRank model is divided into sentences and fed to the PEGASUS model. The model then derives paraphrases for each sentence, making it simpler to understand. The sentences are then merged to provide a final abstract summary. \n\nOur work is related to Dalal et al. (2023), who used the graph-based LexRank algorithm to create the legal document summary. However, as a distinguishing feature of our method, we modified the original LexRank algorithm, seeking a way to guide it in the summarization process and thus improve the generated summary. \n\nSatwick Gupta et al. (2022) compare summary techniques and apply them to legal documents. The work method covers the stages of Dataset Acquisition, Data Cleaning, Text summarization, Summary evaluation, and Comparative analysis. In the extractive summarization stage, the following techniques are compared: LexRank, TextRank, Reduction, Luhn, Edmundson, Latent Semantic Analysis, and SumBasic. The results show that graph-based approaches such as LexRank and TextRank perform better than approaches based on frequency. Similarly to Satwick Gupta et al. (2022), we attest to the good performance of the graph-based approach but make a comparison with methods that implement topic modeling.",
            "score": 0.4982093153338833,
            "section_title": "Related work",
            "char_start_offset": 17348,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 273
                },
                {
                    "start": 276,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1120
                },
                {
                    "start": 1123,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1438
                },
                {
                    "start": 1441,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2129
                }
            ],
            "ref_mentions": [
                {
                    "start": 8,
                    "end": 31,
                    "matchedPaperCorpusId": "506350"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8837890625
        },
        {
            "corpus_id": "266191505",
            "title": "Prompting LLMs with content plans to enhance the summarization of scientific articles",
            "text": "Automatically summarizing scientific articles is a long-studied challenge within automatic text summarization. Earlier conventional approaches relied heavily on extractive methods, where summarization involves identifying and extracting salient snippets, typically sentences, from the original document. For instance, one common family of techniques scored sentences based on statistical metrics like word frequencies to determine the importance of extraction [35,30,6]. \n\nThere have also been attempts to produce scientific article summaries by analyzing their citation contexts [26,12,28]. \n\nHowever, the current dominant paradigm has shifted decisively toward abstractive methods using neural network architectures. Our work focuses on techniques to enhance state-of-the-art abstractive scientific summarizers based on transformer models, which have become ubiquitous in natural language generation applications. Before discussing our proposed contributions, we first contextualize our work by summarizing prior studies on which we seek to build.",
            "score": 0.4980985389094128,
            "section_title": "Related Work",
            "char_start_offset": 5192,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 470
                },
                {
                    "start": 473,
                    "end": 591
                },
                {
                    "start": 594,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1049
                }
            ],
            "ref_mentions": [
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "15145496"
                },
                {
                    "start": 580,
                    "end": 584,
                    "matchedPaperCorpusId": "6494467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8134765625
        },
        {
            "corpus_id": "258739040",
            "title": "Joint learning of text alignment and abstractive summarization for long documents via unbalanced optimal transport",
            "text": "Text summarization is the procedure of identifying the most important information from source text and producing a concise and readable summary (Mani, 1999). Generally speaking, the summarization models can be divided into two types: extraction or abstraction (Gambhir and Gupta, 2017). The extractive approach directly extracts snippets, such as sentences or phrases from the original documents as the summary. In contrast, the abstractive approach uses natural language generation techniques to produce fluent summaries and it may generate expressions not directly Gigaword (Rush et al., 2015) n e w s GovReport (Huang et al., 2021) government report 9409 296 19.8 553 20.0\n\nFor GovReport, its documents are organized in a form of multi-level sections. The method of dividing its documents to a form of one-level sections is discussed in Section 5.1.\n\nexisting in the source document. Nowadays, neural abstractive text summarization (NATS) models (Shi et al., 2021) based on sequence-to-sequence (Seq2Seq) architecture (Sutskever, Vinyals, and Le, 2014) are prevailing. In practice, different types of documents vary greatly in length.\n\nIn Table 1, we present the length statistics of several popular summarization datasets. It can be observed that news stories are shorter than 800 words on average. In contrast, the average length of research papers exceeds 3000 words, and the average length of government reports even exceeds 9000 words. Most existing NATS models treat source document and summary as two single sequences, which works well in summarizing documents of short and medium lengths. However, limited by the underlying neural architectures of NATS models, this practice leads to some de facto difficulties when applied to long documents. Previous studies show that vanilla LSTM (Hochreiter and Schmidhuber, 1997) and vanilla Transformer (Vaswani et al., 2017) can effectively handle sequences of several hundred words at most (Khandelwal et al., 2018;Dai et al., 2019). Besides, the memory and time complexities of computing Transformer grow quadratically with the sequence length. This constraint also limits the application of Transformer in long documents, since long sequences easily run",
            "score": 0.4970403541451338,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 260,
                    "end": 285,
                    "matchedPaperCorpusId": "24465182"
                },
                {
                    "start": 576,
                    "end": 595,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 614,
                    "end": 634,
                    "matchedPaperCorpusId": "233033613"
                },
                {
                    "start": 949,
                    "end": 967,
                    "matchedPaperCorpusId": "54448559"
                },
                {
                    "start": 1021,
                    "end": 1055,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 1794,
                    "end": 1828,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1853,
                    "end": 1875,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4208984375
        },
        {
            "corpus_id": "252847535",
            "title": "Entity-based De-noising Modeling for Controllable Dialogue Summarization",
            "text": "Text summarization is studied in extractive and abstractive paradigms (Gehrmann et al., 2018). In extractive studies, non-neural approaches utilize various linguistic and statistical features via lexical (Kupiec et al., 1995) and graph-based modeling (Erkan and Radev, 2004), and neural approaches bring about substantial improvements via featurerich distributional representation and hierarchical context modeling (Nallapati et al., 2017;Kedzie et al., 2018). In contrast, abstractive approaches are expected to generate more concise and fluent summaries, which brings about different technical challenges. To foster end-to-end data-driven methods, corpora in news domain (e.g., CNN/Daily Mail (Hermann et al., 2015), NYT (Sandhaus, 2008)) are constructed, and sophisticated neural architectures for abstractive summarization are proposed, such as LSTM-based encoding-decoding (Rush et al., 2015), pointer-generator networks (See et al., 2017), hybrid extractive-abstractive summarizer Gehrmann et al. (2018), and fine-tuning large-scale pre-trained language models (Liu and Lapata, 2019;Lewis et al., 2020). Recently, datasets for summarizing conversations are constructed from meetings (Zhong et al., 2021) or daily chats (Gliwa et al., 2019). Based on the linguistic features of human conversations, many studies pay attention to utilizing conversational analysis for dialogue summarization, such as leveraging dialogue acts (Goo and Chen, 2018), multi-modal features (Li et al., 2019), topic information (Liu et al., 2019), coreference (Liu et al., 2021b), and fine-grained view segmentation with hierarchical modeling (Chen and Yang, 2020). \n\nControllable language generation introduces auxiliary signals to obtain diverse or task-specific outputs. Such tasks include text style transfer (Shen et al., 2017) and paraphrasing (Iyyer et al., 2018).",
            "score": 0.4966894898081759,
            "section_title": "Related Work",
            "char_start_offset": 3807,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1646
                },
                {
                    "start": 1649,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1852
                }
            ],
            "ref_mentions": [
                {
                    "start": 70,
                    "end": 93,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 204,
                    "end": 225,
                    "matchedPaperCorpusId": "5775833"
                },
                {
                    "start": 251,
                    "end": 274,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 415,
                    "end": 439,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 439,
                    "end": 459,
                    "matchedPaperCorpusId": "53083054"
                },
                {
                    "start": 878,
                    "end": 897,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 926,
                    "end": 944,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 987,
                    "end": 1009,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 1067,
                    "end": 1089,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 1089,
                    "end": 1108,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1189,
                    "end": 1209,
                    "matchedPaperCorpusId": "233219904"
                },
                {
                    "start": 1225,
                    "end": 1245,
                    "matchedPaperCorpusId": "208010268"
                },
                {
                    "start": 1472,
                    "end": 1489,
                    "matchedPaperCorpusId": "196199831"
                },
                {
                    "start": 1541,
                    "end": 1560,
                    "matchedPaperCorpusId": "235446920"
                },
                {
                    "start": 1624,
                    "end": 1645,
                    "matchedPaperCorpusId": "222133028"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72900390625
        },
        {
            "corpus_id": "258557792",
            "title": "Latest Trends in Artificial Intelligence Technology: A Scoping Review",
            "text": "MES is an event summarization task. While conventional multi-document summarization tries to generate a summary containing multiple sentences from a set of documents, MES tries to summarize the core event in few words. The purpose of MES is to produce a query-level summary, which is essential for example for search engines (Chen et al., 2022). \n\nIn generation and summarization tasks encoder-decoder neural network models have shown robust representation capabilities. However, capturing relation in different documents and eliminating redundancies are major shortcomings while operating settings with multiple documents. Graph-based methods have presented good performance while handling multiple documents. However, syntactic information loss is a main flaw with graphbased methods (Chen et al., 2022). Chen et al. (2022) has introduced a MES framework called Event-Pg. The Event-Pg contains two different phases; the first phase an event identification phase, and the second phase a generation phase. In the event identification phase events are detected on sentence level and on graph-level. In the generation phase an event-aware pointer generator is used for event sequence generation. Based on the results, Event-Pg was very effective against baseline algorithms such as mBart.",
            "score": 0.4954471251865954,
            "section_title": "Succinct description of the event",
            "char_start_offset": 14595,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 36,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 345
                },
                {
                    "start": 348,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1286
                }
            ],
            "ref_mentions": [
                {
                    "start": 325,
                    "end": 344,
                    "matchedPaperCorpusId": "250374734"
                },
                {
                    "start": 786,
                    "end": 805,
                    "matchedPaperCorpusId": "250374734"
                },
                {
                    "start": 807,
                    "end": 825,
                    "matchedPaperCorpusId": "250374734"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.740234375
        },
        {
            "corpus_id": "264290271",
            "title": "Graph-Based Abstractive Summarization of Extracted Essential Knowledge for Low-Resource Scenarios",
            "text": "The growing availability of unstructured information promotes text summarization solutions, which aim to generate concise synopses that convey the exact semantics of the source [69]. Recently, due to the latest advancements in natural language processing (NLP)from information retrieval [13,50,51,52] to entity relationships acquisition [14,15], classification [10], and event extraction [16]abstractive summarization [4,12,31] is receiving more interest, aiming to paraphrase the most meaningful details of documents instead of just retrieving them (i.e., extractive summarization). In this context, a particularly tough challenge is to process massive sequences, namely the task of long-input summarization. Two subtasks are longand multi-document summarization (LDS and MDS, respectively). LDS [29] wants to capture and condense salient points scattered across a lengthy input (e.g., 10K words). In MDS [41], the synthesis is generated from a pool of sources related to the topic whose concatenation assembles a single long input to enable the summarization task to be addressed as in LDS [68]. Thus, from now on, we refer to \"the long input\" as the source of both LDS and MDS tasks. \n\nState-of-the-art (SOTA) solutions for long-input summarization are built upon transformers [63], which proficiently capture longdistance relations between words 1 with the self-attention mechanism. Nevertheless, such models are denoted by a structural constraint that proportionally links their memory usage to the input size, making Figure 1. Overview of our approach. A long input X (each x \u2208 X is a sentence) is converted into a heterogeneous graph: the pink diamonds represent the keyword nodes K, the violet circles denote the sentence nodes S containing keywords, the green circles indicate the context of S, and the different segments between nodes symbolize edges. The salient sentences are extracted and given to a generative PLM to produce the summary. \n\nthem unduly resource-demanding when processing long texts.",
            "score": 0.49506563511380075,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1186
                },
                {
                    "start": 1189,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 1951
                },
                {
                    "start": 1954,
                    "end": 2012
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 181,
                    "matchedPaperCorpusId": "254952632"
                },
                {
                    "start": 287,
                    "end": 291,
                    "matchedPaperCorpusId": "256461073"
                },
                {
                    "start": 294,
                    "end": 297,
                    "matchedPaperCorpusId": "257882420"
                },
                {
                    "start": 297,
                    "end": 300,
                    "matchedPaperCorpusId": "238639683"
                },
                {
                    "start": 337,
                    "end": 341,
                    "matchedPaperCorpusId": "236460045"
                },
                {
                    "start": 341,
                    "end": 344,
                    "matchedPaperCorpusId": "220847089"
                },
                {
                    "start": 361,
                    "end": 365,
                    "matchedPaperCorpusId": "35329921"
                },
                {
                    "start": 388,
                    "end": 392,
                    "matchedPaperCorpusId": "244678529"
                },
                {
                    "start": 421,
                    "end": 424,
                    "matchedPaperCorpusId": "259292631"
                },
                {
                    "start": 424,
                    "end": 427,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 797,
                    "end": 801,
                    "matchedPaperCorpusId": "250118028"
                },
                {
                    "start": 906,
                    "end": 910,
                    "matchedPaperCorpusId": "226289939"
                },
                {
                    "start": 1092,
                    "end": 1096,
                    "matchedPaperCorpusId": "247519084"
                },
                {
                    "start": 1280,
                    "end": 1284,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "271938110",
            "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
            "text": "Modern text summarization research has shifted from homogeneous graphs with static nodes to heterogeneous networks. These networks allow for the inclusion of diverse node types, which represent wide range of textual elements. In addition, they allow for dynamic updates during the summarization process. The approach of [4] constructed a graph neural network based on word co-occurrences within the document to capture word-level relationships. The study in [42] utilized syntactic graph convolutional networks (GCNs) to model the non-Euclidean structure of documents. This approach effectively captured long-range dependencies beyond simple word order. In addition, an attention mechanism is integrated to focus on relevant content for summarization. The authors of [43] employed a network with three distinct node types: sentences, Elementary Discourse Units (EDUs), and entities. They utilized RST discourse parsing to grasp the relationships between EDUs, which provides a deeper comprehension of the document's structure. \n\nThe research in [44] presented a heterogeneous graph network that incorporated information from both words and sentences. The model focused on redundancy dependencies between sentences, and iteratively refined sentence representations through a redundancy-aware graph. This iterative process aimed to enhance the model's ability to capture the essential meaning of each sentence. Recent research examined novel network structures and attention mechanisms for extractive summarization of long documents [45]. One approach utilized a transformer-based architecture within a heterogeneous network and included distinct node types for tokens, entities, and sentences. The study discovered that a multi-granularity sparse attention mechanism assisted in focusing on important relationships between these diverse nodes during the summarization process. \n\nAnother study proposed a novel Multiplex Graph Convolutional Network (M-GCN) architecture [46], which were effective at capturing various kinds of relationships between words and sentences. The model took into account both relationships within sentences (intra-sentential) and relationships between sentences (inter-sentential) to produce better summaries, particularly for long documents. \n\nMoreover, the work of [14] improved the current heterogeneous graph approaches by adding passage nodes alongside word and sentence nodes.",
            "score": 0.49506449754752213,
            "section_title": "C. NEURAL GRAPH-BASED TEXT SUMMARIZATION",
            "char_start_offset": 12480,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1026
                },
                {
                    "start": 1029,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1875
                },
                {
                    "start": 1878,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2267
                },
                {
                    "start": 2270,
                    "end": 2407
                }
            ],
            "ref_mentions": [
                {
                    "start": 458,
                    "end": 462,
                    "matchedPaperCorpusId": "212747858"
                },
                {
                    "start": 767,
                    "end": 771,
                    "matchedPaperCorpusId": "233189611"
                },
                {
                    "start": 1045,
                    "end": 1049,
                    "matchedPaperCorpusId": "226262332"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.978515625
        },
        {
            "corpus_id": "252604304",
            "title": "Abstractive Summarization of Broadcast News Stories for Estonian",
            "text": "The state-of-the-art neural abstractive summarization models are trained with annotated datasets of hundreds of thousands or millions of data points. At the same time, such quality and quantity are not feasible for most languages, including Estonian. \n\nThis work focuses on achieving state-of-the-art low-resource abstractive summarization results for the Estonian language radio news stories. We propose an approach that consists of two steps: automatically generating the transcript, and applying a stateof-the-art text summarization system to generate the result. To overcome the problems of limited available data for training, transfer learning methods on pre-trained models, multilingual models and machine translation were explored and included in the summarization pipeline.",
            "score": 0.4946408421835438,
            "section_title": "Introduction",
            "char_start_offset": 2108,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 250
                },
                {
                    "start": 253,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 782
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.419921875
        },
        {
            "corpus_id": "271533822",
            "title": "Synthesizing Scientific Summaries: An Extractive and Abstractive Approach",
            "text": "In this paper, we have performed experiments on summarization using a combination of two models for extractive and two models for abstractive summarization. We fine-tuned the pre-trained models on a dataset that is specific to summarization of long documents (in our case, scientific papers) and built a pipeline consisting of two major steps in summarization of those papers. We further performed inference on the created combination of models by using three evaluation metrics and have reported the results. We find that it is also possible to exceed the abstractiveness of ground-truth summaries using a certain combination of hyperparameters. Further work in this direction can be focused on applying techniques to improve the ROUGE-score and abstractiveness scores. Additionally, we would also like to perform experiments in extending this approach towards summarization of long documents in niche areas such as legal documents or biomedical data.",
            "score": 0.49457585486093436,
            "section_title": "VI. CONCLUSION AND FUTURE WORK",
            "char_start_offset": 27371,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 952
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5927734375
        },
        {
            "corpus_id": "254408844",
            "title": "Scientific Paper Extractive Summarization Enhanced by Citation Graphs",
            "text": "Researchers usually write an abstract of a paper by referring to some examples, especially from a large number of papers on the same topic. Hence, in this paper, we propose to leverage citation graphs to enhance scientific paper extractive summarization. Concretely, we propose a multi-granularity unsupervised summarization model and a graph-based supervised summarization model, both of which outperform the state-of-the-art baselines by a large margin. In the future, we will explore the effect of other graph structure information on the summarization task such as pairwise distance information.",
            "score": 0.4938647914665709,
            "section_title": "Conclusion",
            "char_start_offset": 28751,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 599
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "262044956",
            "title": "When Large Language Models Meet Citation: A Survey",
            "text": "The selected sentences from the referenced paper, which have semantic similarities with the publication's sentences, are considered cited statements. This process entails the development of a citation linkage framework using both sequential and treestructured models based on Bio-RoBERTa. In addition, Mao et al. (2022) feed document-level text, i.e., citation text, abstract, and conclusion into the generative models BART-large (Lewis et al., 2020) and PEGASUS-large (Zhang et al., 2020) as training data, using the subsequent trained models to effectively produce comprehensive scientific extreme summarization, i.e. single sentence summaries. \n\nIncorporation with Graph Structure. The above citation-based summarization methods only consider surface document information. However, relevant citations can provide rich literature network information. Thus, some research propose to combine LLMs with Graph-based neural networks to effectively incorporate these additional information. Chen et al. (2022) first shift through comparative citations to identify objects of comparison. They then construct a comparative scientific summarization corpus and put forward the Comparative Graph-based Summarization BERT (CGSUM-BERT). This model is designed to craft comparative summaries with the assistance of citation guidance. Cai et al. (2022) propose a model called COVIDSum for summarizing COVID-19 scientific papers, which leverages a linguistically enriched SciBERT framework. The process begins with the extraction of key sentences from the source papers and the construction of word co-occurrence graphs. A SciBERT-based sequence encoder is then employed to encode the sentences, while a Graph Attention Networks-based graph encoder processes the word co-occurrence graphs. In the final stage, these two encodings are fused to generate an abstract summary of each scientific paper.",
            "score": 0.49370435202257185,
            "section_title": "LLMs for Citation-based summarization",
            "char_start_offset": 11337,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 646
                },
                {
                    "start": 649,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1883
                }
            ],
            "ref_mentions": [
                {
                    "start": 302,
                    "end": 319,
                    "matchedPaperCorpusId": "256460952"
                },
                {
                    "start": 430,
                    "end": 450,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 469,
                    "end": 489,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 987,
                    "end": 1005,
                    "matchedPaperCorpusId": "252819466"
                },
                {
                    "start": 1322,
                    "end": 1339,
                    "matchedPaperCorpusId": "246388617"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90966796875
        },
        {
            "corpus_id": "260829737",
            "title": "Cross-lingual extreme summarization of scholarly documents",
            "text": "The number of scientific publications nowadays is rapidly increasing, causing information overload for researchers and making it hard for scholars to keep up to date with current trends and lines of work. Recent work has tried to address this problem by developing methods for automated summarization in the scholarly domain, but concentrated so far only on monolingual settings, primarily English. In this paper, we consequently explore how state-of-the-art neural abstract summarization models based on a multilingual encoder\u2013decoder architecture can be used to enable cross-lingual extreme summaries of scholarly texts. To this end, we compile a new abstractive cross-lingual summarization dataset for the scholarly domain in four different languages, which enables us to train and evaluate models that process English papers and generate summaries in German, Italian, Chinese and Japanese. We present our new X-SCITLDR dataset for multilingual summarization and thoroughly benchmark different models based on a state-of-the-art multilingual pre-trained model, including a two-stage pipeline approach that independently summarizes and translates, as well as a direct cross-lingual model. We additionally explore the benefits of intermediate-stage training using English monolingual summarization and machine translation as intermediate tasks and analyze performance in zero- and few-shot scenarios. Finally, we investigate how to make our approach more efficient on the basis of knowledge distillation methods, which make it possible to shrink the size of our models, so as to reduce the computational complexity of the summarization inference.",
            "score": 0.49326788528407395,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76171875
        },
        {
            "corpus_id": "254408844",
            "title": "Scientific Paper Extractive Summarization Enhanced by Citation Graphs",
            "text": "In a citation graph, adjacent paper nodes share related scientific terms and topics. The graph thus conveys unique structure information of document-level relatedness that can be utilized in the paper summarization task, for exploring beyond the intra-document information.In this work, we focus on leveraging citation graphs to improve scientific paper extractive summarization under different settings.We first propose a Multi-granularity Unsupervised Summarization model (MUS) as a simple and low-cost solution to the task.MUS finetunes a pre-trained encoder model on the citation graph by link prediction tasks.Then, the abstract sentences are extracted from the corresponding paper considering multi-granularity information.Preliminary results demonstrate that citation graph is helpful even in a simple unsupervised framework.Motivated by this, we next propose a Graph-based Supervised Summarizationmodel (GSS) to achieve more accurate results on the task when large-scale labeled data are available.Apart from employing the link prediction as an auxiliary task, GSS introduces a gated sentence encoder and a graph information fusion module to take advantage of the graph information to polish the sentence representation.Experiments on a public benchmark dataset show that MUS and GSS bring substantial improvements over the prior state-of-the-art model.",
            "score": 0.49281921252649274,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.919921875
        },
        {
            "corpus_id": "257677162",
            "title": "Extracting the evolutionary backbone of scientific domains: The semantic main path network analysis approach based on citation context analysis",
            "text": "Nenkova and H. Saggion respectively). \n\nFinally, the MPN extracted from AANSum_del_Bkg_-Fut (Figure 17) recovered the vanished or shrunk branches about multidocument summarization (Branch 1) and graph-based ranking (Branch 2), and at the same time introduced some new papers, such as C04-1129 for Branch 1 (\"Syntactic Simplification For Improving Content Selection In Multi-Document Summarization\"), P08-1048 for Branch 2 (\"Summarizing Emails with Conversational Cohesion and Subjectivity,\" whose abstract says \"Second, we use two graph-based summarization approaches, \u2026, to extract sentences as summaries.\"), and W09-1802 (\"A Scalable Global Model for Summarization,\" whose abstract says \"We present an Integer Linear Program for \u2026 for automatic summarization.\") and C10-2105 (\"Opinion Summarization with Integer Linear Programming Formulation for Sentence Extraction and Ordering\") for Branch 3 about optimization methods for summarization. \n\nAgain, by gradually adding more citation semantics, the semantic MPNs together proved to be more expressive than the semantics-agnostic counterpart.",
            "score": 0.4913705611506485,
            "section_title": "| Case Study 2: Automatic document summarization",
            "char_start_offset": 28859,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 40,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 942
                },
                {
                    "start": 945,
                    "end": 1093
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.147216796875
        },
        {
            "corpus_id": "253420589",
            "title": "Unsupervised Extractive Summarization With Heterogeneous Graph Embeddings for Chinese Documents",
            "text": "Text summarization is the task of automatically condensing the input document to a shorter version while maintaining its most important information. The ability to condense text information can be applied to various applications such as online news services, answering questions, and opinion mining. Basically, there are two main types of text summarization tasks: extractive and abstractive. Extractive text summarization [1,2,3,4,5,6] directly selects some sentences from the input document to assemble summary. In contrast, abstractive text summarization [7,8,9,10,11] can generate novel words and phrases which didn't appear in the input document. \n\nTo effectively extract important sentences from the input document, we first need to prepare large-scale datasets which consist of a large number of document-summary pairs. However, it is obviously impractical to obtain large-scale and highquality datasets for different text summarization tasks as human annotation becomes a bottleneck. Therefore, unsupervised learning approaches have become the main areas of interest among researchers. Previous efforts [12,13] represent the sentences in the input document as the nodes of a graph and use graph-based ranking algorithms [14,15] to decide the importance of each node in the graph. Although easy to implement and computationally inexpensive, graph-based ranking algorithms usually measure the similarity between nodes based on statistical approaches [16,17] which did not take into account the semantic meanings of context words. \n\nTo better capture sentential meanings, pre-trained language models (PLMs) are integrated into unsupervised extractive summarization methods. PacSum [18] is one of the most promising approaches. Although PLMs-based methods have achieved state-of-the-art results on several summarization datasets, few of them have taken into account the various relations between words and sentences of the input document. \n\nAs documents are composed of various components such as words, phrases, sentences and paragraphs, it is a challenging task to develop an effective model for text representations that can considerably capture the inherent characteristics of texts [19].",
            "score": 0.49093375781131543,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1535
                },
                {
                    "start": 1538,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1942
                },
                {
                    "start": 1945,
                    "end": 2196
                }
            ],
            "ref_mentions": [
                {
                    "start": 426,
                    "end": 428,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 434,
                    "end": 436,
                    "matchedPaperCorpusId": "271933141"
                },
                {
                    "start": 1111,
                    "end": 1115,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 1115,
                    "end": 1118,
                    "matchedPaperCorpusId": "239209682"
                },
                {
                    "start": 1228,
                    "end": 1232,
                    "matchedPaperCorpusId": "216000619"
                },
                {
                    "start": 1460,
                    "end": 1463,
                    "matchedPaperCorpusId": "15879823"
                },
                {
                    "start": 1686,
                    "end": 1690,
                    "matchedPaperCorpusId": "182952892"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8466796875
        },
        {
            "corpus_id": "264429084",
            "title": "EXABSUM: a new text summarization approach for generating extractive and abstractive summaries",
            "text": "More recently, Erera et al. [27] introduced the IBM Science Summarizer, an innovative methodology catering to Computer Science papers. This approach crafts summaries contingent upon user-provided information requisites, be it a natural language inquiry, scientific tasks (e.g., \"Machine Translation\"), datasets, or scholarly venues. \n\nAlthough extractive methods can adeptly identify significant information, they may lack the fluidity and precision inherent in human-generated summaries. Consequently, abstractive ATS approaches strive to enhance sentence coherence by diminishing redundancies, elucidating sentence context, and potentially introducing supplementary phrases into the summary. For the synthesis of the final summary, abstractive techniques generally leverage sentence compression, fusion, or modification mechanisms. Barzilay and McKeown [28] pioneered a system wherein dependency trees represent input phrases, and select words are aligned to integrate these trees into a lattice structure. The lattice is subsequently linearized via tree traversal to generate fusion sentences. [29] introduced an innovative approach to sentence fusion, framing the fusion task as an optimization problem. This unsupervised technique draws on dependency structure alignment, semantic and syntactically informed phrase aggregation, and pruning strategies. Later, Filippova delved into the challenge of condensing a collection of interconnected sentences into a succinct single sentence, termed as multisentence compression, and presented a foundational technique based on shortest paths in word graphs [30]. Her method yielded grammatically sound and informative summaries, subsequently finding application in diverse contemporary summary systems [4,31]. Boudin [32] extended Filippova's approach by addressing Multi-Sentence Compression (MSC) as the task of generating a concise single-sentence summary from a cluster of interconnected sentences. He introduced an N-best reranking algorithm based on the frequency and relevance of keyphrases within the documents, resulting in more informative summaries. Banerjee et al. [33] devised multi-document abstractive summaries using word graphs and Integer Linear Programming (ILP). They clustered akin sentences among pivotal documents and employed word-graphs to identify shortest paths.",
            "score": 0.49075338784435496,
            "section_title": "Related works",
            "char_start_offset": 10051,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 332
                },
                {
                    "start": 335,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2106
                },
                {
                    "start": 2107,
                    "end": 2228
                },
                {
                    "start": 2229,
                    "end": 2335
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 32,
                    "matchedPaperCorpusId": "201670229"
                },
                {
                    "start": 1097,
                    "end": 1101,
                    "matchedPaperCorpusId": "14909308"
                },
                {
                    "start": 1603,
                    "end": 1607,
                    "matchedPaperCorpusId": "14750088"
                },
                {
                    "start": 1748,
                    "end": 1751,
                    "matchedPaperCorpusId": "57992117"
                },
                {
                    "start": 1763,
                    "end": 1767,
                    "matchedPaperCorpusId": "6545121"
                },
                {
                    "start": 2123,
                    "end": 2127,
                    "matchedPaperCorpusId": "15795297"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8212890625
        },
        {
            "corpus_id": "258378312",
            "title": "Enhancing Multi-Document Summarization with Cross-Document Graph-based Information Extraction",
            "text": "For baselines, we mainly compare our model with state-of-the-art multi-document summarization models PRIMERA (Xiao et al., 2022) and RE-FLECT . REFLECT only reports ROUGE scores on the Multi-News dataset and we directly use the reported scores for comparison. Besides, we also include a previous model BART-Graph (Pasunuru et al., 2021), which uses a linearized IE graph to improve summarization. We compare our model with it to see whether encoding the graph structurally improves the summarization performance. We also experiment with three ablation variants of our proposed model: (1) Recognition-Only: for the model with only the entity and event recognition loss;\n\n(2) Alignment-Only: for the model with only the graph encoder and the node-text alignment loss. tween the generated summaries and the reference summaries in terms of overlapping n-grams and longest common subsequence.\n\nFactual Consistency. Intuitively, our proposed IE-enhanced summarization should improve factual consistency of the generated summary with the source documents, since the entities and events in the original documents are mined and memorized by the model through the two proposed IE enhancement loss. Therefore, we include several factuality metrics to measure the improvements in terms of factuality of the generated summaries. Specifically we use FactCC (Kryscinski et al., 2020), Fact-Graph (Ribeiro et al., 2022, EntityPrecision (Nan et al., 2021), SUMMAC (Laban et al., 2022), and BERTSCORE (Pagnoni et al., 2021).\n\nAbstractiveness. To measure abstractiveness of our generated summaries, we use the MINT score (Dreyer et al., 2023), which is based on contiguous and non-contiguous extractive overlaps between summaries and their source documents. Our goal is to measure whether the novelty of the generated summary is sacrificed due to the improvements of factual consistency, e.g., by generating a more extractive summary. Table 2 shows the results of our proposed model, as well as the baselines on the three datasets. In general, the full version of our proposed model outperforms the baselines",
            "score": 0.490568874364584,
            "section_title": "Baselines and Implementation Details",
            "char_start_offset": 19222,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 128,
                    "matchedPaperCorpusId": "247519084"
                },
                {
                    "start": 313,
                    "end": 336,
                    "matchedPaperCorpusId": "235097309"
                },
                {
                    "start": 1343,
                    "end": 1367,
                    "matchedPaperCorpusId": "204976362"
                },
                {
                    "start": 1447,
                    "end": 1467,
                    "matchedPaperCorpusId": "244345901"
                },
                {
                    "start": 1483,
                    "end": 1505,
                    "matchedPaperCorpusId": "233407441"
                },
                {
                    "start": 1602,
                    "end": 1623,
                    "matchedPaperCorpusId": "258309129"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93408203125
        },
        {
            "corpus_id": "272253963",
            "title": "Whale-optimized LSTM networks for enhanced automatic text summarization",
            "text": "Song et al. (2018) report a framework that outperforms existing state-of-the-art models in terms of semantic and syntactic structure. The effectiveness of their model on datasets including CNN/Daily Mail underscores the importance of syntactic and semantic understanding in creating accurate and coherent summaries. Aliakbarpour et al. (2021) propose a novel abstractive summarization model that utilizes a combination of CNN and LSTM with an auxiliary attention mechanism. In their research, Gurusamy et al. ( 2023) have made significant strides in automatic text summarization by developing a hybrid model. This model integrates extractive methods, notably Semantic Latent Dirichlet Allocation (Semantic LDA) and Sentence Concept Mapping, with a transformer-based abstractive approach. Bharathi Mohan et al. (2023) delves into the comparison between BERT and GPT-2 in the context of text summarization, focusing on their performance in processing and summarizing multiple documents. \n\nExisting text summarization methods face numerous challenges. Extractive techniques produce summaries lacking deeper context. Clustering-based approaches struggle with scalability and coherence. Graph-based methods are computationally heavy and struggle with informativeness versus redundancy. Adversarial models may not adapt to specialized domains and miss nuanced features. Similarityfocused techniques overlook semantic relationships. Abstractive models are resource-intensive and can introduce biases. Preprocessing techniques reduce interpretability with noisy data. Optimization methods need tuning for text complexity. CNN-based models and frameworks may not generalize well. Semantic methods are computationally demanding and prone to overfitting. WOA addresses these issues by efficiently selecting key features, reducing text dimensionality, and maintaining essential content. It is scalable, handles large tasks efficiently, adapts to various text types, and integrates well with other methods. WOA ensures coherent summaries by balancing informativeness and redundancy, making it a robust and adaptable solution for improving text summarization quality.",
            "score": 0.4905177257409746,
            "section_title": "Related works",
            "char_start_offset": 10408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 984
                },
                {
                    "start": 987,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2153
                }
            ],
            "ref_mentions": [
                {
                    "start": 316,
                    "end": 342,
                    "matchedPaperCorpusId": "237844753"
                },
                {
                    "start": 797,
                    "end": 816,
                    "matchedPaperCorpusId": "252963446"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90234375
        },
        {
            "corpus_id": "259138479",
            "title": "Modeling Structural Similarities between Documents for Coherence Assessment with Graph Convolutional Networks",
            "text": "Despite showing impressive performance, our graph-based approach still has several limitations. The first one is related to the construction of the sentence graph. At present, we consider two sentences to be semantically related if they share similar nouns. But coherence can be achieved not only by describing similar entities but also by discourse (rhetorical) relations (Jurafsky and Martin, 2021). So it will be an exciting direction to incorporate discourse relations into the construction of a graph. The second one is that we implemented our method using only a plain GCN. Recent work has pointed out that the original GCN can be further improved with more advanced aggregation functions (Xu et al., 2019a) or attention mechanisms (Velickovic et al., 2018). So another interesting direction is to explore the benefits of more powerful graph neural networks for our method, which we leave for future study. Append(score, sim_scores)  uation documents in advance. To overcome this drawback, we implement an inductive GCN inspired by the work in fast GCN (Chen et al., 2018). Specifically, we first construct the doc-subgraph graph based on the training corpus (Section 3.3) and train GCN on this graph (Section 3.4). While evaluating, for each document in the test corpus, we add it to the doc-subgraph graph, normalize the adjacency matrix of the new graph, and predict its label (refer to Algorithm 3). Consequently, our method is in a pure inductive setting. That is, our model does not see the test corpus during training, and its evaluation is performed on individual documents without using the information of other samples in the test corpus. Note that when calculating weights for edges between the newly added document node and subgraph nodes, the inverse document frequency we used in equation ( 1) is the one we computed using only the training corpus.",
            "score": 0.490161307119962,
            "section_title": "Limitations",
            "char_start_offset": 28575,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1868
                }
            ],
            "ref_mentions": [
                {
                    "start": 695,
                    "end": 713,
                    "matchedPaperCorpusId": "52895589"
                },
                {
                    "start": 738,
                    "end": 763,
                    "matchedPaperCorpusId": "3292002"
                },
                {
                    "start": 1059,
                    "end": 1078,
                    "matchedPaperCorpusId": "22191393"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80615234375
        },
        {
            "corpus_id": "251719571",
            "title": "Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization",
            "text": "Text summarization aims at producing a concise and fluent summary while preserving salient content and overall meaning of the source documents. It has been applied in a wide range of real-world applications, e.g., summarizing Web search results for interactive information retrieval  and generating medical summaries from doctor-patient conversation transcripts .\n\nWhile the extractive approach is the dominant approach in commercial systems due to its simplicity and effectiveness (Allahyari et al., 2017), the abstractive approach is getting more attention in the research community as neural language models are used (e.g., Rush et al., 2015;Nallapati et al., 2016;Chopra et al., 2016;Liu & Lapata, 2019b;a;Pasunuru et al., 2021). Compared to the extractive approach where a summary is constructed using extracted sentences, abstractive summarizers paraphrase the idea of the source documents in a new form, and have a potential of generating more concise and coherent summaries.\n\nHowever, good abstractive summarizers are harder to develop since we have to deal with problems like semantic representation, inference and low-resource text generation, which are more challenging than sentence extraction. Recently, large-scale pre-trained language models (PLMs) such as PEGASUS (Zhang et al., 2020), GPT (Radford et al., 2019;Brown et al., 2020), T5 (Raffel et al., 2020), have been applied for abstractive summarization. While these models can produce surprisingly fluent text, the generated summaries often contain factual inconsistencies, caused by distorted or fabricated facts about the source documents, which is known as the hallucination problem (Kry\u015bci\u0144ski et al., 2019;Celikyilmaz et al., 2020;Ji et al., 2022). In addition, since the amount of text in the source documents can be very large, it is expensive to train an end-to-end abstractive model (e.g., an encoder-decoder transformer model) given the memory constraints of current hardware and the latency constraints of applications such as online document summarization for interactive information retrieval. Therefore, a two-stage approach",
            "score": 0.49007335542915653,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 627,
                    "end": 645,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 645,
                    "end": 668,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 668,
                    "end": 688,
                    "matchedPaperCorpusId": "133195"
                },
                {
                    "start": 688,
                    "end": 708,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 710,
                    "end": 732,
                    "matchedPaperCorpusId": "232092834"
                },
                {
                    "start": 1280,
                    "end": 1300,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 1306,
                    "end": 1328,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1352,
                    "end": 1373,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1656,
                    "end": 1681,
                    "matchedPaperCorpusId": "201666437"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8310546875
        },
        {
            "corpus_id": "258999996",
            "title": "Contrastive Hierarchical Discourse Graph for Scientific Document Summarization",
            "text": "We perform a systematic comparison with recent approaches in both extractive and abstractive summarization for completeness. We keep the same train/validation/test splitting in all the experiments and report ROUGE scores from the original papers if available, or scores from (Xiao and Carenini, 2019) otherwise. Specifically, we compare with the following strong baseline approaches: Unsupervised methods: LEAD method that selects the first few sentences as a summary, SumBasic (Vanderwende et al., 2007), graph-based unsupervised models LexRank (Erkan and Radev, 2004), PACSUM (Zheng and Lapata, 2019) and HIPORANK (Dong et al., 2020). Neural extractive models: encoder-decoder based model Cheng&Lapata (Cheng and Lapata, 2016) and SummaRuNNer (Nallapati et al., 2016a); local and global context model ExtSum-LG (Xiao and Carenini, 2019) and its variants ExtSum-LG+RdLoss/MMR (Xiao and Carenini, 2020); language model-based methods SentCLF and SentPTR (Subramanian et al., 2019). Neural abstractive models: pointer network generation model PGN (See et al., 2017), hierarchical attention generation model DiscourseAware (Cohan et al., 2018), and transformer-based generation model TLM-I+E (Subramanian et al., 2019).",
            "score": 0.48883687985769686,
            "section_title": "Baseline Methods",
            "char_start_offset": 19782,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1216
                }
            ],
            "ref_mentions": [
                {
                    "start": 478,
                    "end": 504,
                    "matchedPaperCorpusId": "10496513"
                },
                {
                    "start": 546,
                    "end": 569,
                    "matchedPaperCorpusId": "506350"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.791015625
        },
        {
            "corpus_id": "254275267",
            "title": "Document-Level Abstractive Summarization",
            "text": "With the growth of publicly available text data, the summarization of such contents is essential for their usefulness. A text summary must convey important information from the original text and present a smaller, more manageable, size [1]. The task of automatic text summarization produces a concise and fluent text summary while preserving key information and overall meaning [2]. \n\nApproaches to automatic text summarization can be divided into extractive and abstractive summarization. While the extractive approach produces a summary that is comprised entirely of excerpts from the original text, the abstractive approach generates an output that may contain content that is entirely original. Both approaches have seen significant improvements in recent years by using models based on the Transformer architecture [3]. In particular, the fluency of these language models has allowed for state-of-the-art results for abstractive summarization [4][5][6]. \n\nHowever, Transformers' quadratic memory and time complexities with respect to the sequence length make them very expensive to use, especially with long sequences, as required by document-level summarization. Recent approaches explore different attention mechanisms that are able to reduce the quadratic cost, allowing to process longer sequences [7][8][9]. Additionally, retrieval-enhanced language models exhibit useful memorization qualities while being more efficient than plain models [10]. Although less explored, retrieval has been used to enhance an abstractive summarization model, improving its performance [11]. \n\nOur work will address the problem of document-level summarization by studying how the aforementioned techniques can be used to improve the automatic summarization of very long texts. In particular, we will use the arXiv dataset, consisting of several scientific papers and the corresponding abstracts. The results obtained with Efficient Transformers will be reproduced and used as baselines. Then, we propose a novel retrieval-enhanced approach based on the RETRO architecture which reduces the cost of generating a summary of the entire document by processing smaller chunks. All of our implementations are open source and available in GitHub 12 .",
            "score": 0.4887798669067822,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 382
                },
                {
                    "start": 385,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 958
                },
                {
                    "start": 961,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1582
                },
                {
                    "start": 1585,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2162
                },
                {
                    "start": 2163,
                    "end": 2234
                }
            ],
            "ref_mentions": [
                {
                    "start": 236,
                    "end": 239,
                    "matchedPaperCorpusId": "94818"
                },
                {
                    "start": 820,
                    "end": 823,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 948,
                    "end": 951,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 951,
                    "end": 954,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 954,
                    "end": 957,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 1313,
                    "end": 1316,
                    "matchedPaperCorpusId": "233033613"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "corpus_id": "264146402",
            "title": "Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive Review",
            "text": "In recent years, deep learning has revolutionized natural language processing (NLP) by enabling the development of models that can learn complex representations of language data, leading to significant improvements in performance across a wide range of NLP tasks. Deep learning models for NLP typically use large amounts of data to train deep neural networks, allowing them to learn the patterns and relationships in language data. This is in contrast to traditional NLP approaches, which rely on hand-engineered features and rules to perform NLP tasks. The ability of deep neural networks to learn hierarchical representations of language data, handle variable-length input sequences, and perform well on large datasets makes them well-suited for NLP applications. Driven by the exponential growth of textual data and the increasing demand for condensed, coherent, and informative summaries, text summarization has been a critical research area in the field of NLP. Applying deep learning to text summarization refers to the use of deep neural networks to perform text summarization tasks. In this survey, we begin with a review of fashionable text summarization tasks in recent years, including extractive, abstractive, multi-document, and so on. Next, we discuss most deep learning-based models and their experimental results on these tasks. The paper also covers datasets and data representation for summarization tasks. Finally, we delve into the opportunities and challenges associated with summarization tasks and their corresponding methodologies, aiming to inspire future research efforts to advance the field further. A goal of our survey is to explain how these methods differ in their requirements as understanding them is essential for choosing a technique suited for a specific setting.",
            "score": 0.4884765342271638,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68115234375
        },
        {
            "corpus_id": "268594273",
            "title": "Unsupervised Extractive Dialogue Summarization in Hyperdimensional Space",
            "text": "Extractive summarization selects n most representative sentences from a document as its summary.For the scope of this paper, we assume n is known.Extractive summarizers are used both as stand-alone systems and, more recently, as part of hierarchical summarization schemes.The integration of extractive summarizers into hybrid summary pipelines demands the summarizers to be both efficient and accurate.Therefore, we explore a wide range of baseline extractive summarizers that specialize in both spectrums.\n\nTextrank [11] is a graph-based summarizer that identifies nodes based on lexical centrality.PacSum [12] also utilizes sentence centrality to identify the most important sentences in a document.Each sentence in the document is embedded with a neural language model and added as a node to a sentence graph.OTExtSum [13] selects a set of summary sentences that minimizes the semantic distance between the candidate summary set and the original document.Following the previous literature, we also include Lead-n [14] as our baseline.Lead-n selects the n-leading sentences as the summary, exploiting the lead bias typically present in documents.We use BERT variants of PacSum and OTExtSum as our baselines for their state-of-the-art performance.",
            "score": 0.4875112788417966,
            "section_title": "Extractive summarization",
            "char_start_offset": 3241,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 96,
                    "end": 146
                },
                {
                    "start": 146,
                    "end": 272
                },
                {
                    "start": 272,
                    "end": 402
                },
                {
                    "start": 402,
                    "end": 506
                },
                {
                    "start": 508,
                    "end": 600
                },
                {
                    "start": 600,
                    "end": 701
                },
                {
                    "start": 701,
                    "end": 812
                },
                {
                    "start": 812,
                    "end": 958
                },
                {
                    "start": 958,
                    "end": 1037
                },
                {
                    "start": 1037,
                    "end": 1148
                },
                {
                    "start": 1148,
                    "end": 1248
                }
            ],
            "ref_mentions": [
                {
                    "start": 517,
                    "end": 521,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "182952892"
                },
                {
                    "start": 821,
                    "end": 825,
                    "matchedPaperCorpusId": "248299685"
                },
                {
                    "start": 1016,
                    "end": 1020,
                    "matchedPaperCorpusId": "224801419"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8740234375
        },
        {
            "corpus_id": "253763913",
            "title": "N-GPETS: Neural Attention Graph-Based Pretrained Statistical Model for Extractive Text Summarization",
            "text": "Te authors of [50] developed a hybrid method for producing summaries of long scientifc texts that combined the benefts of both extractive and abstractive designs. Te authors in [51,52] use the deep learning model BERT and RISTECB model to answer important questions related to the COVID-19 research articles. Te authors of [44] demonstrated an excellent tuning-based approach for extractive summarization using the BERT model. Te BERT model was also used by the authors of [7,8,16,36,46] for contextual representation in summarization tasks. Te authors in [53] use the BERT model to automatically generate titles from a huge set of published literature or related work. Additionally, extractive summarization tasks using graph structures have been carried out by exploiting linguistic and statistical information included in sentences [9]. Recent research has combined neural networks with graphs, or (GNNs), and used the encoder-decoder structure for extractive summarization [13,54]. Many researchers nowadays use a heterogeneous graph neural network with multiple updated nodes rather than a homogeneous graph structure with no updated nodes for extractive summarization tasks. Te study [55] proposed a bipartite graph attention network for multihop reading comprehension (RC) across documents that encoded diferent documents and entities together. Te authors in [48] presented an approach that modeled redundancy-aware heterogeneous graphs and refned sentence representation using neural networks for extractive summarization. Te studies [9,56]  4 \n\nComputational Intelligence and Neuroscience between sentences are learned. Te work done by [14] built a sentence-level graph-based model, using BERT for sentence encoding and joint neural network model (NTM) for discovering latent topic information. Te authors in [15] proposed a heterogeneous graph structure for modelling crosssentence relationship between sentences. To represent the relationships between the EDUs, they used three diferent types of nodes, including sentence nodes, EDU nodes, and entity nodes, and RST discourse parsing and leverage external discourse expertise to enhance the model's performance. Te next section goes over the unique model N-GPETS methodology that is proposed in this study in depth.",
            "score": 0.4874881987267635,
            "section_title": "Extractive Text Summarization Approaches and",
            "char_start_offset": 16057,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1551
                },
                {
                    "start": 1554,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2276
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 181,
                    "matchedPaperCorpusId": "246287960"
                },
                {
                    "start": 181,
                    "end": 184,
                    "matchedPaperCorpusId": "246418639"
                },
                {
                    "start": 323,
                    "end": 327,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 476,
                    "end": 478,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 478,
                    "end": 481,
                    "matchedPaperCorpusId": "247041797"
                },
                {
                    "start": 481,
                    "end": 484,
                    "matchedPaperCorpusId": "221845203"
                },
                {
                    "start": 484,
                    "end": 487,
                    "matchedPaperCorpusId": "219036690"
                },
                {
                    "start": 556,
                    "end": 560,
                    "matchedPaperCorpusId": "237588401"
                },
                {
                    "start": 835,
                    "end": 838,
                    "matchedPaperCorpusId": "49656757"
                },
                {
                    "start": 977,
                    "end": 981,
                    "matchedPaperCorpusId": "222310577"
                },
                {
                    "start": 981,
                    "end": 984,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 1190,
                    "end": 1194,
                    "matchedPaperCorpusId": "158046817"
                },
                {
                    "start": 1366,
                    "end": 1370,
                    "matchedPaperCorpusId": "250012576"
                },
                {
                    "start": 1542,
                    "end": 1545,
                    "matchedPaperCorpusId": "49656757"
                },
                {
                    "start": 1545,
                    "end": 1548,
                    "matchedPaperCorpusId": "246588027"
                },
                {
                    "start": 1645,
                    "end": 1649,
                    "matchedPaperCorpusId": "233189611"
                },
                {
                    "start": 1818,
                    "end": 1822,
                    "matchedPaperCorpusId": "220586224"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95947265625
        },
        {
            "corpus_id": "270215715",
            "title": "Mix-of-Granularity: Optimize the Chunking Granularity for Retrieval-Augmented Generation",
            "text": "Graph-based text processing techniques combine research in graphs and text retrieval.Previous works exploit semantic similarities between small snippets (a sentence or several words) and reorganize the text material into a graph using Entity Recognition and Relation Construction algorithms [33,14,52].This approach breaks the constraint of the single dimension in the original text corpus, allowing chunks of the same topic to be grouped as neighbors in a graph.These graph-based text processing methods show great potential in tasks requiring reasoning over long texts or multi-hop reasoning.\n\nRepresentative works in this field include \"abstract embedding\" [13], which prioritizes top-k retrieval based on document abstracts (or summaries), providing a comprehensive understanding of the context.RAPTOR [41] organizes snippets as a tree (a special form of a graph) by recursively clustering them, where all non-leaf nodes correspond to summaries of their child nodes.This processing allows access to information at different granularity levels, resulting in significant improvements in summarization tasks.In GMoE [48], the authors use different expert networks to handle hop-1, hop-2, and mixed hop-1 & hop-2 neighbors of a node in a graph, which inspires our extension from MoG to MoGG.",
            "score": 0.48708453656631523,
            "section_title": "Graph-Based Text Processing",
            "char_start_offset": 9425,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 85,
                    "end": 302
                },
                {
                    "start": 302,
                    "end": 463
                },
                {
                    "start": 463,
                    "end": 594
                },
                {
                    "start": 596,
                    "end": 799
                },
                {
                    "start": 799,
                    "end": 970
                },
                {
                    "start": 970,
                    "end": 1109
                },
                {
                    "start": 1109,
                    "end": 1291
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.896484375
        },
        {
            "corpus_id": "270808749",
            "title": "Abstractive Summarization Model for Summarizing Scientific Article",
            "text": "Text summarization approaches are performed by directly extracting salient sentences from the source documents or rewriting these sentences with words that differ from the source documents [9], [10].Previous researchers have focused on extractive methods to summarize scientific articles because abstractive methods are more difficult and complex because they require advanced Natural Language Processing (NLP) techniques.These studies are summarized in Table 1.In this study [3], they proposed a generic summarizer model that is language-independent and based on a quantum-inspired approach for the extraction of important sentences.In this study [11], they proposed a graph-based framework that can also be applied to scientific articles without any domain or language constraints.This model utilizes the advantages of graph-based, statistical-based, semanticbased, and centrality-based methods.In this study [12], they proposed a regression-based model to highlight salient sentences in scientific articles.They experimented on three different scientific datasets (CSPubSum, AlPubSum, and BioPubSum) to demonstrate the effectiveness of their method.In this study [13], they constructed a large-scale manually annotated dataset (SciSummNet) for summarizing scientific articles.In addition, a hybrid summarization model was proposed.The effectiveness of their corpus on this model and the data-driven neural models was evaluated.In this study [14], they presented a new model for summarizing scientific articles by inspiring SummPipon [15].In this study [16], they constructed a novel corpus (SciTLDR) consisting of scientific papers related to the computer science domain.In addition, a novel model (CATTS) was proposed to evaluate their corpus.The proposed model is suitable for both extractive and abstractive methods.\n\nAbstractive methods are closer to reality in terms of generating a summary by thinking human-like.Recently, researchers have focused on it for generating scientific summaries [9], [10].In this study [5], they presented a graph network-based model based on a sentence-level denoiser and an auto-regressive generator.To demonstrate the effectiveness of their model, PubMed and CORD-19 datasets containing scientific articles in the biomedical domain were used.",
            "score": 0.48582053453847085,
            "section_title": "II. RELATED WORK A. TEXT SUMMARIZATION APPROACHES IN SCIENTIFIC ARTICLES",
            "char_start_offset": 4079,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 199,
                    "end": 422
                },
                {
                    "start": 422,
                    "end": 462
                },
                {
                    "start": 462,
                    "end": 634
                },
                {
                    "start": 634,
                    "end": 783
                },
                {
                    "start": 783,
                    "end": 897
                },
                {
                    "start": 897,
                    "end": 1010
                },
                {
                    "start": 1010,
                    "end": 1152
                },
                {
                    "start": 1152,
                    "end": 1279
                },
                {
                    "start": 1279,
                    "end": 1334
                },
                {
                    "start": 1334,
                    "end": 1430
                },
                {
                    "start": 1430,
                    "end": 1541
                },
                {
                    "start": 1541,
                    "end": 1674
                },
                {
                    "start": 1674,
                    "end": 1747
                },
                {
                    "start": 1747,
                    "end": 1822
                },
                {
                    "start": 1824,
                    "end": 1922
                },
                {
                    "start": 1922,
                    "end": 2009
                },
                {
                    "start": 2009,
                    "end": 2139
                },
                {
                    "start": 2139,
                    "end": 2282
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 198,
                    "matchedPaperCorpusId": "219472430"
                },
                {
                    "start": 476,
                    "end": 479,
                    "matchedPaperCorpusId": "232313770"
                },
                {
                    "start": 648,
                    "end": 652,
                    "matchedPaperCorpusId": "224973389"
                },
                {
                    "start": 911,
                    "end": 915,
                    "matchedPaperCorpusId": "224978229"
                },
                {
                    "start": 2004,
                    "end": 2008,
                    "matchedPaperCorpusId": "219472430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8271484375
        },
        {
            "corpus_id": "250953851",
            "title": "Improving extractive document summarization with sentence centrality",
            "text": "The EDS task aims to extract sentences from the original document to form a summary. The task first encodes the sentences with the help of an encoder to obtain a sentence vector. The sentence vector is then passed through a classification layer to determine whether it should be included in the summary. Nallapati et al [2]; Zhou et al. [19] choose recurrent neural networks (RNN) for sentence encoding, while Wang et al. [20] use Transformer [8]. BERT and other pre-trained language models [21] also perform well in the EDS task. Besides, graph neural networks also have received extensive attention. Yasunaga et al. [22] apply the graph neural network for multi-document summarization. Wang et al. [18] propose to use the heterogeneous graph for the EDS task. \n\nAlthough these methods are effective, they mostly rely on sentence position information to enhance sentence representation. We introduce sentence centrality information in the model and remove sentence position information, which improves model performance and does not cause sentence-leading bias.",
            "score": 0.4853129009429891,
            "section_title": "Extractive document summarization",
            "char_start_offset": 6595,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 761
                },
                {
                    "start": 764,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1062
                }
            ],
            "ref_mentions": [
                {
                    "start": 320,
                    "end": 323,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 337,
                    "end": 341,
                    "matchedPaperCorpusId": "49656757"
                },
                {
                    "start": 443,
                    "end": 446,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 491,
                    "end": 495,
                    "matchedPaperCorpusId": "219036690"
                },
                {
                    "start": 618,
                    "end": 622,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 700,
                    "end": 704,
                    "matchedPaperCorpusId": "216552978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91943359375
        },
        {
            "corpus_id": "253155963",
            "title": "Abstractive Summarization Guided by Latent Hierarchical Document Structure",
            "text": "Neural Abstractive Summarization Rush et al. (2015) first proposed to use a sequence-to-sequence model with an attention mechanism to perform sentence compression. Mendes et al. (2019) demonstrated the advantages and limitations of neural methods based on sentence compression. The pointer-generator networks (PGN; See et al. 2017) enhances the attention model with a copying functionality. PGN has also been further extended to create summarization systems by incorporating the topic information (Liu et al., 2019b), document structural information (Song et al., 2018), semantic information (Hardy and Vlachos, 2018), and was improved by replacing the plain LSTM module with the more advanced Transformer model to overcome the difficulty in modeling long sequence input (Pilault et al., 2020;Wang et al., 2021;Fonseca et al., 2022). For the pretrained models, BERTSum (Liu and Lapata, 2019) adopted the BERT encoder for the summarizer, with a randomly initialized decoder. Lewis et al. (2020) presented BART which pre-trains both the underlying encoder and decoder. Our approach is related to the structural attention model (Balachandran et al., 2021;Liu et al., 2019a), but differs in two major ways: (i) we introduce an adaptive sparse matrix-tree construction to learn a latent hierarchical graph and a novel propagation rule; (ii) we investigate to use the structure information both with the encoder and the decoder for abstractive summarization, and not just the encoder. These shows to be more effective for unsupervised learning of the latent hierarchical structure while can defeat the approach that leverages external graph constructor (Balachandran et al., 2021).",
            "score": 0.4839985067575868,
            "section_title": "Related Work",
            "char_start_offset": 5245,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1675
                }
            ],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 51,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 164,
                    "end": 184,
                    "matchedPaperCorpusId": "102352252"
                },
                {
                    "start": 315,
                    "end": 330,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 497,
                    "end": 516,
                    "matchedPaperCorpusId": "203641742"
                },
                {
                    "start": 550,
                    "end": 569,
                    "matchedPaperCorpusId": "46936631"
                },
                {
                    "start": 592,
                    "end": 617,
                    "matchedPaperCorpusId": "52111191"
                },
                {
                    "start": 771,
                    "end": 793,
                    "matchedPaperCorpusId": "202541012"
                },
                {
                    "start": 793,
                    "end": 811,
                    "matchedPaperCorpusId": "229171633"
                },
                {
                    "start": 811,
                    "end": 832,
                    "matchedPaperCorpusId": "249062562"
                },
                {
                    "start": 974,
                    "end": 993,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1125,
                    "end": 1152,
                    "matchedPaperCorpusId": "231934225"
                },
                {
                    "start": 1152,
                    "end": 1170,
                    "matchedPaperCorpusId": "174800499"
                },
                {
                    "start": 1647,
                    "end": 1674,
                    "matchedPaperCorpusId": "231934225"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96044921875
        },
        {
            "corpus_id": "278250638",
            "title": "Capturing Semantic Relationships Using Full Dependency Forests to Improve Consistency in Long Document Summarization",
            "text": "The state-of-the-art methods in this field are compared. We divide these methods into two types. Abstractive model: generates new sentences by understanding the overall meaning of the text. They usually adopt an encoder-decoder architecture. For example, the TLM-I+E [44] summarizes the extracted text; the PEGASUS [37] achieves summarization by generating missing sentences; the BigBird [7] handles very long text by improving the attention mechanism; and the Dancer [39] improves summarization by decomposing long documents. Extractive Models: These methods focus on extracting key content directly from the text. For example, the ExtSum-LG [45] model focuses on local and global contextual information extraction. There are also graph-based models such as Topic-GraphSum [46] that focuses on inter-sentence relations and topic information, and CHANGES [47] that combines hierarchical structure and contrast learning. In addition, models that utilize sliding windows and dynamic memory such as SSN-DM [48], and models that employ sequence-to-sequence approaches such as HiStruct+ [49], HEGEL [50], and MTGNN [51]. Recently, GoSum [52] combines graph neural networks and reinforcement learning to optimize summaries, while there are also methods to improve the quality of summaries using LLM [53].In addition, we also compared the performance of the large language model Qwen2.5-7B [54] and LLaMA3-7B [55] in the long document summarization task to further evaluate the capabilities of existing models in practical applications.",
            "score": 0.48334336343271944,
            "section_title": "3) MODEL COMPARISON",
            "char_start_offset": 16807,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 57,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1529
                }
            ],
            "ref_mentions": [
                {
                    "start": 267,
                    "end": 271,
                    "matchedPaperCorpusId": "202541012"
                },
                {
                    "start": 315,
                    "end": 319,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 468,
                    "end": 472,
                    "matchedPaperCorpusId": "219425367"
                },
                {
                    "start": 643,
                    "end": 647,
                    "matchedPaperCorpusId": "201697450"
                },
                {
                    "start": 774,
                    "end": 778,
                    "matchedPaperCorpusId": "222310577"
                },
                {
                    "start": 855,
                    "end": 859,
                    "matchedPaperCorpusId": "258999996"
                },
                {
                    "start": 1003,
                    "end": 1007,
                    "matchedPaperCorpusId": "235097475"
                },
                {
                    "start": 1082,
                    "end": 1086,
                    "matchedPaperCorpusId": "247594288"
                },
                {
                    "start": 1094,
                    "end": 1098,
                    "matchedPaperCorpusId": "252780923"
                },
                {
                    "start": 1110,
                    "end": 1114,
                    "matchedPaperCorpusId": "252819335"
                },
                {
                    "start": 1132,
                    "end": 1136,
                    "matchedPaperCorpusId": "253708345"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8916015625
        },
        {
            "corpus_id": "256460883",
            "title": "Learning to Generate Overlap Summaries through Noisy Synthetic Data",
            "text": "Text Summarization: Technically, Semantic Overlap Summarization can be viewed as a multidocument summarization task, i.e., multi-seqto-seq task, with an extra commonality constraint (Bansal et al., 2022). Over the past two decades, many document summarizing approaches have been investigated (Zhong et al., 2019). The two most popular among them are extractive approaches (Cao et al., 2018;Narayan et al., 2018;Wu and Hu, 2018;Zhong et al., 2020) and abstractive approaches (Bae et al., 2019;Liu et al., 2017;Nallapati et al., 2016). Some researchers have also tried combining extractive and abstractive approaches (Chen and Bansal, 2018;Hsu et al., 2018;Zhang et al., 2019). \n\nPre-training/ Fine-Tuning Paradigm: Encoderdecoder-based neural models have recently gained a lot of attraction, especially for abstractive summarization tasks, (Rush et al., 2015;Chopra et al., 2016;Zhou et al., 2017;Paulus et al., 2017). Training a generic language model on a large corpus of data and then transferring/fine-tuning it for the summarization job has become a standard approach (Radford et al., 2019;Devlin et al., 2019;Lewis et al., 2019;Xiao et al., 2020;Yan et al., 2020;Zhang et al., 2019;Raffel et al., 2019). In general, multiple document summarization (Goldstein et al., 2000;Yasunaga et al., 2017;Zhao et al., 2020;Ma et al., 2020;Meena et al., 2014;Lebanoff et al., 2018;Fabbri et al., 2019) is more challenging than single document summarization. However, unlike typical multi-document summarizing tasks, the SOS task aims to summarize multiple alternative narratives with an extra overlapping constraint, i.e., the output should only contain the com-mon information from all the input narratives (Karmaker Santu et al., 2018).",
            "score": 0.48270362699381764,
            "section_title": "Related Works",
            "char_start_offset": 4739,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1731
                }
            ],
            "ref_mentions": [
                {
                    "start": 182,
                    "end": 203,
                    "matchedPaperCorpusId": "252819160"
                },
                {
                    "start": 372,
                    "end": 390,
                    "matchedPaperCorpusId": "51878811"
                },
                {
                    "start": 858,
                    "end": 878,
                    "matchedPaperCorpusId": "133195"
                },
                {
                    "start": 1094,
                    "end": 1114,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1253,
                    "end": 1277,
                    "matchedPaperCorpusId": "8294822"
                },
                {
                    "start": 1299,
                    "end": 1317,
                    "matchedPaperCorpusId": "220633461"
                },
                {
                    "start": 1317,
                    "end": 1333,
                    "matchedPaperCorpusId": "226289939"
                },
                {
                    "start": 1333,
                    "end": 1352,
                    "matchedPaperCorpusId": "39340997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65283203125
        },
        {
            "corpus_id": "263831535",
            "title": "Abstractive Summarization of Large Document Collections Using GPT",
            "text": "Research on transformer-attention mechanisms and large language models (LLMs) have produced impressive results, particularly in natural language processing (NLP) and text analytics. LLMs like BERT Kenton et al. [2019], BART Lewis et al. [2020], GPT Radford et al. [2018], Bard Manyaka [2023], and LLaMA Meta AI [2023] have produced significant research and general public impact. Despite their state-of-the-art performance, the goal of broad, general-purpose use leaves certain tasks only partially solved. This paper focuses on the abstractive summarization of multi-document collections. Systems like GPT can perform abstractive summarization but are currently limited to a small maximum input of 512 to 4,096 terms. Document collections can easily consist of hundreds of documents containing thousands of terms. An intelligent method is needed to manage scale to leverage an LLM's abstractive summarization capabilities. We also propose applying sentiment analysis and visualization to augment the summaries with additional properties presented in an interactive and simple-to-understand visual format. \n\nOur approach performs the following steps to extend GPT's abstractive summarization method to large document collections. \n\n1. Apply the Facebook AI Similarity Search Johnson et al. [2021] (FAISS) to estimate document similarity based on the semantic similarity of pairs of documents. 2. Perform Hierarchical Density-Based Spatial Clustering and Application with Noise Malzer and Baum [2020] (HDBSCAN) using FAISS results to generate semantic topic clusters. 3. Identify topic-representative terms and build a collection of representative term sets for each cluster, where each set contains a representative term and all semantically similar terms Nagwani [2015] in the parent cluster. 4. Use the representative term sets to further reduce topic cluster size by combining sentences in a cluster containing representative terms into semantic chunks based on change points in their semantic content. 5. Use GPT's summarization API to summarize each semantic chunk, followed by its concatenation API to combine the semantic chunk summaries into an abstractive summarization of the original document collection. 6. Perform term-based sentiment analysis on each semantic chunk to generate valence (pleasure) and arousal scores.",
            "score": 0.4821221820756625,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1105
                },
                {
                    "start": 1108,
                    "end": 1229
                },
                {
                    "start": 1232,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 2215
                },
                {
                    "start": 2216,
                    "end": 2330
                }
            ],
            "ref_mentions": [
                {
                    "start": 1275,
                    "end": 1296,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1477,
                    "end": 1499,
                    "matchedPaperCorpusId": "207794266"
                },
                {
                    "start": 1756,
                    "end": 1770,
                    "matchedPaperCorpusId": "18656953"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74853515625
        },
        {
            "corpus_id": "271938110",
            "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
            "text": "Abstractive text summarization is dominated by the sequence-to-sequence framework [22]. Early versions relied on RNNs [23], specifically Long Short-Term Memory (LSTM) units, to encode the input text into a vector representation and then decode it into a summary [3]. However, LSTMs struggle to capture long-range dependencies in lengthy sequences, leading to poor summarization and inefficient use of computational resources [7]. Attention mechanisms [24] were introduced to address this issue. However, they can still suffer from issues like outof-vocabulary (OOV) words and repetitive outputs. The Pointer-Generator Network with Coverage [25], tackles these problems by cleverly controlling the inclusion of words during generation, achieving impressive results. \n\nConventional RNN architectures, although well-suited for NLP tasks due to their ability to handle variable-length sequences, suffer from several limitations. These limitations include challenges in parallelization during training, increased training complexity, and difficulties in capturing long-range dependencies and hierarchical relationships within the data. To address these shortcomings, Google introduced the Transformer model [7], a state-of-the-art architecture that leverages self-attention mechanisms to enable parallel processing of the model. This approach represents a significant leap forward compared to traditional RNNs. \n\nBERT, a powerful pre-trained language model based on Transformers [20], has significantly improved performance in various summarization tasks [26], [27], [28], [29], [30], [31], [32]. Leading models like BART [33] (Bidirectional and Auto-Regressive Transformers), PEGASUS [34] (Pre-training with Extracted Gap-sentences for Abstractive Summarization), and ProphetNet [35], all employ Transformer architecture for abstractive summarization task. The researchers in [36] introduced a groundbreaking approach for summarization and question answering using Large Language Models (LLMs). Their approach ensures LLMs are not overloaded with irrelevant data, saving time and resources. \n\nAdditionally, scholars of [37] explored how knowledge graphs can enhance summaries [37].",
            "score": 0.48144640995675386,
            "section_title": "A. ABSTRACTIVE TEXT SUMMARIZATION",
            "char_start_offset": 7832,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 764
                },
                {
                    "start": 767,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1405
                },
                {
                    "start": 1408,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2086
                },
                {
                    "start": 2089,
                    "end": 2177
                }
            ],
            "ref_mentions": [
                {
                    "start": 82,
                    "end": 86,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 262,
                    "end": 265,
                    "matchedPaperCorpusId": "250012667"
                },
                {
                    "start": 1550,
                    "end": 1554,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 1556,
                    "end": 1560,
                    "matchedPaperCorpusId": "233948337"
                },
                {
                    "start": 1680,
                    "end": 1684,
                    "matchedPaperCorpusId": "209405420"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9482421875
        },
        {
            "corpus_id": "261706338",
            "title": "Hindi Text Summarization Using Sequence to Sequence Neural Network",
            "text": "Telugu [44] Proposed a sentence scoring approach for extractive summarization in Telugu Incorporated statistical and linguistic features Achieved promising results on Telugu news articles Marathi [45] Developed a rule-based approach for extractive summarization in Marathi Conducted evaluations on Marathi news articles with promising results 4. \n\nA \n\nAutomatic summarization is crucial in document processing and information retrieval systems. The proposed approach for text summarization of Hindi text documents is based on linguistic rules and the removal of redundant words and phrases. Proposed a neural network-based approach for abstractive summarization in Marathi Utilized a hierarchical encoder-decoder model State-of-the-art techniques in abstractive text summarization include transformer-based models such as BERTSUM [55], MASS [56], and PEGASUS [57], among others. These models use attention mechanisms to generate summaries that capture the context and semantics of the input text. One promising approach is neural network-based models that generate summaries that are more fluent and natural compared to traditional methods. Zong et al. proposed a new approach called UniLM [58], which employs a transformer-based architecture that can generate summaries of varying lengths with improved coherence and fluency. Another study Yonghua et al. introduced the Concept-Flow framework, which uses a graphbased neural network to capture important concepts in the source document and generates summaries based on these concepts [59]. Tianshui et al. proposed a new approach called SARL [60], which uses a selfattention mechanism to focus on important parts of the source document and reinforce the generation of high-quality summaries. Recently, there has been growing interest in unsupervised methods for text summarization. [61] proposed a novel unsupervised method that leverages a pre-trained language model to generate summaries without needing labeled data. In conclusion, text summarization is an active area of research, with many new and exciting developments in recent years. These include using neural networkbased models, reinforcement learning, unsupervised methods, multi-document summarization, and more interpretable and explainable methods.",
            "score": 0.48138284191728353,
            "section_title": "Demonstrated improved performance compared to traditional methods",
            "char_start_offset": 10364,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 345
                },
                {
                    "start": 348,
                    "end": 349
                },
                {
                    "start": 352,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2264
                }
            ],
            "ref_mentions": [
                {
                    "start": 841,
                    "end": 845,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 1190,
                    "end": 1194,
                    "matchedPaperCorpusId": "251847837"
                },
                {
                    "start": 1535,
                    "end": 1539,
                    "matchedPaperCorpusId": "245353359"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7744140625
        },
        {
            "corpus_id": "247958026",
            "title": "Automatic Text Summarization Methods: A Comprehensive Review",
            "text": "For the text summarization task, (Wang et al., 2019) extended the traditional definition of the domain from categories to data sources. Then used, a multi-domain summary dataset to see how the distance between different domains affects neural summarization model performance. Traditional applications have a major flaw: they use high-dimensional, sparse data, making it impossible to gather relevant information. Word embedding is a neural network technique that produces a considerably smaller word representation than the classic Bag-of-Words (BOW) method. (Alami et al., 2019) has created a text summarization system based on word embeddings, and it showed that the Word2Vec representation outperforms the classic BOW representation. Another summarization approach using word embeddings was given by (Mohd et al., 2020). This study also used Word2Vec as a distributional semantic model that captures the semantics. \n\nCurrent state-of-art systems produce generic summaries that are unrelated to the preferences and expectations of their users. CTRLsum (He, Kryscinski, et al., 2020), a unique framework for controlled summarizing, is presented to address that limitation. This system permits users to interact with the summary system via textual input in a collection of key phrases or descriptive prompts to influence several features of generated summaries. The majority of recent neural network summarization algorithms are either selection-based extraction or generation-based abstraction. (Xu & Durrett, 2020) introduced a neural model based on joint extraction and syntactic compression for single-document summarization. The model selects phrases from the document, identifies plausible compressions based on constituent parses, and rates those compressions using a neural model to construct the final summary. Four algorithms were proposed by (El-Kassas et al., 2020). The first algorithm uses the input document to create a new text graph model representation. The second and third algorithms look for sentences to include in the candidate summary in the built text graph. The fourth algorithm selects the most important sentences when the resulting candidate summary exceeds a userspecified limit. Automatic text summarization is an arduous effort for under-resourced languages like Hindi, and it is still an unsolved topic.",
            "score": 0.48129149224012685,
            "section_title": "Extractive text summarization",
            "char_start_offset": 36414,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 917
                },
                {
                    "start": 920,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2209
                },
                {
                    "start": 2210,
                    "end": 2336
                }
            ],
            "ref_mentions": [
                {
                    "start": 559,
                    "end": 579,
                    "matchedPaperCorpusId": "67892357"
                },
                {
                    "start": 1054,
                    "end": 1084,
                    "matchedPaperCorpusId": "227745074"
                },
                {
                    "start": 1496,
                    "end": 1515,
                    "matchedPaperCorpusId": "59599804"
                },
                {
                    "start": 1853,
                    "end": 1877,
                    "matchedPaperCorpusId": "224973389"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6357421875
        },
        {
            "corpus_id": "258999996",
            "title": "Contrastive Hierarchical Discourse Graph for Scientific Document Summarization",
            "text": "Extractive document summarization aims to extract the most salient sentences from the original document and form the summary as an aggregate of these sentences. Compared to abstractive summarization approaches that suffer from hallucination generation problems (Kry\u015bci\u0144ski et al., 2019;Zhang et al., 2022b), summaries generated in an extractive manner are more fluent, faithful, and grammatically accurate, but may lack coherence across sentences. Recent advances in deep neural networks and pre-trained language models (Devlin et al., 2018;Lewis et al., 2019) have led to significant progress in single document summarization (Nallapati et al., 2016a;Narayan et al., 2018;Liu and Lapata, 2019;Zhong et al., 2020). However, these methods mainly focus on short documents like news articles in CNN/DailyMail (Hermann et al., 2015) and New York Times (Sandhaus, 2008), and struggle when dealing with relatively long documents such as scientific papers. \n\nThe challenges of lengthy scientific paper summarization lie in several aspects. First, the extended input context hinders cross-sentence relation modeling, the critical step of extractive summarization (Wang et al., 2020). Thus, sequential models like RNN are incapable of capturing the long-distance dependency between sentences, and hard to differentiate salient sentences from others. Furthermore, scientific papers tend to cover diverse topics and contain rich hierarchical discourse structure information. The internal hierarchy structure, like sections, paragraphs, sentences, and words, is too complex for sequential models to capture. Scientific papers generally follow a standard discourse structure of problem definition, methodology, experiments and analysis, and conclusions (Xiao and Carenini, 2019). Moreover, the lengthy input context also makes the widely adopted self-attention Transformer-based models (Vaswani et al., 2017) inapplicable. The input length of a scientific paper can range from 2000 to 7, 000 words, which exceeds the input limit of the Transformer due to the quadratic computation complexity of self-attention. Thus, sparse Transformer models like BigBird (Zaheer et al., 2020) and Longformer (Beltagy et al., 2020) are proposed.",
            "score": 0.481245978017874,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 949
                },
                {
                    "start": 952,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 2097
                },
                {
                    "start": 2098,
                    "end": 2216
                }
            ],
            "ref_mentions": [
                {
                    "start": 806,
                    "end": 828,
                    "matchedPaperCorpusId": "6203757"
                },
                {
                    "start": 1873,
                    "end": 1895,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.490234375
        },
        {
            "corpus_id": "271938110",
            "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
            "text": "The primary goal of automatic text summarization is to enable a quick review of documents by generating a coherent and concise summary that retains the original essence of the documents [1]. Text summarization can be classified into two categories: extractive and abstractive. Extractive summarization involves the systematic selection of original sentences and content directly from the source document. Conversely, the abstractive summarization technique utilizes sophisticated procedures to significantly transform and rephrase sentences from the original input documents [2]. Most of the research in text summarization has focused news reports (brief documents), conversation records (collaborative documents), critiques (informal documents), and social media posts (tweets). Conversely, researchers have devoted less attention to summarization in medical domain, specifically for medical scientific documents concerning various diseases. \n\nAbstractive models align more closely with the human nature of summarization and can produce high-quality summaries [3]. Consequently, the research presented in this paper utilizes an abstractive technique for the summarization of medical scientific documents. \n\nModeling the relationships between sentences is a crucial step for identifying the relevant content and main theme of the input text in the process of generating a summary [4]. In recent years, several studies [5], [6], utilized Recurrent Neural Networks (RNNs) to grasp the relationships between sentences. However, recurrent models face challenges in accurately capturing long-range dependencies among sentences and efficiently managing computational resources [7]. Modeling global information, especially topical information, constitutes another critical aspect of summarization. It significantly influences the selection of sentences and provides additional context for understanding documents [3], [8]. However, it has been observed that several previous studies [9], [10] have considered topic models as a standalone source of information, neglecting to investigate a cohesive approach that concurrently improves both the task of text summarization (capturing complex inter-sentence relationships within documents) and the effectiveness of topic models. \n\nIn recent years, there have been notable achievements in applying Graph Neural Networks (GNNs) or Graph Attention Networks (GAT) [11] to summarize documents [4], [12], [13], [14], [15], [16], [17].",
            "score": 0.48051836594576036,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 942
                },
                {
                    "start": 945,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1205
                },
                {
                    "start": 1208,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2267
                },
                {
                    "start": 2270,
                    "end": 2467
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 189,
                    "matchedPaperCorpusId": "219504970"
                },
                {
                    "start": 575,
                    "end": 578,
                    "matchedPaperCorpusId": "202541012"
                },
                {
                    "start": 1061,
                    "end": 1064,
                    "matchedPaperCorpusId": "250012667"
                },
                {
                    "start": 1418,
                    "end": 1421,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 1906,
                    "end": 1909,
                    "matchedPaperCorpusId": "250012667"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.927734375
        },
        {
            "corpus_id": "273662136",
            "title": "GraphLSS: Integrating Lexical, Structural, and Semantic Features for Long Document Extractive Summarization",
            "text": "Graph Structure Developing an effective graph structure for summarization has been challenging, leading to a proliferation of diverse approaches. Wang et al. (2020) proposed connecting sentence nodes to word nodes by establishing undirected associations with the contained words. Subsequently, Jia et al. (2020) extended this by introducing named entity nodes and three other edge types: directed edges for tracking subsequent named entities and words in a sentence, directed edges for entities and words within a sentence, and undirected edges for sentence pairs with trigram overlap. \n\nTopic-GraphSum (Cui et al., 2020) was one of the first attempts to apply graph strategies to long document extractive summarization. It integrated a joint neural topic model to discover latent topics in a document, defining these as intermediate nodes to capture inter-sentence relationships across various genres and lengths. SSN (Cui and Hu, 2021) defined a sliding selector network with dynamic memory. SSN splits a given document into multiple segments, encodes them with BERT (Devlin et al., 2019), and selects salient sentences. Instead of representing the document as a graph, it uses a graph-based memory module, updated iteratively with a GAT (Veli\u010dkovi\u0107 et al., 2018), to allow information to flow across different windows. Heter-GraphLongSum (Phan et al., 2022) utilized words, sentences, and passages as nodes, while considering undirected edges for words in sentences, and directed edges for words in passages and passage to sentences. Instead of pre-trained embeddings, it used CNNs and bidirectional LSTMs for node encoding, yielding outstanding results. MTGNN-SUM (Doan et al., 2022) achieved similar results by capturing both inter and intra-sentence information when combining a homogeneous graph of sentence nodes with a heterogeneous graph of words and sentences, as in Wang et al. (2020). \n\nRecent studies underscore the importance of structural information in long document summarization. HEGEL (Zhang et al., 2022) modeled documents as hypergraphs, capturing semantic edges like keyword coreference, section structure, and latent topics.",
            "score": 0.48051836594576036,
            "section_title": "Previous Work",
            "char_start_offset": 2025,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 585
                },
                {
                    "start": 588,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1897
                },
                {
                    "start": 1900,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2148
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 164,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 294,
                    "end": 311,
                    "matchedPaperCorpusId": "226262332"
                },
                {
                    "start": 603,
                    "end": 621,
                    "matchedPaperCorpusId": "222310577"
                },
                {
                    "start": 919,
                    "end": 937,
                    "matchedPaperCorpusId": "235097475"
                },
                {
                    "start": 1069,
                    "end": 1090,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1240,
                    "end": 1265,
                    "matchedPaperCorpusId": "3292002"
                },
                {
                    "start": 1341,
                    "end": 1360,
                    "matchedPaperCorpusId": "252818946"
                },
                {
                    "start": 1668,
                    "end": 1687,
                    "matchedPaperCorpusId": "252819335"
                },
                {
                    "start": 1878,
                    "end": 1896,
                    "matchedPaperCorpusId": "216552978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96484375
        },
        {
            "corpus_id": "252407628",
            "title": "Summarization Programs: Interpretable Abstractive Summarization with Neural Modular Trees",
            "text": "We assume that we have a document D = {D i } d i=1 consisting of d sentences and a corresponding abstractive summary S = {S i } s i=1 consisting of s sentences. A Summarization Program P = {T i } s i=1 is defined as an (ordered) list of s binary trees where each tree T i = (V i , E i ) is a structured representation of the generative process of each summary sentence S i \u2208 S. Fig. 1 shows an example of an SP with two trees for two summary sentences. The set of nodes V i in each tree consists of single sentences, and the edges E i are labeled with one of the neural modules m \u2208 {paraphrase(\u2022), compression(\u2022), fusion(\u2022, \u2022)}. These modules represent operations over sentences wherein compression(X ) \u2192 Y and paraphrase(X ) \u2192 Y are unary operations and fusion(X , Y ) \u2192 Z is a binary operation. The leaf nodes in each tree are sentences from the document D i \u2208 D, and the root is a summary sentence S i \u2208 S. All other nodes are intermediates sentences generated by executing a neural module (referred to as I1 and I2 in Fig. 1). An edge from a node u \u2208 V i to a node v \u2208 V i labeled with the module m means that v is generated by executing m on u. The summary S is obtained by concatenating the root nodes of the trees in order. We hypothesize that the generative process of each summary sentence can be captured by composing different neural modules that operate over sentences. Following prior work on modular approaches to abstractive summarization (Jing & McKeown, 1999;2000;Lebanoff et al., 2019;2020b), we define the following three neural modules for building Summarization Programs. \n\nFusion Module. Sentence fusion in summarization combines information from multiple sentences (Lebanoff et al., 2020a). We finetune a BART-large model, which takes two sentences as input and outputs one fused sentence.",
            "score": 0.48051836594576036,
            "section_title": "SUMMARIZATION PROGRAM",
            "char_start_offset": 10663,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1592
                },
                {
                    "start": 1595,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1812
                }
            ],
            "ref_mentions": [
                {
                    "start": 1454,
                    "end": 1476,
                    "matchedPaperCorpusId": "9927756"
                },
                {
                    "start": 1476,
                    "end": 1481,
                    "matchedPaperCorpusId": "800331"
                },
                {
                    "start": 1481,
                    "end": 1503,
                    "matchedPaperCorpusId": "173990628"
                },
                {
                    "start": 1503,
                    "end": 1509,
                    "matchedPaperCorpusId": "219559167"
                },
                {
                    "start": 1688,
                    "end": 1712,
                    "matchedPaperCorpusId": "222208881"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89599609375
        },
        {
            "corpus_id": "268248096",
            "title": "A Comprehensive Survey on Process-Oriented Automatic Text Summarization with Exploration of LLM-Based Methods",
            "text": "Abstractive summarization models generate summaries by producing sentences distinct from the original text. This can be achieved through text structuring and combining or via a generative models that being trained on predicting the next token. Abstractive summarization can be classified as either \"rule-based\" or \"generative\" methods. The The Rule-based Summarization methods will be introduced as the following: \n\nTree-based Models: Tree-based models use syntactic trees to structure input text, identify key sentences, and integrate them into coherent summaries. [199] proposed BASTS, which uses a dominator tree to split ASTs into blocks, modeled with Tree-LSTMs for improved code summarization. [200] introduced AST-Trans, leveraging ancestor-descendant and sibling relationships to apply tree-structured attention for efficient encoding. These methods reduce redundancy but may miss important semantic connections by not fully considering broader context. \n\nGraph-based Models: Graph structures represent complex relationships between elements, enabling improved information flow in summarization tasks. [201] introduced \"Opinosis,\" a graph-based abstractive summarizer with nodes representing words and edges reflecting sentence structure. [202] proposed a framework using Abstract Meaning Representation (AMR) graphs, transforming source text into condensed semantic graphs for summary generation. [203] developed GMQS, a graphenhanced multihop summarizer, using semantic relation graphs for better multirelational aggregation. While graph-based methods offer flexibility, they are harder to implement and optimize, with less interpretable outputs than tree-based approaches. \n\nThe Generative Summarization Methods treat summarization as a sequence-to-sequence generation task, leveraging advancements in pre-trained models and large-scale corpora to produce high-quality summaries with enriched linguistic understanding. The methods fall into this category will be introduced in detail as the following: \n\nDL-based Models: Deep-learning-based ATS methods, including RNN, LSTM, GRU, and Transformer models, are wellsuited for sequence-to-sequence tasks, enabling effective text processing and generation.",
            "score": 0.48051836594576036,
            "section_title": "Abstractive Summarization",
            "char_start_offset": 52244,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 961
                },
                {
                    "start": 964,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1683
                },
                {
                    "start": 1686,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2012
                },
                {
                    "start": 2015,
                    "end": 2212
                }
            ],
            "ref_mentions": [
                {
                    "start": 566,
                    "end": 571,
                    "matchedPaperCorpusId": "232233271"
                },
                {
                    "start": 700,
                    "end": 705,
                    "matchedPaperCorpusId": "249305308"
                },
                {
                    "start": 1110,
                    "end": 1115,
                    "matchedPaperCorpusId": "988010"
                },
                {
                    "start": 1406,
                    "end": 1411,
                    "matchedPaperCorpusId": "257813092"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93212890625
        },
        {
            "corpus_id": "246388617",
            "title": "COVIDSum: A linguistically enriched SciBERT-based summarization model for COVID-19 scientific papers",
            "text": "An overall architecture of our proposed linguistically enriched abstractive summarization approach. \n\nX. Cai et al. graph structure G (i.e., the adjacency matrix of the word co-occurrence graph) as input, and then outputs the encoded linguistic features, which can be formulated as: \n\nwhere GAT(\u22c5) represents multiple layers of graph attention networks, i. e., the knowledge encoder, and H gat is the corresponding output node embeddings.",
            "score": 0.48051836594576036,
            "section_title": "Graph attention networks based knowledge encoder",
            "char_start_offset": 25568,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 102,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 438
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69921875
        },
        {
            "corpus_id": "252407628",
            "title": "Summarization Programs: Interpretable Abstractive Summarization with Neural Modular Trees",
            "text": "A small-scale manual study of our generated SPs reveal that the two most common forms of errors include (1) redundant or longer paths in an SP, and (2) fusion module generating non-factual sentences or ignoring one of its source sentences. Two other notable issues, arising out of the independence assumption of summary sentences, are (1) final summary sentences having overlapping information, and (2) incoherence between consecutive sentences. One way to improve this is to add a 'coherence' module on top of the root nodes before generating the final summary, which is an interesting topic for future work. We build SPs using sentences as the fundamental content unit (nodes) due to the relative ease of defining and training neural modules on sentences and the availability of large-scale training data. Summarization may also involve other text manipulation operations that are not fully captured by our modules but our framework allows easy inclusion of other modules. Finally, we envision that SPs could also help in locating and debugging errors in human and model summaries by tracing back the generative process of summary sentences to document sentences. \n\nWe proposed the Summarization Program, a novel framework for interpretable abstractive summarization. We demonstrated its effectiveness by developing SP-SEARCH that identifies summarization programs for human summaries with highly faithful neural modules and SP generation models that produce summaries from source documents. Via an initial simulation study, we also show that SPs improve the interpretability of a summarization model.",
            "score": 0.48051836594576036,
            "section_title": "DISCUSSION AND CONCLUSION",
            "char_start_offset": 27092,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1165
                },
                {
                    "start": 1168,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1603
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.923828125
        },
        {
            "corpus_id": "248780330",
            "title": "Discriminative Marginalized Probabilistic Neural Method for Multi-Document Summarization of Medical Literature",
            "text": "We describe related works on multi-document summarization categorized on model architectures. \n\nFlat solutions. Flat concatenation is a simple yet powerful solution because the generation of the multi-document summary is treated as a singledocument summarization task, thus it can leverage state-of-the-art pre-trained summarization models. \n\nConsequently, processing all documents as a flat input requires models capable of handling long sequences. As previously experimented by DeYoung et al. (2021), Xiao et al. (2021) proposed to leverage the Longformer-Encoder-Decoder model (Beltagy et al., 2020) pre-trained with a novel multi-document summarization specific task. They proved that a long-range Transformer that encodes all documents is a straightforward yet effective solution, and they achieved new state-of-the-art results in several multi-document summarization datasets. However, such models may struggle to handle a massive cluster of topic-related documents since they need to truncate them because of architectural limits. Further, processing all documents in a cluster could be noisy if some of them are not relevant or factual with respect to the summary. \n\nHierarchical solutions. To better preserve crossdocument relations and obtain semantic-rich representations, hierarchical concatenation solutions leverage graph-based techniques to work from word and sentence-level (Wan and Yang, 2006;Liao et al., 2018;Nayeem et al., 2018;Antognini and Faltings, 2019;Li et al., 2020) to documentlevel (Amplayo and Lapata, 2021). Other hierarchical approaches include multi-head pooling and inter-paragraph attention architectures (Liu and Lapata, 2019a), attention models with maximal marginal relevance (Fabbri et al., 2019), and attention across different granularity representations (Jin et al., 2020). Such models are often datasetspecific because of the custom architecture, so they struggle to adapt to other datasets and effectively leverage pre-trained state-of-the-art Transformers. \n\nOur solution.",
            "score": 0.4804074749886743,
            "section_title": "Related Work",
            "char_start_offset": 3656,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 96,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 340
                },
                {
                    "start": 343,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1172
                },
                {
                    "start": 1175,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 2001
                },
                {
                    "start": 2004,
                    "end": 2017
                }
            ],
            "ref_mentions": [
                {
                    "start": 480,
                    "end": 501,
                    "matchedPaperCorpusId": "233231380"
                },
                {
                    "start": 1390,
                    "end": 1410,
                    "matchedPaperCorpusId": "5457260"
                },
                {
                    "start": 1410,
                    "end": 1428,
                    "matchedPaperCorpusId": "49210924"
                },
                {
                    "start": 1428,
                    "end": 1448,
                    "matchedPaperCorpusId": "52011473"
                },
                {
                    "start": 1477,
                    "end": 1493,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 1640,
                    "end": 1663,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 1714,
                    "end": 1735,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 1796,
                    "end": 1814,
                    "matchedPaperCorpusId": "220045815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8857421875
        },
        {
            "corpus_id": "269922001",
            "title": "Leveraging Discourse Structure for Extractive Meeting Summarization",
            "text": "Similarly, Liu et al. (2019) showed that supplementing transcripts with multi-modal information about participants' head orientation and eye gaze can help to identify salient information in a meeting. \n\nIn this paper, we exploit information on discourse structure to improve extractive summarization. While abstractive summarization is generally preferable for spontaneous conversation (Murray et al., 2010), focusing on extractive summarization is valuable for multiple reasons. First, even if it is not entirely immune to hallucinations (Zhang et al., 2023), extractive summarization does not suffer from this phenomenon at the level that its abstractive counterpart does (Cao et al., 2018). On the one hand, this makes extractive summarization an attractive final product in itself in contexts arXiv: 2405.11055v3 [cs.CL] 23 Sep 2024 where reliability is crucial. On the other hand, this, combined with the fact that extractive summarization is easier to evaluate than abstractive summarization, makes it a clearer lens through which to study the interaction between discourse structure and content salience. Finally, as a part of a pipeline for abstractive summarization (Shang et al., 2020), it can be used to reduce the number of tokens given as input to a generative system with context length constraints, such as a transformerbased model. \n\nOur approach involves a novel combination of Graph Neural Networks (GNNs) and graph-based representations of discourse in which each node in a graph represents the content of an individual utterance and each edge represents a semantic relation between two utterances whose nature is specified by a label, e.g., Explanation, Correction, Question-Answer Pair or Acknowledgment. The task of extractive summarization is then couched as one of binary node-classification in which nodes are determined to be important or not, and the content of the nodes judged to be important is what determines the final extractive summary. This approach, in contrast to the generation at the whole graph-level (Feng et al., 2020), allows us to gain fine-grained insight into the interaction between the importance of an utterance in a conversation and its role in the overall discourse.",
            "score": 0.48028478302139305,
            "section_title": "Introduction",
            "char_start_offset": 2195,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 203,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1347
                },
                {
                    "start": 1350,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2217
                }
            ],
            "ref_mentions": [
                {
                    "start": 386,
                    "end": 407,
                    "matchedPaperCorpusId": "381368"
                },
                {
                    "start": 539,
                    "end": 559,
                    "matchedPaperCorpusId": "252118883"
                },
                {
                    "start": 674,
                    "end": 692,
                    "matchedPaperCorpusId": "19198109"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84375
        },
        {
            "corpus_id": "273783699",
            "title": "Optimizing Text Summarization with Sentence Clustering and Natural Language Processing",
            "text": "Automatic Text summarization (ATS) has been extensively researched, with various methods proposed to address its challenges [6]. Early work in extractive summarization focused on approaches such as frequency-based methods, graph-based algorithms like Text Rank, and machine learning techniques [7]. These methods generally perform well in selecting salient sentences but often struggle with generating coherent summaries. Recent advancements in abstractive summarization have been driven by transformer-based models. Models like BERT (Bidirectional Encoder Representations from Transformers) and its variants have achieved state-of-the-art performance on summarization tasks by capturing deep contextual information [8]. BART a more recent model, combines the strengths of both bidirectional and autoregressive transformers, making it particularly effective for text generation tasks [1]. This section provides a summary of relevant research that informs the hybrid summarization approach, which combines extractive and abstractive techniques.",
            "score": 0.4797345839822623,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 3830,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1043
                }
            ],
            "ref_mentions": [
                {
                    "start": 294,
                    "end": 297,
                    "matchedPaperCorpusId": "257848076"
                },
                {
                    "start": 884,
                    "end": 887,
                    "matchedPaperCorpusId": "257766291"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92919921875
        },
        {
            "corpus_id": "267750445",
            "title": "Analysis of Multidomain Abstractive Summarization Using Salience Allocation",
            "text": "Zhu et al. [24] enhanced abstractive summarization accuracy by integrating knowledge graphs, improving factual correctness and ROUGE scores on the CNN/Dailymail dataset. Saito et al. [25] suggested a summarization model combining a saliency model and a sequence-to-sequence model to enhance abstractive summarization performance. Manakul and Gales [26] proposed an innovative summarization approach, merging local self-attention and extractive summarization, improving accuracy in summarizing lengthy documents. Recent research in summarization highlights that integrating extractive summaries into input documents boosts abstractive summarization efficiency, improving accuracy and coherence [25,27]. Rehman at al. [28] analyzed various pre-trained models, including google/pegasus-cnndailymail, T5-base, and facebook/bart-large-cnn, for text summarization across diverse datasets such as CNN-dailymail, SAMSum, and BillSum. Cao et al. [29] introduced a novel summarization method, selecting relevant training data to enhance summarization quality and coherence using appropriate templates to reduce hallucination. Recent studies shown, [30,31] hallucination can be reduced by employing a filter on the training data and utilizing multi-task learning techniques. \n\nRecent studies, investigate using selective attention in NLP models to incorporate prior information, improving summarization, especially at the sentence level [32,33]. Wang et al. [2] introduced SEASON, a model using salience-guided selective attention to enhance abstractive summarization performance effectively.",
            "score": 0.4795886948640047,
            "section_title": "Literature Review",
            "char_start_offset": 4510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1263
                },
                {
                    "start": 1266,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1581
                }
            ],
            "ref_mentions": [
                {
                    "start": 348,
                    "end": 352,
                    "matchedPaperCorpusId": "235262720"
                },
                {
                    "start": 697,
                    "end": 700,
                    "matchedPaperCorpusId": "202541012"
                },
                {
                    "start": 716,
                    "end": 720,
                    "matchedPaperCorpusId": "257687578"
                },
                {
                    "start": 937,
                    "end": 941,
                    "matchedPaperCorpusId": "51878811"
                },
                {
                    "start": 1138,
                    "end": 1142,
                    "matchedPaperCorpusId": "231951460"
                },
                {
                    "start": 1430,
                    "end": 1433,
                    "matchedPaperCorpusId": "248571764"
                },
                {
                    "start": 1447,
                    "end": 1450,
                    "matchedPaperCorpusId": "253098395"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76171875
        },
        {
            "corpus_id": "250301954",
            "title": "Hierarchical Heterogeneous Graph Attention Network for Syntax-Aware Summarization",
            "text": "Therefore, it is natural to guide the neural summarization system with a tree-like text graph that embodies syntactic information so that it can identify summary-worthy content and compose summaries that preserve the vital meaning of the source texts. Besides, the syntactic graph is generally easier to obtain than the semantic graph and thus alleviates the computational issue in previous methods based on constructed complicated semantic graphs. \n\nFurthermore, we choose the constituency parsing tree of the sentence as the text graph for the input of GNN in view of two primary reasons. First, the syntactic dependency relationship between tokens can be inexplicitly reflected via the path between them (as shown in Figure 1), so it has already encoded the information from the dependency-based parsing tree. Second, the constituency tree is a natural fit for extracting sub-phrases from the sentence, which is the exact case when identifying essential phrases to create summaries. Third, the extracted syntactic dependency can reflect the semantic relationship between tokens (Figure 1). That is why we favor the constituency-based tree. \n\nBased on the aforementioned motivation, we will first utilize an off-the-shelf constituency parser to obtain the constituency tree for each sentence. Then, we propose a generic syntax-aware heterogeneous graph attention network to learn the representation for each type of node in this constructed tree-like graph. This proposed GNN model consists of two types of layers. One is the syntax-aware graph attention layer for detecting the syntactic dependency relationship between each constituent pair via meta-path, and the other is the hierarchical graph pooling layer for hierarchically gathering information from the tree. \n\nThe contributions of this work are summarized as follows. \u2022 We propose a novel heterogeneous graph attention network for syntax-aware summarization based on the constituency tree. To the best of our knowledge, we are the first to incorporate constituency syntax for text summarization based on GNN. \u2022 We conduct extensive experiments on five datasets from various domains under abstractive and extractive settings to demonstrate its effectiveness. Our model is reasonably flexible and can be easily adapted into both abstractive and extractive tasks. \u2022 Furthermore, we investigate the potentially increased performance with the initiation of some SOTA pretrained models.",
            "score": 0.4789778527467602,
            "section_title": "Introduction",
            "char_start_offset": 2188,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 448
                },
                {
                    "start": 451,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1142
                },
                {
                    "start": 1145,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1769
                },
                {
                    "start": 1772,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2219
                },
                {
                    "start": 2220,
                    "end": 2322
                },
                {
                    "start": 2323,
                    "end": 2442
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94091796875
        },
        {
            "corpus_id": "248307224",
            "title": "Hybrid multi-document text summarization via categorization based on BERT deep learning models",
            "text": "The proposed approach was a multi-task learning system that had merged two major objectives: \"domain classification (auxiliary task) and abstractive review summarization (primary task)\". In addition, the authors have proposed a weakly supervised LDA model intending to gather knowledge on sentiment lexicon and domain-specific aspect representations, which were fed as input to the hidden neural states for aspect/sentiment-aware review representations. \n\nIn 2018, Qasem A. Al-Radaideh and Dareen Q. Bataineh [5] have developed a hybrid single document approach (ASDKGA) which incorporates domain knowledge, statistical features and genetic algorithm to extract important points. Further, the approach is tested on KALIMAT and EASC. Finally, the proposed approach is compared with ROUGE. \n\nIn 2019, Song et al. [6] introduced ATS for summary sentence creation from different source sentences and preserved the shorter representation with no loss in information. Furthermore, the projected LSTM-CNN based ATS framework (ATSDL) could construct new sentences by exploiting the semantic phrases that were more fine-grained fragments than sentences. Finally, the proposed work has shown reliable results from CNN. \n\nIn 2018, Alamiet al. [7] had constructed a new approach for Traditional ATS using the VAE model to learn the feature space from the \"high-dimensional input data\". \n\nBased on the latent representation generated from VAE, the authors have ranked the sentences. The query-based and graph-based approaches were utilized for investigating the impact of the presented work. In 2020, Tomer and Kumar [8] had introduced a novel hybrid approach for GATS. The proposed work was the amalgamation of FLS for extractive sentence selection with Bi-LSTM for abstractive summary production. Moreover, the network weights were updated using the attention mechanism and Adam optimizer. They have explored the most relevant sentences from the document, fuzzy measures and inference. Finally, an abstractive summary was produced for the significant sentences by feeding the relevant sentences as input to Bi-LSTM.",
            "score": 0.4787591055107836,
            "section_title": "Related works",
            "char_start_offset": 8205,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 453
                },
                {
                    "start": 456,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 787
                },
                {
                    "start": 790,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1208
                },
                {
                    "start": 1211,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2104
                }
            ],
            "ref_mentions": [
                {
                    "start": 509,
                    "end": 512,
                    "matchedPaperCorpusId": "51934657"
                },
                {
                    "start": 811,
                    "end": 814,
                    "matchedPaperCorpusId": "3342465"
                },
                {
                    "start": 1232,
                    "end": 1235,
                    "matchedPaperCorpusId": "139600531"
                },
                {
                    "start": 1604,
                    "end": 1607,
                    "matchedPaperCorpusId": "225199768"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86083984375
        },
        {
            "corpus_id": "245827982",
            "title": "An Unsupervised Masking Objective for Abstractive Multi-Document News Summarization",
            "text": "Non-neural unsupervised MDS has been approached from both extractive (Carbonell and Goldstein, 1998;Gillick and Favre, 2009;Haghighi and Vanderwende, 2009) and abstractive (McKeown and Radev, 1995;Radev and McKeown, 1998;Barzilay et al., 1999;Ganesan et al., 2010) angles. At a high level, extractive methods compute a heuristic ranking of source segments and compose them together. Centroid-based extractive summarization ranks a graph representation of the input document(s), where edges are drawn between segments with similar meanings (Radev et al., 2004;Erkan and Radev, 2004;Mihalcea and Tarau, 2004). Instead of computing centroid-based score functions to produce an output, in our approach we will use them to select a masked document candidate as an unsupervised objective for training a neural abstractive summarization system. \n\nSupervised neural abstractive single-document summarization has received increased interest lately (Rush et al., 2015;See et al., 2017;Gehrmann et al., 2018) despite its many shortcomings (Kry\u015bci\u0144ski et al., 2019), such as a reliance on large human-annotated datasets (Napoles et al., 2012;Hermann et al., 2015). Additionally, supervised abstractive MDS models have also been proposed alongside new datasets (Liu and Lapata, 2019;Liu et al., 2018;Fabbri et al., 2019). Our unsupervised masking objective enables us to train any of the above neural methods discriminatively without ground truth summaries. \n\nResearch into unsupervised neural abstractive multi-document summarization is scarcer. Lebanoff et al. (2018) and Zhang et al. (2018) transferred single-document summarization models to MDS, which required no supervised multidocument data.",
            "score": 0.47873754077363606,
            "section_title": "Related Work",
            "char_start_offset": 5223,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 837
                },
                {
                    "start": 840,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1444
                },
                {
                    "start": 1447,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1686
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 100,
                    "matchedPaperCorpusId": "4508623"
                },
                {
                    "start": 100,
                    "end": 124,
                    "matchedPaperCorpusId": "167874"
                },
                {
                    "start": 124,
                    "end": 155,
                    "matchedPaperCorpusId": "678258"
                },
                {
                    "start": 172,
                    "end": 197,
                    "matchedPaperCorpusId": "2446679"
                },
                {
                    "start": 197,
                    "end": 221,
                    "matchedPaperCorpusId": "10019526"
                },
                {
                    "start": 221,
                    "end": 243,
                    "matchedPaperCorpusId": "7031344"
                },
                {
                    "start": 243,
                    "end": 264,
                    "matchedPaperCorpusId": "988010"
                },
                {
                    "start": 559,
                    "end": 581,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 581,
                    "end": 606,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 939,
                    "end": 958,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 958,
                    "end": 975,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 975,
                    "end": 997,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 1108,
                    "end": 1130,
                    "matchedPaperCorpusId": "9586240"
                },
                {
                    "start": 1130,
                    "end": 1151,
                    "matchedPaperCorpusId": "6203757"
                },
                {
                    "start": 1248,
                    "end": 1270,
                    "matchedPaperCorpusId": "170079112"
                },
                {
                    "start": 1287,
                    "end": 1307,
                    "matchedPaperCorpusId": "174799390"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6748046875
        },
        {
            "corpus_id": "262089204",
            "title": "Using Pre-Trained Language Models for Abstractive DBPEDIA Summarization: A Comparative Study",
            "text": "LLMs for abstractive summarization. Recent years have witnessed a growing interest in summarizing descriptions of real-world entities in knowledge graphs [12,13]. This task, known as text summarization, requires selecting the most essential and salient concepts, entities, and relationships from the knowledge graph, and generating a brief and coherent summary of them. Text summarization can generally be divided into two categories: i) extractive summarization [14], which involves selecting the most salient and informative sentences from a document to create a summary, and ii) abstractive summarization [15], which involves generating a new summary that conveys the main ideas of the original document, potentially using new phrases and sentences that were not present in the original text. Our study focuses on the latter for generating abstractive summaries of DBPEDIA abstracts. \n\nAbstractive summarization is a text-generation process that aims to produce summaries that are fluent and coherent, as well as informative and concise. Previous works have employed deep neural networks and language generation techniques to achieve this goal, often using a sequence-to-sequence (Seq2Seq) architecture with an attention mechanism or transformers. These methods can generate summaries that are more expressive and natural than extractive summaries, which simply select sentences from the original document. For example, See et al. [16] proposed the pointer-generator network, which combines the ability to generate new words with the ability to copy words from the input text. This hybrid approach allows for the generation of more fluent and accurate summaries as demonstrated by the evaluation results on the CNN/Daily Mail dataset, where it outperformed several baselines. Another example is the fine-tuning of pre-trained language models on large-scale summarization datasets, which can lead to substantial improvements in abstractive summarization and generate higher-quality summaries [17]. Pre-trained language models such as T5, BART, and GPT-2 have also achieved outstanding performance in generating high-quality summaries in terms of relevance, fluency, and semantic accuracy [18] Motivated by this success, we propose our approach for employing pre-trained language models to produce abstractive summaries of DBPEDIA abstracts.",
            "score": 0.477696851838859,
            "section_title": "Related Works",
            "char_start_offset": 4794,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 36,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 886
                },
                {
                    "start": 889,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2342
                }
            ],
            "ref_mentions": [
                {
                    "start": 463,
                    "end": 467,
                    "matchedPaperCorpusId": "256969487"
                },
                {
                    "start": 608,
                    "end": 612,
                    "matchedPaperCorpusId": "57992117"
                },
                {
                    "start": 1434,
                    "end": 1438,
                    "matchedPaperCorpusId": "8314118"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85400390625
        },
        {
            "corpus_id": "251594480",
            "title": "Parallel Hierarchical Transformer with Attention Alignment for Abstractive Multi-Document Summarization",
            "text": "Since Sutskever et al. (2014) propose the sequence-to-sequence (seq2seq) model for machine translation, the development of NLP applications has been almost inseparable from this framework. In the field of abstractive summarization, the seq2seq model is first applied by Rush et al. (2015) to summarize sentences. With respect to the recent bloom of the attention mechanism and pre-trained models, great effort has been made to improve neural machine summarization upon extensions of seq2seq (Gehrmann et al., 2018;See et al., 2017;Zhang et al., 2019). With the promising results on single documents (See et al., 2017;Gehrmann et al., 2018;Lewis et al., 2020;Pradhan et al., 2021;Liao et al., 2021;Liang et al., 2021), there are increasing recent attempts to study abstractive multi-document summarization (MDS) in the seq2seq framework (Liu et al., 2018;Lebanoff et al., 2018;Fabbri et al., 2019;Liu and Lapata, 2019;Ma et al., 2020). \n\nThis study makes an exploratory attempt to improve the established abstractive summarization models for multi-document summarization (MDS) utilizing the Transformer (Vaswani et al., 2017) architecture. In comparison to single-document summarization, MDS brings challenges on the representation and coverage of its lengthy and linked sources. Liu et al. (2018) propose a two-stage model to first extractively select the important paragraphs, then train the concatenated flat sequences using the Transformer-decoder with memory compressed attention (T-DMCA). Although the two-stage approach effectively reduces redundant information of source documents and retains salient information as inputs, it fails to take into account the cross-document relationship in its summaries. On the other hand, Liu and Lapata (2019) propose a Hierarchical Transformer (HT) with local and global encoder layers to represent cross-token and cross-document information.",
            "score": 0.4776206877742628,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 934
                },
                {
                    "start": 937,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1885
                }
            ],
            "ref_mentions": [
                {
                    "start": 270,
                    "end": 288,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 491,
                    "end": 514,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 514,
                    "end": 531,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 531,
                    "end": 550,
                    "matchedPaperCorpusId": "155100086"
                },
                {
                    "start": 599,
                    "end": 617,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 617,
                    "end": 639,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 639,
                    "end": 658,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 658,
                    "end": 679,
                    "matchedPaperCorpusId": "228889686"
                },
                {
                    "start": 679,
                    "end": 697,
                    "matchedPaperCorpusId": "233849756"
                },
                {
                    "start": 697,
                    "end": 716,
                    "matchedPaperCorpusId": "226319324"
                },
                {
                    "start": 854,
                    "end": 876,
                    "matchedPaperCorpusId": "52053741"
                },
                {
                    "start": 876,
                    "end": 896,
                    "matchedPaperCorpusId": "174799390"
                },
                {
                    "start": 896,
                    "end": 917,
                    "matchedPaperCorpusId": "170079112"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9609375
        },
        {
            "corpus_id": "277284940",
            "title": "Global and Local Context Fusion in Heterogeneous Graph Neural Network for Summarizing Lengthy Scientific Documents",
            "text": "Conventional truncation models frequently lose crucial meaning, whereas hierarchical and distributed representations fail to effectively capture both global and local contextual dependencies. Graph-based models that incorporate heterogeneous structures aim to improve interconnectivity, but they still face challenges in effectively modeling fine-grained semantic relationships. \n\nDomain-specific approaches like SciBERTSUM and citation graph-based methods often fail to generalize effectively across diverse document types. Additionally, cluster-based and pre-training methods, despite their potential, involve substantial computational overhead, making them impractical for real-time summarization. These challenges emphasize the need for a more efficient and scalable solution that combines semantic-rich representations with a graph-based framework to advance extractive summarization for long scientific documents. \n\nThis study aims to fill this gap by proposing a computationally efficient Heterogeneous Graph Neural Network (HGNN) for extractive summarization of scientific texts. The framework combines GloVe embeddings, CNNs, and BiLSTMs for local and long-range dependencies, enhanced LDA for global context, and a Graph Attention Network (GAT) to refine node representations in a heterogeneous graph. This is the first work to integrate LDA, CNN, and BiLSTM encoders in a GAT-based model for scientific text summarization.",
            "score": 0.4757768092357937,
            "section_title": "C. LONG DOCUMENT EXTRACTIVE SUMMARIZATION",
            "char_start_offset": 18898,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 378
                },
                {
                    "start": 381,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 919
                },
                {
                    "start": 922,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1433
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9765625
        },
        {
            "corpus_id": "248266537",
            "title": "A Survey on Neural Abstractive Summarization Methods and Factual Consistency of Summarization",
            "text": "In this survey, we have summarized and analyzed state-of-the-art neural network-based abstractive summarization methods. We also point out that there are many challenges of these models remain unsolved, especially the issue of factual consistency between the summary and the source. We hope that this survey can help researchers better understand the development of abstractive summarization in recent years, so as to make more scientifically meaningful progress in this field.",
            "score": 0.4757768092357937,
            "section_title": "Conclusion",
            "char_start_offset": 41370,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 477
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7998046875
        },
        {
            "corpus_id": "264614849",
            "title": "FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations",
            "text": "-based Representations for Summarization. A growing body of work focuses on using graph-based representations for improving summarization. Whereas different approaches encode graphs into neural models for multi-document summarization (Fan et al., 2019;Li et al., 2020;Pasunuru et al., 2021;Chen et al., 2021), AMR structures have been shown to benefit both document representation and summary generation (Liu et al., 2015;Liao et al., 2018;Hardy and Vlachos, 2018;Dohare et al., 2018) and have the potential of improving controllability in summarization . The above works are related to FACTGRAPH as they use semantic graphs for content representation, but also different because they utilize graphs for the downstream summarization task, whereas FACTGRAPH employ them for factuality evaluation.\n\nSemantic Representations for Factuality Evaluation. More closely to our work, Goodrich et al. (2019) extract tuples from the document and summary and measure the factual consistency by overlapping metrics. Recently, dependency arc entailment (DAE, Goyal and Durrett, 2020) is used to measure subsentence-level factuality by classifying pairs of words defined by dependency arcs which often describe semantic relations. However, FACTGRAPH is considerably different from those approaches, since it explicitly encodes the entire graph semantic structure into the model. Moreover, while DAE considers semantic edge relations of the summary only, FACTGRAPH encodes the semantic structures of both the input document and summary leading to better factuality performance at both sentence and subsentence levels.",
            "score": 0.4757768092357937,
            "section_title": "Related Work",
            "char_start_offset": 7114,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 234,
                    "end": 252,
                    "matchedPaperCorpusId": "202785778"
                },
                {
                    "start": 252,
                    "end": 268,
                    "matchedPaperCorpusId": "218718706"
                },
                {
                    "start": 268,
                    "end": 290,
                    "matchedPaperCorpusId": "235097309"
                },
                {
                    "start": 290,
                    "end": 308,
                    "matchedPaperCorpusId": "239768911"
                },
                {
                    "start": 404,
                    "end": 422,
                    "matchedPaperCorpusId": "5001921"
                },
                {
                    "start": 422,
                    "end": 440,
                    "matchedPaperCorpusId": "49210924"
                },
                {
                    "start": 440,
                    "end": 464,
                    "matchedPaperCorpusId": "52111191"
                },
                {
                    "start": 464,
                    "end": 484,
                    "matchedPaperCorpusId": "51867906"
                },
                {
                    "start": 875,
                    "end": 897,
                    "matchedPaperCorpusId": "173188138"
                },
                {
                    "start": 1045,
                    "end": 1068,
                    "matchedPaperCorpusId": "222291532"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93701171875
        },
        {
            "corpus_id": "273662136",
            "title": "GraphLSS: Integrating Lexical, Structural, and Semantic Features for Long Document Extractive Summarization",
            "text": "Extractive document summarization condenses documents into summaries by selecting only the most relevant sentences. One intuitive approach is to model cross-sentence relationships using graphs. While prior work considered homogeneous graphs (Tixier et al., 2017;Xu et al., 2020), recent heterogeneous graph proposals have shown high effectiveness (Wang et al., 2020;Jia et al., 2020), as they define complex relationships between multiple semantic units and capture long-distance dependencies. Despite their success in summarizing long documents such as scientific papers, many efforts have been made to devise more effective graph constructions. These vary in their definitions of nodes, often requiring external tools or additional machine learning models (Cui et al., 2020), and of edges, which despite being effective, may lead to complex structures that reduce the intuitiveness of the resulting graphs (Zhang et al., 2022). \n\nThis paper introduces GraphLSS, a graph construction that avoids the need for external learning models to define nodes or edges. GraphLSS utilizes Lexical, Structural, and Semantic features, incorporating two types of nodes (sentences and words) and four types of edges (sentence order, sentences semantic similarity, words semantic similarity, and word-sentence associations). We limit word nodes to nouns, verbs, and adjectives for their high semantic richness (Bugue\u00f1o and Mendoza, 2020;Xiao and Carenini, 2019). Our document graphs are processed with GAT (Veli\u010dkovi\u0107 et al., 2018) models on two summary benchmarks, PubMed and arXiv, which are preprocessed and labeled by us. \n\nOur contributions are: i. A novel heterogeneous graph construction using lexical, structural, and semantic features, ii. State-of-the-art results on both benchmarks compared to previous graph strategies and recent non-graph methods, iii. We share our code, including calculated extractive labels and graph-data creation pipeline, on GitHub1 for reproducibility and collaboration.",
            "score": 0.4757768092357937,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 929
                },
                {
                    "start": 932,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1610
                },
                {
                    "start": 1613,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 1992
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 262,
                    "matchedPaperCorpusId": "42957184"
                },
                {
                    "start": 262,
                    "end": 278,
                    "matchedPaperCorpusId": "219036690"
                },
                {
                    "start": 347,
                    "end": 366,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 366,
                    "end": 383,
                    "matchedPaperCorpusId": "226262332"
                },
                {
                    "start": 758,
                    "end": 776,
                    "matchedPaperCorpusId": "222310577"
                },
                {
                    "start": 908,
                    "end": 928,
                    "matchedPaperCorpusId": "252780923"
                },
                {
                    "start": 1422,
                    "end": 1446,
                    "matchedPaperCorpusId": "201697450"
                },
                {
                    "start": 1491,
                    "end": 1515,
                    "matchedPaperCorpusId": "3292002"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87744140625
        },
        {
            "corpus_id": "272703914",
            "title": "BERT-VBD: Vietnamese Multi-Document Summarization Framework",
            "text": "In the domain of document summarization, two dominant approaches exist: extractive and abstractive. Extractive methods identify and extract key text segments like words, phrases, or sentences to form the summary. On the other hand, abstractive methods generate entirely new text summaries encapsulating the core information from the original documents. A recently emerged hybrid approach integrates both extractive and abstractive methods, aiming to produce summaries of even higher quality. This hybrid approach typically involves extracting a subset of crucial sentences using the extractive method, followed by the abstractive method's application on these selected sentences to generate the final summary (Liu et al. [5]). \n\nExtractive summarization involves extracting salient sentences or phrases from documents and concatenating them into a summary. The current approaches often employ graph-based techniques such as LexRank [2] and Tex-tRank [8] using sentence embeddings or focus on features such as sentence position and term frequency to calculate importance. Recent methods apply machine learning models such as reinforcement learning [10] and deep learning [22] to identify key phrases for summarization. Devlin et al [1] proposed the BERT model and Liu et al [6] proposed the RoBERTa model, two models that have achieved state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity. Reimers et al. [13] proposed SBERT which enables encoding sentences and paragraphs into dense vector representations using pretrained language models. It achieves state-of-the-art results on various sentence embedding benchmarks. In this work, we utilize SBERT for the extractive summarization process. \n\nUnlike extractive methods that directly copy key segments from the source text, abstractive summarization leverages deep learning capabilities in natural language processing (NLP) to generate entirely new sentences or paraphrases for the summary, even using words not present in the original documents. This recent advancement is largely driven by the increasing complexity and power of language models. Early attempts such as Nallapati et al [9] employed recurrent neural networks (RNNs) with attention mechanisms, incorporating auxiliary techniques such as keyword mapping and rare word extraction.",
            "score": 0.4755026393733097,
            "section_title": "Related Work",
            "char_start_offset": 2531,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 726
                },
                {
                    "start": 729,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1734
                },
                {
                    "start": 1737,
                    "end": 2039
                },
                {
                    "start": 2040,
                    "end": 2140
                },
                {
                    "start": 2141,
                    "end": 2337
                }
            ],
            "ref_mentions": [
                {
                    "start": 721,
                    "end": 724,
                    "matchedPaperCorpusId": "232374165"
                },
                {
                    "start": 932,
                    "end": 935,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 950,
                    "end": 953,
                    "matchedPaperCorpusId": "577937"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94482421875
        },
        {
            "corpus_id": "252564120",
            "title": "Unsupervised Multi-Granularity Summarization",
            "text": "For DUC2004 and GranuDUC, we test directly with the Summarizer pre-trained on Multi-News, since these datasets are both in the news domain. \n\nIn all experiments, we use standard pyrouge4 to calculate ROUGE scores. Due to the limitation of computational resources, we truncate an input text to 3,072 tokens for LED models. \n\nBaselines We use the following baselines: \n\nBART (Lewis et al., 2020) is the state-of-the-art sequence-to-sequence pre-trained model for vari-ous generation tasks, including abstractive dialogue generation, question answering, and text summarization. We use BART-large in all the experiments. \n\nPEGASUS (Zhang et al., 2020b) is a powerful generation model with gap-sentences generation as a pretraining objective tailored for abstractive summarization. We use the large version of PEGASUS for comparison. \n\nPEGASUS-event indicates that on top of PEGA-SUS, additional event information is prepended to the input before the \u27e8mask\u27e9 token. We compare it to see if additional event information can be captured without our event-aware pre-training stage. \n\nLED (Beltagy et al., 2020) has the same architecture as BART, except that the attention in the encoder introduces additional local attention and extends the position embedding to 16K tokens by copying the original embedding. The parameters in the LED are initialized by the weights in BART. \n\nLED-Length-Control (LED-LC) is a baseline that we obtained by further pre-training LED. Inspired by Fan et al. (2018), given a document and the desired number of sentences k, we randomly place k sentences in the document with the \u27e8mask\u27e9 token, and let the model recover these sentences. During inference, we input the text and the desired number of sentences as a hint to the model so that it can control the length of the output summary. 5 RIMERA (Xiao et al., 2022) is a pre-trained model for multi-document summarization that reduces the need for dataset-specific architectures and extensive labeled data. It achieves state-ofthe-art results on multi-document summarization datasets under multiple settings.",
            "score": 0.47541039111132877,
            "section_title": "Experimental Setup",
            "char_start_offset": 19609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 142,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 321
                },
                {
                    "start": 324,
                    "end": 365
                },
                {
                    "start": 368,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 616
                },
                {
                    "start": 619,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 828
                },
                {
                    "start": 831,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1072
                },
                {
                    "start": 1075,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1365
                },
                {
                    "start": 1368,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2078
                }
            ],
            "ref_mentions": [
                {
                    "start": 373,
                    "end": 393,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 627,
                    "end": 648,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 1468,
                    "end": 1485,
                    "matchedPaperCorpusId": "22716243"
                },
                {
                    "start": 1816,
                    "end": 1834,
                    "matchedPaperCorpusId": "247519084"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68017578125
        },
        {
            "corpus_id": "271525553",
            "title": "Abstractive text summarization: State of the art, challenges, and improvements",
            "text": "This can aid researchers in finding better approaches to generate high-quality abstractive summarization frameworks. \n\n\u2022 Exploration of future research directions: This study highlights emerging frontiers like personalized summarization, long-document summarization, multi-document summarization, multilingual capabilities, and improved evaluation metrics along with leveraging recent advances in Large Language Models (LLMs). It also highlights future directions to overcome limitations like inadequate representation of meaning, maintaining factual consistency, explainability and interpretability, ethical considerations and bias, and further related concepts to help advance the field. \n\n\u2022 Holistic survey of abstractive summarization: Unlike prior works focused solely on extractive summarization, this review takes a comprehensive approach, encompassing state-of-the-art abstractive summarization methods, along with comparisons and analyses of complexities, challenges, and solutions. It provides researchers with a structured overview to advance abstractive summarization research.",
            "score": 0.47522534412907397,
            "section_title": "Prior Surveys on Abstractive Summarization",
            "char_start_offset": 10808,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 119,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 689
                },
                {
                    "start": 692,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1089
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8115234375
        },
        {
            "corpus_id": "247792830",
            "title": "Entity-Driven Fact-Aware Abstractive Summarization of Biomedical Literature",
            "text": "While abstractive summarization is well studied for summarization of news articles with success attributed to the availability of a massive amount of training data, their applicability to scholarly articles, particularly, in the biomedical domain is limited. Further, although named entities have been extensively studied to convey the semantics of an article (news, scientific, social media) and the saliency of individual sentences [18] within an article, they have not been widely used as part of modeling abstractive summarization. [17] performed entityaware single-document abstractive summarization using reinforcement learning for training. Their pipeline-based approach consists of an entity-aware content selection module and abstract generation module. They evaluate their approach on the CNN/Daily Mail and NYT corpora. [18] perform entitydriven multi-document abstractive summarization of news articles (WikiSum, and Multi-News) using an encoder-decoder framework augmented with Graph Attention Network (GAT). [40] proposed EntityRank, an extension of the LexRank [41] graph-based algorithm, for entity-supported summarization of biomedical abstracts. \n\nThere have been a few recent efforts towards knowledge/factaware abstractive summarization in different domains. [42] introduced a Fact-aware Abstractive Summarization model called FaSum for improving the factual consistency of summaries in the domain of news articles. However, their approach does not leverage named entities for fact retrieval. \n\n[20] extended a transformer-based abstractive summarization model using entities disambiguated and linked to Wikidata knowledge graph and attending to the entities for summarization of news articles. Their approach, however, does not perform named-entity based fact retrieval from the knowledge base constrained by the article to be summarized and the named entities. [43] developed an unsupervised pipeline-based approach for knowledge-infused abstractive summarization for condensing patient-to-clinician diagnostic interviews based on Multi-Sentence Compression [44] and Integer Linear Programming [45].",
            "score": 0.4749582667164757,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 3549,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1163
                },
                {
                    "start": 1166,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1512
                },
                {
                    "start": 1515,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2121
                }
            ],
            "ref_mentions": [
                {
                    "start": 434,
                    "end": 438,
                    "matchedPaperCorpusId": "236478143"
                },
                {
                    "start": 831,
                    "end": 835,
                    "matchedPaperCorpusId": "236478143"
                },
                {
                    "start": 1022,
                    "end": 1026,
                    "matchedPaperCorpusId": "13482248"
                },
                {
                    "start": 1076,
                    "end": 1080,
                    "matchedPaperCorpusId": "506350"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85107421875
        },
        {
            "corpus_id": "267547632",
            "title": "Graph Neural Network and NER-Based Text Summarization",
            "text": "This case study aims to demonstrate the practical outcomes of different summarization methods. We compare the results of using Graph Neural Networks (GNNs) alone and in combination with a Named Entity Recognition (NER) based approach, and a Large Language Model (LLM-GPT) to provide a view on the summaries generated. \n\n1) Method Description: The Various methods used: \n\n\u2022 Integrated GNN-NER Approach: Our proposed approach following the pipeline showcased in the above sections. \n\n\u2022 GNN-only Approach: The process flow is similar to that of our proposed approach, apart from the additional input from NER block. 3) Summarization Results: The summaries obtained from each method can be seen in the figures below:",
            "score": 0.4744087129941039,
            "section_title": "C. Case Study for effectiveness of Novel Approach",
            "char_start_offset": 28737,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 317
                },
                {
                    "start": 320,
                    "end": 368
                },
                {
                    "start": 371,
                    "end": 479
                },
                {
                    "start": 482,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 712
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.859375
        },
        {
            "corpus_id": "271903777",
            "title": "GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization",
            "text": "\u2022 GLIMMER is a fully unsupervised multidocument summarization approach that can run on a CPU. As an out-of-the-box solution, it does not require labeled samples, large-scale corpus or additional settings for hyperparameters, and demonstrates superiority in realworld scenarios. \n\n\u2022 We utilize low-level lexical features from raw texts to automatically determine the number of semantic clusters. To our knowledge, we are the first to employ these characteristics in multi-document summarization. \n\n\u2022 Experiments conducted on Multi-News, Multi-XScience, and DUC-2004 demonstrate that our approach outperforms other nonneural approaches based on automatic evaluation metrics, as well as state-of-the-art pretrained multi-document summarization models in zero-shot scenarios. Human evaluation indicates that summaries generated by GLIM-MER are more readable and factually accurate than those generated by previous non-neural approaches.",
            "score": 0.4742222279131273,
            "section_title": "Introduction",
            "char_start_offset": 4376,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 277
                },
                {
                    "start": 280,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 494
                },
                {
                    "start": 497,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 932
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7646484375
        },
        {
            "corpus_id": "256461312",
            "title": "Are Abstractive Summarization Models truly \u2018Abstractive\u2019? An Empirical Study to Compare the two Forms of Summarization",
            "text": "Both extractive and abstractive summarization techniques have been well studied in the Natural Language Processing (NLP) community. Earlier work in extractive summarization relied on clues such as position of sentences and frequency of words while extracting most important snippets for summaries (Khan and Salim, 2014;Baxendale, 1958). More recently, neural network-based extractive summarization have gained more popularity (Alami et al., 2019;Xu and Durrett, 2019;Chen et al., 2018;Mohsen et al., 2020;Anand and Wagh, 2019;Zhong et al., 2020;Liu et al., 2019). \n\nThere has also been significant work in abstractive summarization (Genest and Lapalme, 2012;Barzilay et al., 1999;Tanaka et al., 2009). Most of the recent approaches use encoder-decoder architectures to generate summaries (Lee et al., 2020;Yao et al., 2020;Iwasaki et al., 2019;Zhang et al., 2019a;Raffel et al., 2019a;Lewis et al., 2019). These methods produce summaries using words that are not present in the source text and hence these methods have gained more popularity over the last few years. Transformer models with selfsupervised training (Devlin et al., 2018;Radford et al., 2018;Raffel et al., 2019a;Yang et al., 2019;Clark et al., 2020;Liu et al., 2019) have shown to perform well on language learning when fine-tuned on various NLP tasks. More recently, BART (Lewis et al., 2019), PEGASUS (Zhang et al., 2019a) and T5 (Raffel et al., 2019b) have shown state-of-the-art performance in abstractive summarization, so we analyze the generated summaries of these models in comparison with SOTA methods in extractive summarization (Zhong et al., 2020;Liu et al., 2019).",
            "score": 0.47384592803961834,
            "section_title": "Related Work",
            "char_start_offset": 4614,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 563
                },
                {
                    "start": 566,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1643
                }
            ],
            "ref_mentions": [
                {
                    "start": 297,
                    "end": 319,
                    "matchedPaperCorpusId": "14876509"
                },
                {
                    "start": 319,
                    "end": 335,
                    "matchedPaperCorpusId": "21241987"
                },
                {
                    "start": 426,
                    "end": 446,
                    "matchedPaperCorpusId": "67892357"
                },
                {
                    "start": 467,
                    "end": 485,
                    "matchedPaperCorpusId": "23503296"
                },
                {
                    "start": 485,
                    "end": 505,
                    "matchedPaperCorpusId": "212718533"
                },
                {
                    "start": 505,
                    "end": 526,
                    "matchedPaperCorpusId": "213817744"
                },
                {
                    "start": 632,
                    "end": 658,
                    "matchedPaperCorpusId": "6218032"
                },
                {
                    "start": 658,
                    "end": 680,
                    "matchedPaperCorpusId": "7031344"
                },
                {
                    "start": 680,
                    "end": 700,
                    "matchedPaperCorpusId": "7710946"
                },
                {
                    "start": 806,
                    "end": 823,
                    "matchedPaperCorpusId": "53208305"
                },
                {
                    "start": 823,
                    "end": 844,
                    "matchedPaperCorpusId": "210696644"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68896484375
        },
        {
            "corpus_id": "271938110",
            "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
            "text": "This study proposed a novel Topic-aware Graph Neural Abstractive Summarization model particularly developed for lengthy scientific medical texts. Our model surpassed sentence-level neural graphs by considering additional semantic elements like words and latent topic nodes. This supplements the graph structure, leading a deeper understanding of the text. The model utilizes a powerful technique called BERT to encode the entire document. This exhaustive understanding of the text enables the model to better grasp the overall semantics and relationships between concepts. LDA is also employed to identify hidden topics within the text and recognizes its underlying themes. This thematic comprehension further improved the model's ability to generate summaries that precisely capture the gist of the document. \n\nIn addition, a Heterogeneous Graph Neural Network is integrated into the framework to handle the complexity of scientific medical text by capturing meaningful connections between words, sentences, and latent topics within the document. The network can effectively model the complex connections within the text using the diverse node types. Finally, a Transformer decoder is utilized to ensure that the generated summaries are accurate, clear, and closely reflect the original text. Moreover, the decoder produced high-quality summaries using the comprehensive understanding and rich relationships captured in the previous stages. \n\nWe evaluated our model against various methods using the publicly available PubMed dataset of medical research papers. The results revealed that our approach outperformed most traditional models and achieved performance closer to the leading methods For future research, we propose several directions to further improve our model: We will investigate the integration of more complex semantic units into the model to enhance its performance and robustness. We plan to explore advanced topic modeling techniques that are effective even in resource-limited environments, aiming to maintain simplicity without sacrificing performance. Additionally, we will examine the potential of advanced, sophisticated decoder components that could more effectively synergize with other neural components.",
            "score": 0.4731360815716472,
            "section_title": "V. CONCLUSION AND FUTURE DIRECTIONS",
            "char_start_offset": 40178,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 809
                },
                {
                    "start": 812,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1441
                },
                {
                    "start": 1444,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2074
                },
                {
                    "start": 2075,
                    "end": 2232
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9736328125
        },
        {
            "corpus_id": "274965768",
            "title": "LitLLMs, LLMs for Literature Review: Are we there yet?",
            "text": "The literature review generation task, a form of multi-document summarization, has been tackled through different strategies. @cite_1 proposed a dataset, MultiXScience, to foster abstractive approaches, while @cite_2 used an extractive step to enhance neural summarization. Our work complements these by introducing intermediate plans to improve generation quality. Meanwhile, @cite_3's Galactica demonstrated potential for language models in scientific tasks, a direction we explore with zero-shot experiments.",
            "score": 0.472067128027291,
            "section_title": "GPT-4 0-shot",
            "char_start_offset": 59551,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 511
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64013671875
        },
        {
            "corpus_id": "248784489",
            "title": "Turkish abstractive text summarization using pretrained sequence-to-sequence models",
            "text": "With the emergence of the Web, there has been an exponential increase in the number of documents made available online from sources such as websites, news, blogs, books, scientific papers, and social media. In parallel to this, it has become increasingly difficult for users to find the information they are interested in due to repetitive and irrelevant content. Moreover, the time and effort that are required to comprehend all these sources are immense. There is a need to automatically digest and extract the essence of all this information since it is impractical for humans to comprehend this vast amount of information through manual efforts. In this regard, text summarization has become an inevitable necessity and a very popular field of study in the past few decades. \n\nText summarization aims at automatically generating a concise piece of text from a long document, which is capable of portraying the most important piece of information in a fluent and salient way (Luhn 1958;Edmundson 1969). There are two main approaches in automatic text summarization: extractive text summarization (Mihalcea and Tarau 2004;Nallapati, Zhai, and Zhou 2017) and abstractive text summarization (Rush, Chopra, and Weston 2015;See, Liu, and Manning 2017;Zhang et al. 2020). Extractive text summarization produces summaries by selecting the most relevant sentences or phrases from the input text without reflecting any changes. Abstractive text summarization, on the other hand, is a more challenging task where the aim is to generate a human like summary through making use of complex natural language understanding and generation capabilities. \n\nAbstractive text summarization has gained much more popularity after the advances in deep learning. Recently, sequence-to-sequence (Seq2Seq) models with encoder-decoder architecture have dominated the field. The underlying components of encoder-decoder networks have shifted from LSTM-based (Hochreiter and Schmidhuber 1997) to transformer-based (Vaswani et al. 2017) models. Language model pretraining (Radford et al. 2018;Devlin et al. 2019) has advanced the state-of-the-art, especially for natural language understanding, in numerous natural language processing (NLP) tasks.",
            "score": 0.4716431227908629,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 778
                },
                {
                    "start": 781,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1639
                },
                {
                    "start": 1642,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2220
                }
            ],
            "ref_mentions": [
                {
                    "start": 978,
                    "end": 989,
                    "matchedPaperCorpusId": "15475171"
                },
                {
                    "start": 989,
                    "end": 1004,
                    "matchedPaperCorpusId": "14728274"
                },
                {
                    "start": 1099,
                    "end": 1124,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 1124,
                    "end": 1155,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 1191,
                    "end": 1222,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 1222,
                    "end": 1249,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1249,
                    "end": 1267,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 1933,
                    "end": 1966,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1988,
                    "end": 2008,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62451171875
        },
        {
            "corpus_id": "268713198",
            "title": "MultiSum: A Multi-Facet Approach for Extractive Social Summarization Utilizing Semantic and Sociological Relationships",
            "text": "Social summarization aims to produce a short and compact summary for a collection of posts on a specific topic. It can efectively alleviate information overloading problem and help people acquire key information on social media quickly. A notable technique in the field of text summarization is the 'Extractive' approach, in which representative sentences are directly extracted from the source document. An alternative method, called the 'Abstractive' approach, generates summaries based on the core idea within the text. Both of them have made great progress within conventional summarization domains, such as news summarization and dialogue summarization (Liu et al. 2022;Liu, Jia, and Zhu 2022;Dixit, Wang, and Chen 2023;Li et al. 2023;Gao et al. 2023). \n\nSimilarly, there have been strides in social summarization. Traditional approaches typically use different types of rank-ing algorithms to get summaries based on the textual content of the posts, such as graph-based ranking algorithms (Erkan and Radev 2004;Dutta et al. 2018;Sharifi, Inouye, and Kalita 2013) and clustering-based methods (Andy, Wijaya, and Callison-Burch 2019;Wang et al. 2019;Gillani et al. 2017). These methods mainly consider each post as a separate unit and extract features independently based on its content. However, because of post's brief content and informal expression, its representation tends to be sparse. Work in recent years has focused on improving the summarization by enriching post representation. Ali et al. (2020) propose a technique for summarizing microblogs on Twitter that simultaneously considers topic sentiments. But this approach still focuses on the content of the posts and ignores the relationships between them. Liu et al. (2021) use graph convolutional network (GCN) on social networks to integrate text content and social relation features into a universal representation. Despite the validity of it, existing studies usually involve only one type of edges when constructing graphs. However, posts often have multiple types of relations, such as social and semantic relationships (Jing, Park, and Tong 2021).",
            "score": 0.47108735754461806,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 757
                },
                {
                    "start": 760,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 1995
                },
                {
                    "start": 1996,
                    "end": 2121
                }
            ],
            "ref_mentions": [
                {
                    "start": 675,
                    "end": 698,
                    "matchedPaperCorpusId": "248780072"
                },
                {
                    "start": 698,
                    "end": 725,
                    "matchedPaperCorpusId": "258866156"
                },
                {
                    "start": 725,
                    "end": 740,
                    "matchedPaperCorpusId": "254877347"
                },
                {
                    "start": 740,
                    "end": 756,
                    "matchedPaperCorpusId": "259370717"
                },
                {
                    "start": 995,
                    "end": 1017,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 1017,
                    "end": 1035,
                    "matchedPaperCorpusId": "69605878"
                },
                {
                    "start": 1035,
                    "end": 1068,
                    "matchedPaperCorpusId": "12834719"
                },
                {
                    "start": 1098,
                    "end": 1137,
                    "matchedPaperCorpusId": "199522154"
                },
                {
                    "start": 1137,
                    "end": 1154,
                    "matchedPaperCorpusId": "86783677"
                },
                {
                    "start": 1154,
                    "end": 1173,
                    "matchedPaperCorpusId": "16677293"
                },
                {
                    "start": 1495,
                    "end": 1512,
                    "matchedPaperCorpusId": "254600165"
                },
                {
                    "start": 1723,
                    "end": 1740,
                    "matchedPaperCorpusId": "240230540"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.431640625
        },
        {
            "corpus_id": "273963834",
            "title": "Mixture of Knowledge Minigraph Agents for Literature Review Generation",
            "text": "To generate a summary that is representative of the overall content, graph-based methods construct external graphs to assist document representation and cross-document relation modeling, achieving promising progress. In this regard, LexRank (Erkan and Radev 2004) and TextRank (Mihalcea and Tarau 2004) first introduced graphs to extractive text summarization in 2004. They compute sentence importance using a graph representation of sentences to extract salient textual units from documents as summarization. In 2020, Wang et al. (Wang et al. 2020) propose to extract salient textual units from documents as summarization using a heterogeneous graph consisting of semantic nodes at several granularity levels of documents. In 2022, Wang et al. (Wang et al. 2022) incorporate knowledge graphs into document encoding and decoding, generating the summary from a knowledge graph template to achieve state-of-the-art performance. \n\nHowever, to the best of our knowledge, no existing work integrates LLMs into graph-based methods to leverage their natural language understanding capabilities for improved graph construction and summary generation.",
            "score": 0.47108735754461806,
            "section_title": "Graphs in MSDS Tasks",
            "char_start_offset": 4625,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 925
                },
                {
                    "start": 928,
                    "end": 1142
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 263,
                    "matchedPaperCorpusId": "506350"
                },
                {
                    "start": 510,
                    "end": 517,
                    "matchedPaperCorpusId": "216342059"
                },
                {
                    "start": 745,
                    "end": 762,
                    "matchedPaperCorpusId": "252185277"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90234375
        },
        {
            "corpus_id": "267750445",
            "title": "Analysis of Multidomain Abstractive Summarization Using Salience Allocation",
            "text": "Nallapati et al. [4] introduced a sequence-to-sequence framework for generating abstractive summaries using bidirectional and unidirectional LSTMs, plus attention. See et al. [10] introduced a Pointer-Generator Network with coverage, aiming to enhance accuracy and reduce redundancy. The pointer-generator model, coupled with a coverage mechanism, addresses the issue of out-of-vocabulary (OOV) words and mitigates the problem of generating repetitive phrases, as integrating in [11,12,13,14]. Vaswani et al. [15] revolutionized NLP with the transformer model, using self-attention to capture long-range dependencies and enable efficient parallel training. The implementation of the transformer architecture, particularly the bidirectional encoder model BERT, has demonstrated improved performance in downstream NLP tasks, including text summarization [16,17]. PEGASUS is a pre-trained model designed for summarization tasks, trained on large corpora and incorporating a gap sentence generation task across 12 downstream summarization tasks [18]. Aksenov et al. [19] proposed the BERT windowing method as a solution to overcome the limitations of input size in BERT-based architectures. Hsu et al. [20] proposed an integrated summarization model combining extractive and abstractive techniques, achieving high ROUGE scores and readability. Gehrmann et al. [21] suggested a bottom-up abstractive summarization method with a data-efficient content selector that improves summarization performance on CNN/Dailymail and NYT datasets, outperforming other content selection models. Li et al. [22] proposed a guiding generation model that merges extractive and abstractive methods, using keywords and a prediction-guide mechanism, improving summary production significantly on the CNN/Dailymail dataset. Jin et al. [23] introduced SemSUM, a summarization model integrating semantic dependency graphs to enhance abstractive summarization, using a sentence encoder, graph encoder, and summary decoder. Zhu et al. [24] enhanced abstractive summarization accuracy by integrating knowledge graphs, improving factual correctness and ROUGE scores on the CNN/Dailymail dataset.",
            "score": 0.47108735754461806,
            "section_title": "Literature Review",
            "char_start_offset": 2517,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2162
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 20,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 175,
                    "end": 179,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 479,
                    "end": 483,
                    "matchedPaperCorpusId": "244119654"
                },
                {
                    "start": 483,
                    "end": 486,
                    "matchedPaperCorpusId": "252819161"
                },
                {
                    "start": 486,
                    "end": 489,
                    "matchedPaperCorpusId": "260366135"
                },
                {
                    "start": 489,
                    "end": 492,
                    "matchedPaperCorpusId": "256868603"
                },
                {
                    "start": 852,
                    "end": 856,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 856,
                    "end": 859,
                    "matchedPaperCorpusId": "269587647"
                },
                {
                    "start": 1041,
                    "end": 1045,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 1198,
                    "end": 1202,
                    "matchedPaperCorpusId": "21723747"
                },
                {
                    "start": 1356,
                    "end": 1360,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 1586,
                    "end": 1590,
                    "matchedPaperCorpusId": "44174597"
                },
                {
                    "start": 1808,
                    "end": 1812,
                    "matchedPaperCorpusId": "214303164"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.953125
        },
        {
            "corpus_id": "256627286",
            "title": "An entity-guided text summarization framework with relational heterogeneous graph neural network",
            "text": "GNNs for summarization Graph neural networks are originally designed for homogeneous graph with nodes of same types and with weighted edges [32,33]. Message propagation algorithms for the traditional GNNs utilize the weighted edges to propagate message from neighboring nodes to calculate node encodings [33]. The original GNN does not consider edge types. Therefore, the relational GNN (R-GNN) is proposed to utilize the edge types by equipping each edge type with a transformation function [19]. The original R-GNN does not consider edge weights. GNNs are effective approach to model the structure of documents and to capture long-distance relationships in text for summarization. For example, the GNN is employed for extractive summarization by constructing the sentence graph with sentences encoded by RNN and edge weights computed by counting discourse relation indicators, and then applying the traditional GNN to calculate the graph [17]. The R-GNN-based summarization approach is is proposed to capture long-distance relationships in long text by constructing the graph on sentences, words and entities with the relations of NEXT, IN and REF [34]. The GNN is used in a discourse-aware neural summarization model to capture long-range dependencies by constructing structural discourse graph based on RST trees and coreference mentions encoded with GNN [18]. Recently, the heterogeneous GNNs with different types of nodes and edges are proposed for summarization [2] or other applications [35][36][37][38][39][40]. For summarization, the heterogeneous graph neural network (HGNN) is \n\nproposed by constructing a sentence-word bipartite graph having only one type of edge to model crosssentence relations [2], and applying GAT [32] for node encodings calculations. For other applications, the heterogeneous GNNs are applied to recommendation [34], or to program reidentification [37].",
            "score": 0.47108735754461806,
            "section_title": "Related work",
            "char_start_offset": 8874,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1588
                },
                {
                    "start": 1591,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1889
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 147,
                    "matchedPaperCorpusId": "9665943"
                },
                {
                    "start": 304,
                    "end": 308,
                    "matchedPaperCorpusId": "9665943"
                },
                {
                    "start": 492,
                    "end": 496,
                    "matchedPaperCorpusId": "5458500"
                },
                {
                    "start": 940,
                    "end": 944,
                    "matchedPaperCorpusId": "6532096"
                },
                {
                    "start": 1359,
                    "end": 1363,
                    "matchedPaperCorpusId": "219036690"
                },
                {
                    "start": 1469,
                    "end": 1472,
                    "matchedPaperCorpusId": "216552978"
                },
                {
                    "start": 1495,
                    "end": 1499,
                    "matchedPaperCorpusId": "198952485"
                },
                {
                    "start": 1503,
                    "end": 1507,
                    "matchedPaperCorpusId": "148571592"
                },
                {
                    "start": 1507,
                    "end": 1511,
                    "matchedPaperCorpusId": "4755450"
                },
                {
                    "start": 1511,
                    "end": 1515,
                    "matchedPaperCorpusId": "81978964"
                },
                {
                    "start": 1515,
                    "end": 1519,
                    "matchedPaperCorpusId": "228843177"
                },
                {
                    "start": 1710,
                    "end": 1713,
                    "matchedPaperCorpusId": "216552978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9560546875
        },
        {
            "corpus_id": "278339382",
            "title": "Survey of Abstract Meaning Representation: Then, Now, Future",
            "text": "Several natural language processing tasks involve transforming input text into output text, such as summarization and machine translation. AMR has been explored as a valuable tool for these tasks, offering a structured semantic representation of text. \n\nSummarization. The application of AMRs for summarization began with abstractive summarization, where the goal is to generate summaries by rephrasing the main ideas of the input text into new sentences. Liu et al. [59] introduced an approach in which each sentence is first parsed into an AMR graph. These graphs are then merged and transformed into a single summary AMR graph that is passed to a text generator to produce the final summary. An example of this process is shown in Fig. 8, which summarizes two sentences: \"I saw Joe's dog, which was running in the garden\" and \"The dog was chasing a cat. \" In the merging step, graph fragments are collapsed into unified concepts by merging nodes with the same label across different sentences. The sub-graph prediction is formulated as an integer linear programming optimization problem to select the most important parts of the merged AMR graph for final summary generation. A similar approach is used for multi-document summarization [58], where the summary is generated from the PENMAN representation of the summary AMR. \n\nDespite the progress made, early work on summarization faced challenges in generating fluent language from AMRs, often producing outputs that resembled a bag-of-words collection. To address these issues, the AMR2Text model [37] introduced a guided language generation approach. Using a seq-to-seq model with attention, the guided model combined decoder probability distributions with those derived from the summary AMR. Sentences exhibiting the highest similarity to the summary AMR graph were identified using the Longest Common Subsequence (LCS) method, and n-gram probabilities were interpolated to refine word selection. Experimental results demonstrated that this guided approach produced more coherent summaries compared to unguided baselines. \n\nAMRs have also been explored for extractive summarization, which aims to retain all critical information by directly selecting salient portions of the text while avoiding incoherencies or unresolved anaphoric references.",
            "score": 0.47108735754461806,
            "section_title": "Text-to-Text Generation",
            "char_start_offset": 54089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 251
                },
                {
                    "start": 254,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1326
                },
                {
                    "start": 1329,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2078
                },
                {
                    "start": 2081,
                    "end": 2301
                }
            ],
            "ref_mentions": [
                {
                    "start": 467,
                    "end": 471,
                    "matchedPaperCorpusId": "5001921"
                },
                {
                    "start": 1239,
                    "end": 1243,
                    "matchedPaperCorpusId": "49210924"
                },
                {
                    "start": 1552,
                    "end": 1556,
                    "matchedPaperCorpusId": "52111191"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87841796875
        },
        {
            "corpus_id": "256461279",
            "title": "Assessing Inter-metric Correlation for Multi-document Summarization Evaluation",
            "text": "For generating abstractive summaries, we use four neural-based abstractive summarization systems. We concatenate the input documents. All pretrained model checkpoints accessed from the Huggingface library (Wolf et al., 2019) were further fine-tuned on PeerSum dataset (Li et al., 2022), except for Pegasus. The systems include BART (Lewis et al., 2019) which combines a bidirectional encoder with an auto-regressive decoder, T5 (Raffel et al., 2019) which is an encoder-decoder model trained using teacher forcing, LED (Longformer Encoder-Decoder) (Beltagy et al., 2020) which is a variant of the Longformer model with both encoder and decoder transformer stacks, also scaling linearly with the input, and Pegasus (Zhang et al., 2020) which is a sequence-to-sequence model with gap-sentences generation as a pretraining objective. \n\nThe system outputs we use in our experiments were generated from 100 samples from the test set. Reviews, comments, and replies were used to generate the summaries.",
            "score": 0.47108735754461806,
            "section_title": "B.2 PeerSum dataset",
            "char_start_offset": 14494,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 830
                },
                {
                    "start": 833,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 996
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80224609375
        },
        {
            "corpus_id": "264429084",
            "title": "EXABSUM: a new text summarization approach for generating extractive and abstractive summaries",
            "text": "-Get To The Point [71]: An abstractive summarization approach featuring coverage and utilizing a hybrid pointer-generator architecture. This technique addresses the challenge faced by conventional abstractive summarization systems on extensive documents, mitigating the generation of repeated and redundant words and phrases. -Fast Abstractive Summarization [36]: Introduces a precise and efficient summarization model that initially selects important sentences and subsequently rewrites them in an abstractive manner-compressing and paraphrasing-to generate a concise final summary. The method employs a novel sentence-level policy gradient technique to connect the non-differentiable calculation between these two neural networks while maintaining linguistic fluency. \n\nTables 7, 8, and 9 provide a comparison between the top-performing configurations determined during our experiments and the summarizers mentioned earlier, focusing on the ROUGE-1, ROUGE-2, DUC 2001, DUC 2002, and CNN collections, respectively. Beginning with the DUC 2001 dataset, our systems (EXABSUM Extractive and EXABSUM Abstractive ) surpass the DE and FEOM systems in terms of both ROUGE-1 and ROUGE-2 scores (see Fig. 7). \n\nUpon analyzing the feature weights derived from DE and FEOM, it becomes evident that both methods employ semantic features to ascertain the significance of sentences. This suggests that semantic techniques play a substantial role in the text summarization process. System T, which stands as the top-performing participant in the DUC 2001 competition, achieved superior ROUGE-2 results. However, it's worth noting that its performance is statistically similar to the outcomes produced by DE, FEOM, and Clas-sifier4J (a supervised approach). On the DUC 2002 dataset, the top three performing systems are EXABSUM Extractive , Parveen, and Strube [19] (See Fig. 8). It is worth noting that the Parveen and Strube [19] approach treats summarization as an optimization task, with an optimization step used to verify non-redundancy and local coherence in the resulting summaries. As expected, incorporating coherence and redundancy elimination approaches improves ATS performance.",
            "score": 0.47108735754461806,
            "section_title": "EXABSUM Abstractive",
            "char_start_offset": 62629,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 769
                },
                {
                    "start": 772,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1200
                },
                {
                    "start": 1203,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2176
                }
            ],
            "ref_mentions": [
                {
                    "start": 18,
                    "end": 22,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 358,
                    "end": 362,
                    "matchedPaperCorpusId": "44129061"
                },
                {
                    "start": 1846,
                    "end": 1850,
                    "matchedPaperCorpusId": "14547015"
                },
                {
                    "start": 1912,
                    "end": 1916,
                    "matchedPaperCorpusId": "14547015"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6142578125
        },
        {
            "corpus_id": "264429084",
            "title": "EXABSUM: a new text summarization approach for generating extractive and abstractive summaries",
            "text": "\u2022 We confront the challenge of generating abstractive summaries by presenting a graph-based summarization model designed to yield resilient abstractive summaries. This model builds upon and extends a pioneering multi-sentence compression and fusion approach, bolstered by a re-ranking method based on key-extraction. Notably, this approach functions independently of any need for training data or acquiring knowledge of the document's structure or domain. \n\nThe paper's structure is delineated as follows. The subsequent section introduces pertinent related works and outlines ATS systems developed to cater to distinct applications. Sect. \"EXABSUM ATS Approach\" delves into the description of our proposed ATS system, EXABSUM. Within this section, we expound upon its primary stages, recommended architecture, and the two methodologies employed for the creation of extractive and abstractive summaries. In Sect. \"Experimental setup\", we detail the experimental framework. Here, we provide insight into the datasets utilized, elucidate the conducted experiments aimed at parameter tuning, and subsequently discuss the evaluation process. The achieved results, compared to the other state-of-the-art systems, are presented in the final part of the section. Finally, Sect. \"Summary and conclusions\" discusses the conclusion and future work.",
            "score": 0.47041991475950246,
            "section_title": "Introduction",
            "char_start_offset": 4337,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 455
                },
                {
                    "start": 458,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1338
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8798828125
        },
        {
            "corpus_id": "261085210",
            "title": "Graph-Based Extractive Text Summarization Sentence Scoring Scheme for Big Data Applications",
            "text": "In recent years, e-commerce web portals have exponentially increased the amount of textual data over the internet in the form of reviews and comments about a product or service. This paper proposes a graph-based sentence scoring scheme for sentence selection to represent the summary of a given text. As an extension of the proposed scheme, graphbased clustering is applied as a post-processing step for the selection of representative sentences as a required summary of a given dataset. It shows that the proposed schemes provide better results than a BERT-based extractive text summarization model. The results show that the graph clustering process is not an ideal selection as a post-processing step for all types of datasets. The datasets with repetitive sentences benefit more from the proposed post-processing model. It can apply to applications where the impact of user reviews and comments may be analyzed. \n\nAs an extension of the work, the authors intend to propose a hybrid scheme of extractive and abstractive text summarization, where the designed model would be capable to handle multi-modal data summarization, where the hybrid framework would be designed as a staged framework. Initially, the extractive model would be trained on salient features of the input, and then the abstractive model would generate concise summaries. The architecture would be Transformer-based with a self-attention supervised training process over a large corpus of text documents. For visual and sequential data, convolutional neural networks and long short-term memory would be combined. This extension would have significant implications in digital content generation and social analytics, where the end-to-end framework would address a wide range of challenges with intelligent solutions.",
            "score": 0.4695494586875052,
            "section_title": "Conclusions and Future Work",
            "char_start_offset": 55757,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 915
                },
                {
                    "start": 918,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1786
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9345703125
        },
        {
            "corpus_id": "266838376",
            "title": "A knowledge-graph based text summarization scheme for mobile edge computing",
            "text": "The investigation into algorithms for the text summarization from multi-documents encounters numerous challenges and intricacies [19]. Redundancy and conflict of information among multi-documents need to be fully dealt with to avoid duplicate or contradictory information. Quality and accuracy need to be fully guaranteed [20] to ensure that the generated abstract is concise and comprehensive. Knowledge differences and language barriers between documents in different fields also need to be fully considered and solved [21]. At present, many researchers have studied the method of generating text summarization. Sanchez-Gomez et al. [22] take medical texts as an example to study the query-oriented algorithm of generating text abstracts. This method better considers the professional knowledge and needs in the medical field and has certain pertinence. It can effectively compress and sort out the important information in medical texts, making the abstracts more informative. Mojrian et al. [23] use the heuristic method of quantum computing and genetic algorithm to extract multi-document text summarization. Quantum heuristic genetic algorithm can find better abstracts in a shorter time, has strong robustness and can deal with various input documents and noise data. However, this method is difficult to implement, requires more technology and resources, and affects the real-time performance and efficiency of the algorithm. Gupta et al. [24] used two-level feature extraction method to generate abstracts. This method used two-level feature extraction, firstly understood the report at a macro level, and then extracted more detailed features, which helped to grasp the core content of the text and clearly show how each element in the abstract was extracted from the original text, so that readers could better understand the abstract. However, because this method needs multi-stage processing, it may take more time to extract abstracts than other single-stage methods. \n\nAiming at the problems existing in the above methods, we propose an automatic multi-document text summarization scheme based on knowledge graphs. Unlike traditional summarization strategies, this study delves into the deep processing and analysis of multi-document texts, fully utilizing semantic information and entity relationships within the knowledge graph. This approach facilitates efficient processing and abstract generation of multi-document texts, offering enhanced support to swiftly comprehend and assimilate vast amounts of MEC text data information.",
            "score": 0.46900068916575066,
            "section_title": "Related work",
            "char_start_offset": 6590,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1981
                },
                {
                    "start": 1984,
                    "end": 2129
                },
                {
                    "start": 2130,
                    "end": 2345
                },
                {
                    "start": 2346,
                    "end": 2547
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 133,
                    "matchedPaperCorpusId": "229173267"
                },
                {
                    "start": 322,
                    "end": 326,
                    "matchedPaperCorpusId": "232125865"
                },
                {
                    "start": 521,
                    "end": 525,
                    "matchedPaperCorpusId": "252880344"
                },
                {
                    "start": 635,
                    "end": 639,
                    "matchedPaperCorpusId": "247430498"
                },
                {
                    "start": 995,
                    "end": 999,
                    "matchedPaperCorpusId": "232313770"
                },
                {
                    "start": 1447,
                    "end": 1451,
                    "matchedPaperCorpusId": "233618825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.646484375
        },
        {
            "corpus_id": "266599825",
            "title": "Summarization of Investment Reports Using Pre-trained Model",
            "text": "As related work of abstractive summarization, there is research by Nallapati et al. [17]. They applied the attentional encoder-decoder for the task of abstractive summarization with very promising results, outperforming state-of-the-art results significantly on two different datasets. Chen et al. proposed an accurate and fast summarization model that first selects salient sentences and then rewrites them abstractively to generate a concise overall summary [18]. Cohan et al. proposed the model for abstractive summarization of single, longer-form documents (e.g., research papers) [19]. Sharma et al. constructed BIGPATENT, the large-scale summarization dataset consisting of 1.3 million patent documents with humanwritten abstractive summaries [20]. Karn et al. proposed the extractive approach into a two-step RL-based summarization task (extractive-then-abstractive) [21]. Mao et al. proposed simple yet effective heuristics for oracle extraction as well as a consistency loss term, which encourages the extractor to approximate the averaged dynamic weights predicted by the generator [22].",
            "score": 0.46892608376767536,
            "section_title": "VII. RELATED WORKS",
            "char_start_offset": 11480,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1097
                }
            ],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 88,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "44129061"
                },
                {
                    "start": 585,
                    "end": 589,
                    "matchedPaperCorpusId": "4894594"
                },
                {
                    "start": 749,
                    "end": 753,
                    "matchedPaperCorpusId": "182953211"
                },
                {
                    "start": 874,
                    "end": 878,
                    "matchedPaperCorpusId": "247476265"
                },
                {
                    "start": 1092,
                    "end": 1096,
                    "matchedPaperCorpusId": "239009689"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.728515625
        },
        {
            "corpus_id": "259289652",
            "title": "Graph-Enhanced Biomedical Abstractive Summarization Via Factual Evidence Extraction",
            "text": "Infusing structured semantic representations into language models is a rising research trend underpinning many natural language processing tasks that require understanding and reasoning capabilities. Decoupling factual non-ambiguous concept units from the lexical surface holds great potential in abstractive summarization, especially in the biomedical domain, where fact selection and rephrasing are made more difficult by specialized jargon and hard factuality constraints. Nevertheless, current graph-augmented contributions rely on extractive binary relations, failing to model real-world n-ary and nested biomedical interactions mentioned in the text. To alleviate this issue, we present EASumm, the first framework for biomedical abstractive summarization empowered by event extraction, namely graph-based representations of relevant medical evidence derived from the source scientific document. By relying on dual text-graph encoders, we prove the promising role of explicit event structures, achieving better or comparable performance than previous state-of-the-art models on the CDSR dataset. We conduct extensive ablation studies, including a wide experimentation of graph representation learning techniques. Finally, we offer some hints to guide future research in the field.",
            "score": 0.4686156720021182,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83349609375
        },
        {
            "corpus_id": "264429084",
            "title": "EXABSUM: a new text summarization approach for generating extractive and abstractive summaries",
            "text": "In this subsection, we will compare the top results achieved by EXABSUM Extractive and EXABSUM Abstractive in generating single-document summaries with the performance of various state-of-the-art summarization techniques. Specifically, we will compare our summarization outcomes with: \n\nI. The best-performing participants in the DUC 2001 and 2002 shared tasks. II. The three most successful summarizers identified in a prior evaluation as documented in [63]. III. Other approaches, both recent and earlier, that utilized the DUC01 and DUC02 datasets, and evaluated their results using ROUGE-1 and ROUGE-2 metrics. The following subsections provide a brief overview of these approaches: -Parveen and Strube [19] introduced an unsupervised graph-based technique for single-document summarization, which considers three essential summary features: significance, non-redundancy, and local coherence. -Autosummarizer [64] is a web service that generates summaries by segmenting and ranking the most crucial sentences. Its single-document summarization method involves selecting the most pertinent sentences from the source document and has demonstrated superior performance compared to other summarizers in previous evaluations [65]. Unfortunately, details regarding the functioning of this system are not available. -Classifier4J [66] is a text summarization and classification toolbox. It performs extractive single-document summarization based on word frequency and constructs the summary from the initial sentences that include any of the top-100 most frequent words in the document. -UnifiedRank [67] is an approach that introduces an innovative unified method for single-document and multi-document summarization simultaneously. It utilizes a graph-based representation along with a unified ranking technique. Mr Bush said that he hoped there would be less carping about the emergency office performance this time adding that the agency took a hit for its reaction to Hurricane Hugo",
            "score": 0.46785737688699336,
            "section_title": "Comparison to baselines",
            "char_start_offset": 58534,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 284
                },
                {
                    "start": 287,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1984
                }
            ],
            "ref_mentions": [
                {
                    "start": 454,
                    "end": 458,
                    "matchedPaperCorpusId": "14651945"
                },
                {
                    "start": 707,
                    "end": 711,
                    "matchedPaperCorpusId": "14547015"
                },
                {
                    "start": 1224,
                    "end": 1228,
                    "matchedPaperCorpusId": "6173174"
                },
                {
                    "start": 1597,
                    "end": 1601,
                    "matchedPaperCorpusId": "17224077"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51513671875
        },
        {
            "corpus_id": "277284940",
            "title": "Global and Local Context Fusion in Heterogeneous Graph Neural Network for Summarizing Lengthy Scientific Documents",
            "text": "By leveraging GloVe embeddings, CNNs, and BiLSTMs, the model effectively captures the local dependencies as well as the overall context. Furthermore, the integration of a heterogeneous neural graph structure and LDA for topic modelling enables the model to extract document-wide semantics and cross-sentence interactions, which further enhances its performance. \n\nCompared to the current state-of-the-art neural graphbased models, the proposed model outperforms Topic-GraphSum across all ROUGE scores on the arXiv dataset and achieves comparable results on the PubMed dataset. The use of a Neural Topic Model (NTM) in Topic-GraphSum offers advantages such as joint optimization with the graph network, but it introduces additional challenges in training configuration. In contrast, our model maintains simplicity and interpretability by employing LDA, without significantly compromising performance. Moreover, the proposed model performs on par with HeterGraphLongSum, which enhances passage nodes to capture higher-order semantic units. \n\nWhile HeterGraphLongSum achieves higher ROUGE scores on the arXiv dataset, our method outperforms in terms of ROUGE-1 on the PubMed dataset and performs nearly on par with HeterGraphLongSum for ROUGE-2 and ROUGE-L. This demonstrates that our model is sensitive to the unique characteristics of different datasets and is capable of effectively identifying and extracting salient content from lengthy scientific documents. \n\nThe proposed model reveals superiority over Pegasus, an existing state-of-the-art abstractive summarization model, particularly when evaluated on the arXiv dataset. This advantage can be attributed to the arXiv dataset's longer average document length and more hierarchical information structure, which pose challenges for traditional sequence-to-sequence models. In contrast, our approach, which leverages a contextual information-rich heterogeneous graph, overcomes these shortcomings. By incorporating both local and global contextual information, our model is particularly well-suited for summarizing large volumes of scientific text.",
            "score": 0.4675115232805881,
            "section_title": "1) PERFORMANCE INSIGHT ON ARXIV AND PUBMED",
            "char_start_offset": 45401,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 361
                },
                {
                    "start": 364,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1037
                },
                {
                    "start": 1040,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1460
                },
                {
                    "start": 1463,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2101
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95556640625
        },
        {
            "corpus_id": "254366408",
            "title": "KATSum: Knowledge-aware Abstractive Text Summarization",
            "text": "Classic abstraction models learn the distribution over the input texts and generate the token-level or sentence-level representations. These vectors are fed into the decoder that produces a summary with the trained model parameters that can maximize the output likelihood. However, due to the unconstrained nature of neural-based language models, the output sometimes is difficult to control. Thus, many researchers attempt to adopt constraints during the text summarygenerating process. For example, Zhou et al. propose a selective gate network to extract key points, leading the summary to be more focused on these extracted keywords [25]. Ercan et al. leverage WordNet in the lexical chain-building algorithm, which can discover the relations between two word senses [5]. Building lexical chains turns out to be a very time-consuming work since it relies on exhaustive algorithms. Litvak et al. [15] propose a graph-based method of both supervised and unsupervised approaches. In the supervised approach, the novel method first converts the documents into word graphs and trains a classifier to check if the corresponding word is included in the document extractive summary. In the unsupervised approach HITS algorithm [10] is utilised to calculate the weight of each node and select the top ones. \n\nThe aforementioned works have achieved promising results. However, the limitations are still presented. Classical abstractive summarization models do not consider the impact of additional reference from the source text but generate summaries only relying on the hidden states of the inputs. Fact-aware models apply a fact-aware approach to guide the generating process, but unrelated information is also used as guidance. Such unrelated information may distract the summarization generator from producing a better summary. Meanwhile, approaches with keyword extraction neglect the relation of these keywords, and the generator may manipulate the facts. \n\nIn this paper, we propose a novel Knowledge-aware Abstractive Text Summarization (KATSum) model, which can address the limitations mentioned above. We utilise the Knowledge Graph to extract triplets, eliminate the noise with a trained classifier and identify useful triplets as auxiliaries to guide the text summarization generation process. The architecture of KATSum has been presented in Figure 1.",
            "score": 0.46697379489071067,
            "section_title": "Keywords Extraction",
            "char_start_offset": 7358,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1300
                },
                {
                    "start": 1303,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1955
                },
                {
                    "start": 1958,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2299
                },
                {
                    "start": 2300,
                    "end": 2358
                }
            ],
            "ref_mentions": [
                {
                    "start": 636,
                    "end": 640,
                    "matchedPaperCorpusId": "1770102"
                },
                {
                    "start": 770,
                    "end": 773,
                    "matchedPaperCorpusId": "486465"
                },
                {
                    "start": 898,
                    "end": 902,
                    "matchedPaperCorpusId": "12222596"
                },
                {
                    "start": 1222,
                    "end": 1226,
                    "matchedPaperCorpusId": "216000619"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.912109375
        },
        {
            "corpus_id": "258865347",
            "title": "Is Summary Useful or Not? An Extrinsic Human Evaluation of Text Summaries on Downstream Tasks",
            "text": "We need to select a few publicly available systems to generate summaries on the three datasets and then studying the usefulness of the summaries. As neural abstractive summarization methods with pretraining have achieved great success in recent years, we mainly focus on these summarization models. A total of six representative neural models are chosen as the abstractive systems, including: \n\n\u2022 BART [20]: a sequence-to-sequence model trained as a denoising autoencoder, which is applicable to various natural language generation tasks. It is fine-tuned on CNN/DailyMail. \u2022 Pegasus [45]: a model pre-trained with self-supervised gapsentence-generation objective designed for abstractive summarization. We use the version fine-tuned on CNN/DailyMail. \u2022 BRIO [24]: a model with a new training paradigm that assigns candidate outputs probability mass according to their quality using contrastive learning. It is also fine-tuned on CNN/DailyMail. \u2022 T5 [36]: a text-to-text transfer learning framework that is pretrained with several unsupervised and supervised objectives, including summarization. \u2022 T0 [39]: a prompt-based model, which is fine-tuned on standard summarization datasets including CNN/DailyMail. \u2022 GPT3 [3]: a prompt-based language model that achieves strong performance in the few-shot setting. In this work, we use OpenAI's text-davinci-002 [32]. We also include two simple extractive systems for comparison: \n\n\u2022 Lead-n: Lead-3 is a simple but commonly used summarization baseline that selects the first three sentences of an article as the summary. we modify the Lead-3 setting and refer to it as the Lead-n model. Lead-n selects the first several sentences that are closest to the summary length we set. \n\n\u2022 Lexrank [7]: a graph-based text summarization model that calculates the importance of sentences by determining the cosine similarity between them, and the sentences with highest scores are selected as the summary.",
            "score": 0.46644915669159726,
            "section_title": "Representative Summarization Systems",
            "char_start_offset": 12108,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 392
                },
                {
                    "start": 395,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1423
                },
                {
                    "start": 1426,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1720
                },
                {
                    "start": 1723,
                    "end": 1938
                }
            ],
            "ref_mentions": [
                {
                    "start": 584,
                    "end": 588,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 950,
                    "end": 954,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1216,
                    "end": 1219,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1733,
                    "end": 1736,
                    "matchedPaperCorpusId": "506350"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79248046875
        },
        {
            "corpus_id": "256416214",
            "title": "Do Multi-Document Summarization Models Synthesize?",
            "text": "Automatic (multi-document) summarization (Nenkova and McKeown, 2011;Maybury, 1999) has been an active subfield within NLP for decades. We have focused our analysis on modern, neural abstractive models for conditional text generation (Bahdanau et al., 2015). In light of their empirical success, we have specifically evaluated a set of Transformer-based (Vaswani et al., 2017) models which have recently been used for multidocument summarization (Beltagy et al., 2020;Zhang et al., 2020;Xiao et al., 2022;Raffel et al., 2020). There has been some work on highlighting conflicting evidence in health literature specifically (Shah et al., 2021b,a), though this focused primarily on highlighting conflicting evidence and explicitly aggregating extracted content. \n\nMultiple works have attempted gauge the difficulty of multi-document summarization. Wolhandler et al. (2022) measures the difficulty of abstractive multi-document news summarization as a function of inputs necessary to produce a final summary; they find that two to four well-chosen documents can cover a news topic sufficiently for the summarizer. They also find systematic reviews are particularly ill-suited to this minimal covering approach. Giorgi et al. (2022) studies the impact of document retrieval behaviors on multidocument summarization performance, and find that models are sensitive to missing inputs. \n\nSentence fusion One view on synthesis might be that is a particular kind of sentence fusion (Barzilay and McKeown, 2005). However, past work on \"fusing\" sentences has assumed that the aim is to generate an output that contains the information common to similar sentences (Thadani and McKeown, 2013). This is intuitive in the context of, e.g., summarizing multiple news articles covering the same event. But here we are interested in the more challenging setting in which the output should reflect an aggregate measure of potentially conflicting evidence or opinions. \n\nReview and opinion summarization considers a similar task to ours: Aggregating (usually product) reviews and opinions into a single coherent text.",
            "score": 0.46644915669159726,
            "section_title": "Related Work",
            "char_start_offset": 29821,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 758
                },
                {
                    "start": 761,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1376
                },
                {
                    "start": 1379,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1945
                },
                {
                    "start": 1948,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 233,
                    "end": 256,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 467,
                    "end": 486,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 504,
                    "end": 524,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 845,
                    "end": 869,
                    "matchedPaperCorpusId": "253098164"
                },
                {
                    "start": 1471,
                    "end": 1499,
                    "matchedPaperCorpusId": "16188305"
                },
                {
                    "start": 1650,
                    "end": 1677,
                    "matchedPaperCorpusId": "12635978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8212890625
        },
        {
            "corpus_id": "268444581",
            "title": "End to End Urdu Abstractive Text Summarization With Dataset and Improvement in Evaluation Metric",
            "text": "Sun et al. used the latent semantic analysis for summarizing the legal documents. They don't use deep neural networks / machine learning methods because every case has its context and the deep neural networks decide based on previous data. Sun et al. used reinforcement learning for text summarization. They produced summaries through reinforcement ranking on semantic link network. They have claimed that semantics of text is not considered while summarizing the text so they ranked sentences on semantic link network. A method was suggested in the literature to extract the association semantic rules and use those rules for the summary generation [25]. As the judicial case have multiple hearings and multiple documents. So the last two papers focused on multiple documents summary but our work is for the single document. \n\nMachine Learning-based models are also used for text summarization tasks [18], [19], [31], [33], [41], [45], [57], [124]. In these types of methods, the Recurrent neural networks, sequence to sequence, and reinforcement models are used. Both extractive and abstractive summaries are extracted by ML-based methods. But in abstractive summary, the GA based and Graph-based methods are used rarely. For abstractive summarization, usually the machine learning, deep neural network and reinforcement models are used. For abstractive summarization supervised as well as unsupervised approaches are present in literature. Following [45] and [41] have used the deep neural networks for supervised learning of summarization. In [45], extractive summarization has been done by recurrent neural network language modeling for English language. Researchers proposed a framework based on recurrent neural network language modeling (RNNLM), which can render not only word usage cues but also long-span structural information of word co-occurrence relationships within spoken documents, bypassing most existing LMbased methods' strict bag-of-words assumption. Additional in-depth descriptions of modeling features and corresponding summary performance of various instantiation techniques and a study of alternative model complexity and combination tactics are included. In addition, comprehensive comparisons with several state-of-the-art unsupervised summarizing algorithms demonstrate the effectiveness of our RNNLMbased techniques.",
            "score": 0.4659939153231575,
            "section_title": "II. LITERATURE REVIEW",
            "char_start_offset": 12622,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 825
                },
                {
                    "start": 828,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2181
                },
                {
                    "start": 2182,
                    "end": 2346
                }
            ],
            "ref_mentions": [
                {
                    "start": 650,
                    "end": 654,
                    "matchedPaperCorpusId": "4778680"
                },
                {
                    "start": 901,
                    "end": 905,
                    "matchedPaperCorpusId": "204246471"
                },
                {
                    "start": 913,
                    "end": 917,
                    "matchedPaperCorpusId": "52004857"
                },
                {
                    "start": 919,
                    "end": 923,
                    "matchedPaperCorpusId": "23503296"
                },
                {
                    "start": 925,
                    "end": 929,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 931,
                    "end": 935,
                    "matchedPaperCorpusId": "7683213"
                },
                {
                    "start": 937,
                    "end": 941,
                    "matchedPaperCorpusId": "62585736"
                },
                {
                    "start": 943,
                    "end": 948,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1453,
                    "end": 1457,
                    "matchedPaperCorpusId": "7683213"
                },
                {
                    "start": 1462,
                    "end": 1466,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 1547,
                    "end": 1551,
                    "matchedPaperCorpusId": "7683213"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55810546875
        },
        {
            "paperId": "4a00ba1bd9a860a314577ff7a9805d3945df6392",
            "corpusId": 276509873,
            "title": "Transforming Multi-Document Abstractive Summarization: A Comparative Study of State-of-the-Art Models and Benchmarks",
            "venue": "2024 International Conference on Artificial Intelligence and Quantum Computation-Based Sensor Application (ICAIQSA)",
            "year": 2024,
            "referenceCount": 50,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICAIQSA64000.2024.10882315?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICAIQSA64000.2024.10882315, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2346517426",
                    "name": "Manish Motghare"
                },
                {
                    "authorId": "2284344112",
                    "name": "Avinash Agrawal"
                }
            ],
            "abstract": "This study presents a comprehensive comparative analysis of state-of-the-art models for multi-document abstractive summarization, focusing on their performance across key datasets, including MultiNews, WikiSum, DUC, and CNN/Daily Mail. Given the rapid proliferation of text data, the ability to generate coherent and succinct summaries from multiple documents holds immense potential for knowledge dissemination, research acceleration, and efficient information retrieval. To achieve this goal, we meticulously evaluated prominent models such as BART, Pegasus, T5, Longformer, Pointer-Generator Networks, and more. Extensive experimentation involved fine-tuning each model for optimal performance using benchmark datasets, measuring their effectiveness through ROUGE-1, ROUGE-2, ROUGE-L scores, and additional precision metrics. Results indicate that PRIMERA outperformed its counterparts, achieving the highest ROUGE scores across the datasets, demonstrating superior summarization quality and coherence. The findings underscore the nuanced capabilities of transformer-based architectures compared to hierarchical and graph-based approaches, offering insights into scalability and generalization for multi-document tasks. This research reflects rigorous data processing and performance tuning, leveraging a custom dataset creation pipeline and deep analysis of generated summaries. Our work advances the field by identifying optimal strategies and architectures for multi-document summarization, paving the way for future innovations and practical applications in natural language processing.",
            "corpus_id": "276509873",
            "text": "This study presents a comprehensive comparative analysis of state-of-the-art models for multi-document abstractive summarization, focusing on their performance across key datasets, including MultiNews, WikiSum, DUC, and CNN/Daily Mail. Given the rapid proliferation of text data, the ability to generate coherent and succinct summaries from multiple documents holds immense potential for knowledge dissemination, research acceleration, and efficient information retrieval. To achieve this goal, we meticulously evaluated prominent models such as BART, Pegasus, T5, Longformer, Pointer-Generator Networks, and more. Extensive experimentation involved fine-tuning each model for optimal performance using benchmark datasets, measuring their effectiveness through ROUGE-1, ROUGE-2, ROUGE-L scores, and additional precision metrics. Results indicate that PRIMERA outperformed its counterparts, achieving the highest ROUGE scores across the datasets, demonstrating superior summarization quality and coherence. The findings underscore the nuanced capabilities of transformer-based architectures compared to hierarchical and graph-based approaches, offering insights into scalability and generalization for multi-document tasks. This research reflects rigorous data processing and performance tuning, leveraging a custom dataset creation pipeline and deep analysis of generated summaries. Our work advances the field by identifying optimal strategies and architectures for multi-document summarization, paving the way for future innovations and practical applications in natural language processing.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8798828125
        },
        {
            "paperId": "d4df2f7f687999231ca5ad41e6439be2a31351a1",
            "corpusId": 259311039,
            "title": "Query-Focused Multi-document Summarization Survey",
            "venue": "International Journal of Advanced Computer Science and Applications",
            "year": 2023,
            "referenceCount": 65,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://thesai.org/Downloads/Volume14No6/Paper_88-Query_Focused_Multi_document_Summarization_Survey.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.14569/ijacsa.2023.0140688?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.14569/ijacsa.2023.0140688, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1396413041",
                    "name": "Entesar Alanzi"
                },
                {
                    "authorId": "2220978538",
                    "name": "Safa Alballaa"
                }
            ],
            "abstract": "\u2014With the exponential growth of textual information on the web and in multimedia, query-focused multi-document summarization (QFMS) has emerged as a critical research area. QFMS aims to generate concise summaries that address user queries and satisfy their information needs. This paper provides a comprehensive survey of state-of-the-art approaches in QFMS, focusing specifically on graph-based and clustering-based methods. Each approach is examined in detail, highlighting its advantages and disadvantages. The survey covers ranking algorithms, sentence selection techniques, redundancy removal methods, evaluation metrics, and available datasets. The principal aim of this paper is to present a thorough analysis of QFMS approaches, providing researchers and practitioners with valuable insights into the field. By surveying existing techniques, the paper identifies the challenges and issues faced in QFMS and discusses potential future directions. Moreover, the paper emphasizes the importance of addressing coherency, ambiguity, vague references, evaluation methods, redundancy, and diversity in QFMS. Performance standards and competing approaches are also discussed, showcasing the advancements made in QFMS. The paper acknowledges the need for improving summarization coherence, readability, and semantic efficiency, while balancing compression ratios and summarizing quality. Additionally, it highlights the potential of hybrid methods and the integration of extractive and abstractive techniques to achieve more human-like summaries.",
            "corpus_id": "259311039",
            "text": "\u2014With the exponential growth of textual information on the web and in multimedia, query-focused multi-document summarization (QFMS) has emerged as a critical research area. QFMS aims to generate concise summaries that address user queries and satisfy their information needs. This paper provides a comprehensive survey of state-of-the-art approaches in QFMS, focusing specifically on graph-based and clustering-based methods. Each approach is examined in detail, highlighting its advantages and disadvantages. The survey covers ranking algorithms, sentence selection techniques, redundancy removal methods, evaluation metrics, and available datasets. The principal aim of this paper is to present a thorough analysis of QFMS approaches, providing researchers and practitioners with valuable insights into the field. By surveying existing techniques, the paper identifies the challenges and issues faced in QFMS and discusses potential future directions. Moreover, the paper emphasizes the importance of addressing coherency, ambiguity, vague references, evaluation methods, redundancy, and diversity in QFMS. Performance standards and competing approaches are also discussed, showcasing the advancements made in QFMS. The paper acknowledges the need for improving summarization coherence, readability, and semantic efficiency, while balancing compression ratios and summarizing quality. Additionally, it highlights the potential of hybrid methods and the integration of extractive and abstractive techniques to achieve more human-like summaries.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.74658203125
        },
        {
            "paperId": "9582ae62853eb452127f3ee76b06abc990045006",
            "corpusId": 276185597,
            "title": "Unlocking Potential of Deep Learning: A Comprehensive Analysis of Legal Case Document Summarization",
            "venue": "International Conference on Communication and Electronics Systems",
            "year": 2024,
            "referenceCount": 58,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCES63552.2024.10860267?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCES63552.2024.10860267, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2344256142",
                    "name": "Sneha Ramhari Suryavanshi"
                },
                {
                    "authorId": "2202012401",
                    "name": "Sandeep A. Thorat"
                }
            ],
            "abstract": "This study explores the application of Natural Language Processing (NLP) for automating the summarization of legal documents, addressing the challenge of managing vast amounts of legal data in India. With over 51 million pending cases across various courts, the accumulation of legal information remains largely inaccessible for public and professional use. Traditional methods rely on manual summarization, which is time-intensive and prone to human error. To tackle this, the study employs keyword extraction, named entity recognition (NER), and rhetorical role labeling techniques for refining content and structure. A novel encoder-decoder BiLSTM-GRU model is proposed, incorporating multi-metric graph-based features and a modified attention mechanism to enhance summarization precision, coherence, and relevance. These innovations improve the generation of concise and accurate summaries, critical for legal applications. Furthermore, the study demonstrates practical implementation through a chatbot designed to process legal judgments and answer user queries, showcasing the real-world applicability of the model. Performance evaluation metrics, such as ROUGE and BLEU, validate the model's effectiveness against state-of-the-art approaches. This research emphasizes the advantages of hybrid summarization methods, combining extractive and abstractive approaches to achieve both accuracy and readability. The findings highlight the potential of NLP-driven tools to revolutionize legal workflows, enhance accessibility, and streamline decision-making processes.",
            "corpus_id": "276185597",
            "text": "This study explores the application of Natural Language Processing (NLP) for automating the summarization of legal documents, addressing the challenge of managing vast amounts of legal data in India. With over 51 million pending cases across various courts, the accumulation of legal information remains largely inaccessible for public and professional use. Traditional methods rely on manual summarization, which is time-intensive and prone to human error. To tackle this, the study employs keyword extraction, named entity recognition (NER), and rhetorical role labeling techniques for refining content and structure. A novel encoder-decoder BiLSTM-GRU model is proposed, incorporating multi-metric graph-based features and a modified attention mechanism to enhance summarization precision, coherence, and relevance. These innovations improve the generation of concise and accurate summaries, critical for legal applications. Furthermore, the study demonstrates practical implementation through a chatbot designed to process legal judgments and answer user queries, showcasing the real-world applicability of the model. Performance evaluation metrics, such as ROUGE and BLEU, validate the model's effectiveness against state-of-the-art approaches. This research emphasizes the advantages of hybrid summarization methods, combining extractive and abstractive approaches to achieve both accuracy and readability. The findings highlight the potential of NLP-driven tools to revolutionize legal workflows, enhance accessibility, and streamline decision-making processes.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.468505859375
        },
        {
            "paperId": "912e053b61cae9c0c12d43e1b3fc602500e65ab5",
            "corpusId": 267617127,
            "title": "RankSum An unsupervised extractive text summarization based on rank fusion",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.05976, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283771061",
                    "name": "A. Joshi"
                },
                {
                    "authorId": "2283769406",
                    "name": "E. Fidalgo"
                },
                {
                    "authorId": "2283769403",
                    "name": "E. Alegre"
                },
                {
                    "authorId": "1398291662",
                    "name": "R. Ala\u00edz-Rodr\u00edguez"
                }
            ],
            "abstract": "In this paper, we propose Ranksum, an approach for extractive text summarization of single documents based on the rank fusion of four multi-dimensional sentence features extracted for each sentence: topic information, semantic content, significant keywords, and position. The Ranksum obtains the sentence saliency rankings corresponding to each feature in an unsupervised way followed by the weighted fusion of the four scores to rank the sentences according to their significance. The scores are generated in completely unsupervised way, and a labeled document set is required to learn the fusion weights. Since we found that the fusion weights can generalize to other datasets, we consider the Ranksum as an unsupervised approach. To determine topic rank, we employ probabilistic topic models whereas semantic information is captured using sentence embeddings. To derive rankings using sentence embeddings, we utilize Siamese networks to produce abstractive sentence representation and then we formulate a novel strategy to arrange them in their order of importance. A graph-based strategy is applied to find the significant keywords and related sentence rankings in the document. We also formulate a sentence novelty measure based on bigrams, trigrams, and sentence embeddings to eliminate redundant sentences from the summary. The ranks of all the sentences computed for each feature are finally fused to get the final score for each sentence in the document. We evaluate our approach on publicly available summarization datasets CNN/DailyMail and DUC 2002. Experimental results show that our approach outperforms other existing state-of-the-art summarization methods.",
            "corpus_id": "267617127",
            "text": "In this paper, we propose Ranksum, an approach for extractive text summarization of single documents based on the rank fusion of four multi-dimensional sentence features extracted for each sentence: topic information, semantic content, significant keywords, and position. The Ranksum obtains the sentence saliency rankings corresponding to each feature in an unsupervised way followed by the weighted fusion of the four scores to rank the sentences according to their significance. The scores are generated in completely unsupervised way, and a labeled document set is required to learn the fusion weights. Since we found that the fusion weights can generalize to other datasets, we consider the Ranksum as an unsupervised approach. To determine topic rank, we employ probabilistic topic models whereas semantic information is captured using sentence embeddings. To derive rankings using sentence embeddings, we utilize Siamese networks to produce abstractive sentence representation and then we formulate a novel strategy to arrange them in their order of importance. A graph-based strategy is applied to find the significant keywords and related sentence rankings in the document. We also formulate a sentence novelty measure based on bigrams, trigrams, and sentence embeddings to eliminate redundant sentences from the summary. The ranks of all the sentences computed for each feature are finally fused to get the final score for each sentence in the document. We evaluate our approach on publicly available summarization datasets CNN/DailyMail and DUC 2002. Experimental results show that our approach outperforms other existing state-of-the-art summarization methods.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.73193359375
        },
        {
            "paperId": "bffd163220cc0c4d28d76f73e7fd542be1ffbd8c",
            "corpusId": 269426124,
            "title": "ADVANCMENTS IN TEXT SUMMARIZATION AND EXTRACTIVE QUESTION- ANSWERING : A MACHINE LEARNING APPROACH",
            "venue": "INTERANTIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ijsrem.com/download/advancments-in-text-summarization-and-extractive-question-answering-a-machine-learning-approach/?wpdmdl=31393&refresh=665bfd39829a41717304633",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.55041/ijsrem31446?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.55041/ijsrem31446, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2298697911",
                    "name": "DR D. Eswara Chaitanya"
                }
            ],
            "abstract": "In the era of social media platforms, the rapid expansion of data mining in the fields of information retrieval and natural language processing emphasizes the crucial need for automated text summarization. At the current time, pretrained word embedding techniques and sequence to sequence models can be effectively repurposed in the realm of social network summarization to efficiently condense significant information with strong encoding capabilities. However, dealing with the challenge of extended text dependency and efficiently utilizing latent topic mapping presents an increasingly significant obstacle for these computational models. In this document, we propose a topic- focused approach for both extractive and abstractive summarization, integrating Question Answering features by employing BERT and Pegasus pretrained models. A comprehensive analysis of the architectural complexities of the pre-existing models utilized in the creation of our model has been presented. The evaluation of our Text Summarization & Question Answering Model was conducted meticulously on a variety of datasets including Multi news, XSum, and CNN/Daily Mail, with experimental results indicating its achievement of state-of-the-art performance levels based on ROGUE scores. Following this, the verification of our results through human evaluation confirms that the summaries generated by our model are in alignment with the performance standards set by humans across various datasets. Furthermore, we have integrated an Extractive Question Answering task into our model, proposing an architecture leveraging BERT and conducting comparative analyses against alternative language models to gauge its efficacy. Keywords\u2014Natural Language Processing (NLP), Text Summarization, Question & Answering (Q&A), Transformer Models, Pegasus, BERT (Bidirectional Encoder Representations from Transformers), Neural Topic Modeling, Extractive Summarization, Abstractive Summarization, Semantic Representation, Topic Embedding",
            "corpus_id": "269426124",
            "text": "In the era of social media platforms, the rapid expansion of data mining in the fields of information retrieval and natural language processing emphasizes the crucial need for automated text summarization. At the current time, pretrained word embedding techniques and sequence to sequence models can be effectively repurposed in the realm of social network summarization to efficiently condense significant information with strong encoding capabilities. However, dealing with the challenge of extended text dependency and efficiently utilizing latent topic mapping presents an increasingly significant obstacle for these computational models. In this document, we propose a topic- focused approach for both extractive and abstractive summarization, integrating Question Answering features by employing BERT and Pegasus pretrained models. A comprehensive analysis of the architectural complexities of the pre-existing models utilized in the creation of our model has been presented. The evaluation of our Text Summarization & Question Answering Model was conducted meticulously on a variety of datasets including Multi news, XSum, and CNN/Daily Mail, with experimental results indicating its achievement of state-of-the-art performance levels based on ROGUE scores. Following this, the verification of our results through human evaluation confirms that the summaries generated by our model are in alignment with the performance standards set by humans across various datasets. Furthermore, we have integrated an Extractive Question Answering task into our model, proposing an architecture leveraging BERT and conducting comparative analyses against alternative language models to gauge its efficacy. Keywords\u2014Natural Language Processing (NLP), Text Summarization, Question & Answering (Q&A), Transformer Models, Pegasus, BERT (Bidirectional Encoder Representations from Transformers), Neural Topic Modeling, Extractive Summarization, Abstractive Summarization, Semantic Representation, Topic Embedding",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.7373046875
        },
        {
            "paperId": "e02f5904e03bf0519d3596989f049e5679ecf8b9",
            "corpusId": 257527684,
            "title": "CoSS: Leveraging Statement Semantics for Code Summarization",
            "venue": "IEEE Transactions on Software Engineering",
            "year": 2023,
            "referenceCount": 66,
            "citationCount": 8,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TSE.2023.3256362?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TSE.2023.3256362, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1382573145",
                    "name": "Chaochen Shi"
                },
                {
                    "authorId": "8811761",
                    "name": "Borui Cai"
                },
                {
                    "authorId": "2202878979",
                    "name": "Yao Zhao"
                },
                {
                    "authorId": "2620563",
                    "name": "Longxiang Gao"
                },
                {
                    "authorId": "145137505",
                    "name": "Keshav Sood"
                },
                {
                    "authorId": "2068334797",
                    "name": "Yong Xiang"
                }
            ],
            "abstract": "Automated code summarization tools allow generating descriptions for code snippets in natural language, which benefits software development and maintenance. Recent studies demonstrate that the quality of generated summaries can be improved by using additional code representations beyond token sequences. The majority of contemporary approaches mainly focus on extracting code syntactic and structural information from abstract syntax trees (ASTs). However, from the view of macro-structures, it is challenging to identify and capture semantically meaningful features due to fine-grained syntactic nodes involved in ASTs. To fill this gap, we investigate how to learn more code semantics and control flow features from the perspective of code statements. Accordingly, we propose a novel model entitled CoSS for code summarization. CoSS adopts a Transformer-based encoder and a graph attention network-based encoder to capture token-level and statement-level semantics from code token sequence and control flow graph, respectively. Then, after receiving two-level embeddings from encoders, a joint decoder with a multi-head attention mechanism predicts output sequences verbatim. Performance evaluations on Java, Python, and Solidity datasets validate that CoSS outperforms nine state-of-the-art (SOTA) neural code summarization models in effectiveness and is competitive in execution efficiency. Further, the ablation study reveals the contribution of each model component.",
            "corpus_id": "257527684",
            "text": "Automated code summarization tools allow generating descriptions for code snippets in natural language, which benefits software development and maintenance. Recent studies demonstrate that the quality of generated summaries can be improved by using additional code representations beyond token sequences. The majority of contemporary approaches mainly focus on extracting code syntactic and structural information from abstract syntax trees (ASTs). However, from the view of macro-structures, it is challenging to identify and capture semantically meaningful features due to fine-grained syntactic nodes involved in ASTs. To fill this gap, we investigate how to learn more code semantics and control flow features from the perspective of code statements. Accordingly, we propose a novel model entitled CoSS for code summarization. CoSS adopts a Transformer-based encoder and a graph attention network-based encoder to capture token-level and statement-level semantics from code token sequence and control flow graph, respectively. Then, after receiving two-level embeddings from encoders, a joint decoder with a multi-head attention mechanism predicts output sequences verbatim. Performance evaluations on Java, Python, and Solidity datasets validate that CoSS outperforms nine state-of-the-art (SOTA) neural code summarization models in effectiveness and is competitive in execution efficiency. Further, the ablation study reveals the contribution of each model component.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.57080078125
        },
        {
            "paperId": "ad57e0f9712366e4eb84b3b3b54057d57ab4cadf",
            "corpusId": 257795384,
            "title": "Structure and Sequence Aligned Code Summarization with Prefix and Suffix Balanced Strategy",
            "venue": "Entropy",
            "year": 2023,
            "referenceCount": 32,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/1099-4300/25/4/570/pdf?version=1679913169",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10138082, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2179081501",
                    "name": "Jianhui Zeng"
                },
                {
                    "authorId": "2211800592",
                    "name": "Zhiheng Qu"
                },
                {
                    "authorId": "2130473823",
                    "name": "Bo Cai"
                }
            ],
            "abstract": "Source code summarization focuses on generating qualified natural language descriptions of a code snippet (e.g., functionality, usage and version). In an actual development environment, descriptions of the code are missing or not consistent with the code due to human factors, which makes it difficult for developers to comprehend and conduct subsequent maintenance. Some existing methods generate summaries from the sequence information of code without considering the structural information. Recently, researchers have adopted the Graph Neural Networks (GNNs) to capture the structural information with modified Abstract Syntax Trees (ASTs) to comprehensively represent a source code, but the alignment method of the two information encoder is hard to decide. In this paper, we propose a source code summarization model named SSCS, a unified transformer-based encoder\u2013decoder architecture, for capturing structural and sequence information. SSCS is designed upon a structure-induced transformer with three main novel improvements. SSCS captures the structural information in a multi-scale aspect with an adapted fusion strategy and adopts a hierarchical encoding strategy to capture the textual information from the perspective of the document. Moreover, SSCS utilizes a bidirectional decoder which generates a summary from opposite direction to balance the generation performance between prefix and suffix. We conduct experiments on two public Java and Python datasets to evaluate our method and the result show that SSCS outperforms the state-of-art code summarization methods.",
            "corpus_id": "257795384",
            "text": "Source code summarization focuses on generating qualified natural language descriptions of a code snippet (e.g., functionality, usage and version). In an actual development environment, descriptions of the code are missing or not consistent with the code due to human factors, which makes it difficult for developers to comprehend and conduct subsequent maintenance. Some existing methods generate summaries from the sequence information of code without considering the structural information. Recently, researchers have adopted the Graph Neural Networks (GNNs) to capture the structural information with modified Abstract Syntax Trees (ASTs) to comprehensively represent a source code, but the alignment method of the two information encoder is hard to decide. In this paper, we propose a source code summarization model named SSCS, a unified transformer-based encoder\u2013decoder architecture, for capturing structural and sequence information. SSCS is designed upon a structure-induced transformer with three main novel improvements. SSCS captures the structural information in a multi-scale aspect with an adapted fusion strategy and adopts a hierarchical encoding strategy to capture the textual information from the perspective of the document. Moreover, SSCS utilizes a bidirectional decoder which generates a summary from opposite direction to balance the generation performance between prefix and suffix. We conduct experiments on two public Java and Python datasets to evaluate our method and the result show that SSCS outperforms the state-of-art code summarization methods.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.4560546875
        },
        {
            "paperId": "a16a971c57f25012a338bcc105e2c21ec6d3c356",
            "corpusId": 274330042,
            "title": "Reviews on Transformer-based Models for Financial Time Series Forecasting",
            "venue": "Applied and Computational Engineering",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.ewadirect.com/proceedings/ace/article/view/17411/pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.54254/2755-2721/96/20241351?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.54254/2755-2721/96/20241351, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2332595512",
                    "name": "Heyi Lin"
                }
            ],
            "abstract": "Abstracts. Relationship extraction is a key step in information extraction and aims to identify entities from text data and identify semantic links between them, which is the foundation of knowledge graph construction. Improving relationship extraction efficiency can effectively enhance the quality of information extraction, which has an impact on the construction of a knowledge graph and subsequent downstream tasks. The paper improves the efficiency and quality of relationship extraction by optimizing the model, choosing the way of relationship extraction and proposing new evaluation parameter criteria. The paper summarizes the current state-of-the-art research results, and briefly describes SIEF, an evaluation mechanism that focuses on important sentences, Coreference Aid, which captures the internal structure of entities as well as the external fine-grained information, and FILR, a framework that extracts global document information and performs multi-granular logical reasoning. Aggregate semi-automated data augmentation by integrating large model languages, remote supervision and construction of domain graphs integrating domain information of entities, cross-article entity-centered relationship extraction capable of capturing connections between different articles, and the introduction of mean-accuracy averaging capable of evaluating the comprehensibility of the model are presented. This paper presents a vision of new and better optimization algorithms by describing the latest approaches to relationship extraction optimization and finding that they all have their own limitations.",
            "corpus_id": "274330042",
            "text": "Abstracts. Relationship extraction is a key step in information extraction and aims to identify entities from text data and identify semantic links between them, which is the foundation of knowledge graph construction. Improving relationship extraction efficiency can effectively enhance the quality of information extraction, which has an impact on the construction of a knowledge graph and subsequent downstream tasks. The paper improves the efficiency and quality of relationship extraction by optimizing the model, choosing the way of relationship extraction and proposing new evaluation parameter criteria. The paper summarizes the current state-of-the-art research results, and briefly describes SIEF, an evaluation mechanism that focuses on important sentences, Coreference Aid, which captures the internal structure of entities as well as the external fine-grained information, and FILR, a framework that extracts global document information and performs multi-granular logical reasoning. Aggregate semi-automated data augmentation by integrating large model languages, remote supervision and construction of domain graphs integrating domain information of entities, cross-article entity-centered relationship extraction capable of capturing connections between different articles, and the introduction of mean-accuracy averaging capable of evaluating the comprehensibility of the model are presented. This paper presents a vision of new and better optimization algorithms by describing the latest approaches to relationship extraction optimization and finding that they all have their own limitations.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.10968017578125
        }
    ],
    "quotes": {
        "cost": 0.185973,
        "quotes": [
            {
                "idx": 0,
                "key": "[247446769 | Golestani et al. | 2022 | Citations: 0]",
                "snippets": "Graph neural networks (GNN) have shown to be effective in NLP applications, including syntactic dependency trees (Marcheggiani and Titov, 2017), neural machine translation (Beck et al., 2018), knowledge graphs (Wang et al., 2018), semantic graphs (Song et al., 2018), sequence-to-graph learning (Johnson, 2017), graph-to-sequence learning (Beck et al., 2018), sentence ordering (Yin et al., 2019), and multi-document summarization (Christensen et al., 2013;Yasunaga et al., 2017). \n\nChristensen et al. (2013) used a GNN in multi-document summarization. They create multi-document graphs which determine pairwise ordering constraints of sentences based on the discourse relationship between them. Kipf and Welling (2017) proposed Graph Convolutional Networks (GCN), which is used in Yasunaga et al. (2017) to generate sentence relation graphs. The final sentence embeddings indicate the graph representation and are utilized as inputs to achieve satisfactory results on multi-document summarization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Graph neural networks (GNN) have shown to be effective in NLP applications, including syntactic dependency trees (Marcheggiani and Titov, 2017), neural machine translation (Beck et al., 2018), knowledge graphs (Wang et al., 2018), semantic graphs (Song et al., 2018), sequence-to-graph learning (Johnson, 2017), graph-to-sequence learning (Beck et al., 2018), sentence ordering (Yin et al., 2019), and multi-document summarization (Christensen et al., 2013;Yasunaga et al., 2017). \n\nChristensen et al. (2013) used a GNN in multi-document summarization. They create multi-document graphs which determine pairwise ordering constraints of sentences based on the discourse relationship between them. Kipf and Welling (2017) proposed Graph Convolutional Networks (GCN), which is used in Yasunaga et al. (2017) to generate sentence relation graphs. The final sentence embeddings indicate the graph representation and are utilized as inputs to achieve satisfactory results on multi-document summarization.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[248496597 | Wang et al. | 2022 | Citations: 1]",
                "snippets": "Dohare et al. (2018) propose to generate abstractive summaries by first generating AMR graphs (abstract meaning representation) for corresponding input stories, extract summary graphs from the AMR graphs, and lastly create summaries from the summary graphs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 257,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 257
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Dohare et al. (2018) propose to generate abstractive summaries by first generating AMR graphs (abstract meaning representation) for corresponding input stories, extract summary graphs from the AMR graphs, and lastly create summaries from the summary graphs."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[248571519 | Sankar et al. | 2022 | Citations: 0]",
                "snippets": "Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10112929 | Li et al. | 2014 | Citations: 46]": "In this paper, we focus on the problem of using sentence compression techniques to improve multi-document summarization. We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status \u2013 remove or retain. Integer liner programming with discriminative training is used to solve the problem. Under this model, we incorporate various constraints to improve the linguistic quality of the compressed sentences. Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary. Compared with state-ofthe-art algorithms, our model has similar ROUGE-2 scores but better linguistic quality on TAC data.",
                    "[218718706 | Li et al. | 2020 | Citations: 136]": "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.",
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."
                },
                "metadata": [
                    {
                        "section_title": "Multi Document Summarization",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1615,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 109
                            },
                            {
                                "start": 110,
                                "end": 276
                            },
                            {
                                "start": 277,
                                "end": 452
                            },
                            {
                                "start": 453,
                                "end": 619
                            },
                            {
                                "start": 620,
                                "end": 994
                            },
                            {
                                "start": 995,
                                "end": 1150
                            },
                            {
                                "start": 1151,
                                "end": 1363
                            },
                            {
                                "start": 1364,
                                "end": 1615
                            }
                        ],
                        "ref_mentions": [
                            "577937",
                            "506350",
                            "10112929",
                            "218718706",
                            "218718706"
                        ],
                        "quote": "Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013)."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[249097545 | Chen et al. | 2022 | Citations: 29]",
                "snippets": "For example, Zhao et al. (Zhao et al., 2020) converted the original documents to a sentence graph, taking both linguistic and deep representation into account, and Liu et al. (Liu et al., 2021) constructed sentence graph based on both the similarities and relative distances in the neighborhood of each sentence. Due to the success of neural abstractive models on single-document summarization, abstractive models are adapted to multi-document summarization. Jin et al. (Jin et al., 2020) proposed a multi-granularity interaction network for extractive and abstractive multi-document summarization, and Zhou et al. (Zhou et al., 2021) augmented the classical Transformer (Vaswani et al., 2017) with a heterogeneous graph consisting of entity nodes. However, MDS aims at synthesizing the similar and removing the redundant information among multiple input documents, while RWG needs to find the specific contributions of individual papers even if their research directions are the same and arrange them in comparative discussion (Hu et al., 2014).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[13756489 | Vaswani et al. | 2017 | Citations: 132444]": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                    "[1948673 | Hu et al. | 2014 | Citations: 61]": "In this paper, we investigate a challenging task of automatic related work generation. Given multiple reference papers as input, the task aims to generate a related work section for a target paper. The generated related work section can be used as a draft for the author to complete his or her final related work section. We propose our Automatic Related Work Generation system called ARWG to address this task. It first exploits a PLSA model to split the sentence set of the given papers into different topic-biased parts, and then applies regression models to learn the importance of the sentences. At last it employs an optimization framework to generate the related work section. Our evaluation results on a test set of 150 target papers along with their reference papers show that our proposed ARWG system can generate related work sections with better quality. A user study is also performed to show ARWG can achieve an improvement over generic multi-document summarization baselines.",
                    "[220045815 | Jin et al. | 2020 | Citations: 99]": "In this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization, which jointly learn semantic representations for words, sentences, and documents. The word representations are used to generate an abstractive summary while the sentence representations are used to produce an extractive summary. We employ attention mechanisms to interact between different granularity of semantic representations, which helps to capture multi-granularity key information and improves the performance of both abstractive and extractive summarization. Experiment results show that our proposed model substantially outperforms all strong baseline methods and achieves the best results on the Multi-News dataset.",
                    "[220633461 | Zhao et al. | 2020 | Citations: 64]": "Obtaining training data for multi-document Summarization (MDS) is time consuming and resource-intensive, so recent neural models can only be trained for limited domains. In this paper, we propose SummPip: an unsupervised method for multi-document summarization, in which we convert the original documents to a sentence graph, taking both linguistic and deep representation into account, then apply spectral clustering to obtain multiple clusters of sentences, and finally compress each cluster to generate the final summary. Experiments on Multi-News and DUC-2004 datasets show that our method is competitive to previous unsupervised methods and is even comparable to the neural supervised approaches. In addition, human evaluation shows our system produces consistent and complete summaries compared to human written ones.",
                    "[235792514 | Liu et al. | 2021 | Citations: 29]": "Supervised summarization has made significant improvements in recent years by leveraging cutting-edge deep learning technologies. However, the true success of supervised methods relies on the availability of large quantity of human-generated summaries of documents, which is highly costly and difficult to obtain in general. This paper proposes an unsupervised approach to extractive text summarization, which uses an automatically constructed sentence graph from each document to select salient sentences for summarization based on both the similarities and relative distances in the neighborhood of each sentences. We further generalize our approach from single-document summarization to a multi-document setting, by aggregating document-level graphs via proximity-based cross-document edges. In our experiments on benchmark datasets, the proposed approach achieved competitive or better results than previous state-of-the-art unsupervised extractive summarization methods in both single-document and multi-document settings, and the performance is competitive to strong supervised baselines.",
                    "[236478143 | Zhou et al. | 2021 | Citations: 28]": ","
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 957,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 283
                            },
                            {
                                "start": 284,
                                "end": 429
                            },
                            {
                                "start": 430,
                                "end": 672
                            },
                            {
                                "start": 673,
                                "end": 957
                            }
                        ],
                        "ref_mentions": [
                            "220633461",
                            "235792514",
                            "220045815",
                            "236478143",
                            "13756489",
                            "1948673"
                        ],
                        "quote": "For example, Zhao et al. (Zhao et al., 2020) converted the original documents to a sentence graph, taking both linguistic and deep representation into account, and Liu et al. (Liu et al., 2021) constructed sentence graph based on both the similarities and relative distances in the neighborhood of each sentence. Due to the success of neural abstractive models on single-document summarization, abstractive models are adapted to multi-document summarization. Jin et al. (Jin et al., 2020) proposed a multi-granularity interaction network for extractive and abstractive multi-document summarization, and Zhou et al. (Zhou et al., 2021) augmented the classical Transformer (Vaswani et al., 2017) with a heterogeneous graph consisting of entity nodes. However, MDS aims at synthesizing the similar and removing the redundant information among multiple input documents, while RWG needs to find the specific contributions of individual papers even if their research directions are the same and arrange them in comparative discussion (Hu et al., 2014)."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[250012667 | Jiang et al. | 2022 | Citations: 11]",
                "snippets": "In this work, we suggest a novel GTASum model (Graph-Based Topic-Aware abstract text summarization). First, the document is encoded with BERT (\u67f4\u7530, 2020) (Bidirectional Encoder Representation from Transformers) to obtain contextual sentence representations; meanwhile, NTM (Ding et al., 2018) is used to learn the potential topic of the document. Second, construct a heterogeneous document graph that contains sentence representation nodes and potential topic nodes, then revise them using a adapted GAT [30]. Finally, the sentence representations containing topic information are fed into a Transformer-based decoder to generate summaries.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[52182945 | Ding et al. | 2018 | Citations: 79]": "Topic models are evaluated based on their ability to describe documents well (i.e. low perplexity) and to produce topics that carry coherent semantic meaning. In topic modeling so far, perplexity is a direct optimization target. However, topic coherence, owing to its challenging computation, is not optimized for and is only evaluated after training. In this work, under a neural variational inference framework, we propose methods to incorporate a topic coherence objective into the training process. We demonstrate that such a coherence-aware topic model exhibits a similar level of perplexity as baseline models but achieves substantially higher topic coherence."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 719,
                        "end": 1335,
                        "sentence_offsets": [
                            {
                                "start": 719,
                                "end": 819
                            },
                            {
                                "start": 820,
                                "end": 1041
                            },
                            {
                                "start": 1042,
                                "end": 1204
                            },
                            {
                                "start": 1205,
                                "end": 1335
                            }
                        ],
                        "ref_mentions": [
                            "226096901",
                            "52182945"
                        ],
                        "quote": "In this work, we suggest a novel GTASum model (Graph-Based Topic-Aware abstract text summarization). First, the document is encoded with BERT (\u67f4\u7530, 2020) (Bidirectional Encoder Representation from Transformers) to obtain contextual sentence representations; meanwhile, NTM (Ding et al., 2018) is used to learn the potential topic of the document. Second, construct a heterogeneous document graph that contains sentence representation nodes and potential topic nodes, then revise them using a adapted GAT [30]. Finally, the sentence representations containing topic information are fed into a Transformer-based decoder to generate summaries."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[250374734 | Chen et al. | 2022 | Citations: 2]",
                "snippets": "Abstractive models based on graphs gain much attention (Yasunaga et al., 2017b;Li et al., 2020). Our model, which also combines a graph-based extractive module and neural abstractive sequence-to-sequence architecture, is a higher level summarization and focuses on the core event summarization from multiple documents, which is more challenging.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Multi-document Summarization",
                        "pdf_hash": "",
                        "start": 1365,
                        "end": 1710,
                        "sentence_offsets": [
                            {
                                "start": 1358,
                                "end": 1461
                            },
                            {
                                "start": 1462,
                                "end": 1710
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Abstractive models based on graphs gain much attention (Yasunaga et al., 2017b;Li et al., 2020). Our model, which also combines a graph-based extractive module and neural abstractive sequence-to-sequence architecture, is a higher level summarization and focuses on the core event summarization from multiple documents, which is more challenging."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[251594480 | Ma et al. | 2022 | Citations: 0]",
                "snippets": "In comparison to single-document summarization, MDS brings challenges on the representation and coverage of its lengthy and linked sources...On the other hand, Liu and Lapata (2019) propose a Hierarchical Transformer (HT) with local and global encoder layers to represent cross-token and cross-document information.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1139,
                        "end": 1277,
                        "sentence_offsets": [
                            {
                                "start": 1139,
                                "end": 1278
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In comparison to single-document summarization, MDS brings challenges on the representation and coverage of its lengthy and linked sources"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1711,
                        "end": 1886,
                        "sentence_offsets": [
                            {
                                "start": 1711,
                                "end": 1885
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "On the other hand, Liu and Lapata (2019) propose a Hierarchical Transformer (HT) with local and global encoder layers to represent cross-token and cross-document information."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[252185277 | Wang et al. | 2022 | Citations: 15]",
                "snippets": "Multi-Document Scientific Summarization (MDSS) aims to produce coherent and concise summaries for clusters of topic-relevant scientific papers. This task requires precise understanding of paper content and accurate modeling of cross-paper relationships. Knowledge graphs convey compact and interpretable structured information for documents, which makes them ideal for content modeling and relationship modeling. In this paper, we present KGSum, an MDSS model centred on knowledge graphs during both the encoding and decoding process. Specifically, in the encoding process, two graph-based modules are proposed to incorporate knowledge graph information into paper encoding, while in the decoding process, we propose a two-stage decoder by first generating knowledge graph information of summary in the form of descriptive sentences, followed by generating the final summary. Empirical results show that the proposed architecture brings substantial improvements over baselines on the Multi-Xscience dataset....GraphSum (Li et al., 2020)) is a neural multi-document summarization model that leverages well-known graphs to produce abstractive summaries. We use TF-IDF graph as the input graph. PEGASUS (Zhang et al., 2019) is a sequence-to-sequence model with gapsentences generation as a pre-training objective tailored for abstractive summarization. Pointer-Generator (See et al., 2017) is an RNN based model with an attention mechanism and allows the system to copy words from the source via pointing for abstractive summarization. BertABS (Liu and Lapata, 2019b) uses a pretrained BERT (Devlin et al., 2019) as the encoder for abstractive summarization. We also report the performance of BertABS with an encoder (SciBertABS) pretrained on scientific articles.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[209405420 | Zhang et al. | 2019 | Citations: 2054]": "Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.",
                    "[218718706 | Li et al. | 2020 | Citations: 136]": "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.",
                    "[52967399 | Devlin et al. | 2019 | Citations: 95215]": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                    "[8314118 | See et al. | 2017 | Citations: 4028]": "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points."
                },
                "metadata": [
                    {
                        "quote": "Multi-Document Scientific Summarization (MDSS) aims to produce coherent and concise summaries for clusters of topic-relevant scientific papers. This task requires precise understanding of paper content and accurate modeling of cross-paper relationships. Knowledge graphs convey compact and interpretable structured information for documents, which makes them ideal for content modeling and relationship modeling. In this paper, we present KGSum, an MDSS model centred on knowledge graphs during both the encoding and decoding process. Specifically, in the encoding process, two graph-based modules are proposed to incorporate knowledge graph information into paper encoding, while in the decoding process, we propose a two-stage decoder by first generating knowledge graph information of summary in the form of descriptive sentences, followed by generating the final summary. Empirical results show that the proposed architecture brings substantial improvements over baselines on the Multi-Xscience dataset",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "section_title": "Conclusion",
                        "pdf_hash": "",
                        "start": 1353,
                        "end": 2104,
                        "sentence_offsets": [
                            {
                                "start": 1353,
                                "end": 1493
                            },
                            {
                                "start": 1494,
                                "end": 1533
                            },
                            {
                                "start": 1534,
                                "end": 1691
                            },
                            {
                                "start": 1692,
                                "end": 1874
                            },
                            {
                                "start": 1875,
                                "end": 1997
                            },
                            {
                                "start": 1998,
                                "end": 2103
                            }
                        ],
                        "ref_mentions": [
                            "218718706",
                            "209405420",
                            "8314118",
                            "52967399"
                        ],
                        "quote": ".GraphSum (Li et al., 2020)) is a neural multi-document summarization model that leverages well-known graphs to produce abstractive summaries. We use TF-IDF graph as the input graph. PEGASUS (Zhang et al., 2019) is a sequence-to-sequence model with gapsentences generation as a pre-training objective tailored for abstractive summarization. Pointer-Generator (See et al., 2017) is an RNN based model with an attention mechanism and allows the system to copy words from the source via pointing for abstractive summarization. BertABS (Liu and Lapata, 2019b) uses a pretrained BERT (Devlin et al., 2019) as the encoder for abstractive summarization. We also report the performance of BertABS with an encoder (SciBertABS) pretrained on scientific articles."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[252280335 | Li et al. | 2022 | Citations: 2]",
                "snippets": "A recently proposed neural abstractive Multi-Document Summarization (MDS) model, Graph-Summ (Li et al., 2020), also considers the input graph structure during decoding. The biggest difference between Graphsum and our proposed SACA is that the former only introduces one graph attention layer in each decoder layer. SACA, on the other hand, injects graph structure into decoding by re-encoding the input graph. Specifically, it re-computes the input graph representation by conditioning it on the newly generated text at each decoding step.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218718706 | Li et al. | 2020 | Citations: 136]": "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines."
                },
                "metadata": [
                    {
                        "section_title": "Related Works",
                        "pdf_hash": "",
                        "start": 1362,
                        "end": 1901,
                        "sentence_offsets": [
                            {
                                "start": 1362,
                                "end": 1530
                            },
                            {
                                "start": 1531,
                                "end": 1676
                            },
                            {
                                "start": 1677,
                                "end": 1771
                            },
                            {
                                "start": 1772,
                                "end": 1901
                            }
                        ],
                        "ref_mentions": [
                            "218718706"
                        ],
                        "quote": "A recently proposed neural abstractive Multi-Document Summarization (MDS) model, Graph-Summ (Li et al., 2020), also considers the input graph structure during decoding. The biggest difference between Graphsum and our proposed SACA is that the former only introduces one graph attention layer in each decoder layer. SACA, on the other hand, injects graph structure into decoding by re-encoding the input graph. Specifically, it re-computes the input graph representation by conditioning it on the newly generated text at each decoding step."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[252780923 | Zhang et al. | 2022 | Citations: 45]",
                "snippets": "The paper proposes HEGEL, a hypergraph neural network for long document summarization by capturing high-order cross-sentence relations. HEGEL updates and learns effective sentence representations with hypergraph transformer layers and fuses different types of sentence dependencies, including latent topics, keywords coreference, and section structure...To address the above issues, we propose HEGEL (HypErGraph transformer for Extractive Long document summarization), a graph-based model designed for summarizing long documents with rich discourse information. To better model high-order cross-sentence relations, we represent a document as a hypergraph, a generalization of graph structure, in which an edge can join any number of vertices...Graph-based models have been exploited for extractive summarization to capture cross-sentence dependencies. Unsupervised graph summarization methods rely on graph connectivity to score and rank sentences Zheng and Lapata, 2019;Dong et al., 2020). Researchers also explore supervised graph neural networks for summarization. Yasunaga et al. (2017) applied Graph Convolutional Network (GCN) on the approximate discourse graph. Xu et al. (2019) proposed to apply GCN on structural discourse graphs based on RST trees and coreference mentions. Cui et al. (2020) leveraged topical information by building topicsentence graphs. Recently, Wang et al. (2020) proposed to construct word-document heterogeneous graphs and use word nodes as the intermediary between sentences. Jing et al. (2021) proposed to use multiplex graph to consider different sentence relations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The paper proposes HEGEL, a hypergraph neural network for long document summarization by capturing high-order cross-sentence relations. HEGEL updates and learns effective sentence representations with hypergraph transformer layers and fuses different types of sentence dependencies, including latent topics, keywords coreference, and section structure",
                        "pdf_hash": ""
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1308,
                        "end": 1696,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "To address the above issues, we propose HEGEL (HypErGraph transformer for Extractive Long document summarization), a graph-based model designed for summarizing long documents with rich discourse information. To better model high-order cross-sentence relations, we represent a document as a hypergraph, a generalization of graph structure, in which an edge can join any number of vertices"
                    },
                    {
                        "section_title": "Graph based summarization",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 859,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Graph-based models have been exploited for extractive summarization to capture cross-sentence dependencies. Unsupervised graph summarization methods rely on graph connectivity to score and rank sentences Zheng and Lapata, 2019;Dong et al., 2020). Researchers also explore supervised graph neural networks for summarization. Yasunaga et al. (2017) applied Graph Convolutional Network (GCN) on the approximate discourse graph. Xu et al. (2019) proposed to apply GCN on structural discourse graphs based on RST trees and coreference mentions. Cui et al. (2020) leveraged topical information by building topicsentence graphs. Recently, Wang et al. (2020) proposed to construct word-document heterogeneous graphs and use word nodes as the intermediary between sentences. Jing et al. (2021) proposed to use multiplex graph to consider different sentence relations."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[253155963 | Qiu et al. | 2022 | Citations: 13]",
                "snippets": "Neural Abstractive Summarization (Rush et al., 2015) first proposed to use a sequence-to-sequence model with an attention mechanism to perform sentence compression. (Mendes et al., 2019) demonstrated the advantages and limitations of neural methods based on sentence compression. The pointer-generator networks (PGN; (See et al., 2017)) enhances the attention model with a copying functionality. PGN has also been further extended to create summarization systems by incorporating the topic information (Liu et al., 2019), document structural information (Song et al., 2018), semantic information (Hardy et al., 2018), and was improved by replacing the plain LSTM module with the more advanced Transformer model to overcome the difficulty in modeling long sequence input (Pilault et al., 2020)(Wang et al., 2020)(Fonseca et al., 2022)...we propose a hierarchy-aware graph neural network (HierGNN) which captures such dependencies through three main steps: 1) learning a hierarchical document structure through a latent structure tree learned by a sparse matrix-tree computation; 2) propagating sentence information over this structure using a novel message-passing node propagation mechanism to identify salient information; 3) using graph-level attention to concentrate the decoder on salient information.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[102352252 | Mendes et al. | 2019 | Citations: 52]": "We present a new neural model for text summarization that first extracts sentences from a document and then compresses them. The pro-posed model offers a balance that sidesteps thedifficulties in abstractive methods while gener-ating more concise summaries than extractivemethods. In addition, our model dynamically determines the length of the output summary based on the gold summaries it observes during training and does not require length constraints typical to extractive summarization. The model achieves state-of-the-art results on the CNN/DailyMail and Newsroom datasets, improving over current extractive and abstractive methods. Human evaluations demonstratethat our model generates concise and informa-tive summaries. We also make available a new dataset of oracle compressive summaries derived automatically from the CNN/DailyMailreference summaries.",
                    "[1918428 | Rush et al. | 2015 | Citations: 2702]": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.",
                    "[202541012 | Pilault et al. | 2020 | Citations: 221]": "We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher ROUGE scores. We provide extensive comparisons with strong baseline methods, prior state of the art work as well as multiple variants of our approach including those using only transformers, only extractive techniques and combinations of the two. We examine these models using four different summarization tasks and datasets: arXiv papers, PubMed papers, the Newsroom and BigPatent datasets. We find that transformer based methods produce summaries with fewer n-gram copies, leading to n-gram copying statistics that are more similar to human generated abstracts. We include a human evaluation, finding that transformers are ranked highly for coherence and fluency, but purely extractive methods score higher for informativeness and relevance. We hope that these architectures and experiments may serve as strong points of comparison for future work. Note: The abstract above was collaboratively written by the authors and one of the models presented in this paper based on an earlier draft of this paper.",
                    "[203641742 | Liu et al. | 2019 | Citations: 101]": "Due to the lack of publicly available resources, conversation summarization has received far less attention than text summarization. As the purpose of conversations is to exchange information between at least two interlocutors, key information about a certain topic is often scattered and spanned across multiple utterances and turns from different speakers. This phenomenon is more pronounced during spoken conversations, where speech characteristics such as backchanneling and false-starts might interrupt the topical flow. Moreover, topic diffusion and (intra-utterance) topic drift are also more common in human-to-human conversations. Such linguistic characteristics of dialogue topics make sentence-level extractive summarization approaches used in spoken documents ill-suited for summarizing conversations. Pointer-generator networks have effectively demonstrated its strength at integrating extractive and abstractive capabilities through neural modeling in text summarization. To the best of our knowledge, to date no one has adopted it for summarizing conversations. In this work, we propose a topic-aware architecture to exploit the inherent hierarchical structure in conversations to further adapt the pointer-generator model. Our approach significantly outperforms competitive baselines, achieves more efficient learning outcomes, and attains more robust performance.",
                    "[229171633 | Wang et al. | 2020 | Citations: 17]": "Like humans, document summarization models can interpret a document\u2019s contents in a number of ways. Unfortunately, the neural models of today are largely black boxes that provide little explanation of how or why they generated a summary in the way they did. Therefore, to begin prying open the black box and to inject a level of control into the substance of the final summary, we developed a novel select-and-generate framework that focuses on explainability. By revealing the latent centrality and interactions between sentences, along with scores for novelty and relevance, users are given a window into the choices a model is making and an opportunity to guide those choices in a more desirable direction. A novel pair-wise matrix captures the sentence interactions, centrality and attribute scores, and a mask with tunable attribute thresholds allows the user to control which sentences are likely to be included in the extraction. A sentence-deployed attention mechanism in the abstractor ensures the final summary emphasizes the desired content. Additionally, the encoder is adaptable, supporting both Transformer- and BERT-based configurations. In a series of experiments assessed with ROUGE metrics and two human evaluations, ESCA outperformed eight state-of-the-art models on the CNN/DailyMail and NYT50 benchmark datasets.",
                    "[249062562 | Fonseca et al. | 2022 | Citations: 14]": "We argue that disentangling content selection from the budget used to cover salient content improves the performance and applicability of abstractive summarizers. Our method, FactorSum, does this disentanglement by factorizing summarization into two steps through an energy function: (1) generation of abstractive summary views covering salient information in subsets of the input document (document views); (2) combination of these views into a final summary, following a budget and content guidance. This guidance may come from different sources, including from an advisor model such as BART or BigBird, or in oracle mode \u2013 from the reference. This factorization achieves significantly higher ROUGE scores on multiple benchmarks for long document summarization, namely PubMed, arXiv, and GovReport. Most notably, our model is effective for domain adaptation. When trained only on PubMed samples, it achieves a 46.29 ROUGE-1 score on arXiv, outperforming PEGASUS trained in domain by a large margin. Our experimental results indicate that the performance gains are due to more flexible budget adaptation and processing of shorter contexts provided by partial document views.",
                    "[46936631 | Song et al. | 2018 | Citations: 76]": "Seq2seq learning has produced promising results on summarization. However, in many cases, system summaries still struggle to keep the meaning of the original intact. They may miss out important words or relations that play critical roles in the syntactic structure of source sentences. In this paper, we present structure-infused copy mechanisms to facilitate copying important words and relations from the source sentence to summary sentence. The approach naturally combines source dependency structure with the copy mechanism of an abstractive sentence summarizer. Experimental results demonstrate the effectiveness of incorporating source-side syntactic information in the system, and our proposed approach compares favorably to state-of-the-art methods.",
                    "[52111191 | Hardy et al. | 2018 | Citations: 69]": "Recent work on abstractive summarization has made progress with neural encoder-decoder architectures. However, such models are often challenged due to their lack of explicit semantic modeling of the source document and its summary. In this paper, we extend previous work on abstractive summarization using Abstract Meaning Representation (AMR) with a neural language generation stage which we guide using the source document. We demonstrate that this guidance improves summarization results by 7.4 and 10.5 points in ROUGE-2 using gold standard AMR parses and parses obtained from an off-the-shelf parser respectively. We also find that the summarization performance on later parses is 2 ROUGE-2 points higher than that of a well-established neural encoder-decoder approach trained on a larger dataset.",
                    "[8314118 | See et al. | 2017 | Citations: 4028]": "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 832,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 163
                            },
                            {
                                "start": 164,
                                "end": 277
                            },
                            {
                                "start": 278,
                                "end": 390
                            },
                            {
                                "start": 391,
                                "end": 833
                            }
                        ],
                        "ref_mentions": [
                            "1918428",
                            "102352252",
                            "8314118",
                            "203641742",
                            "46936631",
                            "52111191",
                            "202541012",
                            "229171633",
                            "249062562"
                        ],
                        "quote": "Neural Abstractive Summarization (Rush et al., 2015) first proposed to use a sequence-to-sequence model with an attention mechanism to perform sentence compression. (Mendes et al., 2019) demonstrated the advantages and limitations of neural methods based on sentence compression. The pointer-generator networks (PGN; (See et al., 2017)) enhances the attention model with a copying functionality. PGN has also been further extended to create summarization systems by incorporating the topic information (Liu et al., 2019), document structural information (Song et al., 2018), semantic information (Hardy et al., 2018), and was improved by replacing the plain LSTM module with the more advanced Transformer model to overcome the difficulty in modeling long sequence input (Pilault et al., 2020)(Wang et al., 2020)(Fonseca et al., 2022)"
                    },
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 282,
                        "end": 752,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "we propose a hierarchy-aware graph neural network (HierGNN) which captures such dependencies through three main steps: 1) learning a hierarchical document structure through a latent structure tree learned by a sparse matrix-tree computation; 2) propagating sentence information over this structure using a novel message-passing node propagation mechanism to identify salient information; 3) using graph-level attention to concentrate the decoder on salient information."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[253763913 | Umair et al. | 2022 | Citations: 3]",
                "snippets": "The authors in [48] presented an approach that modeled redundancy-aware heterogeneous graphs and refned sentence representation using neural networks for extractive summarization. The studies [9]56]...between sentences are learned. Te work done by (Huang et al., 2021) built a sentence-level graph-based model, using BERT for sentence encoding and joint neural network model (NTM) for discovering latent topic information. Te authors in (Bangotra et al., 2020) proposed a heterogeneous graph structure for modelling crosssentence relationship between sentences. To represent the relationships between the EDUs, they used three diferent types of nodes, including sentence nodes, EDU nodes, and entity nodes, and RST discourse parsing and leverage external discourse expertise to enhance the model's performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220586224 | Bangotra et al. | 2020 | Citations: 60]": "The lifetime of a node in wireless sensor networks (WSN) is directly responsible for the longevity of the wireless network. The routing of packets is the most energy-consuming activity for a sensor node. Thus, finding an energy-efficient routing strategy for transmission of packets becomes of utmost importance. The opportunistic routing (OR) protocol is one of the new routing protocol that promises reliability and energy efficiency during transmission of packets in wireless sensor networks (WSN). In this paper, we propose an intelligent opportunistic routing protocol (IOP) using a machine learning technique, to select a relay node from the list of potential forwarder nodes to achieve energy efficiency and reliability in the network. The proposed approach might have applications including e-healthcare services. As the proposed method might achieve reliability in the network because it can connect several healthcare network devices in a better way and good healthcare services might be offered. In addition to this, the proposed method saves energy, therefore, it helps the remote patient to connect with healthcare services for a longer duration with the integration of IoT services.",
                    "[233189611 | Huang et al. | 2021 | Citations: 28]": "Modeling the relations between text spans in a document is a crucial yet challenging problem for extractive summarization. Various kinds of relations exist among text spans of different granularity, such as discourse relations between elementary discourse units and coreference relations between phrase mentions. In this paper, we propose a heterogeneous graph based model for extractive summarization that incorporates both discourse and coreference relations. The heterogeneous graph contains three types of nodes, each corresponds to text spans of different granularity. Experimental results on a benchmark summarization dataset verify the effectiveness of our proposed method."
                },
                "metadata": [
                    {
                        "quote": "The authors in [48] presented an approach that modeled redundancy-aware heterogeneous graphs and refned sentence representation using neural networks for extractive summarization. The studies [9]56]",
                        "pdf_hash": ""
                    },
                    {
                        "section_title": "Extractive Text Summarization Approaches and",
                        "pdf_hash": "",
                        "start": 1598,
                        "end": 2173,
                        "sentence_offsets": [
                            {
                                "start": 1554,
                                "end": 1628
                            },
                            {
                                "start": 1629,
                                "end": 1803
                            },
                            {
                                "start": 1804,
                                "end": 1923
                            },
                            {
                                "start": 1924,
                                "end": 2172
                            }
                        ],
                        "ref_mentions": [
                            "233189611",
                            "220586224"
                        ],
                        "quote": "between sentences are learned. Te work done by (Huang et al., 2021) built a sentence-level graph-based model, using BERT for sentence encoding and joint neural network model (NTM) for discovering latent topic information. Te authors in (Bangotra et al., 2020) proposed a heterogeneous graph structure for modelling crosssentence relationship between sentences. To represent the relationships between the EDUs, they used three diferent types of nodes, including sentence nodes, EDU nodes, and entity nodes, and RST discourse parsing and leverage external discourse expertise to enhance the model's performance."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[257219819 | Zesheng et al. | 2023 | Citations: 4]",
                "snippets": "Wang et al. [28] propose a heterogeneous graph-based neural network for extracting summaries, which contains semantic nodes of different granularity levels except sentences. These extra nodes act as \"intermediaries\" between sentences and enrich cross-sentence relations. The introduction of document nodes allows the graph structure to be flexibly extended from a single document setup to multiple documents. Another work [8] proposes a multiplex graph summary (Multi-GraS) model based on multiplex graph convolutional networks that can be used to extract text summaries. This model not only considers Various types of intersentential relations (such as semantic similarity and natural connection), and intra-sentential relations (such as semantic and syntactic relations between words) are also modeled.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Graph Neural Network",
                        "pdf_hash": "",
                        "start": 472,
                        "end": 1276,
                        "sentence_offsets": [
                            {
                                "start": 449,
                                "end": 645
                            },
                            {
                                "start": 646,
                                "end": 742
                            },
                            {
                                "start": 743,
                                "end": 880
                            },
                            {
                                "start": 881,
                                "end": 1043
                            },
                            {
                                "start": 1044,
                                "end": 1276
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Wang et al. [28] propose a heterogeneous graph-based neural network for extracting summaries, which contains semantic nodes of different granularity levels except sentences. These extra nodes act as \"intermediaries\" between sentences and enrich cross-sentence relations. The introduction of document nodes allows the graph structure to be flexibly extended from a single document setup to multiple documents. Another work [8] proposes a multiplex graph summary (Multi-GraS) model based on multiplex graph convolutional networks that can be used to extract text summaries. This model not only considers Various types of intersentential relations (such as semantic similarity and natural connection), and intra-sentential relations (such as semantic and syntactic relations between words) are also modeled."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[257496469 | Li et al. | 2023 | Citations: 11]",
                "snippets": "Graph-Based Models Although graphs are commonly used to boost text summarization (Wu et al., 2021)(You et al., 2022)(Song et al., 2022), there are only a handful of models which have been proposed to use graphs to encode the documents in abstractive MDS (Li et al., 2020)(Jin et al., 2020)(Li et al., 2021)(Cui et al., 2021). Most of these models only leverage homogeneous graphs as they do not consider different edge types of graphs. For example, MGSum (Jin et al., 2020) constructs a threelevel (i.e., document, sentence, and word levels) hierarchical graph and learns semantics with a multi-level interaction network. GraphSum (Li et al., 2020)) constructs a similarity graph over the paragraphs. It learns a graph representation for the paragraphs and uses a hierarchical graph attention mechanism to guide the summary generation process.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[250301954 | Song et al. | 2022 | Citations: 26]": "The task of summarization often requires a non-trivial understanding of the given text at the semantic level. In this work, we essentially incorporate the constituent structure into the single document summarization via the Graph Neural Networks to learn the semantic meaning of tokens. More specifically, we propose a novel hierarchical heterogeneous graph attention network over constituency-based parse trees for syntax-aware summarization. This approach reflects psychological findings that humans will pinpoint specific selection patterns to construct summaries hierarchically. Extensive experiments demonstrate that our model is effective for both the abstractive and extractive summarization tasks on five benchmark datasets from various domains. Moreover, further performance improvement can be obtained by virtue of state-of-the-art pre-trained models.",
                    "[196170254 | Li et al. | 2021 | Citations: 30]": "The key to realize advanced document summarization is semantic representation of documents. This paper investigates the role of Semantic Link Network in representing and understanding documents for multi-document summarization. It proposes a novel abstractive multi-document summarization framework by first transforming documents into a Semantic Link Network of concepts and events and then transforming the Semantic Link Network into the summary of the documents based on the selection of important concepts and events while keeping semantics coherence. Experiments on benchmark datasets show that the proposed summarization approach significantly outperforms relevant state-of-the-art baselines and the Semantic Link Network plays an important role in representing and understanding documents.",
                    "[218718706 | Li et al. | 2020 | Citations: 136]": "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.",
                    "[220045815 | Jin et al. | 2020 | Citations: 99]": "In this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization, which jointly learn semantic representations for words, sentences, and documents. The word representations are used to generate an abstractive summary while the sentence representations are used to produce an extractive summary. We employ attention mechanisms to interact between different granularity of semantic representations, which helps to capture multi-granularity key information and improves the performance of both abstractive and extractive summarization. Experiment results show that our proposed model substantially outperforms all strong baseline methods and achieves the best results on the Multi-News dataset.",
                    "[235187330 | Wu et al. | 2021 | Citations: 45]": "Abstractive summarization for long-document or multi-document remains challenging for the Seq2Seq architecture, as Seq2Seq is not good at analyzing long-distance relations in text. In this paper, we present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. Further, a graph-based encoder-decoder model is proposed to improve both the document representation and summary generation process by leveraging the graph structure. Specifically, several graph augmentation methods are designed to encode both the explicit and implicit relations in the text while the graph-propagation attention mechanism is developed in the decoder to select salient content into the summary. Empirical results show that the proposed architecture brings substantial improvements for both long-document and multi-document summarization tasks.",
                    "[239050558 | Cui et al. | 2021 | Citations: 41]": "A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and then apply a graph-to-sequence framework to generate summaries. Moreover, we employ a neural topic model to jointly discover latent topics that can act as cross-document semantic units to bridge different documents and provide global information to guide the summary generation. Since topic extraction can be viewed as a special type of summarization that\"summarizes\"texts into a more abstract format, i.e., a topic distribution, we adopt a multi-task learning strategy to jointly train the topic and summarization module, allowing the promotion of each other. Experimental results on the Multi-News dataset demonstrate that our model outperforms previous state-of-the-art MDS models on both Rouge metrics and human evaluation, meanwhile learns high-quality topics.",
                    "[250390945 | You et al. | 2022 | Citations: 12]": "Previous studies on the timeline summarization (TLS) task ignored the information interaction between sentences and dates, and adopted pre-defined unlearnable representations for them. They also considered date selection and event detection as two independent tasks, which makes it impossible to integrate their advantages and obtain a globally optimal summary. In this paper, we present a joint learning-based heterogeneous graph attention network for TLS (HeterTls), in which date selection and event detection are combined into a unified framework to improve the extraction accuracy and remove redundant sentences simultaneously. Our heterogeneous graph involves multiple types of nodes, the representations of which are iteratively learned across the heterogeneous graph attention layer. We evaluated our model on four datasets, and found that it significantly outperformed the current state-of-the-art baselines with regard to ROUGE scores and date selection metrics."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 1144,
                        "end": 1994,
                        "sentence_offsets": [
                            {
                                "start": 1144,
                                "end": 1471
                            },
                            {
                                "start": 1472,
                                "end": 1581
                            },
                            {
                                "start": 1582,
                                "end": 1774
                            },
                            {
                                "start": 1775,
                                "end": 1851
                            },
                            {
                                "start": 1852,
                                "end": 1994
                            }
                        ],
                        "ref_mentions": [
                            "235187330",
                            "250390945",
                            "250301954",
                            "218718706",
                            "220045815",
                            "196170254",
                            "239050558",
                            "220045815",
                            "218718706"
                        ],
                        "quote": "Graph-Based Models Although graphs are commonly used to boost text summarization (Wu et al., 2021)(You et al., 2022)(Song et al., 2022), there are only a handful of models which have been proposed to use graphs to encode the documents in abstractive MDS (Li et al., 2020)(Jin et al., 2020)(Li et al., 2021)(Cui et al., 2021). Most of these models only leverage homogeneous graphs as they do not consider different edge types of graphs. For example, MGSum (Jin et al., 2020) constructs a threelevel (i.e., document, sentence, and word levels) hierarchical graph and learns semantics with a multi-level interaction network. GraphSum (Li et al., 2020)) constructs a similarity graph over the paragraphs. It learns a graph representation for the paragraphs and uses a hierarchical graph attention mechanism to guide the summary generation process."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[258999996 | Zhang et al. | 2023 | Citations: 12]",
                "snippets": "Recently, researchers have also turned to graph neural networks (GNN) as an alternative approach. Graph neural networks have been demonstrated to be effective at tasks with rich relational structure and can preserve global structure information (Yao et al., 2018)Xu et al., 2019;(Zhang et al., 2020). By representing a document as a graph, GNNs update and learn sentence representations by message passing, and turn extractive summarization into a node classification problem. Among all attempts, one popular way is to construct crosssentence similarity graphs (Erkan et al., 2004)Zheng and Lapata, 2019), which uses sentence representation cosine similarity as edge weights to model cross-sentence semantic relations. Xu et al. (2019) proposed using Rhetorical Structure Theory (RST) trees and coreference mentions to capture cross-sentence discourse relations. Wang et al. (2020) proposed constructing a word-document het-erogeneous graph by using words as the intermediary between sentences.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[226262336 | Zhang et al. | 2020 | Citations: 68]": "Text classification is a fundamental problem in natural language processing. Recent studies applied graph neural network (GNN) techniques to capture global word co-occurrence in a corpus. However, previous works are not scalable to large-sized corpus and ignore the heterogeneity of the text graph. To address these problems, we introduce a novel Transformer based heterogeneous graph neural network, namely Text Graph Transformer (TG-Transformer). Our model learns effective node representations by capturing structure and heterogeneity from the text graph. We propose a mini-batch text graph sampling method that significantly reduces computing and memory costs to handle large-sized corpus. Extensive experiments have been conducted on several benchmark datasets, and the results demonstrate that TG-Transformer outperforms state-of-the-art approaches on text classification task.",
                    "[506350 | Erkan et al. | 2004 | Citations: 3097]": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                    "[52284222 | Yao et al. | 2018 | Citations: 1828]": "Text classification is an important and classical problem in natural language processing. There have been a number of studies that applied convolutional neural networks (convolution on regular grid, e.g., sequence) to classification. However, only a limited number of studies have explored the more flexible graph convolutional neural networks (convolution on non-grid, e.g., arbitrary graph) for the task. In this work, we propose to use graph convolutional networks for text classification. We build a single text graph for a corpus based on word co-occurrence and document word relations, then learn a Text Graph Convolutional Network (Text GCN) for the corpus. Our Text GCN is initialized with one-hot representation for word and document, it then jointly learns the embeddings for both words and documents, as supervised by the known class labels for documents. Our experimental results on multiple benchmark datasets demonstrate that a vanilla Text GCN without any external word embeddings or knowledge outperforms state-of-the-art methods for text classification. On the other hand, Text GCN also learns predictive word and document embeddings. In addition, experimental results show that the improvement of Text GCN over state-of-the-art comparison methods become more prominent as we lower the percentage of training data, suggesting the robustness of Text GCN to less training data in text classification."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 121,
                        "end": 1120,
                        "sentence_offsets": [
                            {
                                "start": 121,
                                "end": 218
                            },
                            {
                                "start": 219,
                                "end": 423
                            },
                            {
                                "start": 424,
                                "end": 599
                            },
                            {
                                "start": 600,
                                "end": 844
                            },
                            {
                                "start": 845,
                                "end": 988
                            },
                            {
                                "start": 989,
                                "end": 1120
                            }
                        ],
                        "ref_mentions": [
                            "52284222",
                            "226262336",
                            "506350"
                        ],
                        "quote": "Recently, researchers have also turned to graph neural networks (GNN) as an alternative approach. Graph neural networks have been demonstrated to be effective at tasks with rich relational structure and can preserve global structure information (Yao et al., 2018)Xu et al., 2019;(Zhang et al., 2020). By representing a document as a graph, GNNs update and learn sentence representations by message passing, and turn extractive summarization into a node classification problem. Among all attempts, one popular way is to construct crosssentence similarity graphs (Erkan et al., 2004)Zheng and Lapata, 2019), which uses sentence representation cosine similarity as edge weights to model cross-sentence semantic relations. Xu et al. (2019) proposed using Rhetorical Structure Theory (RST) trees and coreference mentions to capture cross-sentence discourse relations. Wang et al. (2020) proposed constructing a word-document het-erogeneous graph by using words as the intermediary between sentences."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[264146402 | Wang et al. | 2023 | Citations: 4]",
                "snippets": "A heterogeneous GNN, HETERSUMGRAPH (Wang et al., 2020) is introduced for extractive document summarization. This network includes nodes of different granularity levels apart from sentences, which act as intermediaries and enrich cross-sentence relations. This approach allows different sentences to interact considering overlapping word information. Moreover, the graph network can accommodate additional node types, such as document nodes for multi-document summarization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[216552978 | Wang et al. | 2020 | Citations: 282]": "As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. Besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github."
                },
                "metadata": [
                    {
                        "section_title": "Graph neural networks",
                        "pdf_hash": "",
                        "start": 1130,
                        "end": 1589,
                        "sentence_offsets": [
                            {
                                "start": 1130,
                                "end": 1223
                            },
                            {
                                "start": 1224,
                                "end": 1370
                            },
                            {
                                "start": 1371,
                                "end": 1465
                            },
                            {
                                "start": 1466,
                                "end": 1589
                            }
                        ],
                        "ref_mentions": [
                            "216552978"
                        ],
                        "quote": "A heterogeneous GNN, HETERSUMGRAPH (Wang et al., 2020) is introduced for extractive document summarization. This network includes nodes of different granularity levels apart from sentences, which act as intermediaries and enrich cross-sentence relations. This approach allows different sentences to interact considering overlapping word information. Moreover, the graph network can accommodate additional node types, such as document nodes for multi-document summarization."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[265607988 | Roy et al. | 2023 | Citations: 0]",
                "snippets": "(Wang et al., 2020) introduced an heterogeneous graph neural network for extractive summarization which used additional semantic units (words) as intermediate nodes to construct relationships between sentences...Recently, state-ofthe-art solutions on abstractive summarization are built upon the transformer (Vaswani et al., 2017) and BERT (Devlin et al., 2019) models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[13756489 | Vaswani et al. | 2017 | Citations: 132444]": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                    "[216552978 | Wang et al. | 2020 | Citations: 282]": "As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. Besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github.",
                    "[52967399 | Devlin et al. | 2019 | Citations: 95215]": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 247,
                        "end": 468,
                        "sentence_offsets": [
                            {
                                "start": 247,
                                "end": 469
                            }
                        ],
                        "ref_mentions": [
                            "216552978"
                        ],
                        "quote": "(Wang et al., 2020) introduced an heterogeneous graph neural network for extractive summarization which used additional semantic units (words) as intermediate nodes to construct relationships between sentences"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 979,
                        "end": 1137,
                        "sentence_offsets": [
                            {
                                "start": 979,
                                "end": 1136
                            }
                        ],
                        "ref_mentions": [
                            "13756489",
                            "52967399"
                        ],
                        "quote": "Recently, state-ofthe-art solutions on abstractive summarization are built upon the transformer (Vaswani et al., 2017) and BERT (Devlin et al., 2019) models."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[266244733 | Su et al. | 2023 | Citations: 1]",
                "snippets": "The crucial aspect of extractive document summarization lies in understanding the interrelations between sentences. Documents inherently comprise a multitude of sentences, and sentence-level models frequently fail to consider the relationships between distantly-placed sentences, resulting in the omission of significant information in the summary. Moreover, information within documents tends to be distributed sparsely, challenging the efficacy of sentence-level models. In the realm of heterogeneous graph neural networks, it has been observed that semantic nodes with varying levels of granularity encapsulate distinct semantic connections. Initially, the incorporation of edge features into the computation of dynamic graph attention networks is performed to account for node relationships. Subsequently, given the multiplicity of topics in a document or a set of documents, a topic model is employed to extract topic-specific features and the probability distribution linking these topics with sentence nodes.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The crucial aspect of extractive document summarization lies in understanding the interrelations between sentences. Documents inherently comprise a multitude of sentences, and sentence-level models frequently fail to consider the relationships between distantly-placed sentences, resulting in the omission of significant information in the summary. Moreover, information within documents tends to be distributed sparsely, challenging the efficacy of sentence-level models. In the realm of heterogeneous graph neural networks, it has been observed that semantic nodes with varying levels of granularity encapsulate distinct semantic connections. Initially, the incorporation of edge features into the computation of dynamic graph attention networks is performed to account for node relationships. Subsequently, given the multiplicity of topics in a document or a set of documents, a topic model is employed to extract topic-specific features and the probability distribution linking these topics with sentence nodes.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[269762702 | Qu | 2024 | Citations: 0]",
                "snippets": "Zhou et al. (Zhou et al., 2021) presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP.Pasunuru et al. (Pasunuru et al., 2021) presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News (Fabbri et al., 2019) and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model (Lewis et al., 2019) with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. (Pasunuru et al., 2021) utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[174799390 | Fabbri et al. | 2019 | Citations: 590]": "Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting.",
                    "[204960716 | Lewis et al. | 2019 | Citations: 10856]": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
                    "[235097309 | Pasunuru et al. | 2021 | Citations: 73]": "This paper presents an efficient graph-enhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents.",
                    "[236478143 | Zhou et al. | 2021 | Citations: 28]": ","
                },
                "metadata": [
                    {
                        "section_title": "STATE OF THE ART",
                        "pdf_hash": "",
                        "start": 270,
                        "end": 1729,
                        "sentence_offsets": [
                            {
                                "start": 228,
                                "end": 547
                            },
                            {
                                "start": 547,
                                "end": 740
                            },
                            {
                                "start": 740,
                                "end": 879
                            },
                            {
                                "start": 879,
                                "end": 1129
                            },
                            {
                                "start": 1129,
                                "end": 1335
                            },
                            {
                                "start": 1335,
                                "end": 1466
                            },
                            {
                                "start": 1466,
                                "end": 1729
                            }
                        ],
                        "ref_mentions": [
                            "236478143",
                            "235097309",
                            "174799390",
                            "204960716",
                            "235097309"
                        ],
                        "quote": "Zhou et al. (Zhou et al., 2021) presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP.Pasunuru et al. (Pasunuru et al., 2021) presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News (Fabbri et al., 2019) and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model (Lewis et al., 2019) with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. (Pasunuru et al., 2021) utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[269791107 | Zhao et al. | 2024 | Citations: 5]",
                "snippets": "Graph neural networks have been widely used for extractive summarization due to their flexibility and scalability.Dong et al. (2020) proposed an unsupervised graph-based model that combines both sentence similarity and hierarchical discourse structure to rank sentences.Cui et al. (2020) injected latent topic information into graph neural networks to further improve performance.(Wang et al., 2020) constructed a word-document heterogeneous graph using word nodes as intermediate to connect sentences.Zhang et al. (2022) proposed a hypergraph transformer to model long-distance dependency while emphasizing the importance of high-order inter-sentence relations in extraction summarization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[216552978 | Wang et al. | 2020 | Citations: 282]": "As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. Besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github."
                },
                "metadata": [
                    {
                        "section_title": "Graph based Summarization",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 689,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 114
                            },
                            {
                                "start": 114,
                                "end": 270
                            },
                            {
                                "start": 270,
                                "end": 380
                            },
                            {
                                "start": 380,
                                "end": 501
                            },
                            {
                                "start": 501,
                                "end": 689
                            }
                        ],
                        "ref_mentions": [
                            "216552978"
                        ],
                        "quote": "Graph neural networks have been widely used for extractive summarization due to their flexibility and scalability.Dong et al. (2020) proposed an unsupervised graph-based model that combines both sentence similarity and hierarchical discourse structure to rank sentences.Cui et al. (2020) injected latent topic information into graph neural networks to further improve performance.(Wang et al., 2020) constructed a word-document heterogeneous graph using word nodes as intermediate to connect sentences.Zhang et al. (2022) proposed a hypergraph transformer to model long-distance dependency while emphasizing the importance of high-order inter-sentence relations in extraction summarization."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[271525553 | Shakil et al. | 2024 | Citations: 24]",
                "snippets": "A graph-based attention mechanism for abstractive document summarization was introduced by Tan et al. (Tan et al., 2017) that takes into account the relationships between sentences in a document. Traditional Seq2Seq models with attention mechanisms frequently concentrate on the words within a sentence but are unable to recognize the inter-sentence dependencies. The authors aimed to overcome this restriction by embedding the structural information of the document into the attention mechanism. The suggested approach first creates a sentence graph that represents the document, where the nodes are the sentences and the edges are the relationships between them. The attention mechanism then works on this graph, permitting the model to focus on both local and global sentence-level information.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[26698484 | Tan et al. | 2017 | Citations: 298]": "Abstractive summarization is the ultimate goal of document summarization research, but previously it is less investigated due to the immaturity of text generation techniques. Recently impressive progress has been made to abstractive sentence summarization using neural models. Unfortunately, attempts on abstractive document summarization are still in a primitive stage, and the evaluation results are worse than extractive methods on benchmark datasets. In this paper, we review the difficulties of neural abstractive document summarization, and propose a novel graph-based attention mechanism in the sequence-to-sequence framework. The intuition is to address the saliency factor of summarization, which has been overlooked by prior works. Experimental results demonstrate our model is able to achieve considerable improvement over previous neural abstractive models. The data-driven neural abstractive method is also competitive with state-of-the-art extractive methods."
                },
                "metadata": [
                    {
                        "section_title": "Attention Mechanisms",
                        "pdf_hash": "",
                        "start": 434,
                        "end": 1217,
                        "sentence_offsets": [
                            {
                                "start": 434,
                                "end": 615
                            },
                            {
                                "start": 616,
                                "end": 783
                            },
                            {
                                "start": 784,
                                "end": 916
                            },
                            {
                                "start": 917,
                                "end": 1084
                            },
                            {
                                "start": 1085,
                                "end": 1217
                            }
                        ],
                        "ref_mentions": [
                            "26698484"
                        ],
                        "quote": "A graph-based attention mechanism for abstractive document summarization was introduced by Tan et al. (Tan et al., 2017) that takes into account the relationships between sentences in a document. Traditional Seq2Seq models with attention mechanisms frequently concentrate on the words within a sentence but are unable to recognize the inter-sentence dependencies. The authors aimed to overcome this restriction by embedding the structural information of the document into the attention mechanism. The suggested approach first creates a sentence graph that represents the document, where the nodes are the sentences and the edges are the relationships between them. The attention mechanism then works on this graph, permitting the model to focus on both local and global sentence-level information."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[271791043 | Khaliq et al. | 2024 | Citations: 2]",
                "snippets": "Through systematic comparisons, proposed model is tested against well-known benchmarks and recent advances in neural graph-based models. These assessments are not just limited to performance metrics like ROUGE scores but also extend to an examination of how well each model assimilates and processes linguistic and thematic information across different document types. This research examines the performance in relation to various node configurations and graph structures used in competing models, providing a comprehensive view of the proposed model's abilities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Performance Evaluation",
                        "pdf_hash": "",
                        "start": 491,
                        "end": 1054,
                        "sentence_offsets": [
                            {
                                "start": 491,
                                "end": 627
                            },
                            {
                                "start": 628,
                                "end": 859
                            },
                            {
                                "start": 860,
                                "end": 1054
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Through systematic comparisons, proposed model is tested against well-known benchmarks and recent advances in neural graph-based models. These assessments are not just limited to performance metrics like ROUGE scores but also extend to an examination of how well each model assimilates and processes linguistic and thematic information across different document types. This research examines the performance in relation to various node configurations and graph structures used in competing models, providing a comprehensive view of the proposed model's abilities."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[271938110 | Khaliq et al. | 2024 | Citations: 1]",
                "snippets": "In recent years, there have been notable achievements in applying Graph Neural Networks (GNNs) or Graph Attention Networks (GAT) [11] to summarize documents [4], [12], [13], [14], [15], [16], [17]. This success arises from their ability to capture complex inter-sentence relationships within documents. Specifically, GNN models demonstrate proficiency in representing complex structural data by encapsulating semantic units (nodes) and their interconnections (edges). Due to these capabilities, there is growing interest in leveraging the GAT framework alongside topic modeling for abstractive summarization tasks. The researchers in [12] proposed a heterogeneous neural graph structure using BERT to obtain contextual sentence representations. They simultaneously trained with latent topics using the Neural Topic Model (NTM) to model global information. The study of [18] proposed an expanded model that employed NTM for abstractive text summarization. \n\nThe authors of [15] introduced a graph structure enriched with latent topical information. They employed K-Means and Gaussian Mixture Models (GMM) for topic extraction. The study of [3] proposed an innovative Graph-Based Topic-Aware abstractive text summarization model. Initially, documents were encoded with BERT to generate sentence representations. Concurrently, NTM identified potential topics, while GAT refined these sentences and topic nodes. Finally, the sentence embeddings, enriched with topic information, were fed into a Transformer-driven decoder to generate summaries.\n\nModern text summarization research has shifted from homogeneous graphs with static nodes to heterogeneous networks. These networks allow for the inclusion of diverse node types, which represent wide range of textual elements. In addition, they allow for dynamic updates during the summarization process. The approach of [4] constructed a graph neural network based on word co-occurrences within the document to capture word-level relationships. The study in [42] utilized syntactic graph convolutional networks (GCNs) to model the non-Euclidean structure of documents.\n\nThis category includes models like Topic-GraphSum [12], SSN-DM [57] HeterGraphLongSum [14] and GTASum [3] that utilize graph structures to represent document information and relationships between sentences or topics, facilitating summarization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In recent years, there have been notable achievements in applying Graph Neural Networks (GNNs) or Graph Attention Networks (GAT) [11] to summarize documents [4], [12], [13], [14], [15], [16], [17]. This success arises from their ability to capture complex inter-sentence relationships within documents. Specifically, GNN models demonstrate proficiency in representing complex structural data by encapsulating semantic units (nodes) and their interconnections (edges). Due to these capabilities, there is growing interest in leveraging the GAT framework alongside topic modeling for abstractive summarization tasks. The researchers in [12] proposed a heterogeneous neural graph structure using BERT to obtain contextual sentence representations. They simultaneously trained with latent topics using the Neural Topic Model (NTM) to model global information. The study of [18] proposed an expanded model that employed NTM for abstractive text summarization. \n\nThe authors of [15] introduced a graph structure enriched with latent topical information. They employed K-Means and Gaussian Mixture Models (GMM) for topic extraction. The study of [3] proposed an innovative Graph-Based Topic-Aware abstractive text summarization model. Initially, documents were encoded with BERT to generate sentence representations. Concurrently, NTM identified potential topics, while GAT refined these sentences and topic nodes. Finally, the sentence embeddings, enriched with topic information, were fed into a Transformer-driven decoder to generate summaries.\n\nModern text summarization research has shifted from homogeneous graphs with static nodes to heterogeneous networks. These networks allow for the inclusion of diverse node types, which represent wide range of textual elements. In addition, they allow for dynamic updates during the summarization process. The approach of [4] constructed a graph neural network based on word co-occurrences within the document to capture word-level relationships. The study in [42] utilized syntactic graph convolutional networks (GCNs) to model the non-Euclidean structure of documents.\n\nThis category includes models like Topic-GraphSum [12], SSN-DM [57] HeterGraphLongSum [14] and GTASum [3] that utilize graph structures to represent document information and relationships between sentences or topics, facilitating summarization.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[273662136 | Bugueno et al. | 2024 | Citations: 0]",
                "snippets": "Recent studies underscore the importance of structural information in long document summarization. HEGEL (Zhang et al., 2022) modeled documents as hypergraphs, capturing semantic edges like keyword coreference, section structure, and latent topics.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Previous Work",
                        "pdf_hash": "",
                        "start": 1900,
                        "end": 2148,
                        "sentence_offsets": [
                            {
                                "start": 1900,
                                "end": 1998
                            },
                            {
                                "start": 1999,
                                "end": 2148
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Recent studies underscore the importance of structural information in long document summarization. HEGEL (Zhang et al., 2022) modeled documents as hypergraphs, capturing semantic edges like keyword coreference, section structure, and latent topics."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[273784877 | Ragazzi et al. | 2025 | Citations: 0]",
                "snippets": "Despite the proficiency exhibited by language models in text summarization, challenges persist in capturing and aggregating salient information dispersed across a cluster of lengthy sources. To accommodate more input, existing solutions prioritize sparse attention mechanisms, relying on sequence truncation without incorporating graph-based modeling of multiple semantic units to locate essential facets.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 155,
                        "end": 560,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Despite the proficiency exhibited by language models in text summarization, challenges persist in capturing and aggregating salient information dispersed across a cluster of lengthy sources. To accommodate more input, existing solutions prioritize sparse attention mechanisms, relying on sequence truncation without incorporating graph-based modeling of multiple semantic units to locate essential facets."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[277284940 | Umair et al. | 2025 | Citations: 1]",
                "snippets": "In the domain of graph-based approaches, [3] implement a modified version of the GAT designed to identify inter-sentence relationships. To enhance the summarization of lengthy documents, they implemented Neural Topic Modeling (NTM) to identify latent topics. This approach introduced NTM as a new node type, which was then integrated with existing sentence nodes to improve the overall structure and accuracy of the summarization process.\n\nThe researchers of [7] integrated passage nodes into the heterogeneous graph structure alongside the existing word and sentence nodes. In the study [34], the authors attempted to redefine the task of review formalization, focusing on scientific paper summarization using citation graphs. The CGSUM model included information from both the source paper and its references, and enhanced the contextual relevance of summarization. The authors also built the Semantic Scholar Network (SSN) from 141K research papers and 661K citation relations with a broad view of scientific domains.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[233169121 | An et al. | 2021 | Citations: 44]": "Previous work for text summarization in scientific domain mainly focused on the content of the input document, but seldom considering its citation network.\nHowever, scientific papers are full of uncommon domain-specific terms, making it almost impossible for the model to understand its true meaning without the help of the relevant research community.\nIn this paper, we redefine the task of scientific papers summarization by utilizing their citation graph and propose a citation graph-based summarization model CGSum which can incorporate the information of both the source paper and its references.\nIn addition, we construct a novel scientific papers summarization dataset Semantic Scholar Network (SSN) which contains 141K research papers in different domains and 661K citation relationships. The entire dataset constitutes a large connected citation graph.\nExtensive experiments show that our model can achieve competitive performance when compared with the pretrained models even with a simple architecture.\nThe results also indicates the citation graph is crucial to better understand the content of papers and generate high-quality summaries."
                },
                "metadata": [
                    {
                        "section_title": "C. LONG DOCUMENT EXTRACTIVE SUMMARIZATION",
                        "pdf_hash": "",
                        "start": 930,
                        "end": 1950,
                        "sentence_offsets": [
                            {
                                "start": 907,
                                "end": 1121
                            },
                            {
                                "start": 1122,
                                "end": 1257
                            },
                            {
                                "start": 1258,
                                "end": 1380
                            },
                            {
                                "start": 1381,
                                "end": 1560
                            },
                            {
                                "start": 1563,
                                "end": 1697
                            },
                            {
                                "start": 1698,
                                "end": 1850
                            },
                            {
                                "start": 1851,
                                "end": 1990
                            }
                        ],
                        "ref_mentions": [
                            "252818946",
                            "233169121"
                        ],
                        "quote": "In the domain of graph-based approaches, [3] implement a modified version of the GAT designed to identify inter-sentence relationships. To enhance the summarization of lengthy documents, they implemented Neural Topic Modeling (NTM) to identify latent topics. This approach introduced NTM as a new node type, which was then integrated with existing sentence nodes to improve the overall structure and accuracy of the summarization process.\n\nThe researchers of [7] integrated passage nodes into the heterogeneous graph structure alongside the existing word and sentence nodes. In the study [34], the authors attempted to redefine the task of review formalization, focusing on scientific paper summarization using citation graphs. The CGSUM model included information from both the source paper and its references, and enhanced the contextual relevance of summarization. The authors also built the Semantic Scholar Network (SSN) from 141K research papers and 661K citation relations with a broad view of scientific domains."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.030927,
        "cot": "To effectively answer the query about recent advances in graph-based neural approaches for abstractive multi-document summarization and state-of-the-art results, I'll organize the information into meaningful dimensions that capture the key aspects of this field.\n\nFirst, I need to create an Introduction/Background section to provide the foundational understanding of graph-based neural approaches and multi-document summarization.\n\nThen, I'll organize the content into the following dimensions:\n\n1. \"Introduction and Background\": This will provide context about multi-document summarization and graph-based approaches. This should be in synthesis format to give a coherent overview of the field and its evolution.\n\n2. \"Evolution of Graph-based Neural Approaches\": This dimension will trace the development of graph-based methods for abstractive multi-document summarization, showing how the field has evolved. A synthesis format is appropriate to show the progression and connections between approaches.\n\n3. \"Recent Graph Neural Network Architectures\": This will detail specific recent architectures and innovations in graph-based models for abstractive summarization. A list format makes sense here to clearly distinguish between different architectural approaches.\n\n4. \"Heterogeneous Graph Networks\": Many quotes discuss heterogeneous graph structures specifically, which seems to be a significant recent trend worth its own section. A synthesis format will help explain how these approaches work and their advantages.\n\n5. \"Topic-aware Graph Models\": Several quotes mention incorporating topic modeling with graph neural networks, which appears to be an important recent direction. A synthesis format will best illustrate how topic modeling enhances graph-based summarization.\n\n6. \"State-of-the-art Models and Results\": This will focus specifically on models that have achieved SOTA results according to the quotes. A list format will clearly identify the top-performing models.\n\n7. \"Datasets and Evaluation\": This will cover the datasets and metrics used to evaluate these approaches. A synthesis format will help explain how these models are evaluated and compared.\n\nThese dimensions cover all the major aspects of the query, ensuring a comprehensive answer about recent advances in graph-based neural approaches for abstractive multi-document summarization and SOTA results.",
        "plan": {
            "Introduction and Background (synthesis)": [
                0,
                2,
                6,
                24
            ],
            "Evolution of Graph-based Neural Approaches (synthesis)": [
                0,
                2,
                9,
                10,
                13,
                14,
                19
            ],
            "Recent Graph Neural Network Architectures (list)": [
                4,
                7,
                8,
                9,
                10,
                13,
                14,
                20,
                23
            ],
            "Heterogeneous Graph Networks (synthesis)": [
                3,
                12,
                15,
                16,
                17,
                18,
                22,
                25
            ],
            "Topic-aware Graph Models (synthesis)": [
                4,
                11,
                17,
                22
            ],
            "State-of-the-art Models and Results (list)": [
                1,
                5,
                7,
                8,
                18,
                21,
                22
            ],
            "Datasets and Evaluation (synthesis)": [
                7,
                21
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction and Background",
                "tldr": "Graph-based neural approaches have evolved significantly in multi-document summarization, progressing from basic graph ranking methods to sophisticated neural architectures that can process multiple documents while capturing their complex relationships. Recent advances have focused on leveraging graph neural networks to model cross-document relationships and improve information extraction from lengthy source documents. (7 sources)",
                "text": "\nMulti-document summarization (MDS) has evolved through several distinct phases over the years, with graph-based methods playing a crucial role throughout this development. The earliest approaches focused on graph ranking-based extractive methods such as TextRank and LexRank, which used graph structures to identify important sentences in document collections <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper> <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>. These early graph-based methods were followed by syntax and structure-based compression techniques that attempted to address information redundancy and paraphrasing across multiple documents <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper> <Paper corpusId=\"10112929\" paperTitle=\"(Li et al., 2014)\" isShortName></Paper>.\n\nThe field underwent a significant transformation with the introduction of neural sequence-to-sequence abstractive methods around 2017, which built upon advances in single-document summarization and incorporated techniques like pointer generator networks with maximal marginal relevance <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper>. This neural evolution coincided with the development of important datasets like WikiSum and MultiNews, which have become standard benchmarks for evaluating MDS systems.\n\nGraph Neural Networks (GNNs) have demonstrated particular effectiveness in various NLP applications, including multi-document summarization <Paper corpusId=\"247446769\" paperTitle=\"(Golestani et al., 2022)\" isShortName></Paper>. Early applications of GNNs to MDS, such as the work by Christensen et al., created multi-document graphs that captured discourse relationships between sentences <Paper corpusId=\"247446769\" paperTitle=\"(Golestani et al., 2022)\" isShortName></Paper>. The introduction of Graph Convolutional Networks (GCN) by Kipf and Welling further advanced the field, enabling the generation of sentence relation graphs for improved summarization <Paper corpusId=\"247446769\" paperTitle=\"(Golestani et al., 2022)\" isShortName></Paper>.\n\nRecent developments have increasingly focused on leveraging graph representations to pre-synthesize dependencies between articles prior to summarization <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper> <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper>. These graph-based approaches help capture cross-document relations, which is crucial when summarizing lengthy documents <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper>. Additionally, innovative architectures like Hierarchical Transformers have been proposed to better represent cross-token and cross-document information <Paper corpusId=\"251594480\" paperTitle=\"(Ma et al., 2022)\" isShortName></Paper>.\n\nDespite these advances, significant challenges remain in capturing and aggregating salient information distributed across multiple lengthy source documents. Current language models, while proficient at summarization, still struggle with processing extensive inputs. Existing solutions often rely on sequence truncation rather than fully exploiting graph-based modeling of semantic units to identify essential information <Paper corpusId=\"273784877\" paperTitle=\"(Ragazzi et al., 2025)\" isShortName></Paper>. This highlights the continuing need for more sophisticated graph-based neural approaches that can effectively handle the complexity of multi-document summarization.",
                "citations": [
                    {
                        "id": "(Sankar et al., 2022)",
                        "snippets": [
                            "Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013)."
                        ],
                        "paper": {
                            "corpus_id": 248571519,
                            "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization",
                            "authors": [
                                {
                                    "authorId": "2064325789",
                                    "name": "Aiswarya Sankar"
                                },
                                {
                                    "authorId": "145934595",
                                    "name": "Ankit R. Chadha"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.96435546875
                    },
                    {
                        "id": "(Erkan et al., 2004)",
                        "snippets": [
                            "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."
                        ],
                        "paper": {
                            "corpus_id": 506350,
                            "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization",
                            "authors": [
                                {
                                    "authorId": "2158159",
                                    "name": "G\u00fcnes Erkan"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                }
                            ],
                            "year": 2004,
                            "venue": "Journal of Artificial Intelligence Research",
                            "n_citations": 3097
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2014)",
                        "snippets": [
                            "In this paper, we focus on the problem of using sentence compression techniques to improve multi-document summarization. We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status \u2013 remove or retain. Integer liner programming with discriminative training is used to solve the problem. Under this model, we incorporate various constraints to improve the linguistic quality of the compressed sentences. Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary. Compared with state-ofthe-art algorithms, our model has similar ROUGE-2 scores but better linguistic quality on TAC data."
                        ],
                        "paper": {
                            "corpus_id": 10112929,
                            "title": "Improving Multi-documents Summarization by Sentence Compression based on Expanded Constituent Parse Trees",
                            "authors": [
                                {
                                    "authorId": "40475614",
                                    "name": "Chen Li"
                                },
                                {
                                    "authorId": "2152797181",
                                    "name": "Yang Liu"
                                },
                                {
                                    "authorId": "144544919",
                                    "name": "Fei Liu"
                                },
                                {
                                    "authorId": "46586837",
                                    "name": "Lin Zhao"
                                },
                                {
                                    "authorId": "1807350",
                                    "name": "F. Weng"
                                }
                            ],
                            "year": 2014,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 46
                        },
                        "score": 0
                    },
                    {
                        "id": "(Golestani et al., 2022)",
                        "snippets": [
                            "Graph neural networks (GNN) have shown to be effective in NLP applications, including syntactic dependency trees (Marcheggiani and Titov, 2017), neural machine translation (Beck et al., 2018), knowledge graphs (Wang et al., 2018), semantic graphs (Song et al., 2018), sequence-to-graph learning (Johnson, 2017), graph-to-sequence learning (Beck et al., 2018), sentence ordering (Yin et al., 2019), and multi-document summarization (Christensen et al., 2013;Yasunaga et al., 2017). \n\nChristensen et al. (2013) used a GNN in multi-document summarization. They create multi-document graphs which determine pairwise ordering constraints of sentences based on the discourse relationship between them. Kipf and Welling (2017) proposed Graph Convolutional Networks (GCN), which is used in Yasunaga et al. (2017) to generate sentence relation graphs. The final sentence embeddings indicate the graph representation and are utilized as inputs to achieve satisfactory results on multi-document summarization."
                        ],
                        "paper": {
                            "corpus_id": 247446769,
                            "title": "Pruned Graph Neural Network for Short Story Ordering",
                            "authors": [
                                {
                                    "authorId": "2124373879",
                                    "name": "Melika Golestani"
                                },
                                {
                                    "authorId": "19218725",
                                    "name": "Zeinab Borhanifard"
                                },
                                {
                                    "authorId": "51133923",
                                    "name": "Farnaz Tahmasebian"
                                },
                                {
                                    "authorId": "3054779",
                                    "name": "Heshaam Faili"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference of the Italian Association for Artificial Intelligence",
                            "n_citations": 0
                        },
                        "score": 0.97265625
                    },
                    {
                        "id": "(Li et al., 2020)",
                        "snippets": [
                            "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines."
                        ],
                        "paper": {
                            "corpus_id": 218718706,
                            "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "48624966",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "2107521158",
                                    "name": "Xinyan Xiao"
                                },
                                {
                                    "authorId": null,
                                    "name": "Jiachen Liu"
                                },
                                {
                                    "authorId": "40354707",
                                    "name": "Hua Wu"
                                },
                                {
                                    "authorId": "144270731",
                                    "name": "Haifeng Wang"
                                },
                                {
                                    "authorId": "2117218629",
                                    "name": "Junping Du"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 136
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ma et al., 2022)",
                        "snippets": [
                            "In comparison to single-document summarization, MDS brings challenges on the representation and coverage of its lengthy and linked sources",
                            "On the other hand, Liu and Lapata (2019) propose a Hierarchical Transformer (HT) with local and global encoder layers to represent cross-token and cross-document information."
                        ],
                        "paper": {
                            "corpus_id": 251594480,
                            "title": "Parallel Hierarchical Transformer with Attention Alignment for Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "51439912",
                                    "name": "Ye Ma"
                                },
                                {
                                    "authorId": "48373979",
                                    "name": "Lu Zong"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.9609375
                    },
                    {
                        "id": "(Ragazzi et al., 2025)",
                        "snippets": [
                            "Despite the proficiency exhibited by language models in text summarization, challenges persist in capturing and aggregating salient information dispersed across a cluster of lengthy sources. To accommodate more input, existing solutions prioritize sparse attention mechanisms, relying on sequence truncation without incorporating graph-based modeling of multiple semantic units to locate essential facets."
                        ],
                        "paper": {
                            "corpus_id": 273784877,
                            "title": "Cross-Document Distillation via Graph-Based Summarization of Extracted Essential Knowledge",
                            "authors": [
                                {
                                    "authorId": "134327204",
                                    "name": "Luca Ragazzi"
                                },
                                {
                                    "authorId": "2237646991",
                                    "name": "Gianluca Moro"
                                },
                                {
                                    "authorId": "2132084411",
                                    "name": "Lorenzo Valgimigli"
                                },
                                {
                                    "authorId": "2328891652",
                                    "name": "Riccardo Fiorani"
                                }
                            ],
                            "year": 2025,
                            "venue": "IEEE Transactions on Audio, Speech, and Language Processing",
                            "n_citations": 0
                        },
                        "score": 0.986328125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Evolution of Graph-based Neural Approaches",
                "tldr": "Graph-based neural approaches for multi-document summarization have progressed from basic ranking methods to sophisticated architectures that model complex document relationships. Recent innovations include heterogeneous graphs, hypergraphs, and hierarchical structures that better capture cross-document dependencies and improve information selection. (20 sources)",
                "text": "\nThe evolution of graph-based neural approaches for multi-document summarization can be traced through several distinct developmental phases. Early graph-based methods like TextRank and LexRank represented fundamental approaches that used graph ranking to identify important sentences in document collections <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper> <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper>. These methods were followed by syntax and structure-based compression techniques that addressed information redundancy across multiple documents <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper> <Paper corpusId=\"10112929\" paperTitle=\"(Li et al., 2014)\" isShortName></Paper>.\n\nA significant advancement came with the integration of Graph Neural Networks (GNNs) into multi-document summarization systems. Christensen et al. created multi-document graphs that captured discourse relationships between sentences, while Kipf and Welling's Graph Convolutional Networks (GCN) further enabled the generation of sentence relation graphs for improved summarization <Paper corpusId=\"247446769\" paperTitle=\"(Golestani et al., 2022)\" isShortName></Paper>. These early applications demonstrated GNNs' effectiveness in modeling document structures and relationships that are crucial for summarization tasks.\n\nThe introduction of pointer-generator networks by See et al. represented another milestone in neural abstractive summarization <Paper corpusId=\"8314118\" paperTitle=\"(See et al., 2017)\" isShortName></Paper>. This approach was subsequently enhanced by incorporating various types of information, including topic information <Paper corpusId=\"203641742\" paperTitle=\"(Liu et al., 2019)\" isShortName></Paper>, document structural information <Paper corpusId=\"46936631\" paperTitle=\"(Song et al., 2018)\" isShortName></Paper>, and semantic information <Paper corpusId=\"52111191\" paperTitle=\"(Hardy et al., 2018)\" isShortName></Paper>. The model was further improved by replacing LSTM modules with more advanced Transformer architectures to better handle long sequence inputs <Paper corpusId=\"202541012\" paperTitle=\"(Pilault et al., 2020)\" isShortName></Paper> <Paper corpusId=\"229171633\" paperTitle=\"(Wang et al., 2020)\" isShortName></Paper> <Paper corpusId=\"249062562\" paperTitle=\"(Fonseca et al., 2022)\" isShortName></Paper>.\n\nRecent developments have shifted toward more sophisticated graph-based neural architectures. These include heterogeneous graph neural networks that integrate nodes of different granularity levels to enrich cross-sentence relations <Paper corpusId=\"216552978\" paperTitle=\"(Wang et al._1, 2020)\" isShortName></Paper> <Paper corpusId=\"269791107\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>. Other innovations include hypergraph transformers like HEGEL, which capture high-order cross-sentence relations <Paper corpusId=\"252780923\" paperTitle=\"(Zhang et al., 2022)\" isShortName></Paper>, and hierarchical structures such as HierGNN, which learns document structure through latent structure trees <Paper corpusId=\"253155963\" paperTitle=\"(Qiu et al., 2022)\" isShortName></Paper>.\n\nGraph-based models for abstractive multi-document summarization have also evolved to leverage different types of graphs for encoding documents. GraphSum constructs similarity graphs over paragraphs <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper>, while MGSum creates a three-level hierarchical graph spanning document, sentence, and word levels <Paper corpusId=\"220045815\" paperTitle=\"(Jin et al., 2020)\" isShortName></Paper>. More recent approaches have incorporated heterogeneous graphs that consider different edge types and semantic nodes of various granularities <Paper corpusId=\"239050558\" paperTitle=\"(Cui et al., 2021)\" isShortName></Paper> <Paper corpusId=\"257496469\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\nThe integration of topic information into graph-based models has emerged as another important trend. Neural topic models have been jointly used with graph neural networks to discover latent topics that bridge different documents and guide summary generation <Paper corpusId=\"239050558\" paperTitle=\"(Cui et al., 2021)\" isShortName></Paper>. Similarly, approaches like BASS have unified semantic graphs to aggregate co-referent phrases distributed across a wide range of context <Paper corpusId=\"235187330\" paperTitle=\"(Wu et al., 2021)\" isShortName></Paper>.\n\nThese advances reflect the ongoing effort to develop more effective methods for capturing cross-document relationships and improving information selection in multi-document summarization. By better modeling the complex dependencies between documents, sentences, and words, these graph-based neural approaches aim to generate more coherent and informative summaries.",
                "citations": [
                    {
                        "id": "(Erkan et al., 2004)",
                        "snippets": [
                            "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."
                        ],
                        "paper": {
                            "corpus_id": 506350,
                            "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization",
                            "authors": [
                                {
                                    "authorId": "2158159",
                                    "name": "G\u00fcnes Erkan"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                }
                            ],
                            "year": 2004,
                            "venue": "Journal of Artificial Intelligence Research",
                            "n_citations": 3097
                        },
                        "score": 0
                    },
                    {
                        "id": "(Sankar et al., 2022)",
                        "snippets": [
                            "Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013)."
                        ],
                        "paper": {
                            "corpus_id": 248571519,
                            "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization",
                            "authors": [
                                {
                                    "authorId": "2064325789",
                                    "name": "Aiswarya Sankar"
                                },
                                {
                                    "authorId": "145934595",
                                    "name": "Ankit R. Chadha"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.96435546875
                    },
                    {
                        "id": "(Li et al., 2014)",
                        "snippets": [
                            "In this paper, we focus on the problem of using sentence compression techniques to improve multi-document summarization. We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status \u2013 remove or retain. Integer liner programming with discriminative training is used to solve the problem. Under this model, we incorporate various constraints to improve the linguistic quality of the compressed sentences. Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary. Compared with state-ofthe-art algorithms, our model has similar ROUGE-2 scores but better linguistic quality on TAC data."
                        ],
                        "paper": {
                            "corpus_id": 10112929,
                            "title": "Improving Multi-documents Summarization by Sentence Compression based on Expanded Constituent Parse Trees",
                            "authors": [
                                {
                                    "authorId": "40475614",
                                    "name": "Chen Li"
                                },
                                {
                                    "authorId": "2152797181",
                                    "name": "Yang Liu"
                                },
                                {
                                    "authorId": "144544919",
                                    "name": "Fei Liu"
                                },
                                {
                                    "authorId": "46586837",
                                    "name": "Lin Zhao"
                                },
                                {
                                    "authorId": "1807350",
                                    "name": "F. Weng"
                                }
                            ],
                            "year": 2014,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 46
                        },
                        "score": 0
                    },
                    {
                        "id": "(Golestani et al., 2022)",
                        "snippets": [
                            "Graph neural networks (GNN) have shown to be effective in NLP applications, including syntactic dependency trees (Marcheggiani and Titov, 2017), neural machine translation (Beck et al., 2018), knowledge graphs (Wang et al., 2018), semantic graphs (Song et al., 2018), sequence-to-graph learning (Johnson, 2017), graph-to-sequence learning (Beck et al., 2018), sentence ordering (Yin et al., 2019), and multi-document summarization (Christensen et al., 2013;Yasunaga et al., 2017). \n\nChristensen et al. (2013) used a GNN in multi-document summarization. They create multi-document graphs which determine pairwise ordering constraints of sentences based on the discourse relationship between them. Kipf and Welling (2017) proposed Graph Convolutional Networks (GCN), which is used in Yasunaga et al. (2017) to generate sentence relation graphs. The final sentence embeddings indicate the graph representation and are utilized as inputs to achieve satisfactory results on multi-document summarization."
                        ],
                        "paper": {
                            "corpus_id": 247446769,
                            "title": "Pruned Graph Neural Network for Short Story Ordering",
                            "authors": [
                                {
                                    "authorId": "2124373879",
                                    "name": "Melika Golestani"
                                },
                                {
                                    "authorId": "19218725",
                                    "name": "Zeinab Borhanifard"
                                },
                                {
                                    "authorId": "51133923",
                                    "name": "Farnaz Tahmasebian"
                                },
                                {
                                    "authorId": "3054779",
                                    "name": "Heshaam Faili"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference of the Italian Association for Artificial Intelligence",
                            "n_citations": 0
                        },
                        "score": 0.97265625
                    },
                    {
                        "id": "(See et al., 2017)",
                        "snippets": [
                            "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points."
                        ],
                        "paper": {
                            "corpus_id": 8314118,
                            "title": "Get To The Point: Summarization with Pointer-Generator Networks",
                            "authors": [
                                {
                                    "authorId": "13070498",
                                    "name": "A. See"
                                },
                                {
                                    "authorId": "35025299",
                                    "name": "Peter J. Liu"
                                },
                                {
                                    "authorId": "144783904",
                                    "name": "Christopher D. Manning"
                                }
                            ],
                            "year": 2017,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 4028
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2019)",
                        "snippets": [
                            "Due to the lack of publicly available resources, conversation summarization has received far less attention than text summarization. As the purpose of conversations is to exchange information between at least two interlocutors, key information about a certain topic is often scattered and spanned across multiple utterances and turns from different speakers. This phenomenon is more pronounced during spoken conversations, where speech characteristics such as backchanneling and false-starts might interrupt the topical flow. Moreover, topic diffusion and (intra-utterance) topic drift are also more common in human-to-human conversations. Such linguistic characteristics of dialogue topics make sentence-level extractive summarization approaches used in spoken documents ill-suited for summarizing conversations. Pointer-generator networks have effectively demonstrated its strength at integrating extractive and abstractive capabilities through neural modeling in text summarization. To the best of our knowledge, to date no one has adopted it for summarizing conversations. In this work, we propose a topic-aware architecture to exploit the inherent hierarchical structure in conversations to further adapt the pointer-generator model. Our approach significantly outperforms competitive baselines, achieves more efficient learning outcomes, and attains more robust performance."
                        ],
                        "paper": {
                            "corpus_id": 203641742,
                            "title": "Topic-Aware Pointer-Generator Networks for Summarizing Spoken Conversations",
                            "authors": [
                                {
                                    "authorId": "49293155",
                                    "name": "Zhengyuan Liu"
                                },
                                {
                                    "authorId": "114743098",
                                    "name": "A. Ng"
                                },
                                {
                                    "authorId": "51129684",
                                    "name": "Sheldon Lee Shao Guang"
                                },
                                {
                                    "authorId": "1829583",
                                    "name": "AiTi Aw"
                                },
                                {
                                    "authorId": "2185019",
                                    "name": "Nancy F. Chen"
                                }
                            ],
                            "year": 2019,
                            "venue": "Automatic Speech Recognition & Understanding",
                            "n_citations": 101
                        },
                        "score": 0
                    },
                    {
                        "id": "(Song et al., 2018)",
                        "snippets": [
                            "Seq2seq learning has produced promising results on summarization. However, in many cases, system summaries still struggle to keep the meaning of the original intact. They may miss out important words or relations that play critical roles in the syntactic structure of source sentences. In this paper, we present structure-infused copy mechanisms to facilitate copying important words and relations from the source sentence to summary sentence. The approach naturally combines source dependency structure with the copy mechanism of an abstractive sentence summarizer. Experimental results demonstrate the effectiveness of incorporating source-side syntactic information in the system, and our proposed approach compares favorably to state-of-the-art methods."
                        ],
                        "paper": {
                            "corpus_id": 46936631,
                            "title": "Structure-Infused Copy Mechanisms for Abstractive Summarization",
                            "authors": [
                                {
                                    "authorId": "50982080",
                                    "name": "Kaiqiang Song"
                                },
                                {
                                    "authorId": "46586837",
                                    "name": "Lin Zhao"
                                },
                                {
                                    "authorId": "144544919",
                                    "name": "Fei Liu"
                                }
                            ],
                            "year": 2018,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 76
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hardy et al., 2018)",
                        "snippets": [
                            "Recent work on abstractive summarization has made progress with neural encoder-decoder architectures. However, such models are often challenged due to their lack of explicit semantic modeling of the source document and its summary. In this paper, we extend previous work on abstractive summarization using Abstract Meaning Representation (AMR) with a neural language generation stage which we guide using the source document. We demonstrate that this guidance improves summarization results by 7.4 and 10.5 points in ROUGE-2 using gold standard AMR parses and parses obtained from an off-the-shelf parser respectively. We also find that the summarization performance on later parses is 2 ROUGE-2 points higher than that of a well-established neural encoder-decoder approach trained on a larger dataset."
                        ],
                        "paper": {
                            "corpus_id": 52111191,
                            "title": "Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation",
                            "authors": [
                                {
                                    "authorId": "46441201",
                                    "name": "Hardy Hardy"
                                },
                                {
                                    "authorId": "2064056928",
                                    "name": "Andreas Vlachos"
                                }
                            ],
                            "year": 2018,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 69
                        },
                        "score": 0
                    },
                    {
                        "id": "(Pilault et al., 2020)",
                        "snippets": [
                            "We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher ROUGE scores. We provide extensive comparisons with strong baseline methods, prior state of the art work as well as multiple variants of our approach including those using only transformers, only extractive techniques and combinations of the two. We examine these models using four different summarization tasks and datasets: arXiv papers, PubMed papers, the Newsroom and BigPatent datasets. We find that transformer based methods produce summaries with fewer n-gram copies, leading to n-gram copying statistics that are more similar to human generated abstracts. We include a human evaluation, finding that transformers are ranked highly for coherence and fluency, but purely extractive methods score higher for informativeness and relevance. We hope that these architectures and experiments may serve as strong points of comparison for future work. Note: The abstract above was collaboratively written by the authors and one of the models presented in this paper based on an earlier draft of this paper."
                        ],
                        "paper": {
                            "corpus_id": 202541012,
                            "title": "On Extractive and Abstractive Neural Document Summarization with Transformer Language Models",
                            "authors": [
                                {
                                    "authorId": "104354626",
                                    "name": "Jonathan Pilault"
                                },
                                {
                                    "authorId": "144235909",
                                    "name": "Raymond Li"
                                },
                                {
                                    "authorId": "50324141",
                                    "name": "Sandeep Subramanian"
                                },
                                {
                                    "authorId": "1972076",
                                    "name": "C. Pal"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 221
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2020)",
                        "snippets": [
                            "Like humans, document summarization models can interpret a document\u2019s contents in a number of ways. Unfortunately, the neural models of today are largely black boxes that provide little explanation of how or why they generated a summary in the way they did. Therefore, to begin prying open the black box and to inject a level of control into the substance of the final summary, we developed a novel select-and-generate framework that focuses on explainability. By revealing the latent centrality and interactions between sentences, along with scores for novelty and relevance, users are given a window into the choices a model is making and an opportunity to guide those choices in a more desirable direction. A novel pair-wise matrix captures the sentence interactions, centrality and attribute scores, and a mask with tunable attribute thresholds allows the user to control which sentences are likely to be included in the extraction. A sentence-deployed attention mechanism in the abstractor ensures the final summary emphasizes the desired content. Additionally, the encoder is adaptable, supporting both Transformer- and BERT-based configurations. In a series of experiments assessed with ROUGE metrics and two human evaluations, ESCA outperformed eight state-of-the-art models on the CNN/DailyMail and NYT50 benchmark datasets."
                        ],
                        "paper": {
                            "corpus_id": 229171633,
                            "title": "Exploring Explainable Selection to Control Abstractive Summarization",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Haonan Wang"
                                },
                                {
                                    "authorId": "145644809",
                                    "name": "Yang Gao"
                                },
                                {
                                    "authorId": "144843219",
                                    "name": "Yu Bai"
                                },
                                {
                                    "authorId": "1747893",
                                    "name": "Mirella Lapata"
                                },
                                {
                                    "authorId": "4590286",
                                    "name": "Heyan Huang"
                                }
                            ],
                            "year": 2020,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 17
                        },
                        "score": 0
                    },
                    {
                        "id": "(Fonseca et al., 2022)",
                        "snippets": [
                            "We argue that disentangling content selection from the budget used to cover salient content improves the performance and applicability of abstractive summarizers. Our method, FactorSum, does this disentanglement by factorizing summarization into two steps through an energy function: (1) generation of abstractive summary views covering salient information in subsets of the input document (document views); (2) combination of these views into a final summary, following a budget and content guidance. This guidance may come from different sources, including from an advisor model such as BART or BigBird, or in oracle mode \u2013 from the reference. This factorization achieves significantly higher ROUGE scores on multiple benchmarks for long document summarization, namely PubMed, arXiv, and GovReport. Most notably, our model is effective for domain adaptation. When trained only on PubMed samples, it achieves a 46.29 ROUGE-1 score on arXiv, outperforming PEGASUS trained in domain by a large margin. Our experimental results indicate that the performance gains are due to more flexible budget adaptation and processing of shorter contexts provided by partial document views."
                        ],
                        "paper": {
                            "corpus_id": 249062562,
                            "title": "Factorizing Content and Budget Decisions in Abstractive Summarization of Long Documents",
                            "authors": [
                                {
                                    "authorId": "2142910610",
                                    "name": "Marcio Fonseca"
                                },
                                {
                                    "authorId": "7264689",
                                    "name": "Yftah Ziser"
                                },
                                {
                                    "authorId": "40146204",
                                    "name": "Shay B. Cohen"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 14
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al._1, 2020)",
                        "snippets": [
                            "As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. Besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github."
                        ],
                        "paper": {
                            "corpus_id": 216552978,
                            "title": "Heterogeneous Graph Neural Networks for Extractive Document Summarization",
                            "authors": [
                                {
                                    "authorId": "49371126",
                                    "name": "Danqing Wang"
                                },
                                {
                                    "authorId": "144118452",
                                    "name": "Pengfei Liu"
                                },
                                {
                                    "authorId": "3337238",
                                    "name": "Y. Zheng"
                                },
                                {
                                    "authorId": "1767521",
                                    "name": "Xipeng Qiu"
                                },
                                {
                                    "authorId": "1790227",
                                    "name": "Xuanjing Huang"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 282
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhao et al., 2024)",
                        "snippets": [
                            "Graph neural networks have been widely used for extractive summarization due to their flexibility and scalability.Dong et al. (2020) proposed an unsupervised graph-based model that combines both sentence similarity and hierarchical discourse structure to rank sentences.Cui et al. (2020) injected latent topic information into graph neural networks to further improve performance.(Wang et al., 2020) constructed a word-document heterogeneous graph using word nodes as intermediate to connect sentences.Zhang et al. (2022) proposed a hypergraph transformer to model long-distance dependency while emphasizing the importance of high-order inter-sentence relations in extraction summarization."
                        ],
                        "paper": {
                            "corpus_id": 269791107,
                            "title": "Hierarchical Attention Graph for Scientific Document Summarization in Global and Local Level",
                            "authors": [
                                {
                                    "authorId": "2294460391",
                                    "name": "Chenlong Zhao"
                                },
                                {
                                    "authorId": "2294514721",
                                    "name": "Xiwen Zhou"
                                },
                                {
                                    "authorId": "2295151257",
                                    "name": "Xiaopeng Xie"
                                },
                                {
                                    "authorId": "2294558459",
                                    "name": "Yong Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "NAACL-HLT",
                            "n_citations": 5
                        },
                        "score": 0.97802734375
                    },
                    {
                        "id": "(Zhang et al., 2022)",
                        "snippets": [
                            "The paper proposes HEGEL, a hypergraph neural network for long document summarization by capturing high-order cross-sentence relations. HEGEL updates and learns effective sentence representations with hypergraph transformer layers and fuses different types of sentence dependencies, including latent topics, keywords coreference, and section structure",
                            "To address the above issues, we propose HEGEL (HypErGraph transformer for Extractive Long document summarization), a graph-based model designed for summarizing long documents with rich discourse information. To better model high-order cross-sentence relations, we represent a document as a hypergraph, a generalization of graph structure, in which an edge can join any number of vertices",
                            "Graph-based models have been exploited for extractive summarization to capture cross-sentence dependencies. Unsupervised graph summarization methods rely on graph connectivity to score and rank sentences Zheng and Lapata, 2019;Dong et al., 2020). Researchers also explore supervised graph neural networks for summarization. Yasunaga et al. (2017) applied Graph Convolutional Network (GCN) on the approximate discourse graph. Xu et al. (2019) proposed to apply GCN on structural discourse graphs based on RST trees and coreference mentions. Cui et al. (2020) leveraged topical information by building topicsentence graphs. Recently, Wang et al. (2020) proposed to construct word-document heterogeneous graphs and use word nodes as the intermediary between sentences. Jing et al. (2021) proposed to use multiplex graph to consider different sentence relations."
                        ],
                        "paper": {
                            "corpus_id": 252780923,
                            "title": "HEGEL: Hypergraph Transformer for Long Document Summarization",
                            "authors": [
                                {
                                    "authorId": "2135688409",
                                    "name": "Haopeng Zhang"
                                },
                                {
                                    "authorId": null,
                                    "name": "Xiao Liu"
                                },
                                {
                                    "authorId": "2168548350",
                                    "name": "Jiawei Zhang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 45
                        },
                        "score": 0.97119140625
                    },
                    {
                        "id": "(Qiu et al., 2022)",
                        "snippets": [
                            "Neural Abstractive Summarization (Rush et al., 2015) first proposed to use a sequence-to-sequence model with an attention mechanism to perform sentence compression. (Mendes et al., 2019) demonstrated the advantages and limitations of neural methods based on sentence compression. The pointer-generator networks (PGN; (See et al., 2017)) enhances the attention model with a copying functionality. PGN has also been further extended to create summarization systems by incorporating the topic information (Liu et al., 2019), document structural information (Song et al., 2018), semantic information (Hardy et al., 2018), and was improved by replacing the plain LSTM module with the more advanced Transformer model to overcome the difficulty in modeling long sequence input (Pilault et al., 2020)(Wang et al., 2020)(Fonseca et al., 2022)",
                            "we propose a hierarchy-aware graph neural network (HierGNN) which captures such dependencies through three main steps: 1) learning a hierarchical document structure through a latent structure tree learned by a sparse matrix-tree computation; 2) propagating sentence information over this structure using a novel message-passing node propagation mechanism to identify salient information; 3) using graph-level attention to concentrate the decoder on salient information."
                        ],
                        "paper": {
                            "corpus_id": 253155963,
                            "title": "Abstractive Summarization Guided by Latent Hierarchical Document Structure",
                            "authors": [
                                {
                                    "authorId": "2159539050",
                                    "name": "Yifu Qiu"
                                },
                                {
                                    "authorId": "40146204",
                                    "name": "Shay B. Cohen"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 13
                        },
                        "score": 0.97607421875
                    },
                    {
                        "id": "(Li et al., 2020)",
                        "snippets": [
                            "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines."
                        ],
                        "paper": {
                            "corpus_id": 218718706,
                            "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "48624966",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "2107521158",
                                    "name": "Xinyan Xiao"
                                },
                                {
                                    "authorId": null,
                                    "name": "Jiachen Liu"
                                },
                                {
                                    "authorId": "40354707",
                                    "name": "Hua Wu"
                                },
                                {
                                    "authorId": "144270731",
                                    "name": "Haifeng Wang"
                                },
                                {
                                    "authorId": "2117218629",
                                    "name": "Junping Du"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 136
                        },
                        "score": 0
                    },
                    {
                        "id": "(Jin et al., 2020)",
                        "snippets": [
                            "In this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization, which jointly learn semantic representations for words, sentences, and documents. The word representations are used to generate an abstractive summary while the sentence representations are used to produce an extractive summary. We employ attention mechanisms to interact between different granularity of semantic representations, which helps to capture multi-granularity key information and improves the performance of both abstractive and extractive summarization. Experiment results show that our proposed model substantially outperforms all strong baseline methods and achieves the best results on the Multi-News dataset."
                        ],
                        "paper": {
                            "corpus_id": 220045815,
                            "title": "Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "1491212422",
                                    "name": "Hanqi Jin"
                                },
                                {
                                    "authorId": "1751960",
                                    "name": "Tian-ming Wang"
                                },
                                {
                                    "authorId": "145078589",
                                    "name": "Xiaojun Wan"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 99
                        },
                        "score": 0
                    },
                    {
                        "id": "(Cui et al., 2021)",
                        "snippets": [
                            "A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and then apply a graph-to-sequence framework to generate summaries. Moreover, we employ a neural topic model to jointly discover latent topics that can act as cross-document semantic units to bridge different documents and provide global information to guide the summary generation. Since topic extraction can be viewed as a special type of summarization that\"summarizes\"texts into a more abstract format, i.e., a topic distribution, we adopt a multi-task learning strategy to jointly train the topic and summarization module, allowing the promotion of each other. Experimental results on the Multi-News dataset demonstrate that our model outperforms previous state-of-the-art MDS models on both Rouge metrics and human evaluation, meanwhile learns high-quality topics."
                        ],
                        "paper": {
                            "corpus_id": 239050558,
                            "title": "Topic-Guided Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "143738684",
                                    "name": "Peng Cui"
                                },
                                {
                                    "authorId": "2109312896",
                                    "name": "Le Hu"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 41
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "Graph-Based Models Although graphs are commonly used to boost text summarization (Wu et al., 2021)(You et al., 2022)(Song et al., 2022), there are only a handful of models which have been proposed to use graphs to encode the documents in abstractive MDS (Li et al., 2020)(Jin et al., 2020)(Li et al., 2021)(Cui et al., 2021). Most of these models only leverage homogeneous graphs as they do not consider different edge types of graphs. For example, MGSum (Jin et al., 2020) constructs a threelevel (i.e., document, sentence, and word levels) hierarchical graph and learns semantics with a multi-level interaction network. GraphSum (Li et al., 2020)) constructs a similarity graph over the paragraphs. It learns a graph representation for the paragraphs and uses a hierarchical graph attention mechanism to guide the summary generation process."
                        ],
                        "paper": {
                            "corpus_id": 257496469,
                            "title": "Compressed Heterogeneous Graph for Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "2143544766",
                                    "name": "Miao Li"
                                },
                                {
                                    "authorId": "2149459181",
                                    "name": "Jianzhong Qi"
                                },
                                {
                                    "authorId": "1800564",
                                    "name": "Jey Han Lau"
                                }
                            ],
                            "year": 2023,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 11
                        },
                        "score": 0.97021484375
                    },
                    {
                        "id": "(Wu et al., 2021)",
                        "snippets": [
                            "Abstractive summarization for long-document or multi-document remains challenging for the Seq2Seq architecture, as Seq2Seq is not good at analyzing long-distance relations in text. In this paper, we present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. Further, a graph-based encoder-decoder model is proposed to improve both the document representation and summary generation process by leveraging the graph structure. Specifically, several graph augmentation methods are designed to encode both the explicit and implicit relations in the text while the graph-propagation attention mechanism is developed in the decoder to select salient content into the summary. Empirical results show that the proposed architecture brings substantial improvements for both long-document and multi-document summarization tasks."
                        ],
                        "paper": {
                            "corpus_id": 235187330,
                            "title": "BASS: Boosting Abstractive Summarization with Unified Semantic Graph",
                            "authors": [
                                {
                                    "authorId": "2139644141",
                                    "name": "Wenhao Wu"
                                },
                                {
                                    "authorId": "48624966",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "2107521158",
                                    "name": "Xinyan Xiao"
                                },
                                {
                                    "authorId": null,
                                    "name": "Jiachen Liu"
                                },
                                {
                                    "authorId": "2314396",
                                    "name": "Ziqiang Cao"
                                },
                                {
                                    "authorId": "48831399",
                                    "name": "Sujian Li"
                                },
                                {
                                    "authorId": "40354707",
                                    "name": "Hua Wu"
                                },
                                {
                                    "authorId": "144270731",
                                    "name": "Haifeng Wang"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 45
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Recent Graph Neural Network Architectures",
                "tldr": "Recent graph-based neural approaches for abstractive multi-document summarization feature diverse architectures including heterogeneous graphs, hypergraphs, and attention mechanisms to better model cross-document relationships. These models leverage various edge types, node granularities, and specialized encoder-decoder frameworks to improve summary generation quality. (10 sources)",
                "text": "\n- **GraphSum** incorporates document similarity graphs built over paragraphs to capture cross-document relations, crucial for summarizing long documents. It leverages graphs during both encoding and decoding phases, and can be combined with pre-trained language models for improved performance. <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper>\n\n- **MGSum** constructs a three-level hierarchical graph spanning document, sentence, and word levels, learning semantics through a multi-level interaction network. This approach allows the model to capture relationships at different textual granularities. <Paper corpusId=\"220045815\" paperTitle=\"(Jin et al., 2020)\" isShortName></Paper>\n\n- **HeterGNN** represents multiple documents as a heterogeneous graph, considering semantic nodes of different granularities. This approach better captures the complex relationships between document elements and improves information selection. <Paper corpusId=\"239050558\" paperTitle=\"(Cui et al., 2021)\" isShortName></Paper>\n\n- **HEGEL** (HypErGraph transformer for Extractive Long document summarization) uses hypergraph neural networks to capture high-order cross-sentence relations in long documents. The model updates and learns effective sentence representations through hypergraph transformer layers and fuses different types of sentence dependencies, including latent topics, keyword coreference, and section structure. <Paper corpusId=\"252780923\" paperTitle=\"(Zhang et al., 2022)\" isShortName></Paper>\n\n- **HierGNN** presents a hierarchy-aware graph neural network that learns document structure through a latent structure tree. The model captures dependencies through three main steps: learning hierarchical document structure, propagating sentence information over this structure to identify salient content, and using graph-level attention to guide the decoder. <Paper corpusId=\"253155963\" paperTitle=\"(Qiu et al., 2022)\" isShortName></Paper>\n\n- **GTASum** (Graph-Based Topic-Aware abstract text summarization) constructs a heterogeneous document graph containing sentence representation nodes and potential topic nodes. The model first encodes documents with BERT while using a Neural Topic Model to learn potential topics, then revises these representations using a Graph Attention Network before feeding them into a Transformer-based decoder. <Paper corpusId=\"250012667\" paperTitle=\"(Jiang et al., 2022)\" isShortName></Paper>\n\n- **KGSum** leverages knowledge graphs for multi-document scientific summarization, incorporating them in both encoding and decoding processes. Two graph-based modules incorporate knowledge graph information into paper encoding, while a two-stage decoder first generates knowledge graph information as descriptive sentences before producing the final summary. <Paper corpusId=\"252185277\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>\n\n- **SACA** enhances graph structure integration during decoding by re-encoding the input graph. Unlike GraphSum, which only introduces one graph attention layer in each decoder layer, SACA re-computes the input graph representation by conditioning it on newly generated text at each decoding step. <Paper corpusId=\"252280335\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>\n\n- **BASS** (Boosting Abstractive Summarization based on a unified Semantic graph) aggregates co-referent phrases distributed across a wide range of context and conveys rich relations between phrases. This approach helps analyze long-distance relations in text for both long-document and multi-document summarization. <Paper corpusId=\"235187330\" paperTitle=\"(Wu et al., 2021)\" isShortName></Paper>\n\n- **HeterTls** presents a joint learning-based heterogeneous graph attention network for timeline summarization, combining date selection and event detection into a unified framework. The heterogeneous graph involves multiple types of nodes with representations iteratively learned across heterogeneous graph attention layers. <Paper corpusId=\"250390945\" paperTitle=\"(You et al., 2022)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Li et al., 2020)",
                        "snippets": [
                            "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines."
                        ],
                        "paper": {
                            "corpus_id": 218718706,
                            "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "48624966",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "2107521158",
                                    "name": "Xinyan Xiao"
                                },
                                {
                                    "authorId": null,
                                    "name": "Jiachen Liu"
                                },
                                {
                                    "authorId": "40354707",
                                    "name": "Hua Wu"
                                },
                                {
                                    "authorId": "144270731",
                                    "name": "Haifeng Wang"
                                },
                                {
                                    "authorId": "2117218629",
                                    "name": "Junping Du"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 136
                        },
                        "score": 0
                    },
                    {
                        "id": "(Jin et al., 2020)",
                        "snippets": [
                            "In this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization, which jointly learn semantic representations for words, sentences, and documents. The word representations are used to generate an abstractive summary while the sentence representations are used to produce an extractive summary. We employ attention mechanisms to interact between different granularity of semantic representations, which helps to capture multi-granularity key information and improves the performance of both abstractive and extractive summarization. Experiment results show that our proposed model substantially outperforms all strong baseline methods and achieves the best results on the Multi-News dataset."
                        ],
                        "paper": {
                            "corpus_id": 220045815,
                            "title": "Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "1491212422",
                                    "name": "Hanqi Jin"
                                },
                                {
                                    "authorId": "1751960",
                                    "name": "Tian-ming Wang"
                                },
                                {
                                    "authorId": "145078589",
                                    "name": "Xiaojun Wan"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 99
                        },
                        "score": 0
                    },
                    {
                        "id": "(Cui et al., 2021)",
                        "snippets": [
                            "A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and then apply a graph-to-sequence framework to generate summaries. Moreover, we employ a neural topic model to jointly discover latent topics that can act as cross-document semantic units to bridge different documents and provide global information to guide the summary generation. Since topic extraction can be viewed as a special type of summarization that\"summarizes\"texts into a more abstract format, i.e., a topic distribution, we adopt a multi-task learning strategy to jointly train the topic and summarization module, allowing the promotion of each other. Experimental results on the Multi-News dataset demonstrate that our model outperforms previous state-of-the-art MDS models on both Rouge metrics and human evaluation, meanwhile learns high-quality topics."
                        ],
                        "paper": {
                            "corpus_id": 239050558,
                            "title": "Topic-Guided Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "143738684",
                                    "name": "Peng Cui"
                                },
                                {
                                    "authorId": "2109312896",
                                    "name": "Le Hu"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 41
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2022)",
                        "snippets": [
                            "The paper proposes HEGEL, a hypergraph neural network for long document summarization by capturing high-order cross-sentence relations. HEGEL updates and learns effective sentence representations with hypergraph transformer layers and fuses different types of sentence dependencies, including latent topics, keywords coreference, and section structure",
                            "To address the above issues, we propose HEGEL (HypErGraph transformer for Extractive Long document summarization), a graph-based model designed for summarizing long documents with rich discourse information. To better model high-order cross-sentence relations, we represent a document as a hypergraph, a generalization of graph structure, in which an edge can join any number of vertices",
                            "Graph-based models have been exploited for extractive summarization to capture cross-sentence dependencies. Unsupervised graph summarization methods rely on graph connectivity to score and rank sentences Zheng and Lapata, 2019;Dong et al., 2020). Researchers also explore supervised graph neural networks for summarization. Yasunaga et al. (2017) applied Graph Convolutional Network (GCN) on the approximate discourse graph. Xu et al. (2019) proposed to apply GCN on structural discourse graphs based on RST trees and coreference mentions. Cui et al. (2020) leveraged topical information by building topicsentence graphs. Recently, Wang et al. (2020) proposed to construct word-document heterogeneous graphs and use word nodes as the intermediary between sentences. Jing et al. (2021) proposed to use multiplex graph to consider different sentence relations."
                        ],
                        "paper": {
                            "corpus_id": 252780923,
                            "title": "HEGEL: Hypergraph Transformer for Long Document Summarization",
                            "authors": [
                                {
                                    "authorId": "2135688409",
                                    "name": "Haopeng Zhang"
                                },
                                {
                                    "authorId": null,
                                    "name": "Xiao Liu"
                                },
                                {
                                    "authorId": "2168548350",
                                    "name": "Jiawei Zhang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 45
                        },
                        "score": 0.97119140625
                    },
                    {
                        "id": "(Qiu et al., 2022)",
                        "snippets": [
                            "Neural Abstractive Summarization (Rush et al., 2015) first proposed to use a sequence-to-sequence model with an attention mechanism to perform sentence compression. (Mendes et al., 2019) demonstrated the advantages and limitations of neural methods based on sentence compression. The pointer-generator networks (PGN; (See et al., 2017)) enhances the attention model with a copying functionality. PGN has also been further extended to create summarization systems by incorporating the topic information (Liu et al., 2019), document structural information (Song et al., 2018), semantic information (Hardy et al., 2018), and was improved by replacing the plain LSTM module with the more advanced Transformer model to overcome the difficulty in modeling long sequence input (Pilault et al., 2020)(Wang et al., 2020)(Fonseca et al., 2022)",
                            "we propose a hierarchy-aware graph neural network (HierGNN) which captures such dependencies through three main steps: 1) learning a hierarchical document structure through a latent structure tree learned by a sparse matrix-tree computation; 2) propagating sentence information over this structure using a novel message-passing node propagation mechanism to identify salient information; 3) using graph-level attention to concentrate the decoder on salient information."
                        ],
                        "paper": {
                            "corpus_id": 253155963,
                            "title": "Abstractive Summarization Guided by Latent Hierarchical Document Structure",
                            "authors": [
                                {
                                    "authorId": "2159539050",
                                    "name": "Yifu Qiu"
                                },
                                {
                                    "authorId": "40146204",
                                    "name": "Shay B. Cohen"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 13
                        },
                        "score": 0.97607421875
                    },
                    {
                        "id": "(Jiang et al., 2022)",
                        "snippets": [
                            "In this work, we suggest a novel GTASum model (Graph-Based Topic-Aware abstract text summarization). First, the document is encoded with BERT (\u67f4\u7530, 2020) (Bidirectional Encoder Representation from Transformers) to obtain contextual sentence representations; meanwhile, NTM (Ding et al., 2018) is used to learn the potential topic of the document. Second, construct a heterogeneous document graph that contains sentence representation nodes and potential topic nodes, then revise them using a adapted GAT [30]. Finally, the sentence representations containing topic information are fed into a Transformer-based decoder to generate summaries."
                        ],
                        "paper": {
                            "corpus_id": 250012667,
                            "title": "GATSum: Graph-Based Topic-Aware Abstract Text Summarization",
                            "authors": [
                                {
                                    "authorId": "2040863712",
                                    "name": "Ming Jiang"
                                },
                                {
                                    "authorId": "2034370007",
                                    "name": "Yifan Zou"
                                },
                                {
                                    "authorId": "2110980173",
                                    "name": "Jian Xu"
                                },
                                {
                                    "authorId": "2156053270",
                                    "name": "Min Zhang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Information Technology and Control",
                            "n_citations": 11
                        },
                        "score": 0.9765625
                    },
                    {
                        "id": "(Wang et al., 2022)",
                        "snippets": [
                            "Multi-Document Scientific Summarization (MDSS) aims to produce coherent and concise summaries for clusters of topic-relevant scientific papers. This task requires precise understanding of paper content and accurate modeling of cross-paper relationships. Knowledge graphs convey compact and interpretable structured information for documents, which makes them ideal for content modeling and relationship modeling. In this paper, we present KGSum, an MDSS model centred on knowledge graphs during both the encoding and decoding process. Specifically, in the encoding process, two graph-based modules are proposed to incorporate knowledge graph information into paper encoding, while in the decoding process, we propose a two-stage decoder by first generating knowledge graph information of summary in the form of descriptive sentences, followed by generating the final summary. Empirical results show that the proposed architecture brings substantial improvements over baselines on the Multi-Xscience dataset",
                            ".GraphSum (Li et al., 2020)) is a neural multi-document summarization model that leverages well-known graphs to produce abstractive summaries. We use TF-IDF graph as the input graph. PEGASUS (Zhang et al., 2019) is a sequence-to-sequence model with gapsentences generation as a pre-training objective tailored for abstractive summarization. Pointer-Generator (See et al., 2017) is an RNN based model with an attention mechanism and allows the system to copy words from the source via pointing for abstractive summarization. BertABS (Liu and Lapata, 2019b) uses a pretrained BERT (Devlin et al., 2019) as the encoder for abstractive summarization. We also report the performance of BertABS with an encoder (SciBertABS) pretrained on scientific articles."
                        ],
                        "paper": {
                            "corpus_id": 252185277,
                            "title": "Multi-Document Scientific Summarization from a Knowledge Graph-Centric View",
                            "authors": [
                                {
                                    "authorId": "2073437",
                                    "name": "Pancheng Wang"
                                },
                                {
                                    "authorId": "2145340498",
                                    "name": "Shasha Li"
                                },
                                {
                                    "authorId": "2116489",
                                    "name": "Kunyuan Pang"
                                },
                                {
                                    "authorId": "50670961",
                                    "name": "Liangliang He"
                                },
                                {
                                    "authorId": "2108821455",
                                    "name": "Dong Li"
                                },
                                {
                                    "authorId": "1762106",
                                    "name": "Jintao Tang"
                                },
                                {
                                    "authorId": "38972135",
                                    "name": "Ting Wang"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 15
                        },
                        "score": 0.98095703125
                    },
                    {
                        "id": "(Li et al., 2022)",
                        "snippets": [
                            "A recently proposed neural abstractive Multi-Document Summarization (MDS) model, Graph-Summ (Li et al., 2020), also considers the input graph structure during decoding. The biggest difference between Graphsum and our proposed SACA is that the former only introduces one graph attention layer in each decoder layer. SACA, on the other hand, injects graph structure into decoding by re-encoding the input graph. Specifically, it re-computes the input graph representation by conditioning it on the newly generated text at each decoding step."
                        ],
                        "paper": {
                            "corpus_id": 252280335,
                            "title": "Graph-to-Text Generation with Dynamic Structure Pruning",
                            "authors": [
                                {
                                    "authorId": "2154884699",
                                    "name": "Liang Li"
                                },
                                {
                                    "authorId": "9706609",
                                    "name": "Ruiying Geng"
                                },
                                {
                                    "authorId": "2132475886",
                                    "name": "Bowen Li"
                                },
                                {
                                    "authorId": "2112563315",
                                    "name": "Can Ma"
                                },
                                {
                                    "authorId": "35755264",
                                    "name": "Yinliang Yue"
                                },
                                {
                                    "authorId": "66200440",
                                    "name": "Binhua Li"
                                },
                                {
                                    "authorId": "2323761746",
                                    "name": "Yongbin Li"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 2
                        },
                        "score": 0.9609375
                    },
                    {
                        "id": "(Wu et al., 2021)",
                        "snippets": [
                            "Abstractive summarization for long-document or multi-document remains challenging for the Seq2Seq architecture, as Seq2Seq is not good at analyzing long-distance relations in text. In this paper, we present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. Further, a graph-based encoder-decoder model is proposed to improve both the document representation and summary generation process by leveraging the graph structure. Specifically, several graph augmentation methods are designed to encode both the explicit and implicit relations in the text while the graph-propagation attention mechanism is developed in the decoder to select salient content into the summary. Empirical results show that the proposed architecture brings substantial improvements for both long-document and multi-document summarization tasks."
                        ],
                        "paper": {
                            "corpus_id": 235187330,
                            "title": "BASS: Boosting Abstractive Summarization with Unified Semantic Graph",
                            "authors": [
                                {
                                    "authorId": "2139644141",
                                    "name": "Wenhao Wu"
                                },
                                {
                                    "authorId": "48624966",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "2107521158",
                                    "name": "Xinyan Xiao"
                                },
                                {
                                    "authorId": null,
                                    "name": "Jiachen Liu"
                                },
                                {
                                    "authorId": "2314396",
                                    "name": "Ziqiang Cao"
                                },
                                {
                                    "authorId": "48831399",
                                    "name": "Sujian Li"
                                },
                                {
                                    "authorId": "40354707",
                                    "name": "Hua Wu"
                                },
                                {
                                    "authorId": "144270731",
                                    "name": "Haifeng Wang"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 45
                        },
                        "score": 0
                    },
                    {
                        "id": "(You et al., 2022)",
                        "snippets": [
                            "Previous studies on the timeline summarization (TLS) task ignored the information interaction between sentences and dates, and adopted pre-defined unlearnable representations for them. They also considered date selection and event detection as two independent tasks, which makes it impossible to integrate their advantages and obtain a globally optimal summary. In this paper, we present a joint learning-based heterogeneous graph attention network for TLS (HeterTls), in which date selection and event detection are combined into a unified framework to improve the extraction accuracy and remove redundant sentences simultaneously. Our heterogeneous graph involves multiple types of nodes, the representations of which are iteratively learned across the heterogeneous graph attention layer. We evaluated our model on four datasets, and found that it significantly outperformed the current state-of-the-art baselines with regard to ROUGE scores and date selection metrics."
                        ],
                        "paper": {
                            "corpus_id": 250390945,
                            "title": "Joint Learning-based Heterogeneous Graph Attention Network for Timeline Summarization",
                            "authors": [
                                {
                                    "authorId": "2072795135",
                                    "name": "Jingyi You"
                                },
                                {
                                    "authorId": "2115198544",
                                    "name": "Dongyuan Li"
                                },
                                {
                                    "authorId": "2300756",
                                    "name": "Hidetaka Kamigaito"
                                },
                                {
                                    "authorId": "1747395",
                                    "name": "Kotaro Funakoshi"
                                },
                                {
                                    "authorId": "144859189",
                                    "name": "M. Okumura"
                                }
                            ],
                            "year": 2022,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 12
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Heterogeneous Graph Networks",
                "tldr": "Heterogeneous graph networks for multi-document summarization incorporate nodes of different types and granularity levels to better capture cross-document relationships. These models represent documents as complex graphs with various node types (sentences, words, entities, topics) and edge relationships that enable richer information exchange and improve summary quality. (11 sources)",
                "text": "\nHeterogeneous graph neural networks (HGNNs) represent a significant advancement in graph-based approaches for multi-document summarization by introducing diverse node types that go beyond basic sentence relationships. Unlike homogeneous graphs that only model sentence-to-sentence connections, heterogeneous graphs incorporate nodes of varying granularity levels, creating richer representations of document content and relationships <Paper corpusId=\"216552978\" paperTitle=\"(Wang et al._1, 2020)\" isShortName></Paper> <Paper corpusId=\"264146402\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>.\n\nHETERSUMGRAPH pioneered this approach by introducing semantic nodes of different granularity levels alongside sentence nodes. These additional nodes serve as intermediaries between sentences, enriching cross-sentence relationships and enabling more effective information exchange <Paper corpusId=\"216552978\" paperTitle=\"(Wang et al._1, 2020)\" isShortName></Paper> <Paper corpusId=\"265607988\" paperTitle=\"(Roy et al., 2023)\" isShortName></Paper>. A key advantage of this architecture is its flexibility in extending from single-document to multi-document summarization through the addition of document-level nodes <Paper corpusId=\"264146402\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"257219819\" paperTitle=\"(Zesheng et al., 2023)\" isShortName></Paper>.\n\nThe heterogeneous graph structure allows different sentences to interact through overlapping word information, which helps capture semantic connections that might otherwise be missed in sentence-only graphs <Paper corpusId=\"264146402\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. This is particularly valuable in multi-document summarization, where information is often distributed sparsely across multiple documents, challenging the effectiveness of sentence-level models <Paper corpusId=\"266244733\" paperTitle=\"(Su et al., 2023)\" isShortName></Paper>.\n\nMore advanced heterogeneous graph models have further extended this approach. EMSum augments the Transformer architecture with a knowledge graph consisting of text units and entities as nodes, utilizing Graph Attention Networks to capture cross-document information and identify relevant content across multiple documents <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper> <Paper corpusId=\"236478143\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper>. Similarly, BART-Long-Graph integrates graphical information with the pre-trained BART model using a separate graph encoder, demonstrating significant improvements on benchmark datasets like Multi-News and DUC-2004 <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper> <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper>.\n\nRecent heterogeneous graph approaches have also incorporated topic modeling to enhance document representation. Models like GTASum construct heterogeneous document graphs containing both sentence representation nodes and topic nodes <Paper corpusId=\"271938110\" paperTitle=\"(Khaliq et al., 2024)\" isShortName></Paper>. These models initially encode documents with BERT while simultaneously employing Neural Topic Models to identify potential topics, then refine these representations using Graph Attention Networks before generating summaries <Paper corpusId=\"271938110\" paperTitle=\"(Khaliq et al., 2024)\" isShortName></Paper> <Paper corpusId=\"233169121\" paperTitle=\"(An et al., 2021)\" isShortName></Paper>.\n\nThe Multi-GraS model further demonstrates the versatility of heterogeneous graphs by considering various types of intersentential relations (like semantic similarity and natural connections) alongside intrasentential relations between words <Paper corpusId=\"257219819\" paperTitle=\"(Zesheng et al., 2023)\" isShortName></Paper>. This multi-layered approach captures both document-level structure and fine-grained semantic relationships, leading to more comprehensive document representations.\n\nAnother innovative approach involves integrating passage nodes into heterogeneous graph structures alongside word and sentence nodes <Paper corpusId=\"277284940\" paperTitle=\"(Umair et al., 2025)\" isShortName></Paper>. This multi-level architecture helps models better understand document structure and identify important information across different granularity levels, improving summarization performance particularly for long or complex documents.\n\nThe evolution from homogeneous graphs with static nodes to heterogeneous networks with diverse node types represents a significant trend in modern text summarization research <Paper corpusId=\"271938110\" paperTitle=\"(Khaliq et al., 2024)\" isShortName></Paper>. By capturing a wider range of textual elements and enabling dynamic updates during the summarization process, heterogeneous graph networks provide a powerful framework for addressing the complex challenges of multi-document summarization.",
                "citations": [
                    {
                        "id": "(Wang et al._1, 2020)",
                        "snippets": [
                            "As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. Besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github."
                        ],
                        "paper": {
                            "corpus_id": 216552978,
                            "title": "Heterogeneous Graph Neural Networks for Extractive Document Summarization",
                            "authors": [
                                {
                                    "authorId": "49371126",
                                    "name": "Danqing Wang"
                                },
                                {
                                    "authorId": "144118452",
                                    "name": "Pengfei Liu"
                                },
                                {
                                    "authorId": "3337238",
                                    "name": "Y. Zheng"
                                },
                                {
                                    "authorId": "1767521",
                                    "name": "Xipeng Qiu"
                                },
                                {
                                    "authorId": "1790227",
                                    "name": "Xuanjing Huang"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 282
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2023)",
                        "snippets": [
                            "A heterogeneous GNN, HETERSUMGRAPH (Wang et al., 2020) is introduced for extractive document summarization. This network includes nodes of different granularity levels apart from sentences, which act as intermediaries and enrich cross-sentence relations. This approach allows different sentences to interact considering overlapping word information. Moreover, the graph network can accommodate additional node types, such as document nodes for multi-document summarization."
                        ],
                        "paper": {
                            "corpus_id": 264146402,
                            "title": "Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive Review",
                            "authors": [
                                {
                                    "authorId": "2258949345",
                                    "name": "Guanghua Wang"
                                },
                                {
                                    "authorId": "2258731583",
                                    "name": "Weili Wu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Discret. Math. Algorithms Appl.",
                            "n_citations": 4
                        },
                        "score": 0.974609375
                    },
                    {
                        "id": "(Roy et al., 2023)",
                        "snippets": [
                            "(Wang et al., 2020) introduced an heterogeneous graph neural network for extractive summarization which used additional semantic units (words) as intermediate nodes to construct relationships between sentences",
                            "Recently, state-ofthe-art solutions on abstractive summarization are built upon the transformer (Vaswani et al., 2017) and BERT (Devlin et al., 2019) models."
                        ],
                        "paper": {
                            "corpus_id": 265607988,
                            "title": "Generating Extractive and Abstractive Summaries in Parallel from Scientific Articles Incorporating Citing Statements",
                            "authors": [
                                {
                                    "authorId": "2164696118",
                                    "name": "Sudipta Singha Roy"
                                },
                                {
                                    "authorId": "2269458234",
                                    "name": "Robert E. Mercer"
                                }
                            ],
                            "year": 2023,
                            "venue": "NEWSUM",
                            "n_citations": 0
                        },
                        "score": 0.962890625
                    },
                    {
                        "id": "(Zesheng et al., 2023)",
                        "snippets": [
                            "Wang et al. [28] propose a heterogeneous graph-based neural network for extracting summaries, which contains semantic nodes of different granularity levels except sentences. These extra nodes act as \"intermediaries\" between sentences and enrich cross-sentence relations. The introduction of document nodes allows the graph structure to be flexibly extended from a single document setup to multiple documents. Another work [8] proposes a multiplex graph summary (Multi-GraS) model based on multiplex graph convolutional networks that can be used to extract text summaries. This model not only considers Various types of intersentential relations (such as semantic similarity and natural connection), and intra-sentential relations (such as semantic and syntactic relations between words) are also modeled."
                        ],
                        "paper": {
                            "corpus_id": 257219819,
                            "title": "Topic-Selective Graph Network for Topic-Focused Summarization",
                            "authors": [
                                {
                                    "authorId": "1764406",
                                    "name": "Shi Zesheng"
                                },
                                {
                                    "authorId": "2110348767",
                                    "name": "Yucheng Zhou"
                                }
                            ],
                            "year": 2023,
                            "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining",
                            "n_citations": 4
                        },
                        "score": 0.97119140625
                    },
                    {
                        "id": "(Su et al., 2023)",
                        "snippets": [
                            "The crucial aspect of extractive document summarization lies in understanding the interrelations between sentences. Documents inherently comprise a multitude of sentences, and sentence-level models frequently fail to consider the relationships between distantly-placed sentences, resulting in the omission of significant information in the summary. Moreover, information within documents tends to be distributed sparsely, challenging the efficacy of sentence-level models. In the realm of heterogeneous graph neural networks, it has been observed that semantic nodes with varying levels of granularity encapsulate distinct semantic connections. Initially, the incorporation of edge features into the computation of dynamic graph attention networks is performed to account for node relationships. Subsequently, given the multiplicity of topics in a document or a set of documents, a topic model is employed to extract topic-specific features and the probability distribution linking these topics with sentence nodes."
                        ],
                        "paper": {
                            "corpus_id": 266244733,
                            "title": "Multi-granularity adaptive extractive document summarization with heterogeneous graph neural networks",
                            "authors": [
                                {
                                    "authorId": "2274342252",
                                    "name": "Wu Su"
                                },
                                {
                                    "authorId": "2274904693",
                                    "name": "Jin Jiang"
                                },
                                {
                                    "authorId": "2274639188",
                                    "name": "Kaihui Huang"
                                }
                            ],
                            "year": 2023,
                            "venue": "PeerJ Computer Science",
                            "n_citations": 1
                        },
                        "score": 0.97314453125
                    },
                    {
                        "id": "(Qu, 2024)",
                        "snippets": [
                            "Zhou et al. (Zhou et al., 2021) presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP.Pasunuru et al. (Pasunuru et al., 2021) presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News (Fabbri et al., 2019) and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model (Lewis et al., 2019) with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. (Pasunuru et al., 2021) utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features."
                        ],
                        "paper": {
                            "corpus_id": 269762702,
                            "title": "Leveraging Knowledge-aware Methodologies for Multi-document Summarization",
                            "authors": [
                                {
                                    "authorId": "2163451228",
                                    "name": "Yutong Qu"
                                }
                            ],
                            "year": 2024,
                            "venue": "The Web Conference",
                            "n_citations": 0
                        },
                        "score": 0.96484375
                    },
                    {
                        "id": "(Zhou et al., 2021)",
                        "snippets": [
                            ","
                        ],
                        "paper": {
                            "corpus_id": 236478143,
                            "title": "Entity-Aware Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Hao Zhou"
                                },
                                {
                                    "authorId": "2053308860",
                                    "name": "Weidong Ren"
                                },
                                {
                                    "authorId": "150112803",
                                    "name": "Gongshen Liu"
                                },
                                {
                                    "authorId": "153253583",
                                    "name": "Bo Su"
                                },
                                {
                                    "authorId": "143844110",
                                    "name": "Wei Lu"
                                }
                            ],
                            "year": 2021,
                            "venue": "Findings",
                            "n_citations": 28
                        },
                        "score": 0
                    },
                    {
                        "id": "(Pasunuru et al., 2021)",
                        "snippets": [
                            "This paper presents an efficient graph-enhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents."
                        ],
                        "paper": {
                            "corpus_id": 235097309,
                            "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters",
                            "authors": [
                                {
                                    "authorId": "10721120",
                                    "name": "Ramakanth Pasunuru"
                                },
                                {
                                    "authorId": "2940333",
                                    "name": "Mengwen Liu"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                },
                                {
                                    "authorId": "120209444",
                                    "name": "Sujith Ravi"
                                },
                                {
                                    "authorId": "40262269",
                                    "name": "Markus Dreyer"
                                }
                            ],
                            "year": 2021,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 73
                        },
                        "score": 0
                    },
                    {
                        "id": "(Khaliq et al., 2024)",
                        "snippets": [
                            "In recent years, there have been notable achievements in applying Graph Neural Networks (GNNs) or Graph Attention Networks (GAT) [11] to summarize documents [4], [12], [13], [14], [15], [16], [17]. This success arises from their ability to capture complex inter-sentence relationships within documents. Specifically, GNN models demonstrate proficiency in representing complex structural data by encapsulating semantic units (nodes) and their interconnections (edges). Due to these capabilities, there is growing interest in leveraging the GAT framework alongside topic modeling for abstractive summarization tasks. The researchers in [12] proposed a heterogeneous neural graph structure using BERT to obtain contextual sentence representations. They simultaneously trained with latent topics using the Neural Topic Model (NTM) to model global information. The study of [18] proposed an expanded model that employed NTM for abstractive text summarization. \n\nThe authors of [15] introduced a graph structure enriched with latent topical information. They employed K-Means and Gaussian Mixture Models (GMM) for topic extraction. The study of [3] proposed an innovative Graph-Based Topic-Aware abstractive text summarization model. Initially, documents were encoded with BERT to generate sentence representations. Concurrently, NTM identified potential topics, while GAT refined these sentences and topic nodes. Finally, the sentence embeddings, enriched with topic information, were fed into a Transformer-driven decoder to generate summaries.\n\nModern text summarization research has shifted from homogeneous graphs with static nodes to heterogeneous networks. These networks allow for the inclusion of diverse node types, which represent wide range of textual elements. In addition, they allow for dynamic updates during the summarization process. The approach of [4] constructed a graph neural network based on word co-occurrences within the document to capture word-level relationships. The study in [42] utilized syntactic graph convolutional networks (GCNs) to model the non-Euclidean structure of documents.\n\nThis category includes models like Topic-GraphSum [12], SSN-DM [57] HeterGraphLongSum [14] and GTASum [3] that utilize graph structures to represent document information and relationships between sentences or topics, facilitating summarization."
                        ],
                        "paper": {
                            "corpus_id": 271938110,
                            "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
                            "authors": [
                                {
                                    "authorId": "2058964352",
                                    "name": "A. Khaliq"
                                },
                                {
                                    "authorId": "2149073250",
                                    "name": "Atif Khan"
                                },
                                {
                                    "authorId": "2316766711",
                                    "name": "Salman Afsar Awan"
                                },
                                {
                                    "authorId": "2316764100",
                                    "name": "Salman Jan"
                                },
                                {
                                    "authorId": "2257111507",
                                    "name": "Muhammad Umair"
                                },
                                {
                                    "authorId": "38509860",
                                    "name": "M. Zuhairi"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Access",
                            "n_citations": 1
                        },
                        "score": 0.9892578125
                    },
                    {
                        "id": "(An et al., 2021)",
                        "snippets": [
                            "Previous work for text summarization in scientific domain mainly focused on the content of the input document, but seldom considering its citation network.\nHowever, scientific papers are full of uncommon domain-specific terms, making it almost impossible for the model to understand its true meaning without the help of the relevant research community.\nIn this paper, we redefine the task of scientific papers summarization by utilizing their citation graph and propose a citation graph-based summarization model CGSum which can incorporate the information of both the source paper and its references.\nIn addition, we construct a novel scientific papers summarization dataset Semantic Scholar Network (SSN) which contains 141K research papers in different domains and 661K citation relationships. The entire dataset constitutes a large connected citation graph.\nExtensive experiments show that our model can achieve competitive performance when compared with the pretrained models even with a simple architecture.\nThe results also indicates the citation graph is crucial to better understand the content of papers and generate high-quality summaries."
                        ],
                        "paper": {
                            "corpus_id": 233169121,
                            "title": "Enhancing Scientific Papers Summarization with Citation Graph",
                            "authors": [
                                {
                                    "authorId": "2064164220",
                                    "name": "Chen An"
                                },
                                {
                                    "authorId": "1606040932",
                                    "name": "Ming Zhong"
                                },
                                {
                                    "authorId": "2135836407",
                                    "name": "Yiran Chen"
                                },
                                {
                                    "authorId": "49371126",
                                    "name": "Danqing Wang"
                                },
                                {
                                    "authorId": "1767521",
                                    "name": "Xipeng Qiu"
                                },
                                {
                                    "authorId": "1790227",
                                    "name": "Xuanjing Huang"
                                }
                            ],
                            "year": 2021,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 44
                        },
                        "score": 0
                    },
                    {
                        "id": "(Umair et al., 2025)",
                        "snippets": [
                            "In the domain of graph-based approaches, [3] implement a modified version of the GAT designed to identify inter-sentence relationships. To enhance the summarization of lengthy documents, they implemented Neural Topic Modeling (NTM) to identify latent topics. This approach introduced NTM as a new node type, which was then integrated with existing sentence nodes to improve the overall structure and accuracy of the summarization process.\n\nThe researchers of [7] integrated passage nodes into the heterogeneous graph structure alongside the existing word and sentence nodes. In the study [34], the authors attempted to redefine the task of review formalization, focusing on scientific paper summarization using citation graphs. The CGSUM model included information from both the source paper and its references, and enhanced the contextual relevance of summarization. The authors also built the Semantic Scholar Network (SSN) from 141K research papers and 661K citation relations with a broad view of scientific domains."
                        ],
                        "paper": {
                            "corpus_id": 277284940,
                            "title": "Global and Local Context Fusion in Heterogeneous Graph Neural Network for Summarizing Lengthy Scientific Documents",
                            "authors": [
                                {
                                    "authorId": "2257111507",
                                    "name": "Muhammad Umair"
                                },
                                {
                                    "authorId": "2149073250",
                                    "name": "Atif Khan"
                                },
                                {
                                    "authorId": "47122876",
                                    "name": "Fasee Ullah"
                                },
                                {
                                    "authorId": "2282843278",
                                    "name": "Atef Masmoudi"
                                },
                                {
                                    "authorId": "2299236773",
                                    "name": "Muhammed Faheem"
                                }
                            ],
                            "year": 2025,
                            "venue": "IEEE Access",
                            "n_citations": 1
                        },
                        "score": 0.9765625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Topic-aware Graph Models",
                "tldr": "Topic-aware graph models enhance multi-document summarization by incorporating latent topic information into graph structures, allowing better representation of document content relationships. These models typically combine neural topic modeling with graph neural networks to discover semantic themes across documents and use this information to guide the summarization process. (6 sources)",
                "text": "\nTopic-aware graph models represent a significant advancement in graph-based neural approaches for multi-document summarization by integrating topic modeling with graph neural networks. These models address a fundamental challenge in multi-document summarization: understanding the thematic relationships across documents and identifying the most relevant information based on these themes.\n\nThe GTASum (Graph-Based Topic-Aware abstract text summarization) model exemplifies this approach by constructing a heterogeneous document graph containing both sentence representation nodes and potential topic nodes <Paper corpusId=\"250012667\" paperTitle=\"(Jiang et al., 2022)\" isShortName></Paper>. This model employs a dual-process approach, simultaneously encoding documents with BERT to obtain contextual sentence representations while using Neural Topic Models (NTM) to learn potential topics within the documents <Paper corpusId=\"52182945\" paperTitle=\"(Ding et al., 2018)\" isShortName></Paper>. The sentence representations are then revised using a Graph Attention Network before being fed into a Transformer-based decoder to generate summaries <Paper corpusId=\"250012667\" paperTitle=\"(Jiang et al., 2022)\" isShortName></Paper>.\n\nSimilar approaches have been developed by other researchers, such as Huang et al., who built a sentence-level graph-based model that combines BERT for sentence encoding with joint neural topic modeling for discovering latent topic information <Paper corpusId=\"233189611\" paperTitle=\"(Huang et al., 2021)\" isShortName></Paper>. This integration helps capture global information and thematic relationships that might be missed in purely structural approaches <Paper corpusId=\"253763913\" paperTitle=\"(Umair et al., 2022)\" isShortName></Paper>.\n\nThe effectiveness of topic-aware graph models stems from their ability to address the challenge of sparsely distributed information in documents. By incorporating topic modeling, these approaches can better handle relationships between distantly-placed sentences and capture significant information that might otherwise be omitted in sentence-level models <Paper corpusId=\"266244733\" paperTitle=\"(Su et al., 2023)\" isShortName></Paper>. Topic models are particularly valuable for extracting topic-specific features and establishing probability distributions that link topics with sentence nodes, providing a more comprehensive understanding of document content <Paper corpusId=\"266244733\" paperTitle=\"(Su et al., 2023)\" isShortName></Paper>.\n\nRecent research has explored various approaches to topic integration in graph structures. Some models use K-Means and Gaussian Mixture Models for topic extraction, while others employ Neural Topic Models <Paper corpusId=\"271938110\" paperTitle=\"(Khaliq et al., 2024)\" isShortName></Paper>. Models like Topic-GraphSum, SSN-DM, HeterGraphLongSum, and GTASum all leverage graph structures to represent relationships between sentences and topics, facilitating more effective summarization <Paper corpusId=\"271938110\" paperTitle=\"(Khaliq et al., 2024)\" isShortName></Paper>.\n\nThe evolution of topic-aware graph models reflects the broader trend in text summarization research: moving from homogeneous graphs with static nodes to heterogeneous networks that can represent a wider range of textual elements and allow for dynamic updates during the summarization process <Paper corpusId=\"271938110\" paperTitle=\"(Khaliq et al., 2024)\" isShortName></Paper>. By combining the structural modeling capabilities of graph neural networks with the semantic understanding provided by topic models, these approaches offer a more comprehensive framework for addressing the complex challenges of multi-document summarization.",
                "citations": [
                    {
                        "id": "(Jiang et al., 2022)",
                        "snippets": [
                            "In this work, we suggest a novel GTASum model (Graph-Based Topic-Aware abstract text summarization). First, the document is encoded with BERT (\u67f4\u7530, 2020) (Bidirectional Encoder Representation from Transformers) to obtain contextual sentence representations; meanwhile, NTM (Ding et al., 2018) is used to learn the potential topic of the document. Second, construct a heterogeneous document graph that contains sentence representation nodes and potential topic nodes, then revise them using a adapted GAT [30]. Finally, the sentence representations containing topic information are fed into a Transformer-based decoder to generate summaries."
                        ],
                        "paper": {
                            "corpus_id": 250012667,
                            "title": "GATSum: Graph-Based Topic-Aware Abstract Text Summarization",
                            "authors": [
                                {
                                    "authorId": "2040863712",
                                    "name": "Ming Jiang"
                                },
                                {
                                    "authorId": "2034370007",
                                    "name": "Yifan Zou"
                                },
                                {
                                    "authorId": "2110980173",
                                    "name": "Jian Xu"
                                },
                                {
                                    "authorId": "2156053270",
                                    "name": "Min Zhang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Information Technology and Control",
                            "n_citations": 11
                        },
                        "score": 0.9765625
                    },
                    {
                        "id": "(Ding et al., 2018)",
                        "snippets": [
                            "Topic models are evaluated based on their ability to describe documents well (i.e. low perplexity) and to produce topics that carry coherent semantic meaning. In topic modeling so far, perplexity is a direct optimization target. However, topic coherence, owing to its challenging computation, is not optimized for and is only evaluated after training. In this work, under a neural variational inference framework, we propose methods to incorporate a topic coherence objective into the training process. We demonstrate that such a coherence-aware topic model exhibits a similar level of perplexity as baseline models but achieves substantially higher topic coherence."
                        ],
                        "paper": {
                            "corpus_id": 52182945,
                            "title": "Coherence-Aware Neural Topic Modeling",
                            "authors": [
                                {
                                    "authorId": "2058085578",
                                    "name": "Ran Ding"
                                },
                                {
                                    "authorId": "1701451",
                                    "name": "Ramesh Nallapati"
                                },
                                {
                                    "authorId": "144028698",
                                    "name": "Bing Xiang"
                                }
                            ],
                            "year": 2018,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 79
                        },
                        "score": 0
                    },
                    {
                        "id": "(Huang et al., 2021)",
                        "snippets": [
                            "Modeling the relations between text spans in a document is a crucial yet challenging problem for extractive summarization. Various kinds of relations exist among text spans of different granularity, such as discourse relations between elementary discourse units and coreference relations between phrase mentions. In this paper, we propose a heterogeneous graph based model for extractive summarization that incorporates both discourse and coreference relations. The heterogeneous graph contains three types of nodes, each corresponds to text spans of different granularity. Experimental results on a benchmark summarization dataset verify the effectiveness of our proposed method."
                        ],
                        "paper": {
                            "corpus_id": 233189611,
                            "title": "Extractive Summarization Considering Discourse and Coreference Relations based on Heterogeneous Graph",
                            "authors": [
                                {
                                    "authorId": "1796312335",
                                    "name": "Yin Jou Huang"
                                },
                                {
                                    "authorId": "1795664",
                                    "name": "S. Kurohashi"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
                            "n_citations": 28
                        },
                        "score": 0
                    },
                    {
                        "id": "(Umair et al., 2022)",
                        "snippets": [
                            "The authors in [48] presented an approach that modeled redundancy-aware heterogeneous graphs and refned sentence representation using neural networks for extractive summarization. The studies [9]56]",
                            "between sentences are learned. Te work done by (Huang et al., 2021) built a sentence-level graph-based model, using BERT for sentence encoding and joint neural network model (NTM) for discovering latent topic information. Te authors in (Bangotra et al., 2020) proposed a heterogeneous graph structure for modelling crosssentence relationship between sentences. To represent the relationships between the EDUs, they used three diferent types of nodes, including sentence nodes, EDU nodes, and entity nodes, and RST discourse parsing and leverage external discourse expertise to enhance the model's performance."
                        ],
                        "paper": {
                            "corpus_id": 253763913,
                            "title": "N-GPETS: Neural Attention Graph-Based Pretrained Statistical Model for Extractive Text Summarization",
                            "authors": [
                                {
                                    "authorId": "2257111507",
                                    "name": "Muhammad Umair"
                                },
                                {
                                    "authorId": "46487765",
                                    "name": "I. Alam"
                                },
                                {
                                    "authorId": "2149073250",
                                    "name": "Atif Khan"
                                },
                                {
                                    "authorId": "143835958",
                                    "name": "Inayat Khan"
                                },
                                {
                                    "authorId": "2175498951",
                                    "name": "Niamat Ullah"
                                },
                                {
                                    "authorId": "2191729128",
                                    "name": "Mohammad Yusuf Momand"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computational Intelligence and Neuroscience",
                            "n_citations": 3
                        },
                        "score": 0.95947265625
                    },
                    {
                        "id": "(Su et al., 2023)",
                        "snippets": [
                            "The crucial aspect of extractive document summarization lies in understanding the interrelations between sentences. Documents inherently comprise a multitude of sentences, and sentence-level models frequently fail to consider the relationships between distantly-placed sentences, resulting in the omission of significant information in the summary. Moreover, information within documents tends to be distributed sparsely, challenging the efficacy of sentence-level models. In the realm of heterogeneous graph neural networks, it has been observed that semantic nodes with varying levels of granularity encapsulate distinct semantic connections. Initially, the incorporation of edge features into the computation of dynamic graph attention networks is performed to account for node relationships. Subsequently, given the multiplicity of topics in a document or a set of documents, a topic model is employed to extract topic-specific features and the probability distribution linking these topics with sentence nodes."
                        ],
                        "paper": {
                            "corpus_id": 266244733,
                            "title": "Multi-granularity adaptive extractive document summarization with heterogeneous graph neural networks",
                            "authors": [
                                {
                                    "authorId": "2274342252",
                                    "name": "Wu Su"
                                },
                                {
                                    "authorId": "2274904693",
                                    "name": "Jin Jiang"
                                },
                                {
                                    "authorId": "2274639188",
                                    "name": "Kaihui Huang"
                                }
                            ],
                            "year": 2023,
                            "venue": "PeerJ Computer Science",
                            "n_citations": 1
                        },
                        "score": 0.97314453125
                    },
                    {
                        "id": "(Khaliq et al., 2024)",
                        "snippets": [
                            "In recent years, there have been notable achievements in applying Graph Neural Networks (GNNs) or Graph Attention Networks (GAT) [11] to summarize documents [4], [12], [13], [14], [15], [16], [17]. This success arises from their ability to capture complex inter-sentence relationships within documents. Specifically, GNN models demonstrate proficiency in representing complex structural data by encapsulating semantic units (nodes) and their interconnections (edges). Due to these capabilities, there is growing interest in leveraging the GAT framework alongside topic modeling for abstractive summarization tasks. The researchers in [12] proposed a heterogeneous neural graph structure using BERT to obtain contextual sentence representations. They simultaneously trained with latent topics using the Neural Topic Model (NTM) to model global information. The study of [18] proposed an expanded model that employed NTM for abstractive text summarization. \n\nThe authors of [15] introduced a graph structure enriched with latent topical information. They employed K-Means and Gaussian Mixture Models (GMM) for topic extraction. The study of [3] proposed an innovative Graph-Based Topic-Aware abstractive text summarization model. Initially, documents were encoded with BERT to generate sentence representations. Concurrently, NTM identified potential topics, while GAT refined these sentences and topic nodes. Finally, the sentence embeddings, enriched with topic information, were fed into a Transformer-driven decoder to generate summaries.\n\nModern text summarization research has shifted from homogeneous graphs with static nodes to heterogeneous networks. These networks allow for the inclusion of diverse node types, which represent wide range of textual elements. In addition, they allow for dynamic updates during the summarization process. The approach of [4] constructed a graph neural network based on word co-occurrences within the document to capture word-level relationships. The study in [42] utilized syntactic graph convolutional networks (GCNs) to model the non-Euclidean structure of documents.\n\nThis category includes models like Topic-GraphSum [12], SSN-DM [57] HeterGraphLongSum [14] and GTASum [3] that utilize graph structures to represent document information and relationships between sentences or topics, facilitating summarization."
                        ],
                        "paper": {
                            "corpus_id": 271938110,
                            "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
                            "authors": [
                                {
                                    "authorId": "2058964352",
                                    "name": "A. Khaliq"
                                },
                                {
                                    "authorId": "2149073250",
                                    "name": "Atif Khan"
                                },
                                {
                                    "authorId": "2316766711",
                                    "name": "Salman Afsar Awan"
                                },
                                {
                                    "authorId": "2316764100",
                                    "name": "Salman Jan"
                                },
                                {
                                    "authorId": "2257111507",
                                    "name": "Muhammad Umair"
                                },
                                {
                                    "authorId": "38509860",
                                    "name": "M. Zuhairi"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Access",
                            "n_citations": 1
                        },
                        "score": 0.9892578125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "State-of-the-art Models and Results",
                "tldr": "Recent graph-based neural models for multi-document summarization have achieved impressive performance on benchmark datasets through various architectural innovations. Key advances include integrating knowledge graphs, combining pre-trained language models with graph structures, and developing specialized encoding-decoding mechanisms. (15 sources)",
                "text": "\n- **GraphSum** has emerged as a significant baseline in graph-based neural approaches for multi-document summarization. The model leverages graph representations of documents such as similarity graphs and discourse graphs to more effectively process multiple input documents. GraphSum utilizes these graphs both during encoding to capture cross-document relations and during the generation process to guide summary production. When combined with pre-trained language models, GraphSum demonstrates substantial improvements over previous baselines on the WikiSum and MultiNews datasets <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper>.\n\n- **SACA** (Structure-Aware Cross-Attention) improves upon GraphSum by enhancing how graph structures are integrated during decoding. While GraphSum only introduces one graph attention layer in each decoder layer, SACA re-encodes the input graph by re-computing the graph representation conditioned on newly generated text at each decoding step. This allows for more dynamic incorporation of graph information throughout the generation process <Paper corpusId=\"252280335\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>.\n\n- **KGSum** centers on knowledge graphs during both encoding and decoding processes for multi-document scientific summarization. In the encoding process, two graph-based modules incorporate knowledge graph information into paper encoding. The decoding process uses a two-stage approach, first generating knowledge graph information as descriptive sentences before producing the final summary. Empirical results show that KGSum brings substantial improvements over baselines including GraphSum, PEGASUS <Paper corpusId=\"209405420\" paperTitle=\"(Zhang et al., 2019)\" isShortName></Paper>, Pointer-Generator <Paper corpusId=\"8314118\" paperTitle=\"(See et al., 2017)\" isShortName></Paper>, and BertABS <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper> on the Multi-Xscience dataset <Paper corpusId=\"252185277\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>.\n\n- **EMSum** is an entity-aware model that augments the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks. This design enables EMSum to capture cross-document information and identify relevant information among documents. The knowledge graph is constructed using extracted semantic entities through co-reference resolution tools from AllenNLP <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper> <Paper corpusId=\"236478143\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper>.\n\n- **BART-Long-Graph** combines the pre-trained BART sequence-to-sequence Transformer model <Paper corpusId=\"204960716\" paperTitle=\"(Lewis et al., 2019)\" isShortName></Paper> with Longformer's local and global attention mechanisms for encoding long texts. It integrates graphical information through a separate graph encoder. The semantic knowledge graph is constructed using AllenNLP at the document level and OpenIE at the sentence level to capture multi-level semantic information. BART-Long-Graph achieved remarkable results on benchmark datasets including Multi-News <Paper corpusId=\"174799390\" paperTitle=\"(Fabbri et al., 2019)\" isShortName></Paper> and DUC-2004, showing an average 1.8 ROUGE score improvement over previous work <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper> <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper>.\n\n- **Topic-GraphSum**, **SSN-DM**, **HeterGraphLongSum**, and **GTASum** all utilize graph structures to represent document information and relationships between sentences or topics. These models construct graphs that integrate topic information, with some using Neural Topic Models to identify potential topics and others employing techniques like K-Means and Gaussian Mixture Models for topic extraction. Graph Attention Networks are commonly used to refine sentence and topic node representations before generating summaries <Paper corpusId=\"271938110\" paperTitle=\"(Khaliq et al., 2024)\" isShortName></Paper>.\n\n- **AMR-based models** take a different approach by first generating Abstract Meaning Representation (AMR) graphs for input documents, then extracting summary graphs from these AMR graphs, and finally creating summaries from the summary graphs. This semantic representation approach provides a structured way to capture document meaning for summarization <Paper corpusId=\"248496597\" paperTitle=\"(Wang et al._1, 2022)\" isShortName></Paper>.\n\nPerformance evaluations of these models extend beyond standard ROUGE scores to examine how well each model assimilates and processes linguistic and thematic information across different document types. Comprehensive assessments also analyze performance in relation to various node configurations and graph structures used in competing models <Paper corpusId=\"271791043\" paperTitle=\"(Khaliq et al._1, 2024)\" isShortName></Paper>. While graph-based models have shown significant advantages, hybrid approaches that combine graph-based extractive modules with neural abstractive sequence-to-sequence architectures have proven particularly effective for challenging multi-document summarization tasks <Paper corpusId=\"250374734\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Li et al., 2020)",
                        "snippets": [
                            "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines."
                        ],
                        "paper": {
                            "corpus_id": 218718706,
                            "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "48624966",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "2107521158",
                                    "name": "Xinyan Xiao"
                                },
                                {
                                    "authorId": null,
                                    "name": "Jiachen Liu"
                                },
                                {
                                    "authorId": "40354707",
                                    "name": "Hua Wu"
                                },
                                {
                                    "authorId": "144270731",
                                    "name": "Haifeng Wang"
                                },
                                {
                                    "authorId": "2117218629",
                                    "name": "Junping Du"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 136
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2022)",
                        "snippets": [
                            "A recently proposed neural abstractive Multi-Document Summarization (MDS) model, Graph-Summ (Li et al., 2020), also considers the input graph structure during decoding. The biggest difference between Graphsum and our proposed SACA is that the former only introduces one graph attention layer in each decoder layer. SACA, on the other hand, injects graph structure into decoding by re-encoding the input graph. Specifically, it re-computes the input graph representation by conditioning it on the newly generated text at each decoding step."
                        ],
                        "paper": {
                            "corpus_id": 252280335,
                            "title": "Graph-to-Text Generation with Dynamic Structure Pruning",
                            "authors": [
                                {
                                    "authorId": "2154884699",
                                    "name": "Liang Li"
                                },
                                {
                                    "authorId": "9706609",
                                    "name": "Ruiying Geng"
                                },
                                {
                                    "authorId": "2132475886",
                                    "name": "Bowen Li"
                                },
                                {
                                    "authorId": "2112563315",
                                    "name": "Can Ma"
                                },
                                {
                                    "authorId": "35755264",
                                    "name": "Yinliang Yue"
                                },
                                {
                                    "authorId": "66200440",
                                    "name": "Binhua Li"
                                },
                                {
                                    "authorId": "2323761746",
                                    "name": "Yongbin Li"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 2
                        },
                        "score": 0.9609375
                    },
                    {
                        "id": "(Zhang et al., 2019)",
                        "snippets": [
                            "Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets."
                        ],
                        "paper": {
                            "corpus_id": 209405420,
                            "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization",
                            "authors": [
                                {
                                    "authorId": "47540100",
                                    "name": "Jingqing Zhang"
                                },
                                {
                                    "authorId": "2143397386",
                                    "name": "Yao Zhao"
                                },
                                {
                                    "authorId": "144413479",
                                    "name": "Mohammad Saleh"
                                },
                                {
                                    "authorId": "35025299",
                                    "name": "Peter J. Liu"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 2054
                        },
                        "score": 0
                    },
                    {
                        "id": "(See et al., 2017)",
                        "snippets": [
                            "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points."
                        ],
                        "paper": {
                            "corpus_id": 8314118,
                            "title": "Get To The Point: Summarization with Pointer-Generator Networks",
                            "authors": [
                                {
                                    "authorId": "13070498",
                                    "name": "A. See"
                                },
                                {
                                    "authorId": "35025299",
                                    "name": "Peter J. Liu"
                                },
                                {
                                    "authorId": "144783904",
                                    "name": "Christopher D. Manning"
                                }
                            ],
                            "year": 2017,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 4028
                        },
                        "score": 0
                    },
                    {
                        "id": "(Devlin et al., 2019)",
                        "snippets": [
                            "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                        ],
                        "paper": {
                            "corpus_id": 52967399,
                            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                            "authors": [
                                {
                                    "authorId": "39172707",
                                    "name": "Jacob Devlin"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                },
                                {
                                    "authorId": "2544107",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "3259253",
                                    "name": "Kristina Toutanova"
                                }
                            ],
                            "year": 2019,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 95215
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2022)",
                        "snippets": [
                            "Multi-Document Scientific Summarization (MDSS) aims to produce coherent and concise summaries for clusters of topic-relevant scientific papers. This task requires precise understanding of paper content and accurate modeling of cross-paper relationships. Knowledge graphs convey compact and interpretable structured information for documents, which makes them ideal for content modeling and relationship modeling. In this paper, we present KGSum, an MDSS model centred on knowledge graphs during both the encoding and decoding process. Specifically, in the encoding process, two graph-based modules are proposed to incorporate knowledge graph information into paper encoding, while in the decoding process, we propose a two-stage decoder by first generating knowledge graph information of summary in the form of descriptive sentences, followed by generating the final summary. Empirical results show that the proposed architecture brings substantial improvements over baselines on the Multi-Xscience dataset",
                            ".GraphSum (Li et al., 2020)) is a neural multi-document summarization model that leverages well-known graphs to produce abstractive summaries. We use TF-IDF graph as the input graph. PEGASUS (Zhang et al., 2019) is a sequence-to-sequence model with gapsentences generation as a pre-training objective tailored for abstractive summarization. Pointer-Generator (See et al., 2017) is an RNN based model with an attention mechanism and allows the system to copy words from the source via pointing for abstractive summarization. BertABS (Liu and Lapata, 2019b) uses a pretrained BERT (Devlin et al., 2019) as the encoder for abstractive summarization. We also report the performance of BertABS with an encoder (SciBertABS) pretrained on scientific articles."
                        ],
                        "paper": {
                            "corpus_id": 252185277,
                            "title": "Multi-Document Scientific Summarization from a Knowledge Graph-Centric View",
                            "authors": [
                                {
                                    "authorId": "2073437",
                                    "name": "Pancheng Wang"
                                },
                                {
                                    "authorId": "2145340498",
                                    "name": "Shasha Li"
                                },
                                {
                                    "authorId": "2116489",
                                    "name": "Kunyuan Pang"
                                },
                                {
                                    "authorId": "50670961",
                                    "name": "Liangliang He"
                                },
                                {
                                    "authorId": "2108821455",
                                    "name": "Dong Li"
                                },
                                {
                                    "authorId": "1762106",
                                    "name": "Jintao Tang"
                                },
                                {
                                    "authorId": "38972135",
                                    "name": "Ting Wang"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 15
                        },
                        "score": 0.98095703125
                    },
                    {
                        "id": "(Qu, 2024)",
                        "snippets": [
                            "Zhou et al. (Zhou et al., 2021) presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP.Pasunuru et al. (Pasunuru et al., 2021) presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News (Fabbri et al., 2019) and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model (Lewis et al., 2019) with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. (Pasunuru et al., 2021) utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features."
                        ],
                        "paper": {
                            "corpus_id": 269762702,
                            "title": "Leveraging Knowledge-aware Methodologies for Multi-document Summarization",
                            "authors": [
                                {
                                    "authorId": "2163451228",
                                    "name": "Yutong Qu"
                                }
                            ],
                            "year": 2024,
                            "venue": "The Web Conference",
                            "n_citations": 0
                        },
                        "score": 0.96484375
                    },
                    {
                        "id": "(Zhou et al., 2021)",
                        "snippets": [
                            ","
                        ],
                        "paper": {
                            "corpus_id": 236478143,
                            "title": "Entity-Aware Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Hao Zhou"
                                },
                                {
                                    "authorId": "2053308860",
                                    "name": "Weidong Ren"
                                },
                                {
                                    "authorId": "150112803",
                                    "name": "Gongshen Liu"
                                },
                                {
                                    "authorId": "153253583",
                                    "name": "Bo Su"
                                },
                                {
                                    "authorId": "143844110",
                                    "name": "Wei Lu"
                                }
                            ],
                            "year": 2021,
                            "venue": "Findings",
                            "n_citations": 28
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lewis et al., 2019)",
                        "snippets": [
                            "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance."
                        ],
                        "paper": {
                            "corpus_id": 204960716,
                            "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
                            "authors": [
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "11323179",
                                    "name": "Yinhan Liu"
                                },
                                {
                                    "authorId": "39589154",
                                    "name": "Naman Goyal"
                                },
                                {
                                    "authorId": "2320509",
                                    "name": "Marjan Ghazvininejad"
                                },
                                {
                                    "authorId": "113947684",
                                    "name": "Abdel-rahman Mohamed"
                                },
                                {
                                    "authorId": "39455775",
                                    "name": "Omer Levy"
                                },
                                {
                                    "authorId": "1759422",
                                    "name": "Veselin Stoyanov"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 10856
                        },
                        "score": 0
                    },
                    {
                        "id": "(Fabbri et al., 2019)",
                        "snippets": [
                            "Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting."
                        ],
                        "paper": {
                            "corpus_id": 174799390,
                            "title": "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
                            "authors": [
                                {
                                    "authorId": "46255971",
                                    "name": "Alexander R. Fabbri"
                                },
                                {
                                    "authorId": "46331602",
                                    "name": "Irene Li"
                                },
                                {
                                    "authorId": "2106009217",
                                    "name": "Tianwei She"
                                },
                                {
                                    "authorId": "50341789",
                                    "name": "Suyi Li"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 590
                        },
                        "score": 0
                    },
                    {
                        "id": "(Pasunuru et al., 2021)",
                        "snippets": [
                            "This paper presents an efficient graph-enhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents."
                        ],
                        "paper": {
                            "corpus_id": 235097309,
                            "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters",
                            "authors": [
                                {
                                    "authorId": "10721120",
                                    "name": "Ramakanth Pasunuru"
                                },
                                {
                                    "authorId": "2940333",
                                    "name": "Mengwen Liu"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                },
                                {
                                    "authorId": "120209444",
                                    "name": "Sujith Ravi"
                                },
                                {
                                    "authorId": "40262269",
                                    "name": "Markus Dreyer"
                                }
                            ],
                            "year": 2021,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 73
                        },
                        "score": 0
                    },
                    {
                        "id": "(Khaliq et al., 2024)",
                        "snippets": [
                            "In recent years, there have been notable achievements in applying Graph Neural Networks (GNNs) or Graph Attention Networks (GAT) [11] to summarize documents [4], [12], [13], [14], [15], [16], [17]. This success arises from their ability to capture complex inter-sentence relationships within documents. Specifically, GNN models demonstrate proficiency in representing complex structural data by encapsulating semantic units (nodes) and their interconnections (edges). Due to these capabilities, there is growing interest in leveraging the GAT framework alongside topic modeling for abstractive summarization tasks. The researchers in [12] proposed a heterogeneous neural graph structure using BERT to obtain contextual sentence representations. They simultaneously trained with latent topics using the Neural Topic Model (NTM) to model global information. The study of [18] proposed an expanded model that employed NTM for abstractive text summarization. \n\nThe authors of [15] introduced a graph structure enriched with latent topical information. They employed K-Means and Gaussian Mixture Models (GMM) for topic extraction. The study of [3] proposed an innovative Graph-Based Topic-Aware abstractive text summarization model. Initially, documents were encoded with BERT to generate sentence representations. Concurrently, NTM identified potential topics, while GAT refined these sentences and topic nodes. Finally, the sentence embeddings, enriched with topic information, were fed into a Transformer-driven decoder to generate summaries.\n\nModern text summarization research has shifted from homogeneous graphs with static nodes to heterogeneous networks. These networks allow for the inclusion of diverse node types, which represent wide range of textual elements. In addition, they allow for dynamic updates during the summarization process. The approach of [4] constructed a graph neural network based on word co-occurrences within the document to capture word-level relationships. The study in [42] utilized syntactic graph convolutional networks (GCNs) to model the non-Euclidean structure of documents.\n\nThis category includes models like Topic-GraphSum [12], SSN-DM [57] HeterGraphLongSum [14] and GTASum [3] that utilize graph structures to represent document information and relationships between sentences or topics, facilitating summarization."
                        ],
                        "paper": {
                            "corpus_id": 271938110,
                            "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization",
                            "authors": [
                                {
                                    "authorId": "2058964352",
                                    "name": "A. Khaliq"
                                },
                                {
                                    "authorId": "2149073250",
                                    "name": "Atif Khan"
                                },
                                {
                                    "authorId": "2316766711",
                                    "name": "Salman Afsar Awan"
                                },
                                {
                                    "authorId": "2316764100",
                                    "name": "Salman Jan"
                                },
                                {
                                    "authorId": "2257111507",
                                    "name": "Muhammad Umair"
                                },
                                {
                                    "authorId": "38509860",
                                    "name": "M. Zuhairi"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Access",
                            "n_citations": 1
                        },
                        "score": 0.9892578125
                    },
                    {
                        "id": "(Wang et al._1, 2022)",
                        "snippets": [
                            "Dohare et al. (2018) propose to generate abstractive summaries by first generating AMR graphs (abstract meaning representation) for corresponding input stories, extract summary graphs from the AMR graphs, and lastly create summaries from the summary graphs."
                        ],
                        "paper": {
                            "corpus_id": 248496597,
                            "title": "Large-Scale Multi-Document Summarization with Information Extraction and Compression",
                            "authors": [
                                {
                                    "authorId": "2152171283",
                                    "name": "Ning Wang"
                                },
                                {
                                    "authorId": "2140162523",
                                    "name": "Han Liu"
                                },
                                {
                                    "authorId": "1753376",
                                    "name": "D. Klabjan"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.96240234375
                    },
                    {
                        "id": "(Khaliq et al._1, 2024)",
                        "snippets": [
                            "Through systematic comparisons, proposed model is tested against well-known benchmarks and recent advances in neural graph-based models. These assessments are not just limited to performance metrics like ROUGE scores but also extend to an examination of how well each model assimilates and processes linguistic and thematic information across different document types. This research examines the performance in relation to various node configurations and graph structures used in competing models, providing a comprehensive view of the proposed model's abilities."
                        ],
                        "paper": {
                            "corpus_id": 271791043,
                            "title": "Enhanced Topic-Aware Summarization Using Statistical Graph Neural Networks",
                            "authors": [
                                {
                                    "authorId": "2058964352",
                                    "name": "A. Khaliq"
                                },
                                {
                                    "authorId": "2269634624",
                                    "name": "Salman Afsar Awan"
                                },
                                {
                                    "authorId": "2315394721",
                                    "name": "Fahad Ahmad"
                                },
                                {
                                    "authorId": "2315423888",
                                    "name": "Muhammad Azam Zia"
                                },
                                {
                                    "authorId": "2255632961",
                                    "name": "Muhammad Zafar Iqbal"
                                }
                            ],
                            "year": 2024,
                            "venue": "Computers, Materials &amp; Continua",
                            "n_citations": 2
                        },
                        "score": 0.97802734375
                    },
                    {
                        "id": "(Chen et al., 2022)",
                        "snippets": [
                            "Abstractive models based on graphs gain much attention (Yasunaga et al., 2017b;Li et al., 2020). Our model, which also combines a graph-based extractive module and neural abstractive sequence-to-sequence architecture, is a higher level summarization and focuses on the core event summarization from multiple documents, which is more challenging."
                        ],
                        "paper": {
                            "corpus_id": 250374734,
                            "title": "Two-phase Multi-document Event Summarization on Core Event Graphs",
                            "authors": [
                                {
                                    "authorId": "48354529",
                                    "name": "Zengjian Chen"
                                },
                                {
                                    "authorId": "2116315442",
                                    "name": "Jin Xu"
                                },
                                {
                                    "authorId": "145865588",
                                    "name": "M. Liao"
                                },
                                {
                                    "authorId": "2138967543",
                                    "name": "Tong Xue"
                                },
                                {
                                    "authorId": "2190820519",
                                    "name": "Kun He"
                                }
                            ],
                            "year": 2022,
                            "venue": "Journal of Artificial Intelligence Research",
                            "n_citations": 2
                        },
                        "score": 0.9775390625
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Datasets and Evaluation",
                "tldr": "Evaluation of graph-based neural approaches for multi-document summarization relies on several benchmark datasets including Multi-News, WikiSum, DUC, and Multi-Xscience. Performance is typically measured using ROUGE metrics, though more comprehensive assessments increasingly examine models' abilities to process linguistic and thematic information across different document types. (6 sources)",
                "text": "\nMulti-document summarization models are evaluated across several established benchmark datasets that vary in domain, document length, and summary characteristics. The Multi-Xscience dataset has emerged as an important benchmark for Multi-Document Scientific Summarization (MDSS), specifically designed to evaluate models' ability to process clusters of topic-relevant scientific papers and generate coherent summaries <Paper corpusId=\"252185277\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. This dataset presents particular challenges as it requires precise understanding of paper content and accurate modeling of cross-paper relationships.\n\nWikiSum and MultiNews represent other commonly used benchmarks for evaluating graph-based neural approaches. These datasets have been instrumental in demonstrating the performance improvements achieved by models like GraphSum, which leverages graph representations to capture cross-document relations <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper>. The DUC-2004 dataset, though older, continues to serve as a standard evaluation benchmark, particularly for comparing newer approaches against established baselines.\n\nThe predominant evaluation metrics for multi-document summarization remain the ROUGE scores (Recall-Oriented Understudy for Gisting Evaluation), which measure the overlap of n-grams between system-generated summaries and reference summaries. However, as the field advances, researchers are increasingly employing more comprehensive assessment approaches that go beyond simple ROUGE scores. These evaluations examine how effectively models assimilate and process linguistic and thematic information across different document types <Paper corpusId=\"271791043\" paperTitle=\"(Khaliq et al._1, 2024)\" isShortName></Paper>.\n\nComparative evaluations typically measure graph-based approaches against strong baselines including PEGASUS <Paper corpusId=\"209405420\" paperTitle=\"(Zhang et al., 2019)\" isShortName></Paper>, a sequence-to-sequence model with gap-sentences generation as a pre-training objective; Pointer-Generator <Paper corpusId=\"8314118\" paperTitle=\"(See et al., 2017)\" isShortName></Paper>, which uses an attention mechanism with the ability to copy words from source documents; and BertABS, which utilizes pre-trained BERT encoders <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>. Domain-specific variants, such as SciBertABS with encoders pre-trained on scientific articles, are also used as comparisons for specialized summarization tasks <Paper corpusId=\"252185277\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>.\n\nRecent evaluation approaches have become more nuanced, examining performance in relation to various node configurations and graph structures employed by competing models. These comprehensive assessments provide deeper insights into each model's abilities beyond simple performance metrics <Paper corpusId=\"271791043\" paperTitle=\"(Khaliq et al._1, 2024)\" isShortName></Paper>. Such thorough evaluations are particularly valuable as the field continues to advance, with researchers exploring increasingly complex graph representations and neural architectures for multi-document summarization.",
                "citations": [
                    {
                        "id": "(Wang et al., 2022)",
                        "snippets": [
                            "Multi-Document Scientific Summarization (MDSS) aims to produce coherent and concise summaries for clusters of topic-relevant scientific papers. This task requires precise understanding of paper content and accurate modeling of cross-paper relationships. Knowledge graphs convey compact and interpretable structured information for documents, which makes them ideal for content modeling and relationship modeling. In this paper, we present KGSum, an MDSS model centred on knowledge graphs during both the encoding and decoding process. Specifically, in the encoding process, two graph-based modules are proposed to incorporate knowledge graph information into paper encoding, while in the decoding process, we propose a two-stage decoder by first generating knowledge graph information of summary in the form of descriptive sentences, followed by generating the final summary. Empirical results show that the proposed architecture brings substantial improvements over baselines on the Multi-Xscience dataset",
                            ".GraphSum (Li et al., 2020)) is a neural multi-document summarization model that leverages well-known graphs to produce abstractive summaries. We use TF-IDF graph as the input graph. PEGASUS (Zhang et al., 2019) is a sequence-to-sequence model with gapsentences generation as a pre-training objective tailored for abstractive summarization. Pointer-Generator (See et al., 2017) is an RNN based model with an attention mechanism and allows the system to copy words from the source via pointing for abstractive summarization. BertABS (Liu and Lapata, 2019b) uses a pretrained BERT (Devlin et al., 2019) as the encoder for abstractive summarization. We also report the performance of BertABS with an encoder (SciBertABS) pretrained on scientific articles."
                        ],
                        "paper": {
                            "corpus_id": 252185277,
                            "title": "Multi-Document Scientific Summarization from a Knowledge Graph-Centric View",
                            "authors": [
                                {
                                    "authorId": "2073437",
                                    "name": "Pancheng Wang"
                                },
                                {
                                    "authorId": "2145340498",
                                    "name": "Shasha Li"
                                },
                                {
                                    "authorId": "2116489",
                                    "name": "Kunyuan Pang"
                                },
                                {
                                    "authorId": "50670961",
                                    "name": "Liangliang He"
                                },
                                {
                                    "authorId": "2108821455",
                                    "name": "Dong Li"
                                },
                                {
                                    "authorId": "1762106",
                                    "name": "Jintao Tang"
                                },
                                {
                                    "authorId": "38972135",
                                    "name": "Ting Wang"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 15
                        },
                        "score": 0.98095703125
                    },
                    {
                        "id": "(Li et al., 2020)",
                        "snippets": [
                            "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines."
                        ],
                        "paper": {
                            "corpus_id": 218718706,
                            "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization",
                            "authors": [
                                {
                                    "authorId": "48624966",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "2107521158",
                                    "name": "Xinyan Xiao"
                                },
                                {
                                    "authorId": null,
                                    "name": "Jiachen Liu"
                                },
                                {
                                    "authorId": "40354707",
                                    "name": "Hua Wu"
                                },
                                {
                                    "authorId": "144270731",
                                    "name": "Haifeng Wang"
                                },
                                {
                                    "authorId": "2117218629",
                                    "name": "Junping Du"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 136
                        },
                        "score": 0
                    },
                    {
                        "id": "(Khaliq et al._1, 2024)",
                        "snippets": [
                            "Through systematic comparisons, proposed model is tested against well-known benchmarks and recent advances in neural graph-based models. These assessments are not just limited to performance metrics like ROUGE scores but also extend to an examination of how well each model assimilates and processes linguistic and thematic information across different document types. This research examines the performance in relation to various node configurations and graph structures used in competing models, providing a comprehensive view of the proposed model's abilities."
                        ],
                        "paper": {
                            "corpus_id": 271791043,
                            "title": "Enhanced Topic-Aware Summarization Using Statistical Graph Neural Networks",
                            "authors": [
                                {
                                    "authorId": "2058964352",
                                    "name": "A. Khaliq"
                                },
                                {
                                    "authorId": "2269634624",
                                    "name": "Salman Afsar Awan"
                                },
                                {
                                    "authorId": "2315394721",
                                    "name": "Fahad Ahmad"
                                },
                                {
                                    "authorId": "2315423888",
                                    "name": "Muhammad Azam Zia"
                                },
                                {
                                    "authorId": "2255632961",
                                    "name": "Muhammad Zafar Iqbal"
                                }
                            ],
                            "year": 2024,
                            "venue": "Computers, Materials &amp; Continua",
                            "n_citations": 2
                        },
                        "score": 0.97802734375
                    },
                    {
                        "id": "(Zhang et al., 2019)",
                        "snippets": [
                            "Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets."
                        ],
                        "paper": {
                            "corpus_id": 209405420,
                            "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization",
                            "authors": [
                                {
                                    "authorId": "47540100",
                                    "name": "Jingqing Zhang"
                                },
                                {
                                    "authorId": "2143397386",
                                    "name": "Yao Zhao"
                                },
                                {
                                    "authorId": "144413479",
                                    "name": "Mohammad Saleh"
                                },
                                {
                                    "authorId": "35025299",
                                    "name": "Peter J. Liu"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 2054
                        },
                        "score": 0
                    },
                    {
                        "id": "(See et al., 2017)",
                        "snippets": [
                            "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points."
                        ],
                        "paper": {
                            "corpus_id": 8314118,
                            "title": "Get To The Point: Summarization with Pointer-Generator Networks",
                            "authors": [
                                {
                                    "authorId": "13070498",
                                    "name": "A. See"
                                },
                                {
                                    "authorId": "35025299",
                                    "name": "Peter J. Liu"
                                },
                                {
                                    "authorId": "144783904",
                                    "name": "Christopher D. Manning"
                                }
                            ],
                            "year": 2017,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 4028
                        },
                        "score": 0
                    },
                    {
                        "id": "(Devlin et al., 2019)",
                        "snippets": [
                            "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                        ],
                        "paper": {
                            "corpus_id": 52967399,
                            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                            "authors": [
                                {
                                    "authorId": "39172707",
                                    "name": "Jacob Devlin"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                },
                                {
                                    "authorId": "2544107",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "3259253",
                                    "name": "Kristina Toutanova"
                                }
                            ],
                            "year": 2019,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 95215
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.252543
    }
}