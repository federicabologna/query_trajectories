{
    "query": "What are the most effective and recent techniques for measuring data difficulty\u2014such as cross-entropy, model uncertainty, and learning percentage\u2014for constructing curricula in large language model training?",
    "user_id": "lib_user",
    "task_id": "15918c79-f566-4c1e-91c6-62c345ecfeb5",
    "timestamp": "2025-06-24T00:38:43.791047",
    "n_retrieval": 256,
    "n_retrieved": 255,
    "n_candidates": 37,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.562737,
    "decomposed_query": {
        "rewritten_query": "Most effective techniques for measuring data difficulty (cross-entropy, model uncertainty, learning percentage) for constructing curricula in large language model training.",
        "keyword_query": "measuring data difficulty cross-entropy model uncertainty learning percentage curricula large language model training",
        "search_filters": {
            "year": "2022-2025",
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010017,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Exploring the Learning Difficulty of Data: Theory and Measure",
            "venue": "ACM Transactions on Knowledge Discovery from Data",
            "year": 2022,
            "reference_count": 84,
            "citation_count": 6,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2205.07427",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.07427, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2152350103",
                    "name": "Weiyao Zhu"
                },
                {
                    "authorId": "2061463125",
                    "name": "Ou Wu"
                },
                {
                    "authorId": "2165378509",
                    "name": "Fengguang Su"
                },
                {
                    "authorId": "2165450358",
                    "name": "Yingjun Deng"
                }
            ],
            "abstract": "\u2018\u2018Easy/hard sample\u201d is a popular parlance in machine learning. Learning difficulty of samples refers to how easy/hard a sample is during a learning procedure. An increasing need of measuring learning difficulty demonstrates its importance in machine learning (e.g., difficulty-based weighting learning strategies). Previous literature has proposed a number of learning difficulty measures. However, no comprehensive investigation for learning difficulty is available to date, resulting in that nearly all existing measures are heuristically defined without a rigorous theoretical foundation. This study attempts to conduct a pilot theoretical study for learning difficulty of samples. First, influential factors for learning difficulty are summarized. Under various situations conducted by summarized influential factors, correlations between learning difficulty and two vital criteria of machine learning, namely, generalization error and model complexity, are revealed. Second, a theoretical definition of learning difficulty is proposed on the basis of these two criteria. A practical measure of learning difficulty is proposed under the direction of the theoretical definition by importing the bias-variance trade-off theory. Subsequently, the rationality of theoretical definition and the practical measure are demonstrated, respectively, by analysis of several classical weighting methods and abundant experiments realized under all situations conducted by summarized influential factors. The mentioned weighting methods can be reasonably explained under the proposed theoretical definition and concerned propositions. The comparison in these experiments indicates that the proposed measure significantly outperforms the other measures throughout the experiments.",
            "corpus_id": 248810776,
            "sentences": [
                {
                    "corpus_id": "248810776",
                    "title": "Exploring the Learning Difficulty of Data: Theory and Measure",
                    "text": "Learning difficulty is considered as an intrinsic property of data in machine learning [46,75]. Existing measurements are usually based on heuristic cues or inspirations, and they can be divided into the following main categories: \n\n\u2022 Loss-based measurement. This category directly uses the loss as the measure. Most measures fall into this category because it is simple yet effective in various learning tasks. Some methods [68] directly utilize the loss in one epoch as the degree of difficulty. Accordingly, the degrees for the same samples vary in different epochs. Some others utilize the average loss [45] during the partial or whole training procedure for measurement. \u2022 Cross-validation-based measurement. This category adopts a crossvalidation strategy [65]. For example, five-fold cross-validation is performed, and the whole cross-validation is repeated ten times. Consequently, each ELDD training sample receives ten predictions. The value of error predictions is used as the indicator of difficulty. \u2022 Uncertainty-based measurement. This category uses the (model) uncertainty of a sample to measure the difficulty. Aguilar et al. [3] identified hard samples based on the epistemic uncertainty (also known as the model uncertainty). They leveraged the Bayesian Neural Network [67] to infer the epistemic uncertainty. \u2022 Margin-based measurement. This category uses the margin (distance) of a sample to the underlying decision surface as the measurement. The rationale is that a small margin denotes a large difficulty [42,68]. \u2022 Gradient-based measurement. This category uses the loss gradient of a sample to measure the difficulty. Agarwal and Hooker [2] proposed the variance of gradients (VOG) across different epochs to rank data from difficult to easy. They considered that samples with high VOG values are far more difficult for the model to learn. Santiago et al. [54] applied the norm of the gradients to measure the difficulty, and high norms indicate large difficulty for learning. \n\nThe above-mentioned categories are highly correlated. For example, margin-based measurement is indeed a loss-based one when margin-based loss (e.g., hinge loss) is used.",
                    "score": 0.562215273814821,
                    "section_title": "Learning Difficulty Measurement",
                    "char_start_offset": 5000,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 95
                        },
                        {
                            "start": 96,
                            "end": 230
                        },
                        {
                            "start": 233,
                            "end": 258
                        },
                        {
                            "start": 259,
                            "end": 311
                        },
                        {
                            "start": 312,
                            "end": 411
                        },
                        {
                            "start": 412,
                            "end": 497
                        },
                        {
                            "start": 498,
                            "end": 569
                        },
                        {
                            "start": 570,
                            "end": 675
                        },
                        {
                            "start": 676,
                            "end": 713
                        },
                        {
                            "start": 714,
                            "end": 767
                        },
                        {
                            "start": 768,
                            "end": 875
                        },
                        {
                            "start": 876,
                            "end": 941
                        },
                        {
                            "start": 942,
                            "end": 1012
                        },
                        {
                            "start": 1013,
                            "end": 1045
                        },
                        {
                            "start": 1046,
                            "end": 1127
                        },
                        {
                            "start": 1128,
                            "end": 1244
                        },
                        {
                            "start": 1245,
                            "end": 1328
                        },
                        {
                            "start": 1329,
                            "end": 1356
                        },
                        {
                            "start": 1357,
                            "end": 1464
                        },
                        {
                            "start": 1465,
                            "end": 1537
                        },
                        {
                            "start": 1538,
                            "end": 1567
                        },
                        {
                            "start": 1568,
                            "end": 1643
                        },
                        {
                            "start": 1644,
                            "end": 1768
                        },
                        {
                            "start": 1769,
                            "end": 1865
                        },
                        {
                            "start": 1866,
                            "end": 2002
                        },
                        {
                            "start": 2005,
                            "end": 2058
                        },
                        {
                            "start": 2059,
                            "end": 2174
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 425,
                            "end": 429,
                            "matchedPaperCorpusId": "231807280"
                        },
                        {
                            "start": 762,
                            "end": 766,
                            "matchedPaperCorpusId": "202540591"
                        },
                        {
                            "start": 1143,
                            "end": 1146,
                            "matchedPaperCorpusId": "215414127"
                        },
                        {
                            "start": 1288,
                            "end": 1292,
                            "matchedPaperCorpusId": "53715584"
                        },
                        {
                            "start": 1533,
                            "end": 1536,
                            "matchedPaperCorpusId": "231807280"
                        },
                        {
                            "start": 1663,
                            "end": 1666,
                            "matchedPaperCorpusId": "221319579"
                        },
                        {
                            "start": 1882,
                            "end": 1886,
                            "matchedPaperCorpusId": "225499092"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89306640625
                },
                {
                    "corpus_id": "248810776",
                    "title": "Exploring the Learning Difficulty of Data: Theory and Measure",
                    "text": "The partition of training data into different subsets according to their learning difficulties and adoption of separate learning schemes (e.g., weighting) are proven to be useful in many learning tasks [21,43,46,68]. The learning difficulty investigated in this study refers to the degrees of easy or hard to learn of training samples in a given learning task. Although learning difficulty has no formal and consensus definition, it has been widely discussed and utilized in previous machine learning literature, including noise-aware, curriculum, and metric learning. \n\nNumerous methods are proposed to measure the learning difficulty of a training sample. The most common practice is to leverage the training output (e.g., loss and the predicted value on the true category) of a sample to construct the measurements. In Self-paced Learning (SPL) [26,68], the training loss is used to determine whether a sample is easy or not, and easy samples are first learned. We assume that p i,yi is the prediction on the ground-truth category for a training sample x i . In object detection, the value of (1 \u2212 p i,yi ) is used to indicate the learning difficulty for x i [43] . Given that the training output in an epoch may be unreliable, some methods utilize the average training output of a sample during the training to measure the difficulty. Huang et al. [33] designed a cyclic training procedure, and the model is trained from underfitting to over-fitting in one cycle. The average training loss in the whole cyclic procedure is used as the noisy indicator for a training sample. Feng et al. [20] utilized the magnitude of the loss gradient to measure the learning difficulty of a training sample. A large gradient magnitude indicates a high degree of difficulty. \n\nDue to lack of a theoretical basis, different learning difficulty measures are based on different heuristic cues or empirical observations, resulting that each measure usually only suits specific application scenarios. A clearer understanding of the essence of a sample's learning difficulty can at least facilitate explaining difficulty-based weighting methods and designing more effective learning difficulty measures. However, we are still far from concluding that we have a comprehensive understanding of learning difficulty: \n\n(1) There is no formal definition of the learning difficulty of a sample. Different studies exhibit different understandings of learning difficulty.",
                    "score": 0.45111663388989065,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 216
                        },
                        {
                            "start": 217,
                            "end": 360
                        },
                        {
                            "start": 361,
                            "end": 568
                        },
                        {
                            "start": 571,
                            "end": 657
                        },
                        {
                            "start": 658,
                            "end": 818
                        },
                        {
                            "start": 819,
                            "end": 964
                        },
                        {
                            "start": 965,
                            "end": 1061
                        },
                        {
                            "start": 1062,
                            "end": 1168
                        },
                        {
                            "start": 1169,
                            "end": 1338
                        },
                        {
                            "start": 1339,
                            "end": 1467
                        },
                        {
                            "start": 1468,
                            "end": 1577
                        },
                        {
                            "start": 1578,
                            "end": 1695
                        },
                        {
                            "start": 1696,
                            "end": 1761
                        },
                        {
                            "start": 1764,
                            "end": 1982
                        },
                        {
                            "start": 1983,
                            "end": 2184
                        },
                        {
                            "start": 2185,
                            "end": 2293
                        },
                        {
                            "start": 2296,
                            "end": 2369
                        },
                        {
                            "start": 2370,
                            "end": 2444
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 202,
                            "end": 206,
                            "matchedPaperCorpusId": "6644398"
                        },
                        {
                            "start": 206,
                            "end": 209,
                            "matchedPaperCorpusId": "47252984"
                        },
                        {
                            "start": 212,
                            "end": 215,
                            "matchedPaperCorpusId": "231807280"
                        },
                        {
                            "start": 848,
                            "end": 852,
                            "matchedPaperCorpusId": "52902973"
                        },
                        {
                            "start": 852,
                            "end": 855,
                            "matchedPaperCorpusId": "231807280"
                        },
                        {
                            "start": 1162,
                            "end": 1166,
                            "matchedPaperCorpusId": "47252984"
                        },
                        {
                            "start": 1352,
                            "end": 1356,
                            "matchedPaperCorpusId": "207995300"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.73583984375
                }
            ],
            "relevance_judgement": 0.89306640625,
            "relevance_judgment_input_expanded": "# Title: Exploring the Learning Difficulty of Data: Theory and Measure\n# Venue: ACM Transactions on Knowledge Discovery from Data\n# Authors: Weiyao Zhu, Ou Wu, Fengguang Su, Yingjun Deng\n## Abstract\n\u2018\u2018Easy/hard sample\u201d is a popular parlance in machine learning. Learning difficulty of samples refers to how easy/hard a sample is during a learning procedure. An increasing need of measuring learning difficulty demonstrates its importance in machine learning (e.g., difficulty-based weighting learning strategies). Previous literature has proposed a number of learning difficulty measures. However, no comprehensive investigation for learning difficulty is available to date, resulting in that nearly all existing measures are heuristically defined without a rigorous theoretical foundation. This study attempts to conduct a pilot theoretical study for learning difficulty of samples. First, influential factors for learning difficulty are summarized. Under various situations conducted by summarized influential factors, correlations between learning difficulty and two vital criteria of machine learning, namely, generalization error and model complexity, are revealed. Second, a theoretical definition of learning difficulty is proposed on the basis of these two criteria. A practical measure of learning difficulty is proposed under the direction of the theoretical definition by importing the bias-variance trade-off theory. Subsequently, the rationality of theoretical definition and the practical measure are demonstrated, respectively, by analysis of several classical weighting methods and abundant experiments realized under all situations conducted by summarized influential factors. The mentioned weighting methods can be reasonably explained under the proposed theoretical definition and concerned propositions. The comparison in these experiments indicates that the proposed measure significantly outperforms the other measures throughout the experiments.\n## Introduction\nThe partition of training data into different subsets according to their learning difficulties and adoption of separate learning schemes (e.g., weighting) are proven to be useful in many learning tasks [21,43,46,68]. The learning difficulty investigated in this study refers to the degrees of easy or hard to learn of training samples in a given learning task. Although learning difficulty has no formal and consensus definition, it has been widely discussed and utilized in previous machine learning literature, including noise-aware, curriculum, and metric learning. \n\nNumerous methods are proposed to measure the learning difficulty of a training sample. The most common practice is to leverage the training output (e.g., loss and the predicted value on the true category) of a sample to construct the measurements. In Self-paced Learning (SPL) [26,68], the training loss is used to determine whether a sample is easy or not, and easy samples are first learned. We assume that p i,yi is the prediction on the ground-truth category for a training sample x i . In object detection, the value of (1 \u2212 p i,yi ) is used to indicate the learning difficulty for x i [43] . Given that the training output in an epoch may be unreliable, some methods utilize the average training output of a sample during the training to measure the difficulty. Huang et al. [33] designed a cyclic training procedure, and the model is trained from underfitting to over-fitting in one cycle. The average training loss in the whole cyclic procedure is used as the noisy indicator for a training sample. Feng et al. [20] utilized the magnitude of the loss gradient to measure the learning difficulty of a training sample. A large gradient magnitude indicates a high degree of difficulty. \n\nDue to lack of a theoretical basis, different learning difficulty measures are based on different heuristic cues or empirical observations, resulting that each measure usually only suits specific application scenarios. A clearer understanding of the essence of a sample's learning difficulty can at least facilitate explaining difficulty-based weighting methods and designing more effective learning difficulty measures. However, we are still far from concluding that we have a comprehensive understanding of learning difficulty: \n\n(1) There is no formal definition of the learning difficulty of a sample. Different studies exhibit different understandings of learning difficulty.\n\n## Learning Difficulty Measurement\nLearning difficulty is considered as an intrinsic property of data in machine learning [46,75]. Existing measurements are usually based on heuristic cues or inspirations, and they can be divided into the following main categories: \n\n\u2022 Loss-based measurement. This category directly uses the loss as the measure. Most measures fall into this category because it is simple yet effective in various learning tasks. Some methods [68] directly utilize the loss in one epoch as the degree of difficulty. Accordingly, the degrees for the same samples vary in different epochs. Some others utilize the average loss [45] during the partial or whole training procedure for measurement. \u2022 Cross-validation-based measurement. This category adopts a crossvalidation strategy [65]. For example, five-fold cross-validation is performed, and the whole cross-validation is repeated ten times. Consequently, each ELDD training sample receives ten predictions. The value of error predictions is used as the indicator of difficulty. \u2022 Uncertainty-based measurement. This category uses the (model) uncertainty of a sample to measure the difficulty. Aguilar et al. [3] identified hard samples based on the epistemic uncertainty (also known as the model uncertainty). They leveraged the Bayesian Neural Network [67] to infer the epistemic uncertainty. \u2022 Margin-based measurement. This category uses the margin (distance) of a sample to the underlying decision surface as the measurement. The rationale is that a small margin denotes a large difficulty [42,68]. \u2022 Gradient-based measurement. This category uses the loss gradient of a sample to measure the difficulty. Agarwal and Hooker [2] proposed the variance of gradients (VOG) across different epochs to rank data from difficult to easy. They considered that samples with high VOG values are far more difficult for the model to learn. Santiago et al. [54] applied the norm of the gradients to measure the difficulty, and high norms indicate large difficulty for learning. \n\nThe above-mentioned categories are highly correlated. For example, margin-based measurement is indeed a loss-based one when margin-based loss (e.g., hinge loss) is used.",
            "reference_string": "[248810776 | Zhu et al. | 2022 | Citations: 6]"
        },
        {
            "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 24,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.07490, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2301165169",
                    "name": "Jisu Kim"
                },
                {
                    "authorId": "2301167177",
                    "name": "Juhwan Lee"
                }
            ],
            "abstract": "The rapid advancement of Large Language Models (LLMs) has improved text understanding and generation but poses challenges in computational resources. This study proposes a curriculum learning-inspired, data-centric training strategy that begins with simpler tasks and progresses to more complex ones, using criteria such as prompt length, attention scores, and loss values to structure the training data. Experiments with Mistral-7B (Jiang et al., 2023) and Gemma-7B (Team et al., 2024) models demonstrate that curriculum learning slightly improves performance compared to traditional random data shuffling. Notably, we observed that sorting data based on our proposed attention criteria generally led to better performance. This approach offers a sustainable method to enhance LLM performance without increasing model size or dataset volume, addressing scalability challenges in LLM training.",
            "corpus_id": 269756933,
            "sentences": [
                {
                    "corpus_id": "269756933",
                    "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
                    "text": "Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021;Nagatsuka et al., 2023).However, these metrics may not fully reflect the complexity of a dataset.It is essential to assess data difficulty from the model's perspective, rather than relying solely on data-specific metrics.Our research proposes a new approach to calculate the degree of difficulty based on a model-centric perspective.By organizing the training dataset according to difficulty using our new metric, we aim to improve the model's performance compared to random shuffling.\n\n3 Methods for Quantitative Difficulty Measurement\n\nIn this section, we introduce a novel methodology for training models that begins with easier tasks and methodically progresses to more challenging ones.This approach requires organizing data by its level of complexity, for which we have established three principal criteria.By arranging the data in an order that goes from less to more difficult, this approach establishes a structured progression for learning.",
                    "score": 0.4185848365485682,
                    "section_title": "Related Work",
                    "char_start_offset": 5440,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 164
                        },
                        {
                            "start": 164,
                            "end": 237
                        },
                        {
                            "start": 237,
                            "end": 361
                        },
                        {
                            "start": 361,
                            "end": 473
                        },
                        {
                            "start": 473,
                            "end": 625
                        },
                        {
                            "start": 627,
                            "end": 676
                        },
                        {
                            "start": 678,
                            "end": 831
                        },
                        {
                            "start": 831,
                            "end": 953
                        },
                        {
                            "start": 953,
                            "end": 1090
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 140,
                            "end": 163,
                            "matchedPaperCorpusId": "255221201"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88037109375
                },
                {
                    "corpus_id": "269756933",
                    "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
                    "text": "A high loss signifies a substantial discrepancy between the predicted and actual outcomes, indicating that the model perceives the data as challenging.\n\nWe establish a direct correlation between high loss values and increased difficulty.Therefore, prompts that generate higher loss are considered more challenging for the model to learn.This method computes the cross-entropy loss associated with the answer segment of the prompt.",
                    "score": 0.3058360598962842,
                    "section_title": "Loss",
                    "char_start_offset": 8187,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 151
                        },
                        {
                            "start": 153,
                            "end": 237
                        },
                        {
                            "start": 237,
                            "end": 337
                        },
                        {
                            "start": 337,
                            "end": 430
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83935546875
                },
                {
                    "corpus_id": "269756933",
                    "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
                    "text": "Curriculum learning, introduced by Bengio et al. in 2009(Bengio et al., 2009), is a learning strategy where models are trained on data from simple to complex, mirroring human learning processes.This approach enhances model performance by establishing a foundational understanding before introducing more complex data.It has been widely applied in natural language processing (NLP) and computer vision.(Guo et al., 2018;Antonios Platanios et al., 2019;Tay et al., 2019).\n\nIn NLP, curriculum learning has proven to be effective by organizing the training sequence to begin with simpler texts and progressively transition to more complex ones.The complexity of these texts is assessed based on various criteria, including text length, the rarity of vocabulary, and edit distance.This structured approach has successfully demonstrated the efficacy of curriculum learning in the training of Pretrained Language Models (PLMs) (Chang et al., 2021;Nagatsuka et al., 2023).\n\nRecent advancements in large-scale language and multimodal models have highlighted the effective integration of curriculum learning with cutting-edge techniques, significantly boosting performance across various applications.Notably, projects such as WavLLM (Hu et al., 2024) and AutoWebGLM (Lai et al., 2024) showcase the potential of these integrations, tailoring the learning process to gradually introduce more complex tasks, thereby enhancing model robustness and application specificity.Alongside these innovations, further research has validated the effectiveness of curriculum learning in instruction tuning for language models.This approach not only systematically improves models' abilities to handle progressively challenging tasks but also optimizes their learning trajectory for better performance in specialized tasks.By methodically structuring the training process, these initiatives have set a new standard in the development and refinement of AI models, marking a significant step forward in their evolution.\n\nIn the development of training data for curricu-lum learning, accurately measuring data difficulty is crucial.Specifically, for LLMs (Large Language Models), determining the degree of data difficulty is challenging.Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021;Nagatsuka et al., 2023).",
                    "score": 0.43745283469435003,
                    "section_title": "Related Work",
                    "char_start_offset": 3231,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 194
                        },
                        {
                            "start": 194,
                            "end": 317
                        },
                        {
                            "start": 317,
                            "end": 401
                        },
                        {
                            "start": 401,
                            "end": 469
                        },
                        {
                            "start": 471,
                            "end": 640
                        },
                        {
                            "start": 640,
                            "end": 776
                        },
                        {
                            "start": 776,
                            "end": 964
                        },
                        {
                            "start": 966,
                            "end": 1191
                        },
                        {
                            "start": 1191,
                            "end": 1459
                        },
                        {
                            "start": 1459,
                            "end": 1602
                        },
                        {
                            "start": 1602,
                            "end": 1798
                        },
                        {
                            "start": 1798,
                            "end": 1992
                        },
                        {
                            "start": 1994,
                            "end": 2104
                        },
                        {
                            "start": 2104,
                            "end": 2209
                        },
                        {
                            "start": 2209,
                            "end": 2373
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 35,
                            "end": 56,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 56,
                            "end": 77,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 401,
                            "end": 419,
                            "matchedPaperCorpusId": "51920640"
                        },
                        {
                            "start": 940,
                            "end": 963,
                            "matchedPaperCorpusId": "255221201"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82275390625
                }
            ],
            "relevance_judgement": 0.88037109375,
            "relevance_judgment_input_expanded": "# Title: Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning\n# Venue: arXiv.org\n# Authors: Jisu Kim, Juhwan Lee\n## Abstract\nThe rapid advancement of Large Language Models (LLMs) has improved text understanding and generation but poses challenges in computational resources. This study proposes a curriculum learning-inspired, data-centric training strategy that begins with simpler tasks and progresses to more complex ones, using criteria such as prompt length, attention scores, and loss values to structure the training data. Experiments with Mistral-7B (Jiang et al., 2023) and Gemma-7B (Team et al., 2024) models demonstrate that curriculum learning slightly improves performance compared to traditional random data shuffling. Notably, we observed that sorting data based on our proposed attention criteria generally led to better performance. This approach offers a sustainable method to enhance LLM performance without increasing model size or dataset volume, addressing scalability challenges in LLM training.\n## Related Work\nCurriculum learning, introduced by Bengio et al. in 2009(Bengio et al., 2009), is a learning strategy where models are trained on data from simple to complex, mirroring human learning processes.This approach enhances model performance by establishing a foundational understanding before introducing more complex data.It has been widely applied in natural language processing (NLP) and computer vision.(Guo et al., 2018;Antonios Platanios et al., 2019;Tay et al., 2019).\n\nIn NLP, curriculum learning has proven to be effective by organizing the training sequence to begin with simpler texts and progressively transition to more complex ones.The complexity of these texts is assessed based on various criteria, including text length, the rarity of vocabulary, and edit distance.This structured approach has successfully demonstrated the efficacy of curriculum learning in the training of Pretrained Language Models (PLMs) (Chang et al., 2021;Nagatsuka et al., 2023).\n\nRecent advancements in large-scale language and multimodal models have highlighted the effective integration of curriculum learning with cutting-edge techniques, significantly boosting performance across various applications.Notably, projects such as WavLLM (Hu et al., 2024) and AutoWebGLM (Lai et al., 2024) showcase the potential of these integrations, tailoring the learning process to gradually introduce more complex tasks, thereby enhancing model robustness and application specificity.Alongside these innovations, further research has validated the effectiveness of curriculum learning in instruction tuning for language models.This approach not only systematically improves models' abilities to handle progressively challenging tasks but also optimizes their learning trajectory for better performance in specialized tasks.By methodically structuring the training process, these initiatives have set a new standard in the development and refinement of AI models, marking a significant step forward in their evolution.\n\nIn the development of training data for curricu-lum learning, accurately measuring data difficulty is crucial.Specifically, for LLMs (Large Language Models), determining the degree of data difficulty is challenging.Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021;Nagatsuka et al., 2023).\n...\nTraditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021;Nagatsuka et al., 2023).However, these metrics may not fully reflect the complexity of a dataset.It is essential to assess data difficulty from the model's perspective, rather than relying solely on data-specific metrics.Our research proposes a new approach to calculate the degree of difficulty based on a model-centric perspective.By organizing the training dataset according to difficulty using our new metric, we aim to improve the model's performance compared to random shuffling.\n\n3 Methods for Quantitative Difficulty Measurement\n\nIn this section, we introduce a novel methodology for training models that begins with easier tasks and methodically progresses to more challenging ones.This approach requires organizing data by its level of complexity, for which we have established three principal criteria.By arranging the data in an order that goes from less to more difficult, this approach establishes a structured progression for learning.\n\n## Loss\nA high loss signifies a substantial discrepancy between the predicted and actual outcomes, indicating that the model perceives the data as challenging.\n\nWe establish a direct correlation between high loss values and increased difficulty.Therefore, prompts that generate higher loss are considered more challenging for the model to learn.This method computes the cross-entropy loss associated with the answer segment of the prompt.",
            "reference_string": "[269756933 | Kim et al. | 2024 | Citations: 10]"
        },
        {
            "title": "LeRaC: Learning Rate Curriculum",
            "venue": "International Journal of Computer Vision",
            "year": 2022,
            "reference_count": 78,
            "citation_count": 9,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2205.09180",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.09180, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2154573729",
                    "name": "Florinel-Alin Croitoru"
                },
                {
                    "authorId": "103931166",
                    "name": "Nicolae-C\u0103t\u0103lin Ristea"
                },
                {
                    "authorId": "1817759",
                    "name": "Radu Tudor Ionescu"
                },
                {
                    "authorId": "1703601",
                    "name": "N. Sebe"
                }
            ],
            "abstract": "Most curriculum learning methods require an approach to sort the data samples by difficulty, which is often cumbersome to perform. In this work, we propose a novel curriculum learning approach termed Learning Rate Curriculum (LeRaC), which leverages the use of a different learning rate for each layer of a neural network to create a data-agnostic curriculum during the initial training epochs. More specifically, LeRaC assigns higher learning rates to neural layers closer to the input, gradually decreasing the learning rates as the layers are placed farther away from the input. The learning rates increase at various paces during the first training iterations, until they all reach the same value. From this point on, the neural model is trained as usual. This creates a model-level curriculum learning strategy that does not require sorting the examples by difficulty and is compatible with any neural network, generating higher performance levels regardless of the architecture. We conduct comprehensive experiments on 12 data sets from the computer vision (CIFAR-10, CIFAR-100, Tiny ImageNet, ImageNet-1K, Food-101, UTKFace, PASCAL VOC), language (BoolQ, QNLI, RTE) and audio (ESC-50, CREMA-D) domains, considering various convolutional (ResNet-18, Wide-ResNet-50, DenseNet-121, YOLOv5), recurrent (LSTM) and transformer (CvT, BERT, SepTr) architectures. We compare our approach with the conventional training regime, as well as with Curriculum by Smoothing (CBS), a state-of-the-art data-agnostic curriculum learning approach. Unlike CBS, our performance improvements over the standard training regime are consistent across all data sets and models. Furthermore, we significantly surpass CBS in terms of training time (there is no additional cost over the standard training regime for LeRaC). Our code is freely available at: https://github.com/CroitoruAlin/LeRaC.",
            "corpus_id": 248887633,
            "sentences": [
                {
                    "corpus_id": "248887633",
                    "title": "LeRaC: Learning Rate Curriculum",
                    "text": "Curriculum learning was initially introduced by Bengio et al. [1] as a training strategy that helps machine learning models to generalize better when the training examples are presented in the ascending order of their difficulty. Extensive surveys on curriculum learning methods, including the most recent advancements on the topic, were conducted by Soviany et al. [2] and Wang et al. [4]. In the former survey, Soviany et al. [2] emphasized that curriculum learning is not only applied at the data level, but also with respect to the other components involved in a machine learning approach, namely at the model level, the task level and the objective (loss) level. Regardless of the component on which curriculum learning is applied, the technique has demonstrated its effectiveness on a broad range of machine learning tasks, from computer vision [1,7,[27][28][29][30][31][32][33][34] to natural language processing [1,[35][36][37][38] and audio processing [39,40]. \n\nThe main challenge for the methods that build the curriculum at the data level is measuring the difficulty of the data samples, which is required to order the samples from easy to hard. Most studies have addressed the problem with human input [41][42][43] or metrics based on domain-specific heuristics. For instance, the text length [36,[44][45][46] and the word frequency [1,38] have been employed in natural language processing. In computer vision, the samples containing fewer and larger objects have been considered to be easier in some works [32,33]. Other solutions employed difficulty estimators [47] or even the confidence level of the predictions made by the neural network [48,49] to approximate the complexity of the data samples. Other studies [27][28][29] used the error of a previously trained model to estimate the difficulty of each sample. Such solutions have shown their utility in specific application domains. Nonetheless, measuring the difficulty remains problematic when implementing standard (data-level) curriculum learning strategies, at least in some application domains. Therefore, several alternatives have emerged over time, handling the drawback and improving the conventional curriculum learning approach.",
                    "score": 0.43564314479386546,
                    "section_title": "Curriculum Learning",
                    "char_start_offset": 6876,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 229
                        },
                        {
                            "start": 230,
                            "end": 390
                        },
                        {
                            "start": 391,
                            "end": 667
                        },
                        {
                            "start": 668,
                            "end": 969
                        },
                        {
                            "start": 972,
                            "end": 1157
                        },
                        {
                            "start": 1158,
                            "end": 1275
                        },
                        {
                            "start": 1276,
                            "end": 1403
                        },
                        {
                            "start": 1404,
                            "end": 1528
                        },
                        {
                            "start": 1529,
                            "end": 1714
                        },
                        {
                            "start": 1715,
                            "end": 1829
                        },
                        {
                            "start": 1830,
                            "end": 1902
                        },
                        {
                            "start": 1903,
                            "end": 2070
                        },
                        {
                            "start": 2071,
                            "end": 2209
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 366,
                            "end": 369,
                            "matchedPaperCorpusId": "231709290"
                        },
                        {
                            "start": 386,
                            "end": 389,
                            "matchedPaperCorpusId": "232362223"
                        },
                        {
                            "start": 428,
                            "end": 431,
                            "matchedPaperCorpusId": "231709290"
                        },
                        {
                            "start": 854,
                            "end": 856,
                            "matchedPaperCorpusId": "220301592"
                        },
                        {
                            "start": 856,
                            "end": 860,
                            "matchedPaperCorpusId": "266998701"
                        },
                        {
                            "start": 860,
                            "end": 864,
                            "matchedPaperCorpusId": "254246401"
                        },
                        {
                            "start": 864,
                            "end": 868,
                            "matchedPaperCorpusId": "256808576"
                        },
                        {
                            "start": 868,
                            "end": 872,
                            "matchedPaperCorpusId": "8105909"
                        },
                        {
                            "start": 872,
                            "end": 876,
                            "matchedPaperCorpusId": "51876228"
                        },
                        {
                            "start": 876,
                            "end": 880,
                            "matchedPaperCorpusId": "6954583"
                        },
                        {
                            "start": 884,
                            "end": 888,
                            "matchedPaperCorpusId": "5658192"
                        },
                        {
                            "start": 923,
                            "end": 927,
                            "matchedPaperCorpusId": "85498775"
                        },
                        {
                            "start": 927,
                            "end": 931,
                            "matchedPaperCorpusId": "26468344"
                        },
                        {
                            "start": 931,
                            "end": 935,
                            "matchedPaperCorpusId": "1900277"
                        },
                        {
                            "start": 935,
                            "end": 939,
                            "matchedPaperCorpusId": "51606954"
                        },
                        {
                            "start": 961,
                            "end": 965,
                            "matchedPaperCorpusId": "19805513"
                        },
                        {
                            "start": 965,
                            "end": 968,
                            "matchedPaperCorpusId": "11590585"
                        },
                        {
                            "start": 1215,
                            "end": 1219,
                            "matchedPaperCorpusId": "8502955"
                        },
                        {
                            "start": 1219,
                            "end": 1223,
                            "matchedPaperCorpusId": "204539326"
                        },
                        {
                            "start": 1223,
                            "end": 1227,
                            "matchedPaperCorpusId": "221995570"
                        },
                        {
                            "start": 1306,
                            "end": 1310,
                            "matchedPaperCorpusId": "26468344"
                        },
                        {
                            "start": 1314,
                            "end": 1318,
                            "matchedPaperCorpusId": "166228313"
                        },
                        {
                            "start": 1318,
                            "end": 1322,
                            "matchedPaperCorpusId": "233433844"
                        },
                        {
                            "start": 1349,
                            "end": 1352,
                            "matchedPaperCorpusId": "51606954"
                        },
                        {
                            "start": 1520,
                            "end": 1524,
                            "matchedPaperCorpusId": "6954583"
                        },
                        {
                            "start": 1576,
                            "end": 1580,
                            "matchedPaperCorpusId": "879067"
                        },
                        {
                            "start": 1656,
                            "end": 1660,
                            "matchedPaperCorpusId": "10364203"
                        },
                        {
                            "start": 1660,
                            "end": 1663,
                            "matchedPaperCorpusId": "102350936"
                        },
                        {
                            "start": 1729,
                            "end": 1733,
                            "matchedPaperCorpusId": "266998701"
                        },
                        {
                            "start": 1733,
                            "end": 1737,
                            "matchedPaperCorpusId": "254246401"
                        },
                        {
                            "start": 1737,
                            "end": 1741,
                            "matchedPaperCorpusId": "256808576"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.869140625
                }
            ],
            "relevance_judgement": 0.869140625,
            "relevance_judgment_input_expanded": "# Title: LeRaC: Learning Rate Curriculum\n# Venue: International Journal of Computer Vision\n# Authors: Florinel-Alin Croitoru, Nicolae-C\u0103t\u0103lin Ristea, Radu Tudor Ionescu, N. Sebe\n## Abstract\nMost curriculum learning methods require an approach to sort the data samples by difficulty, which is often cumbersome to perform. In this work, we propose a novel curriculum learning approach termed Learning Rate Curriculum (LeRaC), which leverages the use of a different learning rate for each layer of a neural network to create a data-agnostic curriculum during the initial training epochs. More specifically, LeRaC assigns higher learning rates to neural layers closer to the input, gradually decreasing the learning rates as the layers are placed farther away from the input. The learning rates increase at various paces during the first training iterations, until they all reach the same value. From this point on, the neural model is trained as usual. This creates a model-level curriculum learning strategy that does not require sorting the examples by difficulty and is compatible with any neural network, generating higher performance levels regardless of the architecture. We conduct comprehensive experiments on 12 data sets from the computer vision (CIFAR-10, CIFAR-100, Tiny ImageNet, ImageNet-1K, Food-101, UTKFace, PASCAL VOC), language (BoolQ, QNLI, RTE) and audio (ESC-50, CREMA-D) domains, considering various convolutional (ResNet-18, Wide-ResNet-50, DenseNet-121, YOLOv5), recurrent (LSTM) and transformer (CvT, BERT, SepTr) architectures. We compare our approach with the conventional training regime, as well as with Curriculum by Smoothing (CBS), a state-of-the-art data-agnostic curriculum learning approach. Unlike CBS, our performance improvements over the standard training regime are consistent across all data sets and models. Furthermore, we significantly surpass CBS in terms of training time (there is no additional cost over the standard training regime for LeRaC). Our code is freely available at: https://github.com/CroitoruAlin/LeRaC.\n## Curriculum Learning\nCurriculum learning was initially introduced by Bengio et al. [1] as a training strategy that helps machine learning models to generalize better when the training examples are presented in the ascending order of their difficulty. Extensive surveys on curriculum learning methods, including the most recent advancements on the topic, were conducted by Soviany et al. [2] and Wang et al. [4]. In the former survey, Soviany et al. [2] emphasized that curriculum learning is not only applied at the data level, but also with respect to the other components involved in a machine learning approach, namely at the model level, the task level and the objective (loss) level. Regardless of the component on which curriculum learning is applied, the technique has demonstrated its effectiveness on a broad range of machine learning tasks, from computer vision [1,7,[27][28][29][30][31][32][33][34] to natural language processing [1,[35][36][37][38] and audio processing [39,40]. \n\nThe main challenge for the methods that build the curriculum at the data level is measuring the difficulty of the data samples, which is required to order the samples from easy to hard. Most studies have addressed the problem with human input [41][42][43] or metrics based on domain-specific heuristics. For instance, the text length [36,[44][45][46] and the word frequency [1,38] have been employed in natural language processing. In computer vision, the samples containing fewer and larger objects have been considered to be easier in some works [32,33]. Other solutions employed difficulty estimators [47] or even the confidence level of the predictions made by the neural network [48,49] to approximate the complexity of the data samples. Other studies [27][28][29] used the error of a previously trained model to estimate the difficulty of each sample. Such solutions have shown their utility in specific application domains. Nonetheless, measuring the difficulty remains problematic when implementing standard (data-level) curriculum learning strategies, at least in some application domains. Therefore, several alternatives have emerged over time, handling the drawback and improving the conventional curriculum learning approach.",
            "reference_string": "[248887633 | Croitoru et al. | 2022 | Citations: 9]"
        },
        {
            "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 21,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.10430, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "7565696",
                    "name": "Dheeraj Mekala"
                },
                {
                    "authorId": "2284673632",
                    "name": "Alex Nguyen"
                },
                {
                    "authorId": "2284595153",
                    "name": "Jingbo Shang"
                }
            ],
            "abstract": "Instruction-tuning language models has become a crucial step in aligning them for general use. Typically, this process involves extensive training on large datasets, incurring high training costs. In this paper, we introduce a novel training data selection based on the learning percentage of the samples. We assert that current language models possess the capability to autonomously select high-quality training data, leading to comparable or improved performance compared to training on the entire dataset. Our experiments span different-sized models, revealing that this characteristic holds for models ranging from 1B (small) to 13B (large) in size. Moreover, we demonstrate an interesting finding that the data hardness transfers across model sizes, and a smaller 350M model can effectively curate high-quality training data with hard samples for a larger 13B model, resulting in an equally or superior instruction-tuned model compared to training on the complete dataset. Utilizing open-sourced OPT and Llama-2 models up to 13B in size, two publicly available instruction-tuning training datasets and evaluated by both automatic metrics&humans, our paper introduces a novel approach to training data selection, showcasing a more efficient alternative.",
            "corpus_id": 267740312,
            "sentences": [
                {
                    "corpus_id": "267740312",
                    "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
                    "text": "In this paper, we introduce a learning percentagebased metric for assessing the difficulty of samples. We demonstrate that language models ranging from 1B to 13B sizes can self-select high-quality training data by employing this metric. Additionally, we empirically validate the transferability of data hardness across different model sizes, showcasing the efficient curation of high-quality training data by smaller models. Furthermore, we propose an optimized version of the metric that offers increased speed while maintaining equal efficacy. In future, we aim to explore methods for automatically transforming easy training samples into more challenging ones. \n\nOur examination reveals prevalence of noisy samples within the LP(1) and LP app (1) subsets of data. The detection and mitigation of noisy samples are imperative to mitigate their influence on the dataset. We leave this for future work.",
                    "score": 0.3058360598962842,
                    "section_title": "Conclusion",
                    "char_start_offset": 22528,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 102
                        },
                        {
                            "start": 103,
                            "end": 236
                        },
                        {
                            "start": 237,
                            "end": 424
                        },
                        {
                            "start": 425,
                            "end": 545
                        },
                        {
                            "start": 546,
                            "end": 663
                        },
                        {
                            "start": 666,
                            "end": 766
                        },
                        {
                            "start": 767,
                            "end": 871
                        },
                        {
                            "start": 872,
                            "end": 902
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.861328125
                },
                {
                    "corpus_id": "267740312",
                    "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
                    "text": "In this paper, we delve into the measurement of sample difficulty from the model's perspective. Drawing inspiration from the learning order metric in (Mekala et al., 2022a), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered. We then select the most difficult sample subsets based on this ranking and instruction-tune a language model. Our experiments involve two instruction-tuning datasets, Alpaca-Data (Taori et al., 2023), and Dolly (Conover et al., 2023), with performance measured using automated metrics such as Al-pacaEval (Li et al., 2023b) and human evaluation. \n\nOur main findings indicate that language models can autonomously select training data, achieving performance equal to or better than training on the entire dataset. Furthermore, this characteristic scales across different model sizes, ranging from smaller ones (1B) to larger ones (13B) \n\n1 in parameters. As the size of the language model increases, we observe a consistent reduction in the minimum amount of data needed to surpass the performance of a model trained on the entire dataset. Interestingly, we observe that the data hardness also transfers across models, meaning samples considered difficult by smaller models are similarly challenging for larger models. Moreover, we note that this transferability improves with the size of the smaller model, eventually achieving comparable quality, beyond a size threshold, to that attained by selfselection conducted by larger models. Our study employs open-sourced models such as OPT (Zhang et al., 2022) and Llama-2 (Touvron et al., 2023) to support these findings. The remainder of the paper is structured as follows: initially, we describe the experimental setup encompassing the language models, the datasets employed, and the evaluation metrics utilized (section 2). Subsequently, we present our learning percentage-based difficulty metric and analyze it in detail (section 3). Following this, we optimize the proposed metric and introduce an equally effective, approximate, and faster metric (section 4). Ultimately, we analyze the challenging data identified through this metric (section 5).",
                    "score": 0.468397373739596,
                    "section_title": "arXiv:2402.10430v1 [cs.CL] 16 Feb 2024",
                    "char_start_offset": 1531,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 95
                        },
                        {
                            "start": 96,
                            "end": 330
                        },
                        {
                            "start": 331,
                            "end": 429
                        },
                        {
                            "start": 430,
                            "end": 539
                        },
                        {
                            "start": 540,
                            "end": 775
                        },
                        {
                            "start": 778,
                            "end": 942
                        },
                        {
                            "start": 943,
                            "end": 1064
                        },
                        {
                            "start": 1067,
                            "end": 1083
                        },
                        {
                            "start": 1084,
                            "end": 1268
                        },
                        {
                            "start": 1269,
                            "end": 1447
                        },
                        {
                            "start": 1448,
                            "end": 1664
                        },
                        {
                            "start": 1665,
                            "end": 1797
                        },
                        {
                            "start": 1798,
                            "end": 2002
                        },
                        {
                            "start": 2003,
                            "end": 2113
                        },
                        {
                            "start": 2114,
                            "end": 2241
                        },
                        {
                            "start": 2242,
                            "end": 2329
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 150,
                            "end": 172,
                            "matchedPaperCorpusId": "249060677"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8408203125
                }
            ],
            "relevance_judgement": 0.861328125,
            "relevance_judgment_input_expanded": "# Title: Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Dheeraj Mekala, Alex Nguyen, Jingbo Shang\n## Abstract\nInstruction-tuning language models has become a crucial step in aligning them for general use. Typically, this process involves extensive training on large datasets, incurring high training costs. In this paper, we introduce a novel training data selection based on the learning percentage of the samples. We assert that current language models possess the capability to autonomously select high-quality training data, leading to comparable or improved performance compared to training on the entire dataset. Our experiments span different-sized models, revealing that this characteristic holds for models ranging from 1B (small) to 13B (large) in size. Moreover, we demonstrate an interesting finding that the data hardness transfers across model sizes, and a smaller 350M model can effectively curate high-quality training data with hard samples for a larger 13B model, resulting in an equally or superior instruction-tuned model compared to training on the complete dataset. Utilizing open-sourced OPT and Llama-2 models up to 13B in size, two publicly available instruction-tuning training datasets and evaluated by both automatic metrics&humans, our paper introduces a novel approach to training data selection, showcasing a more efficient alternative.\n## arXiv:2402.10430v1 [cs.CL] 16 Feb 2024\nIn this paper, we delve into the measurement of sample difficulty from the model's perspective. Drawing inspiration from the learning order metric in (Mekala et al., 2022a), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered. We then select the most difficult sample subsets based on this ranking and instruction-tune a language model. Our experiments involve two instruction-tuning datasets, Alpaca-Data (Taori et al., 2023), and Dolly (Conover et al., 2023), with performance measured using automated metrics such as Al-pacaEval (Li et al., 2023b) and human evaluation. \n\nOur main findings indicate that language models can autonomously select training data, achieving performance equal to or better than training on the entire dataset. Furthermore, this characteristic scales across different model sizes, ranging from smaller ones (1B) to larger ones (13B) \n\n1 in parameters. As the size of the language model increases, we observe a consistent reduction in the minimum amount of data needed to surpass the performance of a model trained on the entire dataset. Interestingly, we observe that the data hardness also transfers across models, meaning samples considered difficult by smaller models are similarly challenging for larger models. Moreover, we note that this transferability improves with the size of the smaller model, eventually achieving comparable quality, beyond a size threshold, to that attained by selfselection conducted by larger models. Our study employs open-sourced models such as OPT (Zhang et al., 2022) and Llama-2 (Touvron et al., 2023) to support these findings. The remainder of the paper is structured as follows: initially, we describe the experimental setup encompassing the language models, the datasets employed, and the evaluation metrics utilized (section 2). Subsequently, we present our learning percentage-based difficulty metric and analyze it in detail (section 3). Following this, we optimize the proposed metric and introduce an equally effective, approximate, and faster metric (section 4). Ultimately, we analyze the challenging data identified through this metric (section 5).\n\n## Conclusion\nIn this paper, we introduce a learning percentagebased metric for assessing the difficulty of samples. We demonstrate that language models ranging from 1B to 13B sizes can self-select high-quality training data by employing this metric. Additionally, we empirically validate the transferability of data hardness across different model sizes, showcasing the efficient curation of high-quality training data by smaller models. Furthermore, we propose an optimized version of the metric that offers increased speed while maintaining equal efficacy. In future, we aim to explore methods for automatically transforming easy training samples into more challenging ones. \n\nOur examination reveals prevalence of noisy samples within the LP(1) and LP app (1) subsets of data. The detection and mitigation of noisy samples are imperative to mitigate their influence on the dataset. We leave this for future work.",
            "reference_string": "[267740312 | Mekala et al. | 2024 | Citations: 21]"
        },
        {
            "title": "Data Optimization in Deep Learning: A Survey",
            "venue": "IEEE Transactions on Knowledge and Data Engineering",
            "year": 2023,
            "reference_count": 379,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.16499, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261492235",
                    "name": "Ou Wu"
                },
                {
                    "authorId": "7304193",
                    "name": "Rujing Yao"
                }
            ],
            "abstract": "Large-scale, high-quality data are considered an essential factor for the successful application of many deep learning techniques. Meanwhile, numerous real-world deep learning tasks still have to contend with the lack of sufficient amounts of high-quality data. Additionally, issues such as model robustness, fairness, and trustworthiness are also closely related to training data. Consequently, a huge number of studies in the existing literature have focused on the data aspect in deep learning tasks. Some typical data optimization techniques include data augmentation, logit perturbation, sample weighting, and data condensation. These techniques usually come from different deep learning divisions and their theoretical inspirations or heuristic motivations may seem unrelated to each other. This study aims to organize a wide range of existing data optimization methodologies for deep learning from the previous literature, and makes the effort to construct a comprehensive taxonomy for them. The constructed taxonomy considers the diversity of split dimensions, and deep sub-taxonomies are constructed for each dimension. On the basis of the taxonomy, connections among the extensive data optimization methods for deep learning are built in terms of five aspects. We probe into rendering several promising and interesting future directions. The constructed taxonomy and the revealed connections will enlighten the better understanding of existing methods and the design of novel data optimization techniques. Furthermore, our aspiration for this survey is to promote data optimization as an independent subdivision of deep learning.",
            "corpus_id": 264451551,
            "sentences": [
                {
                    "corpus_id": "264451551",
                    "title": "Data Optimization in Deep Learning: A Survey",
                    "text": "The accurate measurement of learning difficulty for each training sample is of great importance because several deep learning paradigms employ adaptive learning strategies based on the level of learning difficulty. For instance, curriculum learning [71] holds the perspective that easy samples should receive more focus in the early training stages, while hard samples should be given more attention in the later stages of training. Some other studies [72] hold the opposite perspective that hard samples should be prioritized throughout the training procedure. \n\nAs shown in Fig. 7, there are five major manners to measure learning difficulty of samples, namely, lossbased, gradient-based, uncertainty-based, multi-trainingbased, and distance-based. Obviously, the measures for learning difficulty are quite similar to those for cleanliness. In fact, some studies consider that noisy samples are those quite difficult to learn and divide samples into easy/medium/hard/noisy. Paul et al. [73] proposed the error l2-norm score to measure difficulty. Zhu et al. [74] established a formal definition for learning difficulty of samples inspired by the bias-variance trade-off theorem and proposed a new learning difficulty measures. Sorscher et al. [75] defined the cosine distance of a sample to its nearest cluster center as the sample's difficulty measure and applied it in sample selection. \u2022 Uncertainty. This type contains two sub-types, namely, aleatory uncertainty and epistemic uncertainty [76]. The former is also called data uncertainty and occurs when training samples are imperfect, e.g., noisy. Therefore, the cleanliness degree can be used as a measure of data uncertainty [77]. Epistemic uncertainty is also called model uncertainty. It appears when the learning strategy is imperfect. Model uncertainty can be calculated based on information entropy of the DNN prediction or the variance of multiple predictions output by a DNN with the dropout trick [78]. \u2022 Diversity. This type aims to identify the diversity of a subset of training samples. The subset is usually a category. The measurement for subset diversity is useful in the design of data augmentation strategy for the subset [79] and data selection [59]. Friedman and Dieng [80] leveraged the exponential of the Shannon entropy of the eigenvalues of a similarity matrix, namely, vendi score to measure diversity.",
                    "score": 0.33092007672030155,
                    "section_title": "A. Data perception",
                    "char_start_offset": 31128,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 214
                        },
                        {
                            "start": 215,
                            "end": 432
                        },
                        {
                            "start": 433,
                            "end": 561
                        },
                        {
                            "start": 564,
                            "end": 750
                        },
                        {
                            "start": 751,
                            "end": 842
                        },
                        {
                            "start": 843,
                            "end": 975
                        },
                        {
                            "start": 976,
                            "end": 1048
                        },
                        {
                            "start": 1049,
                            "end": 1228
                        },
                        {
                            "start": 1229,
                            "end": 1390
                        },
                        {
                            "start": 1391,
                            "end": 1405
                        },
                        {
                            "start": 1406,
                            "end": 1500
                        },
                        {
                            "start": 1501,
                            "end": 1604
                        },
                        {
                            "start": 1605,
                            "end": 1689
                        },
                        {
                            "start": 1690,
                            "end": 1745
                        },
                        {
                            "start": 1746,
                            "end": 1797
                        },
                        {
                            "start": 1798,
                            "end": 1969
                        },
                        {
                            "start": 1970,
                            "end": 1982
                        },
                        {
                            "start": 1983,
                            "end": 2056
                        },
                        {
                            "start": 2057,
                            "end": 2090
                        },
                        {
                            "start": 2091,
                            "end": 2226
                        },
                        {
                            "start": 2227,
                            "end": 2384
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 249,
                            "end": 253,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 452,
                            "end": 456,
                            "matchedPaperCorpusId": "47252984"
                        },
                        {
                            "start": 1495,
                            "end": 1499,
                            "matchedPaperCorpusId": "226307260"
                        },
                        {
                            "start": 1964,
                            "end": 1968,
                            "matchedPaperCorpusId": "71134"
                        },
                        {
                            "start": 2197,
                            "end": 2201,
                            "matchedPaperCorpusId": "174799713"
                        },
                        {
                            "start": 2221,
                            "end": 2225,
                            "matchedPaperCorpusId": "252494342"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85693359375
                }
            ],
            "relevance_judgement": 0.85693359375,
            "relevance_judgment_input_expanded": "# Title: Data Optimization in Deep Learning: A Survey\n# Venue: IEEE Transactions on Knowledge and Data Engineering\n# Authors: Ou Wu, Rujing Yao\n## Abstract\nLarge-scale, high-quality data are considered an essential factor for the successful application of many deep learning techniques. Meanwhile, numerous real-world deep learning tasks still have to contend with the lack of sufficient amounts of high-quality data. Additionally, issues such as model robustness, fairness, and trustworthiness are also closely related to training data. Consequently, a huge number of studies in the existing literature have focused on the data aspect in deep learning tasks. Some typical data optimization techniques include data augmentation, logit perturbation, sample weighting, and data condensation. These techniques usually come from different deep learning divisions and their theoretical inspirations or heuristic motivations may seem unrelated to each other. This study aims to organize a wide range of existing data optimization methodologies for deep learning from the previous literature, and makes the effort to construct a comprehensive taxonomy for them. The constructed taxonomy considers the diversity of split dimensions, and deep sub-taxonomies are constructed for each dimension. On the basis of the taxonomy, connections among the extensive data optimization methods for deep learning are built in terms of five aspects. We probe into rendering several promising and interesting future directions. The constructed taxonomy and the revealed connections will enlighten the better understanding of existing methods and the design of novel data optimization techniques. Furthermore, our aspiration for this survey is to promote data optimization as an independent subdivision of deep learning.\n## A. Data perception\nThe accurate measurement of learning difficulty for each training sample is of great importance because several deep learning paradigms employ adaptive learning strategies based on the level of learning difficulty. For instance, curriculum learning [71] holds the perspective that easy samples should receive more focus in the early training stages, while hard samples should be given more attention in the later stages of training. Some other studies [72] hold the opposite perspective that hard samples should be prioritized throughout the training procedure. \n\nAs shown in Fig. 7, there are five major manners to measure learning difficulty of samples, namely, lossbased, gradient-based, uncertainty-based, multi-trainingbased, and distance-based. Obviously, the measures for learning difficulty are quite similar to those for cleanliness. In fact, some studies consider that noisy samples are those quite difficult to learn and divide samples into easy/medium/hard/noisy. Paul et al. [73] proposed the error l2-norm score to measure difficulty. Zhu et al. [74] established a formal definition for learning difficulty of samples inspired by the bias-variance trade-off theorem and proposed a new learning difficulty measures. Sorscher et al. [75] defined the cosine distance of a sample to its nearest cluster center as the sample's difficulty measure and applied it in sample selection. \u2022 Uncertainty. This type contains two sub-types, namely, aleatory uncertainty and epistemic uncertainty [76]. The former is also called data uncertainty and occurs when training samples are imperfect, e.g., noisy. Therefore, the cleanliness degree can be used as a measure of data uncertainty [77]. Epistemic uncertainty is also called model uncertainty. It appears when the learning strategy is imperfect. Model uncertainty can be calculated based on information entropy of the DNN prediction or the variance of multiple predictions output by a DNN with the dropout trick [78]. \u2022 Diversity. This type aims to identify the diversity of a subset of training samples. The subset is usually a category. The measurement for subset diversity is useful in the design of data augmentation strategy for the subset [79] and data selection [59]. Friedman and Dieng [80] leveraged the exponential of the Shannon entropy of the eigenvalues of a similarity matrix, namely, vendi score to measure diversity.",
            "reference_string": "[264451551 | Wu et al. | 2023 | Citations: 1]"
        },
        {
            "title": "Human-in-the-loop machine learning: a state of the art",
            "venue": "Artificial Intelligence Review",
            "year": 2022,
            "reference_count": 188,
            "citation_count": 417,
            "influential_citation_count": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s10462-022-10246-w.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10462-022-10246-w?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10462-022-10246-w, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1402122878",
                    "name": "E. Mosqueira-Rey"
                },
                {
                    "authorId": "1401777431",
                    "name": "Elena Hern\u00e1ndez-Pereira"
                },
                {
                    "authorId": "1403901547",
                    "name": "David Alonso-R\u00edos"
                },
                {
                    "authorId": "1403863088",
                    "name": "Jos\u00e9 Bobes-Bascar\u00e1n"
                },
                {
                    "authorId": "1402123073",
                    "name": "\u00c1. Fern\u00e1ndez-Leal"
                }
            ],
            "abstract": "Researchers are defining new types of interactions between humans and machine learning algorithms generically called human-in-the-loop machine learning. Depending on who is in control of the learning process, we can identify: active learning, in which the system remains in control; interactive machine learning, in which there is a closer interaction between users and learning systems; and machine teaching, where human domain experts have control over the learning process. Aside from control, humans can also be involved in the learning process in other ways. In curriculum learning human domain experts try to impose some structure on the examples presented to improve the learning; in explainable AI the focus is on the ability of the model to explain to humans why a given solution was chosen. This collaboration between AI models and humans should not be limited only to the learning process; if we go further, we can see other terms that arise such as Usable and Useful AI. In this paper we review the state of the art of the techniques involved in the new forms of relationship between humans and ML algorithms. Our contribution is not merely listing the different approaches, but to provide definitions clarifying confusing, varied and sometimes contradictory terms; to elucidate and determine the boundaries between the different methods; and to correlate all the techniques searching for the connections and influences between them.",
            "corpus_id": 251663102,
            "sentences": [
                {
                    "corpus_id": "251663102",
                    "title": "Human-in-the-loop machine learning: a state of the art",
                    "text": "Researchers have manually designed various difficulty measurers mainly based on the data characteristics of specific tasks and most of them designed for image in Computer Vision (CV) and text data in Natural Language Processing (NLP). Among data characteristics, complexity, diversity and noise estimation are considered. Complexity represents the structural complexity of a particular example, such that examples with higher complexity have more dimensions and are thus harder to be captured by the models (e.g. sentence length in NLP tasks). Diversity stands for the distributional diversity of a group of examples (e.g. regular or irregular shapes in CV tasks) where a larger value of diversity means the data is more diverse and is more difficult for model learning. Larger diversity sometimes also makes the data noisier. So, another characteristic to be studied is noise estimation which estimates the noise level of examples and defines cleaner data as easier. \n\nOther interesting difficulty measurers include human-annotation based Image Difficulty Scores (Soviany et al. 2020;Ionescu et al. 2016) which are proposed to measure the difficulty of an image by collecting the response times of human annotators answering \"Yes\" or \"No\" at identifying objects in images. Intuitively, longer response time corresponds to harder image examples. These measures can be considered separately but also correlated. For example, high complexity and high diversity bring more degrees of freedom to the data, which needs a model with larger capacity and bigger effort of training.",
                    "score": 0.4234275792862424,
                    "section_title": "Difficulty measurers",
                    "char_start_offset": 75476,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 234
                        },
                        {
                            "start": 235,
                            "end": 321
                        },
                        {
                            "start": 322,
                            "end": 512
                        },
                        {
                            "start": 513,
                            "end": 543
                        },
                        {
                            "start": 544,
                            "end": 622
                        },
                        {
                            "start": 623,
                            "end": 770
                        },
                        {
                            "start": 771,
                            "end": 826
                        },
                        {
                            "start": 827,
                            "end": 967
                        },
                        {
                            "start": 970,
                            "end": 1273
                        },
                        {
                            "start": 1274,
                            "end": 1345
                        },
                        {
                            "start": 1346,
                            "end": 1410
                        },
                        {
                            "start": 1411,
                            "end": 1573
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1064,
                            "end": 1085,
                            "matchedPaperCorpusId": "204801162"
                        },
                        {
                            "start": 1085,
                            "end": 1105,
                            "matchedPaperCorpusId": "879067"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8388671875
                }
            ],
            "relevance_judgement": 0.8388671875,
            "relevance_judgment_input_expanded": "# Title: Human-in-the-loop machine learning: a state of the art\n# Venue: Artificial Intelligence Review\n# Authors: E. Mosqueira-Rey, Elena Hern\u00e1ndez-Pereira, David Alonso-R\u00edos, Jos\u00e9 Bobes-Bascar\u00e1n, \u00c1. Fern\u00e1ndez-Leal\n## Abstract\nResearchers are defining new types of interactions between humans and machine learning algorithms generically called human-in-the-loop machine learning. Depending on who is in control of the learning process, we can identify: active learning, in which the system remains in control; interactive machine learning, in which there is a closer interaction between users and learning systems; and machine teaching, where human domain experts have control over the learning process. Aside from control, humans can also be involved in the learning process in other ways. In curriculum learning human domain experts try to impose some structure on the examples presented to improve the learning; in explainable AI the focus is on the ability of the model to explain to humans why a given solution was chosen. This collaboration between AI models and humans should not be limited only to the learning process; if we go further, we can see other terms that arise such as Usable and Useful AI. In this paper we review the state of the art of the techniques involved in the new forms of relationship between humans and ML algorithms. Our contribution is not merely listing the different approaches, but to provide definitions clarifying confusing, varied and sometimes contradictory terms; to elucidate and determine the boundaries between the different methods; and to correlate all the techniques searching for the connections and influences between them.\n## Difficulty measurers\nResearchers have manually designed various difficulty measurers mainly based on the data characteristics of specific tasks and most of them designed for image in Computer Vision (CV) and text data in Natural Language Processing (NLP). Among data characteristics, complexity, diversity and noise estimation are considered. Complexity represents the structural complexity of a particular example, such that examples with higher complexity have more dimensions and are thus harder to be captured by the models (e.g. sentence length in NLP tasks). Diversity stands for the distributional diversity of a group of examples (e.g. regular or irregular shapes in CV tasks) where a larger value of diversity means the data is more diverse and is more difficult for model learning. Larger diversity sometimes also makes the data noisier. So, another characteristic to be studied is noise estimation which estimates the noise level of examples and defines cleaner data as easier. \n\nOther interesting difficulty measurers include human-annotation based Image Difficulty Scores (Soviany et al. 2020;Ionescu et al. 2016) which are proposed to measure the difficulty of an image by collecting the response times of human annotators answering \"Yes\" or \"No\" at identifying objects in images. Intuitively, longer response time corresponds to harder image examples. These measures can be considered separately but also correlated. For example, high complexity and high diversity bring more degrees of freedom to the data, which needs a model with larger capacity and bigger effort of training.",
            "reference_string": "[251663102 | Mosqueira-Rey et al. | 2022 | Citations: 417]"
        },
        {
            "title": "An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 45,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.13534, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "50295874",
                    "name": "Xuemei Tang"
                },
                {
                    "authorId": "2284863194",
                    "name": "Qi Su"
                }
            ],
            "abstract": "Sequence labeling models often benefit from incorporating external knowledge. However, this practice introduces data heterogeneity and complicates the model with additional modules, leading to increased expenses for training a high-performing model. To address this challenge, we propose a two-stage curriculum learning (TCL) framework specifically designed for sequence labeling tasks. The TCL framework enhances training by gradually introducing data instances from easy to hard, aiming to improve both performance and training speed. Furthermore, we explore different metrics for assessing the difficulty levels of sequence labeling tasks. Through extensive experimentation on six Chinese word segmentation (CWS) and Part-of-speech tagging (POS) datasets, we demonstrate the effectiveness of our model in enhancing the performance of sequence labeling models. Additionally, our analysis indicates that TCL accelerates training and alleviates the slow training problem associated with complex models.",
            "corpus_id": 267770178,
            "sentences": [
                {
                    "corpus_id": "267770178",
                    "title": "An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling",
                    "text": "The former determines the relative \"easiness\" of each data example, which can be pre-defined or determined automatically. The latter determines the sequence of data subsets during the training process,guided by the judgment from the difficulty measurer (Wang et al., 2021), using either a linear or non-linear scheduler. When both components are designed based on human prior knowledge, it is referred to as pre-defined CL. On the other hand, if one of the components is driven by a data-driven algorithm, it is known as automatic CL, exemplified by self-paced learning and transfer teacher. Predefined difficulty measurer is typically designed by considering data characteristics such as sentence length and word rarity. In general, designing difficulty metrics specifically tailored to the task at hand is more efficient and typically involves data-driven algorithms. For instance, Mohiuddin et al. (2022) developed three deterministic scoring methods for machine translation that leverage the similarity of a parallel sentence pairs and cross-entropy. Zhu et al. (2021) evaluated sample difficulty in dialogue generation based on model loss. Lobov et al. (2022) utilized metrics such as sentence length, average confidence, and perplexity value to assess difficulty in named entity recognition (NER). However, these metrics primarily concentrate on the sentence-level and do not adequately address token-and word-level difficulty.",
                    "score": 0.311939090775342,
                    "section_title": "Related Work",
                    "char_start_offset": 5939,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 121
                        },
                        {
                            "start": 122,
                            "end": 320
                        },
                        {
                            "start": 321,
                            "end": 423
                        },
                        {
                            "start": 424,
                            "end": 591
                        },
                        {
                            "start": 592,
                            "end": 721
                        },
                        {
                            "start": 722,
                            "end": 869
                        },
                        {
                            "start": 870,
                            "end": 1054
                        },
                        {
                            "start": 1055,
                            "end": 1144
                        },
                        {
                            "start": 1145,
                            "end": 1303
                        },
                        {
                            "start": 1304,
                            "end": 1433
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1055,
                            "end": 1072,
                            "matchedPaperCorpusId": "244119160"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8076171875
                }
            ],
            "relevance_judgement": 0.8076171875,
            "relevance_judgment_input_expanded": "# Title: An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling\n# Venue: arXiv.org\n# Authors: Xuemei Tang, Qi Su\n## Abstract\nSequence labeling models often benefit from incorporating external knowledge. However, this practice introduces data heterogeneity and complicates the model with additional modules, leading to increased expenses for training a high-performing model. To address this challenge, we propose a two-stage curriculum learning (TCL) framework specifically designed for sequence labeling tasks. The TCL framework enhances training by gradually introducing data instances from easy to hard, aiming to improve both performance and training speed. Furthermore, we explore different metrics for assessing the difficulty levels of sequence labeling tasks. Through extensive experimentation on six Chinese word segmentation (CWS) and Part-of-speech tagging (POS) datasets, we demonstrate the effectiveness of our model in enhancing the performance of sequence labeling models. Additionally, our analysis indicates that TCL accelerates training and alleviates the slow training problem associated with complex models.\n## Related Work\nThe former determines the relative \"easiness\" of each data example, which can be pre-defined or determined automatically. The latter determines the sequence of data subsets during the training process,guided by the judgment from the difficulty measurer (Wang et al., 2021), using either a linear or non-linear scheduler. When both components are designed based on human prior knowledge, it is referred to as pre-defined CL. On the other hand, if one of the components is driven by a data-driven algorithm, it is known as automatic CL, exemplified by self-paced learning and transfer teacher. Predefined difficulty measurer is typically designed by considering data characteristics such as sentence length and word rarity. In general, designing difficulty metrics specifically tailored to the task at hand is more efficient and typically involves data-driven algorithms. For instance, Mohiuddin et al. (2022) developed three deterministic scoring methods for machine translation that leverage the similarity of a parallel sentence pairs and cross-entropy. Zhu et al. (2021) evaluated sample difficulty in dialogue generation based on model loss. Lobov et al. (2022) utilized metrics such as sentence length, average confidence, and perplexity value to assess difficulty in named entity recognition (NER). However, these metrics primarily concentrate on the sentence-level and do not adequately address token-and word-level difficulty.",
            "reference_string": "[267770178 | Tang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "The Efficiency Spectrum of Large Language Models: An Algorithmic Survey",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 319,
            "citation_count": 23,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.00678, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257191230",
                    "name": "Tianyu Ding"
                },
                {
                    "authorId": "2257252148",
                    "name": "Tianyi Chen"
                },
                {
                    "authorId": "2268495701",
                    "name": "Haidong Zhu"
                },
                {
                    "authorId": "2257278916",
                    "name": "Jiachen Jiang"
                },
                {
                    "authorId": "2269696935",
                    "name": "Yiqi Zhong"
                },
                {
                    "authorId": "2257235854",
                    "name": "Jinxin Zhou"
                },
                {
                    "authorId": "2269161996",
                    "name": "Guangzhi Wang"
                },
                {
                    "authorId": "2269697742",
                    "name": "Zhihui Zhu"
                },
                {
                    "authorId": "15623770",
                    "name": "Ilya Zharkov"
                },
                {
                    "authorId": "46225943",
                    "name": "Luming Liang"
                }
            ],
            "abstract": "The rapid growth of Large Language Models (LLMs) has been a driving force in transforming various domains, reshaping the artificial general intelligence landscape. However, the increasing computational and memory demands of these models present substantial challenges, hindering both academic research and practical applications. To address these issues, a wide array of methods, including both algorithmic and hardware solutions, have been developed to enhance the efficiency of LLMs. This survey delivers a comprehensive review of algorithmic advancements aimed at improving LLM efficiency. Unlike other surveys that typically focus on specific areas such as training or model compression, this paper examines the multi-faceted dimensions of efficiency essential for the end-to-end algorithmic development of LLMs. Specifically, it covers various topics related to efficiency, including scaling laws, data utilization, architectural innovations, training and tuning strategies, and inference techniques. This paper aims to serve as a valuable resource for researchers and practitioners, laying the groundwork for future innovations in this critical research area. Our repository of relevant references is maintained at url{https://github.com/tding1/Efficient-LLM-Survey}.",
            "corpus_id": 265552007,
            "sentences": [
                {
                    "corpus_id": "265552007",
                    "title": "The Efficiency Spectrum of Large Language Models: An Algorithmic Survey",
                    "text": "Curriculum learning [18,75] is a strategy that aims to improve the model training efficiency by carefully designing the feeding order of the instances in the training data. The principle of this approach is to initiate training with simpler samples or subtasks and progressively escalate to more challenging ones. Two critical components are integral to the design of a curriculum learning method. (i) The difficulty metric (or difficulty criterion), responsible for ranking the training samples based on their complexity. This metric serves as a guide to categorize training instances from the simplest to the most complex. (ii) The pacing function (also known as curriculum scheduler or arrangement), which determines the rate at which these ranked samples are fed to the model training. This function modulates the learning curve, ensuring that the model is not overwhelmed by the complexity of the tasks too early. Difficulty Metric. In the realm of curriculum learning for natural language processing, the most widely used difficulty metric is perhaps the sequence length [84,157,158]. The underlying assumption is that processing longer sentences poses greater challenges than shorter ones. Another prevalent metric is vocabulary rarity [285,334], based on the intuition that sentences with less frequently used words in the training set are inherently more complex to comprehend. In addition, this metric could be measured by the uncertainty sampling principle in active learning, where the uncertainty indicated by other pre-trained models could serves as a gauge of difficulty as well. \n\nFor fine-tuning in specific downstream tasks, researchers have innovated task-specific difficulty metrics. A notable example is in paraphrase generation, where Kadotani et al. [124] proposed using the edit distance between paraphrased sentence pairs as a metric, approximating the extent of required transformations. These custom metrics, tailored to the nuances of specific tasks like paraphrase generation, often outperform general metrics such as sentence length or word rarity. In the context of neural machine translation (NMT), defining sample difficulty is more complex. In addition to sentence length and word rarity, some studies [139] have incorporated linguistically-motivated features. For instance, the number of coordinating conjunctions in a target sentence has been used to estimate sentence complexity.",
                    "score": 0.33092007672030155,
                    "section_title": "Curriculum Learning",
                    "char_start_offset": 27786,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 172
                        },
                        {
                            "start": 173,
                            "end": 313
                        },
                        {
                            "start": 314,
                            "end": 397
                        },
                        {
                            "start": 398,
                            "end": 522
                        },
                        {
                            "start": 523,
                            "end": 624
                        },
                        {
                            "start": 625,
                            "end": 789
                        },
                        {
                            "start": 790,
                            "end": 918
                        },
                        {
                            "start": 919,
                            "end": 937
                        },
                        {
                            "start": 938,
                            "end": 1090
                        },
                        {
                            "start": 1091,
                            "end": 1196
                        },
                        {
                            "start": 1197,
                            "end": 1386
                        },
                        {
                            "start": 1387,
                            "end": 1594
                        },
                        {
                            "start": 1597,
                            "end": 1703
                        },
                        {
                            "start": 1704,
                            "end": 1913
                        },
                        {
                            "start": 1914,
                            "end": 2078
                        },
                        {
                            "start": 2079,
                            "end": 2174
                        },
                        {
                            "start": 2175,
                            "end": 2294
                        },
                        {
                            "start": 2295,
                            "end": 2416
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 20,
                            "end": 24,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 24,
                            "end": 27,
                            "matchedPaperCorpusId": "2105042"
                        },
                        {
                            "start": 1077,
                            "end": 1081,
                            "matchedPaperCorpusId": "255185900"
                        },
                        {
                            "start": 1085,
                            "end": 1089,
                            "matchedPaperCorpusId": "252917575"
                        },
                        {
                            "start": 1248,
                            "end": 1252,
                            "matchedPaperCorpusId": "211102338"
                        },
                        {
                            "start": 1773,
                            "end": 1778,
                            "matchedPaperCorpusId": "237329957"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80517578125
                }
            ],
            "relevance_judgement": 0.80517578125,
            "relevance_judgment_input_expanded": "# Title: The Efficiency Spectrum of Large Language Models: An Algorithmic Survey\n# Venue: arXiv.org\n# Authors: Tianyu Ding, Tianyi Chen, Haidong Zhu, Jiachen Jiang, Yiqi Zhong, Jinxin Zhou, Guangzhi Wang, Zhihui Zhu, Ilya Zharkov, Luming Liang\n## Abstract\nThe rapid growth of Large Language Models (LLMs) has been a driving force in transforming various domains, reshaping the artificial general intelligence landscape. However, the increasing computational and memory demands of these models present substantial challenges, hindering both academic research and practical applications. To address these issues, a wide array of methods, including both algorithmic and hardware solutions, have been developed to enhance the efficiency of LLMs. This survey delivers a comprehensive review of algorithmic advancements aimed at improving LLM efficiency. Unlike other surveys that typically focus on specific areas such as training or model compression, this paper examines the multi-faceted dimensions of efficiency essential for the end-to-end algorithmic development of LLMs. Specifically, it covers various topics related to efficiency, including scaling laws, data utilization, architectural innovations, training and tuning strategies, and inference techniques. This paper aims to serve as a valuable resource for researchers and practitioners, laying the groundwork for future innovations in this critical research area. Our repository of relevant references is maintained at url{https://github.com/tding1/Efficient-LLM-Survey}.\n## Curriculum Learning\nCurriculum learning [18,75] is a strategy that aims to improve the model training efficiency by carefully designing the feeding order of the instances in the training data. The principle of this approach is to initiate training with simpler samples or subtasks and progressively escalate to more challenging ones. Two critical components are integral to the design of a curriculum learning method. (i) The difficulty metric (or difficulty criterion), responsible for ranking the training samples based on their complexity. This metric serves as a guide to categorize training instances from the simplest to the most complex. (ii) The pacing function (also known as curriculum scheduler or arrangement), which determines the rate at which these ranked samples are fed to the model training. This function modulates the learning curve, ensuring that the model is not overwhelmed by the complexity of the tasks too early. Difficulty Metric. In the realm of curriculum learning for natural language processing, the most widely used difficulty metric is perhaps the sequence length [84,157,158]. The underlying assumption is that processing longer sentences poses greater challenges than shorter ones. Another prevalent metric is vocabulary rarity [285,334], based on the intuition that sentences with less frequently used words in the training set are inherently more complex to comprehend. In addition, this metric could be measured by the uncertainty sampling principle in active learning, where the uncertainty indicated by other pre-trained models could serves as a gauge of difficulty as well. \n\nFor fine-tuning in specific downstream tasks, researchers have innovated task-specific difficulty metrics. A notable example is in paraphrase generation, where Kadotani et al. [124] proposed using the edit distance between paraphrased sentence pairs as a metric, approximating the extent of required transformations. These custom metrics, tailored to the nuances of specific tasks like paraphrase generation, often outperform general metrics such as sentence length or word rarity. In the context of neural machine translation (NMT), defining sample difficulty is more complex. In addition to sentence length and word rarity, some studies [139] have incorporated linguistically-motivated features. For instance, the number of coordinating conjunctions in a target sentence has been used to estimate sentence complexity.",
            "reference_string": "[265552007 | Ding et al. | 2023 | Citations: 23]"
        },
        {
            "title": "A Review of the Evaluation System for Curriculum Learning",
            "venue": "Electronics",
            "year": 2023,
            "reference_count": 169,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2079-9292/12/7/1676/pdf?version=1680350130",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics12071676?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics12071676, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144102068",
                    "name": "Fengchun Liu"
                },
                {
                    "authorId": "2213622721",
                    "name": "Tong Zhang"
                },
                {
                    "authorId": "1806923",
                    "name": "Chunying Zhang"
                },
                {
                    "authorId": "145412331",
                    "name": "Lu Liu"
                },
                {
                    "authorId": "2108947838",
                    "name": "Liya Wang"
                },
                {
                    "authorId": "2213836973",
                    "name": "Bin Liu"
                }
            ],
            "abstract": "In recent years, deep learning models have been more and more widely used in various fields and have become a research hotspot for various tasks in artificial intelligence, but there are significant limitations in non-convex optimization problems. As a model training strategy for non-convex optimization, curriculum learning advocates that models learn in the order of easier to more difficult data, mimicking the basic idea of gradual human learning as they learn curriculum. This strategy has been widely used in the fields of computer vision, natural language processing, and reinforcement learning; it can effectively solve the non-convex optimization problem and improve the generalization ability and convergence speed of models. This paper first introduces the application of curriculum learning at three major levels: data, task, and model, and summarizes the evaluators designed using curriculum learning methods in various domains, including difficulty evaluators, training schedulers, and loss evaluators, which correspond to the three stages of difficulty evaluation, training schedule, and loss evaluation in the application of curriculum learning to model training. We also discuss how to choose an appropriate evaluation system and the differences between terms used in different types of research. Finally, we summarize five methods similar to curriculum learning in the field of machine learning and provide a summary and outlook of the curriculum learning evaluation system.",
            "corpus_id": 257937538,
            "sentences": [
                {
                    "corpus_id": "257937538",
                    "title": "A Review of the Evaluation System for Curriculum Learning",
                    "text": "Figure 4 illustrates the cross-review method. In the process of model training, samples with larger losses are harder to learn for the current stage of the model. Conversely, samples with smaller losses prove that the model has been able to correctly predict or classify that sample, which should reduce the sampling probability of that sample. Using sample loss [25,29] as a difficulty evaluation metric, such as Negative Log Likelihood loss [79], square loss [36], and cross-entropy loss [42,110,111] as difficulty scores, is widely used in self-paced learning [18] and its variant methods [22,112]. For example, self-paced learning [18] controls the model to start sampling from samples with smaller losses for training through the coordination of the SPregularizer and modulates the regularizer to keep decreasing during the training process, guiding the model to gradually sample samples with larger losses. Cross entropy is used as a measure of transferability [42], domain relevance [44], uncertainty [113], and representativeness [36,114], as in Shu et al. [42], where cross entropy loss is used as a measure of sample transferability for solving sample noises of the source domain and distribution shift across domains. In particular, cross entropy is used as a measure of domain relevance in neural machine translation, such as when using the model cross entropy as a sentence divergence score [44]. Where a higher divergence score indicates that the sentence has more in-domain features and is more likely to be different from samples in the generic domain, thus enabling learning from common to individual samples in different domains for better generalization. Zhang [110] and Wang et al. [115] used the cross-entropy of two models for measuring domain relevance and noise level, including the cross-entropy difference between two models trained using out-of-domain data and in-domain data (Moore-Lewis Method) and the degree of change in cross-entropy for selecting generaldomain data for model training (Cynical Data Selection) [110], such as assessing the domain relevance of sentences using the cross-entropy of in-domain and general-domain language models [115] (Equation ( 5)).",
                    "score": 0.48686526903984245,
                    "section_title": "Self-Scoring",
                    "char_start_offset": 38321,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 45
                        },
                        {
                            "start": 46,
                            "end": 162
                        },
                        {
                            "start": 163,
                            "end": 344
                        },
                        {
                            "start": 345,
                            "end": 601
                        },
                        {
                            "start": 602,
                            "end": 912
                        },
                        {
                            "start": 913,
                            "end": 1228
                        },
                        {
                            "start": 1229,
                            "end": 1409
                        },
                        {
                            "start": 1410,
                            "end": 1673
                        },
                        {
                            "start": 1674,
                            "end": 2196
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 363,
                            "end": 367,
                            "matchedPaperCorpusId": "243865335"
                        },
                        {
                            "start": 461,
                            "end": 465,
                            "matchedPaperCorpusId": "57784927"
                        },
                        {
                            "start": 490,
                            "end": 494,
                            "matchedPaperCorpusId": "69357784"
                        },
                        {
                            "start": 498,
                            "end": 502,
                            "matchedPaperCorpusId": "52952193"
                        },
                        {
                            "start": 563,
                            "end": 567,
                            "matchedPaperCorpusId": "1977996"
                        },
                        {
                            "start": 592,
                            "end": 596,
                            "matchedPaperCorpusId": "37032764"
                        },
                        {
                            "start": 596,
                            "end": 600,
                            "matchedPaperCorpusId": "9431293"
                        },
                        {
                            "start": 635,
                            "end": 639,
                            "matchedPaperCorpusId": "1977996"
                        },
                        {
                            "start": 967,
                            "end": 971,
                            "matchedPaperCorpusId": "69357784"
                        },
                        {
                            "start": 990,
                            "end": 994,
                            "matchedPaperCorpusId": "232104951"
                        },
                        {
                            "start": 1008,
                            "end": 1013,
                            "matchedPaperCorpusId": "220047761"
                        },
                        {
                            "start": 1038,
                            "end": 1042,
                            "matchedPaperCorpusId": "57784927"
                        },
                        {
                            "start": 1065,
                            "end": 1069,
                            "matchedPaperCorpusId": "69357784"
                        },
                        {
                            "start": 1404,
                            "end": 1408,
                            "matchedPaperCorpusId": "232104951"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.79248046875
                },
                {
                    "corpus_id": "257937538",
                    "title": "A Review of the Evaluation System for Curriculum Learning",
                    "text": "Zhang [110] and Wang et al. [115] used the cross-entropy of two models for measuring domain relevance and noise level, including the cross-entropy difference between two models trained using out-of-domain data and in-domain data (Moore-Lewis Method) and the degree of change in cross-entropy for selecting generaldomain data for model training (Cynical Data Selection) [110], such as assessing the domain relevance of sentences using the cross-entropy of in-domain and general-domain language models [115] (Equation ( 5)). \n\nIn particular, Mousavi et al. [116] proposed to use two parameters, entropy and mean alpha angle, to obtain direct scattering mechanism information for measuring the degree of complexity of each pixel, which is used to calculate the complexity of each PolSAR im- In the process of model training, samples with larger losses are harder to learn for the current stage of the model. Conversely, samples with smaller losses prove that the model has been able to correctly predict or classify that sample, which should reduce the sampling probability of that sample. Using sample loss [25,29] as a difficulty evaluation metric, such as Negative Log Likelihood loss [79], square loss [36], and cross-entropy loss [42,110,111] as difficulty scores, is widely used in self-paced learning [18] and its variant methods [22,112]. For example, self-paced learning [18] controls the model to start sampling from samples with smaller losses for training through the coordination of the SP-regularizer and modulates the regularizer to keep decreasing during the training process, guiding the model to gradually sample samples with larger losses. Cross entropy is used as a measure of transferability [42], domain relevance [44], uncertainty [113], and representativeness [36,114], as in Shu et al. [42], where cross entropy loss is used as a measure of sample transferability for solving sample noises of the source domain and distribution shift across domains. In particular, cross entropy is used as a measure of domain relevance in neural machine translation, such as when using the model cross entropy as a sentence divergence score [44].",
                    "score": 0.474848022781657,
                    "section_title": "Self-Scoring",
                    "char_start_offset": 39995,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 522
                        },
                        {
                            "start": 525,
                            "end": 904
                        },
                        {
                            "start": 905,
                            "end": 1086
                        },
                        {
                            "start": 1087,
                            "end": 1343
                        },
                        {
                            "start": 1344,
                            "end": 1655
                        },
                        {
                            "start": 1656,
                            "end": 1971
                        },
                        {
                            "start": 1972,
                            "end": 2152
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 555,
                            "end": 560,
                            "matchedPaperCorpusId": "245502373"
                        },
                        {
                            "start": 1105,
                            "end": 1109,
                            "matchedPaperCorpusId": "243865335"
                        },
                        {
                            "start": 1203,
                            "end": 1207,
                            "matchedPaperCorpusId": "57784927"
                        },
                        {
                            "start": 1232,
                            "end": 1236,
                            "matchedPaperCorpusId": "69357784"
                        },
                        {
                            "start": 1240,
                            "end": 1244,
                            "matchedPaperCorpusId": "52952193"
                        },
                        {
                            "start": 1305,
                            "end": 1309,
                            "matchedPaperCorpusId": "1977996"
                        },
                        {
                            "start": 1334,
                            "end": 1338,
                            "matchedPaperCorpusId": "37032764"
                        },
                        {
                            "start": 1338,
                            "end": 1342,
                            "matchedPaperCorpusId": "9431293"
                        },
                        {
                            "start": 1377,
                            "end": 1381,
                            "matchedPaperCorpusId": "1977996"
                        },
                        {
                            "start": 1710,
                            "end": 1714,
                            "matchedPaperCorpusId": "69357784"
                        },
                        {
                            "start": 1733,
                            "end": 1737,
                            "matchedPaperCorpusId": "232104951"
                        },
                        {
                            "start": 1751,
                            "end": 1756,
                            "matchedPaperCorpusId": "220047761"
                        },
                        {
                            "start": 1781,
                            "end": 1785,
                            "matchedPaperCorpusId": "57784927"
                        },
                        {
                            "start": 1808,
                            "end": 1812,
                            "matchedPaperCorpusId": "69357784"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69189453125
                },
                {
                    "corpus_id": "257937538",
                    "title": "A Review of the Evaluation System for Curriculum Learning",
                    "text": "In particular, cross entropy is used as a measure of domain relevance in neural machine translation, such as when using the model cross entropy as a sentence divergence score [44]. Where a higher divergence score indicates that the sentence has more in-domain features and is more likely to be different from samples in the generic domain, thus enabling learning from common to individual samples in different domains for better generalization. Zhang [110] and Wang et al. [115] used the cross-entropy of two models for measuring domain relevance and noise level, including the cross-entropy difference between two models trained using out-of-domain data and in-domain data (Moore-Lewis Method) and the degree of change in cross-entropy for selecting general-domain data for model training (Cynical Data Selection) [110], such as assessing the domain relevance of sentences using the cross-entropy of in-domain and general-domain language models [115] (Equation ( 5)). \n\nIn particular, Mousavi et al. [116] proposed to use two parameters, entropy and mean alpha angle, to obtain direct scattering mechanism information for measuring the degree of complexity of each pixel, which is used to calculate the complexity of each PolSAR image patch. \n\nIn practice, research focusing on the instantaneous loss values of samples, as mentioned above, requires evaluating all samples before selecting them at each step, which involves additional inference on unselected samples, and that work is very costly in training. Rather than focusing on the instantaneous loss values of a sample, some studies have focused on its loss value during training, calculating the change in model loss over two consecutive training iterations [107] as a difficulty metric, proving that a sample is very difficult when its loss fluctuates between maximum and minimum values during the sequence. Zhou et al. [117] proposed the exponential moving average (EMA) method for the detection of clean and pseudo-labeled samples. When a sample's loss consistently maintains a low value during training, then its label has a higher probability of being correct, and when a sample's EMA consistency loss remains constant during training, then its pseudo-label is more reliable, achieving the selection of clean, correctly pseudo-labeled data for training and avoiding the inclusion of harmful noisy data.",
                    "score": 0.44824992188435697,
                    "section_title": "Self-Scoring",
                    "char_start_offset": 41967,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 180
                        },
                        {
                            "start": 181,
                            "end": 444
                        },
                        {
                            "start": 445,
                            "end": 968
                        },
                        {
                            "start": 971,
                            "end": 1242
                        },
                        {
                            "start": 1245,
                            "end": 1509
                        },
                        {
                            "start": 1510,
                            "end": 1866
                        },
                        {
                            "start": 1867,
                            "end": 1992
                        },
                        {
                            "start": 1993,
                            "end": 2365
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 175,
                            "end": 179,
                            "matchedPaperCorpusId": "232104951"
                        },
                        {
                            "start": 1001,
                            "end": 1006,
                            "matchedPaperCorpusId": "245502373"
                        },
                        {
                            "start": 1716,
                            "end": 1721,
                            "matchedPaperCorpusId": "227275560"
                        },
                        {
                            "start": 1879,
                            "end": 1884,
                            "matchedPaperCorpusId": "233452059"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6904296875
                }
            ],
            "relevance_judgement": 0.79248046875,
            "relevance_judgment_input_expanded": "# Title: A Review of the Evaluation System for Curriculum Learning\n# Venue: Electronics\n# Authors: Fengchun Liu, Tong Zhang, Chunying Zhang, Lu Liu, Liya Wang, Bin Liu\n## Abstract\nIn recent years, deep learning models have been more and more widely used in various fields and have become a research hotspot for various tasks in artificial intelligence, but there are significant limitations in non-convex optimization problems. As a model training strategy for non-convex optimization, curriculum learning advocates that models learn in the order of easier to more difficult data, mimicking the basic idea of gradual human learning as they learn curriculum. This strategy has been widely used in the fields of computer vision, natural language processing, and reinforcement learning; it can effectively solve the non-convex optimization problem and improve the generalization ability and convergence speed of models. This paper first introduces the application of curriculum learning at three major levels: data, task, and model, and summarizes the evaluators designed using curriculum learning methods in various domains, including difficulty evaluators, training schedulers, and loss evaluators, which correspond to the three stages of difficulty evaluation, training schedule, and loss evaluation in the application of curriculum learning to model training. We also discuss how to choose an appropriate evaluation system and the differences between terms used in different types of research. Finally, we summarize five methods similar to curriculum learning in the field of machine learning and provide a summary and outlook of the curriculum learning evaluation system.\n## Self-Scoring\nFigure 4 illustrates the cross-review method. In the process of model training, samples with larger losses are harder to learn for the current stage of the model. Conversely, samples with smaller losses prove that the model has been able to correctly predict or classify that sample, which should reduce the sampling probability of that sample. Using sample loss [25,29] as a difficulty evaluation metric, such as Negative Log Likelihood loss [79], square loss [36], and cross-entropy loss [42,110,111] as difficulty scores, is widely used in self-paced learning [18] and its variant methods [22,112]. For example, self-paced learning [18] controls the model to start sampling from samples with smaller losses for training through the coordination of the SPregularizer and modulates the regularizer to keep decreasing during the training process, guiding the model to gradually sample samples with larger losses. Cross entropy is used as a measure of transferability [42], domain relevance [44], uncertainty [113], and representativeness [36,114], as in Shu et al. [42], where cross entropy loss is used as a measure of sample transferability for solving sample noises of the source domain and distribution shift across domains. In particular, cross entropy is used as a measure of domain relevance in neural machine translation, such as when using the model cross entropy as a sentence divergence score [44]. Where a higher divergence score indicates that the sentence has more in-domain features and is more likely to be different from samples in the generic domain, thus enabling learning from common to individual samples in different domains for better generalization. Zhang [110] and Wang et al. [115] used the cross-entropy of two models for measuring domain relevance and noise level, including the cross-entropy difference between two models trained using out-of-domain data and in-domain data (Moore-Lewis Method) and the degree of change in cross-entropy for selecting generaldomain data for model training (Cynical Data Selection) [110], such as assessing the domain relevance of sentences using the cross-entropy of in-domain and general-domain language models [115] (Equation ( 5)).\n...\nZhang [110] and Wang et al. [115] used the cross-entropy of two models for measuring domain relevance and noise level, including the cross-entropy difference between two models trained using out-of-domain data and in-domain data (Moore-Lewis Method) and the degree of change in cross-entropy for selecting generaldomain data for model training (Cynical Data Selection) [110], such as assessing the domain relevance of sentences using the cross-entropy of in-domain and general-domain language models [115] (Equation ( 5)). \n\nIn particular, Mousavi et al. [116] proposed to use two parameters, entropy and mean alpha angle, to obtain direct scattering mechanism information for measuring the degree of complexity of each pixel, which is used to calculate the complexity of each PolSAR im- In the process of model training, samples with larger losses are harder to learn for the current stage of the model. Conversely, samples with smaller losses prove that the model has been able to correctly predict or classify that sample, which should reduce the sampling probability of that sample. Using sample loss [25,29] as a difficulty evaluation metric, such as Negative Log Likelihood loss [79], square loss [36], and cross-entropy loss [42,110,111] as difficulty scores, is widely used in self-paced learning [18] and its variant methods [22,112]. For example, self-paced learning [18] controls the model to start sampling from samples with smaller losses for training through the coordination of the SP-regularizer and modulates the regularizer to keep decreasing during the training process, guiding the model to gradually sample samples with larger losses. Cross entropy is used as a measure of transferability [42], domain relevance [44], uncertainty [113], and representativeness [36,114], as in Shu et al. [42], where cross entropy loss is used as a measure of sample transferability for solving sample noises of the source domain and distribution shift across domains. In particular, cross entropy is used as a measure of domain relevance in neural machine translation, such as when using the model cross entropy as a sentence divergence score [44].\n...\nIn particular, cross entropy is used as a measure of domain relevance in neural machine translation, such as when using the model cross entropy as a sentence divergence score [44]. Where a higher divergence score indicates that the sentence has more in-domain features and is more likely to be different from samples in the generic domain, thus enabling learning from common to individual samples in different domains for better generalization. Zhang [110] and Wang et al. [115] used the cross-entropy of two models for measuring domain relevance and noise level, including the cross-entropy difference between two models trained using out-of-domain data and in-domain data (Moore-Lewis Method) and the degree of change in cross-entropy for selecting general-domain data for model training (Cynical Data Selection) [110], such as assessing the domain relevance of sentences using the cross-entropy of in-domain and general-domain language models [115] (Equation ( 5)). \n\nIn particular, Mousavi et al. [116] proposed to use two parameters, entropy and mean alpha angle, to obtain direct scattering mechanism information for measuring the degree of complexity of each pixel, which is used to calculate the complexity of each PolSAR image patch. \n\nIn practice, research focusing on the instantaneous loss values of samples, as mentioned above, requires evaluating all samples before selecting them at each step, which involves additional inference on unselected samples, and that work is very costly in training. Rather than focusing on the instantaneous loss values of a sample, some studies have focused on its loss value during training, calculating the change in model loss over two consecutive training iterations [107] as a difficulty metric, proving that a sample is very difficult when its loss fluctuates between maximum and minimum values during the sequence. Zhou et al. [117] proposed the exponential moving average (EMA) method for the detection of clean and pseudo-labeled samples. When a sample's loss consistently maintains a low value during training, then its label has a higher probability of being correct, and when a sample's EMA consistency loss remains constant during training, then its pseudo-label is more reliable, achieving the selection of clean, correctly pseudo-labeled data for training and avoiding the inclusion of harmful noisy data.",
            "reference_string": "[257937538 | Liu et al. | 2023 | Citations: 6]"
        },
        {
            "title": "Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 60,
            "citation_count": 34,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.20541, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2172356226",
                    "name": "Zachary Ankner"
                },
                {
                    "authorId": "73066558",
                    "name": "Cody Blakeney"
                },
                {
                    "authorId": "34824875",
                    "name": "Kartik K. Sreenivasan"
                },
                {
                    "authorId": "2304322161",
                    "name": "Max Marion"
                },
                {
                    "authorId": "2028252288",
                    "name": "Matthew L. Leavitt"
                },
                {
                    "authorId": "1690452",
                    "name": "Mansheej Paul"
                }
            ],
            "abstract": "In this work, we investigate whether small language models can determine high-quality subsets of large-scale text datasets that improve the performance of larger language models. While existing work has shown that pruning based on the perplexity of a larger model can yield high-quality data, we investigate whether smaller models can be used for perplexity-based pruning and how pruning is affected by the domain composition of the data being pruned. We demonstrate that for multiple dataset compositions, perplexity-based pruning of pretraining data can \\emph{significantly} improve downstream task performance: pruning based on perplexities computed with a 125 million parameter model improves the average performance on downstream tasks of a 3 billion parameter model by up to 2.04 and achieves up to a $1.45\\times$ reduction in pretraining steps to reach commensurate baseline performance. Furthermore, we demonstrate that such perplexity-based data pruning also yields downstream performance gains in the over-trained and data-constrained regimes.",
            "corpus_id": 270199394,
            "sentences": [
                {
                    "corpus_id": "270199394",
                    "title": "Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models",
                    "text": "A large focus of the machine learning community has been improving the performance of large language models (LLMs) while reducing their training costs.In this work, we consider how to improve the quality of an LLM by improving the quality of its pretraining data.Although there are many techniques to improve data quality, such as augmenting training samples with additional information (Li et al., 2024;Korbak et al., 2023), in this work we focus on the predominant method of data pruning: intelligently selecting a high-quality subset of a larger dataset to train on.\n\nData pruning is commonly used for quality filtering of noisy text data.Simple approaches include using symbolic rules (Bane et al., 2022;Raffel et al., 2020) or using simple classifiers to determine high-quality samples (Wenzek et al., 2020).However, in addition to basic quality filtering, more complex data pruning techniques are also applied to datasets to further improve their quality.Xie et al. (2023b) perform importance resampling where importance scores are calculated based on feature similarity to a target text.Tirumala et al. (2023) prune datasets by deduplicating and diversifying data based on a pretrained language model's embeddings of the text samples.Xie et al. (2023a) re-weight domain proportions based on learnability as determined by a smaller proxy model.Marion et al. (2023) investigate data pruning based on multiple neural heuristics of sample difficulty, ultimately concluding that the perplexity of a sample under a reference language model is the best pruning metric.\n\nIn this work, we thoroughly investigate the impact that data pruning based on sample perplexity (Marion et al., 2023) has on LLM pretraining.In particular, we focus on the interplay between pretraining dataset composition and pruning methodology.We further evaluate perplexity pruning in the overtrained and data-constrained regimes.We also investigate whether evaluating the quality of data interventions based on upstream test set perplexity is a sound methodology for gauging downstream performance.",
                    "score": 0.31458772771458765,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 151
                        },
                        {
                            "start": 151,
                            "end": 263
                        },
                        {
                            "start": 263,
                            "end": 569
                        },
                        {
                            "start": 571,
                            "end": 642
                        },
                        {
                            "start": 642,
                            "end": 813
                        },
                        {
                            "start": 813,
                            "end": 961
                        },
                        {
                            "start": 961,
                            "end": 1094
                        },
                        {
                            "start": 1094,
                            "end": 1241
                        },
                        {
                            "start": 1241,
                            "end": 1350
                        },
                        {
                            "start": 1350,
                            "end": 1568
                        },
                        {
                            "start": 1570,
                            "end": 1711
                        },
                        {
                            "start": 1711,
                            "end": 1816
                        },
                        {
                            "start": 1816,
                            "end": 1903
                        },
                        {
                            "start": 1903,
                            "end": 2072
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 387,
                            "end": 404,
                            "matchedPaperCorpusId": "260866107"
                        },
                        {
                            "start": 404,
                            "end": 424,
                            "matchedPaperCorpusId": "257020046"
                        },
                        {
                            "start": 689,
                            "end": 708,
                            "matchedPaperCorpusId": "252186406"
                        },
                        {
                            "start": 708,
                            "end": 728,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 791,
                            "end": 812,
                            "matchedPaperCorpusId": "207870323"
                        },
                        {
                            "start": 961,
                            "end": 979,
                            "matchedPaperCorpusId": "256627727"
                        },
                        {
                            "start": 1094,
                            "end": 1116,
                            "matchedPaperCorpusId": "261076313"
                        },
                        {
                            "start": 1241,
                            "end": 1259,
                            "matchedPaperCorpusId": "258741043"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.775390625
                }
            ],
            "relevance_judgement": 0.775390625,
            "relevance_judgment_input_expanded": "# Title: Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models\n# Venue: International Conference on Learning Representations\n# Authors: Zachary Ankner, Cody Blakeney, Kartik K. Sreenivasan, Max Marion, Matthew L. Leavitt, Mansheej Paul\n## Abstract\nIn this work, we investigate whether small language models can determine high-quality subsets of large-scale text datasets that improve the performance of larger language models. While existing work has shown that pruning based on the perplexity of a larger model can yield high-quality data, we investigate whether smaller models can be used for perplexity-based pruning and how pruning is affected by the domain composition of the data being pruned. We demonstrate that for multiple dataset compositions, perplexity-based pruning of pretraining data can \\emph{significantly} improve downstream task performance: pruning based on perplexities computed with a 125 million parameter model improves the average performance on downstream tasks of a 3 billion parameter model by up to 2.04 and achieves up to a $1.45\\times$ reduction in pretraining steps to reach commensurate baseline performance. Furthermore, we demonstrate that such perplexity-based data pruning also yields downstream performance gains in the over-trained and data-constrained regimes.\n## Introduction\nA large focus of the machine learning community has been improving the performance of large language models (LLMs) while reducing their training costs.In this work, we consider how to improve the quality of an LLM by improving the quality of its pretraining data.Although there are many techniques to improve data quality, such as augmenting training samples with additional information (Li et al., 2024;Korbak et al., 2023), in this work we focus on the predominant method of data pruning: intelligently selecting a high-quality subset of a larger dataset to train on.\n\nData pruning is commonly used for quality filtering of noisy text data.Simple approaches include using symbolic rules (Bane et al., 2022;Raffel et al., 2020) or using simple classifiers to determine high-quality samples (Wenzek et al., 2020).However, in addition to basic quality filtering, more complex data pruning techniques are also applied to datasets to further improve their quality.Xie et al. (2023b) perform importance resampling where importance scores are calculated based on feature similarity to a target text.Tirumala et al. (2023) prune datasets by deduplicating and diversifying data based on a pretrained language model's embeddings of the text samples.Xie et al. (2023a) re-weight domain proportions based on learnability as determined by a smaller proxy model.Marion et al. (2023) investigate data pruning based on multiple neural heuristics of sample difficulty, ultimately concluding that the perplexity of a sample under a reference language model is the best pruning metric.\n\nIn this work, we thoroughly investigate the impact that data pruning based on sample perplexity (Marion et al., 2023) has on LLM pretraining.In particular, we focus on the interplay between pretraining dataset composition and pruning methodology.We further evaluate perplexity pruning in the overtrained and data-constrained regimes.We also investigate whether evaluating the quality of data interventions based on upstream test set perplexity is a sound methodology for gauging downstream performance.",
            "reference_string": "[270199394 | Ankner et al. | 2024 | Citations: 34]"
        },
        {
            "title": "Symmetric Self-Paced Learning for Domain Generalization",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 43,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/29639/31084",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i15.29639?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i15.29639, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2110600983",
                    "name": "Di Zhao"
                },
                {
                    "authorId": "34930533",
                    "name": "Yun Sing Koh"
                },
                {
                    "authorId": "2276344048",
                    "name": "Gillian Dobbie"
                },
                {
                    "authorId": "2293565669",
                    "name": "Hongsheng Hu"
                },
                {
                    "authorId": "2293453267",
                    "name": "Philippe Fournier-Viger"
                }
            ],
            "abstract": "Deep learning methods often suffer performance degradation due to domain shift, where discrepancies exist between training and testing data distributions.\nDomain generalization mitigates this problem by leveraging information from multiple source domains to enhance model generalization capabilities for unseen domains.\nHowever, existing domain generalization methods typically present examples to the model in a random manner, overlooking the potential benefits of structured data presentation.\nTo bridge this gap, we propose a novel learning strategy, Symmetric Self-Paced Learning (SSPL), for domain generalization.\nSSPL consists of a Symmetric Self-Paced training scheduler and a Gradient-based Difficulty Measure (GDM).\nSpecifically, the proposed training scheduler initially focuses on easy examples, gradually shifting emphasis to harder examples as training progresses.\nGDM dynamically evaluates example difficulty through the gradient magnitude with respect to the example itself.\nExperiments across five popular benchmark datasets demonstrate the effectiveness of the proposed learning strategy.",
            "corpus_id": 268696658,
            "sentences": [
                {
                    "corpus_id": "268696658",
                    "title": "Symmetric Self-Paced Learning for Domain Generalization",
                    "text": "Current methods evaluate example difficulty through predefined metrics, such as sentence length, or dynamically update it based on training loss, such as cross-entropy loss. However, predefined difficulty measures do not integrate the model's feedback, and training loss, focusing only on the difference between predictions and ground truth, raises an inaccurate difficulty measurement issue when different examples yield identical training losses. \n\nTo address these limitations, we propose the Gradientbased Difficulty Measure (GDM), which evaluates example difficulty through dynamic measurement of the gradient magnitude with respect to the example itself. Unlike training loss, the gradient avoids inaccurate difficulty assessment by taking input features into account. As a result, even if examples yield the same loss, their gradients can differ. Additionally, loss landscapes can encompass plateaus or saddle points, where training loss remains relatively stable even with substantial shifts in model parameters. In these scenarios, utilizing training loss for evaluating example difficulty can be misleading. Conversely, the gradient provides finergrained insights into changes in model parameters, making the gradient magnitude a more informative approach for evaluating difficulty. The GDM is computed with Equation 8, where \u03be x denotes the difficulty of the example x. ColorJitter. An Empirical Risk Minimization (ERM) baseline is also included, which merges data from all source domains without utilizing domain generalization techniques. Evaluation Metrics. We adopt the leave-one-out-test evaluation strategy as the evaluation metric following the prior works (Li et al. 2017b;Carlucci et al. 2019;Li et al. 2019;Zhou et al. 2022). Specifically, we select one domain as the test domain at a time and use the remaining domains as the source domains for training. We report the accuracy for each separate domain. Performance measures are reported as top-1 classification accuracy (%) averaged over ten runs, along with their corresponding 95% confidence intervals. \n\nNetwork Structure. The network structure is chosen by following the previous work (Carlucci et al. 2019;Li et al. 2019;Zhou et al. 2020). In the Digits dataset, images are resized to 32 \u00d7 32 and converted to RGB by replicating channels. The backbone of the neural network is constructed by 3 \u00d7 3 Conv layers (64 kernels), each followed by a",
                    "score": 0.39611444828693737,
                    "section_title": "Gradient-based Difficulty Measure",
                    "char_start_offset": 9632,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 173
                        },
                        {
                            "start": 174,
                            "end": 448
                        },
                        {
                            "start": 451,
                            "end": 660
                        },
                        {
                            "start": 661,
                            "end": 774
                        },
                        {
                            "start": 775,
                            "end": 853
                        },
                        {
                            "start": 854,
                            "end": 1020
                        },
                        {
                            "start": 1021,
                            "end": 1117
                        },
                        {
                            "start": 1118,
                            "end": 1292
                        },
                        {
                            "start": 1293,
                            "end": 1380
                        },
                        {
                            "start": 1381,
                            "end": 1393
                        },
                        {
                            "start": 1394,
                            "end": 1551
                        },
                        {
                            "start": 1552,
                            "end": 1571
                        },
                        {
                            "start": 1572,
                            "end": 1746
                        },
                        {
                            "start": 1747,
                            "end": 1876
                        },
                        {
                            "start": 1877,
                            "end": 1925
                        },
                        {
                            "start": 1926,
                            "end": 2077
                        },
                        {
                            "start": 2080,
                            "end": 2098
                        },
                        {
                            "start": 2099,
                            "end": 2217
                        },
                        {
                            "start": 2218,
                            "end": 2316
                        },
                        {
                            "start": 2317,
                            "end": 2420
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1675,
                            "end": 1692,
                            "matchedPaperCorpusId": "6037691"
                        },
                        {
                            "start": 1692,
                            "end": 1713,
                            "matchedPaperCorpusId": "81978372"
                        },
                        {
                            "start": 1713,
                            "end": 1728,
                            "matchedPaperCorpusId": "59553457"
                        },
                        {
                            "start": 2162,
                            "end": 2184,
                            "matchedPaperCorpusId": "81978372"
                        },
                        {
                            "start": 2184,
                            "end": 2199,
                            "matchedPaperCorpusId": "59553457"
                        },
                        {
                            "start": 2199,
                            "end": 2215,
                            "matchedPaperCorpusId": "212718063"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7685546875
                }
            ],
            "relevance_judgement": 0.7685546875,
            "relevance_judgment_input_expanded": "# Title: Symmetric Self-Paced Learning for Domain Generalization\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Di Zhao, Yun Sing Koh, Gillian Dobbie, Hongsheng Hu, Philippe Fournier-Viger\n## Abstract\nDeep learning methods often suffer performance degradation due to domain shift, where discrepancies exist between training and testing data distributions.\nDomain generalization mitigates this problem by leveraging information from multiple source domains to enhance model generalization capabilities for unseen domains.\nHowever, existing domain generalization methods typically present examples to the model in a random manner, overlooking the potential benefits of structured data presentation.\nTo bridge this gap, we propose a novel learning strategy, Symmetric Self-Paced Learning (SSPL), for domain generalization.\nSSPL consists of a Symmetric Self-Paced training scheduler and a Gradient-based Difficulty Measure (GDM).\nSpecifically, the proposed training scheduler initially focuses on easy examples, gradually shifting emphasis to harder examples as training progresses.\nGDM dynamically evaluates example difficulty through the gradient magnitude with respect to the example itself.\nExperiments across five popular benchmark datasets demonstrate the effectiveness of the proposed learning strategy.\n## Gradient-based Difficulty Measure\nCurrent methods evaluate example difficulty through predefined metrics, such as sentence length, or dynamically update it based on training loss, such as cross-entropy loss. However, predefined difficulty measures do not integrate the model's feedback, and training loss, focusing only on the difference between predictions and ground truth, raises an inaccurate difficulty measurement issue when different examples yield identical training losses. \n\nTo address these limitations, we propose the Gradientbased Difficulty Measure (GDM), which evaluates example difficulty through dynamic measurement of the gradient magnitude with respect to the example itself. Unlike training loss, the gradient avoids inaccurate difficulty assessment by taking input features into account. As a result, even if examples yield the same loss, their gradients can differ. Additionally, loss landscapes can encompass plateaus or saddle points, where training loss remains relatively stable even with substantial shifts in model parameters. In these scenarios, utilizing training loss for evaluating example difficulty can be misleading. Conversely, the gradient provides finergrained insights into changes in model parameters, making the gradient magnitude a more informative approach for evaluating difficulty. The GDM is computed with Equation 8, where \u03be x denotes the difficulty of the example x. ColorJitter. An Empirical Risk Minimization (ERM) baseline is also included, which merges data from all source domains without utilizing domain generalization techniques. Evaluation Metrics. We adopt the leave-one-out-test evaluation strategy as the evaluation metric following the prior works (Li et al. 2017b;Carlucci et al. 2019;Li et al. 2019;Zhou et al. 2022). Specifically, we select one domain as the test domain at a time and use the remaining domains as the source domains for training. We report the accuracy for each separate domain. Performance measures are reported as top-1 classification accuracy (%) averaged over ten runs, along with their corresponding 95% confidence intervals. \n\nNetwork Structure. The network structure is chosen by following the previous work (Carlucci et al. 2019;Li et al. 2019;Zhou et al. 2020). In the Digits dataset, images are resized to 32 \u00d7 32 and converted to RGB by replicating channels. The backbone of the neural network is constructed by 3 \u00d7 3 Conv layers (64 kernels), each followed by a",
            "reference_string": "[268696658 | Zhao et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Ling-CL: Understanding NLP Models through Linguistic Curricula",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 52,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.20121, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1659451954",
                    "name": "Mohamed Elgaar"
                },
                {
                    "authorId": "143656058",
                    "name": "Hadi Amiri"
                }
            ],
            "abstract": "We employ a characterization of linguistic complexity from psycholinguistic and language acquisition research to develop data-driven curricula to understand the underlying linguistic knowledge that models learn to address NLP tasks. The novelty of our approach is in the development of linguistic curricula derived from data, existing knowledge about linguistic complexity, and model behavior during training. By analyzing several benchmark NLP datasets, our curriculum learning approaches identify sets of linguistic metrics (indices) that inform the challenges and reasoning required to address each task. Our work will inform future research in all NLP areas, allowing linguistic complexity to be considered early in the research and development process. In addition, our work prompts an examination of gold standards and fair evaluation in NLP.",
            "corpus_id": 264819795,
            "sentences": [
                {
                    "corpus_id": "264819795",
                    "title": "Ling-CL: Understanding NLP Models through Linguistic Curricula",
                    "text": "Second, existing approaches quantify the difficulty of data based on instantaneous training loss. However, training loss provides noisy estimates of sample difficulty due to data memorization (Zhang et al., 2017;Arpit et al., 2017) in neural models. We will address both issues as part of this research. \n\nThe contributions of this paper are: \n\n\u2022 incorporating human-verified linguistic complexity information to establish an effective scoring function for assessing the difficulty of text data with respect to NLP models, \u2022 deriving linguistic curricula for NLP models based on linguistic complexity of data and model behavior during training, and \u2022 identifying the core sets of linguistic complexity indices that most contribute to learning NLP tasks by models. \n\nWe evaluate our approach on several NLP tasks that require significant linguistic knowledge and reasoning to be addressed. Experimental results show that our approach can uncover latent linguistic knowledge that is most important for addressing NLP tasks. In addition, our approach obtains consistent performance gain over competing models. Source code and data is available at https://github.com/CLU-UML/Ling-CL. We present a framework for multiview curriculum learning using linguistic complexity indices. Our framework estimates the importance of various linguistic complexity indices, aggregates the resulting importance scores to determine the difficulty of samples for learning NLP tasks, and develops novel curricula for training models using complexity indices. The list of all indices used is in Appendix A.",
                    "score": 0.5037594567063713,
                    "section_title": "Introduction",
                    "char_start_offset": 3511,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 97
                        },
                        {
                            "start": 98,
                            "end": 249
                        },
                        {
                            "start": 250,
                            "end": 303
                        },
                        {
                            "start": 306,
                            "end": 342
                        },
                        {
                            "start": 345,
                            "end": 763
                        },
                        {
                            "start": 766,
                            "end": 888
                        },
                        {
                            "start": 889,
                            "end": 1021
                        },
                        {
                            "start": 1022,
                            "end": 1106
                        },
                        {
                            "start": 1107,
                            "end": 1179
                        },
                        {
                            "start": 1180,
                            "end": 1273
                        },
                        {
                            "start": 1274,
                            "end": 1535
                        },
                        {
                            "start": 1536,
                            "end": 1582
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 192,
                            "end": 212,
                            "matchedPaperCorpusId": "6212000"
                        },
                        {
                            "start": 212,
                            "end": 231,
                            "matchedPaperCorpusId": "11455421"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76025390625
                }
            ],
            "relevance_judgement": 0.76025390625,
            "relevance_judgment_input_expanded": "# Title: Ling-CL: Understanding NLP Models through Linguistic Curricula\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Mohamed Elgaar, Hadi Amiri\n## Abstract\nWe employ a characterization of linguistic complexity from psycholinguistic and language acquisition research to develop data-driven curricula to understand the underlying linguistic knowledge that models learn to address NLP tasks. The novelty of our approach is in the development of linguistic curricula derived from data, existing knowledge about linguistic complexity, and model behavior during training. By analyzing several benchmark NLP datasets, our curriculum learning approaches identify sets of linguistic metrics (indices) that inform the challenges and reasoning required to address each task. Our work will inform future research in all NLP areas, allowing linguistic complexity to be considered early in the research and development process. In addition, our work prompts an examination of gold standards and fair evaluation in NLP.\n## Introduction\nSecond, existing approaches quantify the difficulty of data based on instantaneous training loss. However, training loss provides noisy estimates of sample difficulty due to data memorization (Zhang et al., 2017;Arpit et al., 2017) in neural models. We will address both issues as part of this research. \n\nThe contributions of this paper are: \n\n\u2022 incorporating human-verified linguistic complexity information to establish an effective scoring function for assessing the difficulty of text data with respect to NLP models, \u2022 deriving linguistic curricula for NLP models based on linguistic complexity of data and model behavior during training, and \u2022 identifying the core sets of linguistic complexity indices that most contribute to learning NLP tasks by models. \n\nWe evaluate our approach on several NLP tasks that require significant linguistic knowledge and reasoning to be addressed. Experimental results show that our approach can uncover latent linguistic knowledge that is most important for addressing NLP tasks. In addition, our approach obtains consistent performance gain over competing models. Source code and data is available at https://github.com/CLU-UML/Ling-CL. We present a framework for multiview curriculum learning using linguistic complexity indices. Our framework estimates the importance of various linguistic complexity indices, aggregates the resulting importance scores to determine the difficulty of samples for learning NLP tasks, and develops novel curricula for training models using complexity indices. The list of all indices used is in Appendix A.",
            "reference_string": "[264819795 | Elgaar et al. | 2023 | Citations: 2]"
        },
        {
            "title": "Data Complexity: A New Perspective for Analyzing the Difficulty of Defect Prediction Tasks",
            "venue": "ACM Transactions on Software Engineering and Methodology",
            "year": 2023,
            "reference_count": 149,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3649596",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.03615, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1384264537",
                    "name": "Xiaohui Wan"
                },
                {
                    "authorId": "2115548938",
                    "name": "Zheng Zheng"
                },
                {
                    "authorId": "2407876",
                    "name": "Fangyun Qin"
                },
                {
                    "authorId": "2216488259",
                    "name": "Xuhui Lu"
                }
            ],
            "abstract": "Defect prediction is crucial for software quality assurance and has been extensively researched over recent decades. However, prior studies rarely focus on data complexity in defect prediction tasks, and even less on understanding the difficulties of these tasks from the perspective of data complexity. In this article, we conduct an empirical study to estimate the hardness of over 33,000 instances, employing a set of measures to characterize the inherent difficulty of instances and the characteristics of defect datasets. Our findings indicate that: (1) instance hardness in both classes displays a right-skewed distribution, with the defective class exhibiting a more scattered distribution; (2) class overlap is the primary factor influencing instance hardness and can be characterized through feature, structural, and instance-level overlap; (3) no universal preprocessing technique is applicable to all datasets, and it may not consistently reduce data complexity, fortunately, dataset complexity measures can help identify suitable techniques for specific datasets; (4) integrating data complexity information into the learning process can enhance an algorithm\u2019s learning capacity. In summary, this empirical study highlights the crucial role of data complexity in defect prediction tasks, and provides a novel perspective for advancing research in defect prediction techniques.",
            "corpus_id": 258546757,
            "sentences": [
                {
                    "corpus_id": "258546757",
                    "title": "Data Complexity: A New Perspective for Analyzing the Difficulty of Defect Prediction Tasks",
                    "text": "In the field of machine learning, there has been growing interest in characterizing the complexity of both individual instances and entire datasets, known as data complexity analysis. The concept of instance hardness was initially proposed by M. Smith et al. [105], which estimates the probability that a given instance in a particular dataset will be misclassified. Limited training samples are distributed in a high-dimensional and sparse feature space, which naturally results in certain samples being difficult to classify. For example, overlapping instances and minority instances in small disjuncts tend to have a relatively high instance hardness value, as a ML algorithm has to overfit the limited training instances to learn an accurate decision boundary on the entire dataset. Recently, some researchers have explored incorporating the information of instance hardness into the learning progress to improve the performance of ML models or alleviate data complexity issues [19,42,59,60,107,132]. Moreover, Zhou et al. expanded on the notion of instance hardness by proposing dynamic instance hardness and integrated it into curriculum learning, effectively improving the efficiency and performance of curriculum learning approaches [106,133]. \n\nInstance hardness values reveal instances that are prone to misclassification, while hardness measures explain why they are difficult to classify. M. Smith et al. [107] proposed a set of interpretable hardness measures by examining the learning mechanisms of several simple ML algorithms. These measures provide insights into why particular instances are challenging to classify and how to detect them. Specifically, these measures reveal the difficulty of a classification problem at the instance level, rather than at the aggregate level of the entire dataset. However, these measures can also be averaged to estimate the hardness of the dataset as a whole. After that, Arruda et al. proposed new instance hardness measures as a supplement to the previous work [4]. An open-source Python library named PyHard2 has been developed to implement all of the measures proposed in [107] and [4]. Recent work on dynamic classifier selection has demonstrated the utility of instance hardness measures in identifying classifiers that perform well in confusing or overlapping areas of the dataset, providing indications of local competency.",
                    "score": 0.30887401304497075,
                    "section_title": "Prior Research on Data Complexity Analysis",
                    "char_start_offset": 14516,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 183
                        },
                        {
                            "start": 184,
                            "end": 366
                        },
                        {
                            "start": 367,
                            "end": 527
                        },
                        {
                            "start": 528,
                            "end": 786
                        },
                        {
                            "start": 787,
                            "end": 1004
                        },
                        {
                            "start": 1005,
                            "end": 1251
                        },
                        {
                            "start": 1254,
                            "end": 1400
                        },
                        {
                            "start": 1401,
                            "end": 1542
                        },
                        {
                            "start": 1543,
                            "end": 1656
                        },
                        {
                            "start": 1657,
                            "end": 1816
                        },
                        {
                            "start": 1817,
                            "end": 1913
                        },
                        {
                            "start": 1914,
                            "end": 2021
                        },
                        {
                            "start": 2022,
                            "end": 2144
                        },
                        {
                            "start": 2145,
                            "end": 2385
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 982,
                            "end": 986,
                            "matchedPaperCorpusId": "231580628"
                        },
                        {
                            "start": 986,
                            "end": 989,
                            "matchedPaperCorpusId": "57365456"
                        },
                        {
                            "start": 989,
                            "end": 992,
                            "matchedPaperCorpusId": "202542410"
                        },
                        {
                            "start": 992,
                            "end": 995,
                            "matchedPaperCorpusId": "224703399"
                        },
                        {
                            "start": 995,
                            "end": 999,
                            "matchedPaperCorpusId": "11674608"
                        },
                        {
                            "start": 999,
                            "end": 1003,
                            "matchedPaperCorpusId": "249861618"
                        },
                        {
                            "start": 1241,
                            "end": 1246,
                            "matchedPaperCorpusId": "265098679"
                        },
                        {
                            "start": 1246,
                            "end": 1250,
                            "matchedPaperCorpusId": "227275560"
                        },
                        {
                            "start": 1417,
                            "end": 1422,
                            "matchedPaperCorpusId": "11674608"
                        },
                        {
                            "start": 2017,
                            "end": 2020,
                            "matchedPaperCorpusId": "224817007"
                        },
                        {
                            "start": 2130,
                            "end": 2135,
                            "matchedPaperCorpusId": "11674608"
                        },
                        {
                            "start": 2140,
                            "end": 2143,
                            "matchedPaperCorpusId": "224817007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7568359375
                }
            ],
            "relevance_judgement": 0.7568359375,
            "relevance_judgment_input_expanded": "# Title: Data Complexity: A New Perspective for Analyzing the Difficulty of Defect Prediction Tasks\n# Venue: ACM Transactions on Software Engineering and Methodology\n# Authors: Xiaohui Wan, Zheng Zheng, Fangyun Qin, Xuhui Lu\n## Abstract\nDefect prediction is crucial for software quality assurance and has been extensively researched over recent decades. However, prior studies rarely focus on data complexity in defect prediction tasks, and even less on understanding the difficulties of these tasks from the perspective of data complexity. In this article, we conduct an empirical study to estimate the hardness of over 33,000 instances, employing a set of measures to characterize the inherent difficulty of instances and the characteristics of defect datasets. Our findings indicate that: (1) instance hardness in both classes displays a right-skewed distribution, with the defective class exhibiting a more scattered distribution; (2) class overlap is the primary factor influencing instance hardness and can be characterized through feature, structural, and instance-level overlap; (3) no universal preprocessing technique is applicable to all datasets, and it may not consistently reduce data complexity, fortunately, dataset complexity measures can help identify suitable techniques for specific datasets; (4) integrating data complexity information into the learning process can enhance an algorithm\u2019s learning capacity. In summary, this empirical study highlights the crucial role of data complexity in defect prediction tasks, and provides a novel perspective for advancing research in defect prediction techniques.\n## Prior Research on Data Complexity Analysis\nIn the field of machine learning, there has been growing interest in characterizing the complexity of both individual instances and entire datasets, known as data complexity analysis. The concept of instance hardness was initially proposed by M. Smith et al. [105], which estimates the probability that a given instance in a particular dataset will be misclassified. Limited training samples are distributed in a high-dimensional and sparse feature space, which naturally results in certain samples being difficult to classify. For example, overlapping instances and minority instances in small disjuncts tend to have a relatively high instance hardness value, as a ML algorithm has to overfit the limited training instances to learn an accurate decision boundary on the entire dataset. Recently, some researchers have explored incorporating the information of instance hardness into the learning progress to improve the performance of ML models or alleviate data complexity issues [19,42,59,60,107,132]. Moreover, Zhou et al. expanded on the notion of instance hardness by proposing dynamic instance hardness and integrated it into curriculum learning, effectively improving the efficiency and performance of curriculum learning approaches [106,133]. \n\nInstance hardness values reveal instances that are prone to misclassification, while hardness measures explain why they are difficult to classify. M. Smith et al. [107] proposed a set of interpretable hardness measures by examining the learning mechanisms of several simple ML algorithms. These measures provide insights into why particular instances are challenging to classify and how to detect them. Specifically, these measures reveal the difficulty of a classification problem at the instance level, rather than at the aggregate level of the entire dataset. However, these measures can also be averaged to estimate the hardness of the dataset as a whole. After that, Arruda et al. proposed new instance hardness measures as a supplement to the previous work [4]. An open-source Python library named PyHard2 has been developed to implement all of the measures proposed in [107] and [4]. Recent work on dynamic classifier selection has demonstrated the utility of instance hardness measures in identifying classifiers that perform well in confusing or overlapping areas of the dataset, providing indications of local competency.",
            "reference_string": "[258546757 | Wan et al. | 2023 | Citations: 1]"
        },
        {
            "title": "Comprehensive Benchmarking of Entropy and Margin Based Scoring Metrics for Data Selection",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 17,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.16302, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268493467",
                    "name": "Anusha Sabbineni"
                },
                {
                    "authorId": "2268492466",
                    "name": "Nikhil Anand"
                },
                {
                    "authorId": "2268489661",
                    "name": "Maria Minakova"
                }
            ],
            "abstract": "While data selection methods have been studied extensively in active learning, data pruning, and data augmentation settings, there is little evidence for the efficacy of these methods in industry scale settings, particularly in low-resource languages. Our work presents ways of assessing prospective training examples in those settings for their\"usefulness\"or\"difficulty\". We also demonstrate how these measures can be used in selecting important examples for training supervised machine learning models. We primarily experiment with entropy and Error L2-Norm (EL2N) scores. We use these metrics to curate high quality datasets from a large pool of \\textit{Weak Signal Labeled} data, which assigns no-defect high confidence hypotheses during inference as ground truth labels. We then conduct training data augmentation experiments using these de-identified datasets and demonstrate that score-based selection can result in a 2% decrease in semantic error rate and 4%-7% decrease in domain classification error rate when compared to the baseline technique of random selection.",
            "corpus_id": 265466496,
            "sentences": [
                {
                    "corpus_id": "265466496",
                    "title": "Comprehensive Benchmarking of Entropy and Margin Based Scoring Metrics for Data Selection",
                    "text": "While data selection methods have been studied extensively in active learning, data pruning, and data augmentation settings, there is little evidence for the efficacy of these methods in industry scale settings, particularly in low-resource languages. Our work presents ways of assessing prospective training examples in those settings for their\"usefulness\"or\"difficulty\". We also demonstrate how these measures can be used in selecting important examples for training supervised machine learning models. We primarily experiment with entropy and Error L2-Norm (EL2N) scores. We use these metrics to curate high quality datasets from a large pool of \\textit{Weak Signal Labeled} data, which assigns no-defect high confidence hypotheses during inference as ground truth labels. We then conduct training data augmentation experiments using these de-identified datasets and demonstrate that score-based selection can result in a 2% decrease in semantic error rate and 4%-7% decrease in domain classification error rate when compared to the baseline technique of random selection.",
                    "score": 0.32297905362763957,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75537109375
                }
            ],
            "relevance_judgement": 0.75537109375,
            "relevance_judgment_input_expanded": "# Title: Comprehensive Benchmarking of Entropy and Margin Based Scoring Metrics for Data Selection\n# Venue: arXiv.org\n# Authors: Anusha Sabbineni, Nikhil Anand, Maria Minakova\n## Abstract\nWhile data selection methods have been studied extensively in active learning, data pruning, and data augmentation settings, there is little evidence for the efficacy of these methods in industry scale settings, particularly in low-resource languages. Our work presents ways of assessing prospective training examples in those settings for their\"usefulness\"or\"difficulty\". We also demonstrate how these measures can be used in selecting important examples for training supervised machine learning models. We primarily experiment with entropy and Error L2-Norm (EL2N) scores. We use these metrics to curate high quality datasets from a large pool of \\textit{Weak Signal Labeled} data, which assigns no-defect high confidence hypotheses during inference as ground truth labels. We then conduct training data augmentation experiments using these de-identified datasets and demonstrate that score-based selection can result in a 2% decrease in semantic error rate and 4%-7% decrease in domain classification error rate when compared to the baseline technique of random selection.\n",
            "reference_string": "[265466496 | Sabbineni et al. | 2023 | Citations: 0]"
        },
        {
            "title": "Structural-Entropy-Based Sample Selection for Efficient and Effective Learning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 80,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.02268, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2324006090",
                    "name": "Tianchi Xie"
                },
                {
                    "authorId": "2316675548",
                    "name": "Jiangning Zhu"
                },
                {
                    "authorId": "2324704341",
                    "name": "Guozu Ma"
                },
                {
                    "authorId": "2324761714",
                    "name": "Minzhi Lin"
                },
                {
                    "authorId": "2324226156",
                    "name": "Wei Chen"
                },
                {
                    "authorId": "1604645934",
                    "name": "Weikai Yang"
                },
                {
                    "authorId": "2257322120",
                    "name": "Shixia Liu"
                }
            ],
            "abstract": "Sample selection improves the efficiency and effectiveness of machine learning models by providing informative and representative samples. Typically, samples can be modeled as a sample graph, where nodes are samples and edges represent their similarities. Most existing methods are based on local information, such as the training difficulty of samples, thereby overlooking global information, such as connectivity patterns. This oversight can result in suboptimal selection because global information is crucial for ensuring that the selected samples well represent the structural properties of the graph. To address this issue, we employ structural entropy to quantify global information and losslessly decompose it from the whole graph to individual nodes using the Shapley value. Based on the decomposition, we present $\\textbf{S}$tructural-$\\textbf{E}$ntropy-based sample $\\textbf{S}$election ($\\textbf{SES}$), a method that integrates both global and local information to select informative and representative samples. SES begins by constructing a $k$NN-graph among samples based on their similarities. It then measures sample importance by combining structural entropy (global metric) with training difficulty (local metric). Finally, SES applies importance-biased blue noise sampling to select a set of diverse and representative samples. Comprehensive experiments on three learning scenarios -- supervised learning, active learning, and continual learning -- clearly demonstrate the effectiveness of our method.",
            "corpus_id": 273098749,
            "sentences": [
                {
                    "corpus_id": "273098749",
                    "title": "Structural-Entropy-Based Sample Selection for Efficient and Effective Learning",
                    "text": "Existing sample selection methods primarily utilize local information. They can be classified into two categories based on the information utilized: attribute-based methods and connection-based methods. \n\nAttribute-based methods rely on the attributes of individual samples. A commonly used attribute is the training difficulty, which is typically assessed from two perspectives: confidence and error. Metrics that measure model confidence include the entropy of the prediction vector (Coleman et al., 2020) and the variance of the predicted probabilities across training epochs (Swayamdipta et al., 2020). Metrics that measure model error include EL2N (Paul et al., 2021), which calculates the L 2 norm of the error vector, and the Forgetting score (Toneva et al., 2019), which tracks the frequency of misclassifications after initial correct classifications. AUM (Pleiss et al., 2020) combines both perspectives by measuring the confidence for correct classifications and the error for misclassifications. Based on these metrics, several sample selection methods have been developed. One simple yet effective method is selecting the most difficult samples, as they have a larger impact on the model performance (Paul et al., 2021). However, this method overlooks easy samples, which are crucial for model training when data budgets are limited (Sorscher et al., 2022). To address this issue, CCS (Zheng et al., 2022) divides the dataset into strata based on training difficulty and performs random sampling within each stratum. InfoBatch (Qin et al., 2023) retains some easy samples and enhances their influence by upscaling their gradient. Another line of work uses the gradient as the attribute and aims to match the average gradient of the selected samples with that of all samples (Mirzasoleiman et al., 2019;Killamsetty et al., 2021). However, these gradients depend on the model's current state during training, limiting the applicability of the selected samples to other models. \n\nConnection-based methods utilize local connections within the sample graph to optimize sample diversity and coverage. GraphCut (Iyer et al., 2021) selects samples with weak connections among them to promote diversity while maintaining strong connections to unselected samples for better coverage.",
                    "score": 0.3577321202318,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 5602,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 70
                        },
                        {
                            "start": 71,
                            "end": 202
                        },
                        {
                            "start": 205,
                            "end": 274
                        },
                        {
                            "start": 275,
                            "end": 401
                        },
                        {
                            "start": 402,
                            "end": 606
                        },
                        {
                            "start": 607,
                            "end": 860
                        },
                        {
                            "start": 861,
                            "end": 1007
                        },
                        {
                            "start": 1008,
                            "end": 1085
                        },
                        {
                            "start": 1086,
                            "end": 1233
                        },
                        {
                            "start": 1234,
                            "end": 1370
                        },
                        {
                            "start": 1371,
                            "end": 1529
                        },
                        {
                            "start": 1530,
                            "end": 1642
                        },
                        {
                            "start": 1643,
                            "end": 1841
                        },
                        {
                            "start": 1842,
                            "end": 1987
                        },
                        {
                            "start": 1990,
                            "end": 2107
                        },
                        {
                            "start": 2108,
                            "end": 2286
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 485,
                            "end": 507,
                            "matchedPaperCorpusId": "195750622"
                        },
                        {
                            "start": 579,
                            "end": 605,
                            "matchedPaperCorpusId": "221856637"
                        },
                        {
                            "start": 653,
                            "end": 672,
                            "matchedPaperCorpusId": "235898952"
                        },
                        {
                            "start": 750,
                            "end": 771,
                            "matchedPaperCorpusId": "55481903"
                        },
                        {
                            "start": 865,
                            "end": 886,
                            "matchedPaperCorpusId": "210932316"
                        },
                        {
                            "start": 1213,
                            "end": 1232,
                            "matchedPaperCorpusId": "235898952"
                        },
                        {
                            "start": 1346,
                            "end": 1369,
                            "matchedPaperCorpusId": "250113273"
                        },
                        {
                            "start": 1398,
                            "end": 1418,
                            "matchedPaperCorpusId": "253224188"
                        },
                        {
                            "start": 1540,
                            "end": 1558,
                            "matchedPaperCorpusId": "257427625"
                        },
                        {
                            "start": 1787,
                            "end": 1815,
                            "matchedPaperCorpusId": "211259075"
                        },
                        {
                            "start": 1815,
                            "end": 1840,
                            "matchedPaperCorpusId": "235421747"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75048828125
                }
            ],
            "relevance_judgement": 0.75048828125,
            "relevance_judgment_input_expanded": "# Title: Structural-Entropy-Based Sample Selection for Efficient and Effective Learning\n# Venue: arXiv.org\n# Authors: Tianchi Xie, Jiangning Zhu, Guozu Ma, Minzhi Lin, Wei Chen, Weikai Yang, Shixia Liu\n## Abstract\nSample selection improves the efficiency and effectiveness of machine learning models by providing informative and representative samples. Typically, samples can be modeled as a sample graph, where nodes are samples and edges represent their similarities. Most existing methods are based on local information, such as the training difficulty of samples, thereby overlooking global information, such as connectivity patterns. This oversight can result in suboptimal selection because global information is crucial for ensuring that the selected samples well represent the structural properties of the graph. To address this issue, we employ structural entropy to quantify global information and losslessly decompose it from the whole graph to individual nodes using the Shapley value. Based on the decomposition, we present $\\textbf{S}$tructural-$\\textbf{E}$ntropy-based sample $\\textbf{S}$election ($\\textbf{SES}$), a method that integrates both global and local information to select informative and representative samples. SES begins by constructing a $k$NN-graph among samples based on their similarities. It then measures sample importance by combining structural entropy (global metric) with training difficulty (local metric). Finally, SES applies importance-biased blue noise sampling to select a set of diverse and representative samples. Comprehensive experiments on three learning scenarios -- supervised learning, active learning, and continual learning -- clearly demonstrate the effectiveness of our method.\n## RELATED WORK\nExisting sample selection methods primarily utilize local information. They can be classified into two categories based on the information utilized: attribute-based methods and connection-based methods. \n\nAttribute-based methods rely on the attributes of individual samples. A commonly used attribute is the training difficulty, which is typically assessed from two perspectives: confidence and error. Metrics that measure model confidence include the entropy of the prediction vector (Coleman et al., 2020) and the variance of the predicted probabilities across training epochs (Swayamdipta et al., 2020). Metrics that measure model error include EL2N (Paul et al., 2021), which calculates the L 2 norm of the error vector, and the Forgetting score (Toneva et al., 2019), which tracks the frequency of misclassifications after initial correct classifications. AUM (Pleiss et al., 2020) combines both perspectives by measuring the confidence for correct classifications and the error for misclassifications. Based on these metrics, several sample selection methods have been developed. One simple yet effective method is selecting the most difficult samples, as they have a larger impact on the model performance (Paul et al., 2021). However, this method overlooks easy samples, which are crucial for model training when data budgets are limited (Sorscher et al., 2022). To address this issue, CCS (Zheng et al., 2022) divides the dataset into strata based on training difficulty and performs random sampling within each stratum. InfoBatch (Qin et al., 2023) retains some easy samples and enhances their influence by upscaling their gradient. Another line of work uses the gradient as the attribute and aims to match the average gradient of the selected samples with that of all samples (Mirzasoleiman et al., 2019;Killamsetty et al., 2021). However, these gradients depend on the model's current state during training, limiting the applicability of the selected samples to other models. \n\nConnection-based methods utilize local connections within the sample graph to optimize sample diversity and coverage. GraphCut (Iyer et al., 2021) selects samples with weak connections among them to promote diversity while maintaining strong connections to unselected samples for better coverage.",
            "reference_string": "[273098749 | Xie et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Answering from Sure to Uncertain: Uncertainty-Aware Curriculum Learning for Video Question Answering",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 66,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.01510, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2266086772",
                    "name": "Haopeng Li"
                },
                {
                    "authorId": "2066160726",
                    "name": "Qiuhong Ke"
                },
                {
                    "authorId": "2146826373",
                    "name": "Mingming Gong"
                },
                {
                    "authorId": "2277600596",
                    "name": "Tom Drummond"
                }
            ],
            "abstract": "While significant advancements have been made in video question answering (VideoQA), the potential benefits of enhancing model generalization through tailored difficulty scheduling have been largely overlooked in existing research. This paper seeks to bridge that gap by incorporating VideoQA into a curriculum learning (CL) framework that progressively trains models from simpler to more complex data. Recognizing that conventional self-paced CL methods rely on training loss for difficulty measurement, which might not accurately reflect the intricacies of video-question pairs, we introduce the concept of uncertainty-aware CL. Here, uncertainty serves as the guiding principle for dynamically adjusting the difficulty. Furthermore, we address the challenge posed by uncertainty by presenting a probabilistic modeling approach for VideoQA. Specifically, we conceptualize VideoQA as a stochastic computation graph, where the hidden representations are treated as stochastic variables. This yields two distinct types of uncertainty: one related to the inherent uncertainty in the data and another pertaining to the model's confidence. In practice, we seamlessly integrate the VideoQA model into our framework and conduct comprehensive experiments. The findings affirm that our approach not only achieves enhanced performance but also effectively quantifies uncertainty in the context of VideoQA.",
            "corpus_id": 266741778,
            "sentences": [
                {
                    "corpus_id": "266741778",
                    "title": "Answering from Sure to Uncertain: Uncertainty-Aware Curriculum Learning for Video Question Answering",
                    "text": "Curriculum learning (CL) [18] emulates the human learning process by starting with easier tasks and gradually progressing to more challenging ones. Two central components of CL are the difficulty measure and the training scheduler. In the case of self-paced CL (SPL, where difficulty is measured during training), the loss function is often used as the difficulty measure [19]- [22]. Initially, during training, samples with higher losses are excluded from optimization. As training advances, the threshold is gradually increased to incorporate more complex data into the optimization process. However, relying solely on loss might not accurately represent the inherent difficulty of data, as difficulty is an intrinsic attribute of samples and should be independent of ground truth labels. To overcome this limitation, we propose employing uncertainty as the difficulty measure for SPL. To the best of our knowledge, [44] is the only work that also uses uncertainty for CL. However, our method is essentially different from it: 1) We derive uncertainty by probabilistic modeling, while it obtains data uncertainty by a pretrained language model (predefined CL); 2) We perform CL by re-weighting the data, while it adopts baby step [18] to arrange data; 3) We focus on VideoQA, while it addresses neural machine translation. The pretrained model and the training scheduler based on baby step make [44] more complex to implement than ours.",
                    "score": 0.3685763966626367,
                    "section_title": "B. Curriculum Learning",
                    "char_start_offset": 6196,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 147
                        },
                        {
                            "start": 148,
                            "end": 231
                        },
                        {
                            "start": 232,
                            "end": 383
                        },
                        {
                            "start": 384,
                            "end": 470
                        },
                        {
                            "start": 471,
                            "end": 593
                        },
                        {
                            "start": 594,
                            "end": 790
                        },
                        {
                            "start": 791,
                            "end": 887
                        },
                        {
                            "start": 888,
                            "end": 974
                        },
                        {
                            "start": 975,
                            "end": 1324
                        },
                        {
                            "start": 1325,
                            "end": 1438
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 25,
                            "end": 29,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 372,
                            "end": 376,
                            "matchedPaperCorpusId": "1977996"
                        },
                        {
                            "start": 378,
                            "end": 382,
                            "matchedPaperCorpusId": "94785766"
                        },
                        {
                            "start": 1232,
                            "end": 1236,
                            "matchedPaperCorpusId": "873046"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7392578125
                }
            ],
            "relevance_judgement": 0.7392578125,
            "relevance_judgment_input_expanded": "# Title: Answering from Sure to Uncertain: Uncertainty-Aware Curriculum Learning for Video Question Answering\n# Venue: arXiv.org\n# Authors: Haopeng Li, Qiuhong Ke, Mingming Gong, Tom Drummond\n## Abstract\nWhile significant advancements have been made in video question answering (VideoQA), the potential benefits of enhancing model generalization through tailored difficulty scheduling have been largely overlooked in existing research. This paper seeks to bridge that gap by incorporating VideoQA into a curriculum learning (CL) framework that progressively trains models from simpler to more complex data. Recognizing that conventional self-paced CL methods rely on training loss for difficulty measurement, which might not accurately reflect the intricacies of video-question pairs, we introduce the concept of uncertainty-aware CL. Here, uncertainty serves as the guiding principle for dynamically adjusting the difficulty. Furthermore, we address the challenge posed by uncertainty by presenting a probabilistic modeling approach for VideoQA. Specifically, we conceptualize VideoQA as a stochastic computation graph, where the hidden representations are treated as stochastic variables. This yields two distinct types of uncertainty: one related to the inherent uncertainty in the data and another pertaining to the model's confidence. In practice, we seamlessly integrate the VideoQA model into our framework and conduct comprehensive experiments. The findings affirm that our approach not only achieves enhanced performance but also effectively quantifies uncertainty in the context of VideoQA.\n## B. Curriculum Learning\nCurriculum learning (CL) [18] emulates the human learning process by starting with easier tasks and gradually progressing to more challenging ones. Two central components of CL are the difficulty measure and the training scheduler. In the case of self-paced CL (SPL, where difficulty is measured during training), the loss function is often used as the difficulty measure [19]- [22]. Initially, during training, samples with higher losses are excluded from optimization. As training advances, the threshold is gradually increased to incorporate more complex data into the optimization process. However, relying solely on loss might not accurately represent the inherent difficulty of data, as difficulty is an intrinsic attribute of samples and should be independent of ground truth labels. To overcome this limitation, we propose employing uncertainty as the difficulty measure for SPL. To the best of our knowledge, [44] is the only work that also uses uncertainty for CL. However, our method is essentially different from it: 1) We derive uncertainty by probabilistic modeling, while it obtains data uncertainty by a pretrained language model (predefined CL); 2) We perform CL by re-weighting the data, while it adopts baby step [18] to arrange data; 3) We focus on VideoQA, while it addresses neural machine translation. The pretrained model and the training scheduler based on baby step make [44] more complex to implement than ours.",
            "reference_string": "[266741778 | Li et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Curriculum Learning: Theories, Approaches, Applications, Tools, and Future Directions in the Era of Large Language Models",
            "venue": "The Web Conference",
            "year": 2024,
            "reference_count": 20,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3589335.3641257",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3589335.3641257?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3589335.3641257, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256599610",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2261888564",
                    "name": "Yuwei Zhou"
                },
                {
                    "authorId": "2191043236",
                    "name": "Hong Chen"
                },
                {
                    "authorId": "2156154955",
                    "name": "Wenwu Zhu"
                }
            ],
            "abstract": "This tutorial focuses on curriculum learning (CL), an important topic in machine learning, which gains an increasing amount of attention in the research community. CL is a learning paradigm that enables machines to learn from easy data to hard data, imitating the meaningful procedure of human learning with curricula. As an easy-to-use plug-in, CL has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision, natural language processing, data mining, reinforcement learning, etc. Therefore, it is essential introducing CL to more scholars and researchers in the machine learning community. However, there have been no tutorials on CL so far, motivating the organization of our tutorial on CL at WWW 2024. To give a comprehensive tutorial on CL, we plan to organize it from the following aspects: (1) theories, (2) approaches, (3) applications, (4) tools and (5) future directions. First, we introduce the motivations, theories and insights behind CL. Second, we advocate novel, high-quality approaches, as well as innovative solutions to the challenging problems in CL. Then we present the applications of CL in various scenarios, followed by some relevant tools. In the end, we discuss open questions and the future direction in the era of large language models. We believe this topic is at the core of the scope of WWW and is attractive to the audience interested in machine learning from both academia and industry.",
            "corpus_id": 269762685,
            "sentences": [
                {
                    "corpus_id": "269762685",
                    "title": "Curriculum Learning: Theories, Approaches, Applications, Tools, and Future Directions in the Era of Large Language Models",
                    "text": "A general framework for curriculum design consists of two core components: Difficulty Measurer and Training Scheduler, which decide two things respectively: 1) What kind of training data is supposed to be easier than other data?2) When should we present harder data for training, and how much more?Thus, we can divide existing CL methods into two types: when both the Difficulty Measurer and Training Scheduler are designed by human prior knowledge with no data-driven algorithms involved, we call the CL method predefined CL.If any (or both) of the two components are learned by data-driven models or algorithms, then we denote the CL method as automatic CL.\n\nIn the early stages, predefined CL takes the mainstream.However, this type of predefined approach is not flexible and general enough for widespread applications.In 2010, Kumar et al. propose self-paced learning (SPL), enabling automatic curriculum scheduling by ordering data according to their training loss.Subsequently, a variety of automatic curriculum learning methods have continued to emerge.For example, transfer learning methods employ teacher models to offer student models curricula.Reinforcement learning methods allow teacher models to adapt curriculum based on the feedback from student models.In addition, there are other ones based on Bayesian optimization, meta-learning, and adversarial learning for implementing automatic curriculum learning.All representative approaches of both categories will be reviewed and discussed in this tutorial.",
                    "score": 0.32973882748331895,
                    "section_title": "Approaches",
                    "char_start_offset": 5464,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 228
                        },
                        {
                            "start": 228,
                            "end": 298
                        },
                        {
                            "start": 298,
                            "end": 526
                        },
                        {
                            "start": 526,
                            "end": 659
                        },
                        {
                            "start": 661,
                            "end": 717
                        },
                        {
                            "start": 717,
                            "end": 822
                        },
                        {
                            "start": 822,
                            "end": 970
                        },
                        {
                            "start": 970,
                            "end": 1060
                        },
                        {
                            "start": 1060,
                            "end": 1155
                        },
                        {
                            "start": 1155,
                            "end": 1269
                        },
                        {
                            "start": 1269,
                            "end": 1422
                        },
                        {
                            "start": 1422,
                            "end": 1519
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.736328125
                }
            ],
            "relevance_judgement": 0.736328125,
            "relevance_judgment_input_expanded": "# Title: Curriculum Learning: Theories, Approaches, Applications, Tools, and Future Directions in the Era of Large Language Models\n# Venue: The Web Conference\n# Authors: Xin Wang, Yuwei Zhou, Hong Chen, Wenwu Zhu\n## Abstract\nThis tutorial focuses on curriculum learning (CL), an important topic in machine learning, which gains an increasing amount of attention in the research community. CL is a learning paradigm that enables machines to learn from easy data to hard data, imitating the meaningful procedure of human learning with curricula. As an easy-to-use plug-in, CL has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision, natural language processing, data mining, reinforcement learning, etc. Therefore, it is essential introducing CL to more scholars and researchers in the machine learning community. However, there have been no tutorials on CL so far, motivating the organization of our tutorial on CL at WWW 2024. To give a comprehensive tutorial on CL, we plan to organize it from the following aspects: (1) theories, (2) approaches, (3) applications, (4) tools and (5) future directions. First, we introduce the motivations, theories and insights behind CL. Second, we advocate novel, high-quality approaches, as well as innovative solutions to the challenging problems in CL. Then we present the applications of CL in various scenarios, followed by some relevant tools. In the end, we discuss open questions and the future direction in the era of large language models. We believe this topic is at the core of the scope of WWW and is attractive to the audience interested in machine learning from both academia and industry.\n## Approaches\nA general framework for curriculum design consists of two core components: Difficulty Measurer and Training Scheduler, which decide two things respectively: 1) What kind of training data is supposed to be easier than other data?2) When should we present harder data for training, and how much more?Thus, we can divide existing CL methods into two types: when both the Difficulty Measurer and Training Scheduler are designed by human prior knowledge with no data-driven algorithms involved, we call the CL method predefined CL.If any (or both) of the two components are learned by data-driven models or algorithms, then we denote the CL method as automatic CL.\n\nIn the early stages, predefined CL takes the mainstream.However, this type of predefined approach is not flexible and general enough for widespread applications.In 2010, Kumar et al. propose self-paced learning (SPL), enabling automatic curriculum scheduling by ordering data according to their training loss.Subsequently, a variety of automatic curriculum learning methods have continued to emerge.For example, transfer learning methods employ teacher models to offer student models curricula.Reinforcement learning methods allow teacher models to adapt curriculum based on the feedback from student models.In addition, there are other ones based on Bayesian optimization, meta-learning, and adversarial learning for implementing automatic curriculum learning.All representative approaches of both categories will be reviewed and discussed in this tutorial.",
            "reference_string": "[269762685 | Wang et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 86,
            "citation_count": 4,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2210.12499",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.12499, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48810605",
                    "name": "Fenia Christopoulou"
                },
                {
                    "authorId": "2346538",
                    "name": "Gerasimos Lampouras"
                },
                {
                    "authorId": "2676143",
                    "name": "Ignacio Iacobacci"
                }
            ],
            "abstract": "Curriculum Learning (CL) is a technique of training models via ranking examples in a typically increasing difficulty trend with the aim of accelerating convergence and improving generalisability. Current approaches for Natural Language Understanding (NLU) tasks use CL to improve in-distribution data performance often via heuristic-oriented or task-agnostic difficulties. In this work, instead, we employ CL for NLU by taking advantage of training dynamics as difficulty metrics, i.e., statistics that measure the behavior of the model at hand on specific task-data instances during training and propose modifications of existing CL schedulers based on these statistics. Differently from existing works, we focus on evaluating models on in-distribution (ID), out-of-distribution (OOD) as well as zero-shot (ZS) cross-lingual transfer datasets. We show across several NLU tasks that CL with training dynamics can result in better performance mostly on zero-shot cross-lingual transfer and OOD settings with improvements up by 8.5% in certain cases. Overall, experiments indicate that training dynamics can lead to better performing models with smoother training compared to other difficulty metrics while being 20% faster on average. In addition, through analysis we shed light on the correlations of task-specific versus task-agnostic metrics.",
            "corpus_id": 247694098,
            "sentences": [
                {
                    "corpus_id": "247694098",
                    "title": "Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU",
                    "text": "Typically, CL requires a measure of difficulty for each example in the training set. Existing methods using CL in NLU tasks rely on heuristics such as sentence length, word rarity, depth of the dependency tree (Platanios et al., 2019;Tay et al., 2019), metrics based on item-response theory (Lalor and Yu, 2020) or task-agnostic model metrics such as perplexity (Zhou et al., 2020). Such metrics have been employed to either improve in-distribution performance on NLU or Machine Translation. However, their effect is still underexplored on other settings. \n\nIn this study instead, we propose to adopt training dynamics (Swayamdipta et al., 2020, TD) as difficulty measures for CL and fine-tune models with curricula on downstream tasks. TD were re-cently proposed as a set of statistics collected during the course of a model's training to automatically evaluate dataset quality, by identifying annotation artifacts. These statistics, offer a 3-dimensional view of a model's uncertainty towards each training example classifying them into distinct areas-easy, ambiguous and hard examples for a model to learn. \n\nWe test a series of easy-to-hard curricula using TD, namely TD-CL, with existing schedulers as well as novel modifications of those and experiment with other task-specific and task-agnostic metrics. We show performances and training times on three settings: in-distribution (ID), out-of-distribution (OOD) and zero-shot (ZS) transfer to languages different than English. To the best of our knowledge, no prior work on NLU considers the impact of CL on all these settings. To consolidate our findings, we evaluate models on different classification tasks, including Natural Language Inference, Paraphrase Identification, Commonsense Causal Reasoning and Document Classification. \n\nOur findings suggest that TD-CL provides better zero-shot cross-lingual transfer up to 1.2% over prior work and can gain an average speedup of 20%, up to 51% in certain cases. In ID settings CL has minimal to no impact, while in OOD settings models trained with TD-CL can boost performance up to 8.5% on a different domain.",
                    "score": 0.31872029292056714,
                    "section_title": "Introduction",
                    "char_start_offset": 1638,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 84
                        },
                        {
                            "start": 85,
                            "end": 382
                        },
                        {
                            "start": 383,
                            "end": 491
                        },
                        {
                            "start": 492,
                            "end": 555
                        },
                        {
                            "start": 558,
                            "end": 736
                        },
                        {
                            "start": 737,
                            "end": 916
                        },
                        {
                            "start": 917,
                            "end": 1109
                        },
                        {
                            "start": 1112,
                            "end": 1310
                        },
                        {
                            "start": 1311,
                            "end": 1482
                        },
                        {
                            "start": 1483,
                            "end": 1583
                        },
                        {
                            "start": 1584,
                            "end": 1789
                        },
                        {
                            "start": 1792,
                            "end": 1967
                        },
                        {
                            "start": 1968,
                            "end": 2115
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 210,
                            "end": 234,
                            "matchedPaperCorpusId": "85498775"
                        },
                        {
                            "start": 234,
                            "end": 251,
                            "matchedPaperCorpusId": "166228313"
                        },
                        {
                            "start": 291,
                            "end": 311,
                            "matchedPaperCorpusId": "226226711"
                        },
                        {
                            "start": 362,
                            "end": 381,
                            "matchedPaperCorpusId": "220047761"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72509765625
                },
                {
                    "corpus_id": "247694098",
                    "title": "Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU",
                    "text": "As aforementioned, we use training dynamics (Swayamdipta et al., 2020), i.e. statistics originally introduced to analyse dataset quality, as difficulty metrics. The suitability of such statistics to serve as difficulty measures for CL is encapsulated in three core aspects. Firstly, training dynamics are straightforward. They can be easily obtained by training a single model on the target dataset and keeping statistics about its predictions on the training set. Secondly, training dynamics correlate well with model uncertainty and follow a similar trend to human (dis)agreement in terms of data annotation, essentially combining the view of both worlds. Finally, training dynamics manifest a clear pattern of separating instances into distinct areaseasy, ambiguous and hard examples for a model to learn-something that aligns well with the ideas behind Curriculum Learning. The difficulty of an example (x i , y i ) can be determined by a function f , where an example i is considered more difficult than example j if f (x i , y i ) > f (x j , y j ). We list three difficulty metrics that use statistics during the course of a model's training, as follows: CONFIDENCE (CONF) of an example x i is the average probability assigned to the gold label y i by a model with parameters \u03b8 across a number of epochs E. This is a continuous metric with higher values corresponding to easier examples. \n\nVARIABILITY (VAR) of an example x i is the standard deviation of the probabilities assigned to the gold label y i across E epochs. It is a continuous metric with higher values indicating greater uncertainty for a training example. \n\nConfidence and correctness are the primary metrics that we use in our curricula since low and high values correspond to hard and easy examples respectively. On the other hand, variability is used as an auxiliary metric since only high scores clearly represent uncertain examples while low scores offer no important information on their own.",
                    "score": 0.4572523256838916,
                    "section_title": "Difficulty Metrics",
                    "char_start_offset": 9233,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 160
                        },
                        {
                            "start": 161,
                            "end": 273
                        },
                        {
                            "start": 274,
                            "end": 321
                        },
                        {
                            "start": 322,
                            "end": 464
                        },
                        {
                            "start": 465,
                            "end": 657
                        },
                        {
                            "start": 658,
                            "end": 877
                        },
                        {
                            "start": 878,
                            "end": 1054
                        },
                        {
                            "start": 1055,
                            "end": 1393
                        },
                        {
                            "start": 1396,
                            "end": 1526
                        },
                        {
                            "start": 1527,
                            "end": 1626
                        },
                        {
                            "start": 1629,
                            "end": 1785
                        },
                        {
                            "start": 1786,
                            "end": 1969
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 44,
                            "end": 70,
                            "matchedPaperCorpusId": "221856637"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6904296875
                },
                {
                    "corpus_id": "247694098",
                    "title": "Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU",
                    "text": "Curriculum Learning (CL) is a technique of training models via ranking examples in a typically increasing difficulty trend with the aim of accelerating convergence and improving generalisability. Current approaches for Natural Language Understanding (NLU) tasks use CL to improve in-distribution data performance often via heuristic-oriented or task-agnostic difficulties. In this work, instead, we employ CL for NLU by taking advantage of training dynamics as difficulty metrics, i.e., statistics that measure the behavior of the model at hand on specific task-data instances during training and propose modifications of existing CL schedulers based on these statistics. Differently from existing works, we focus on evaluating models on in-distribution (ID), out-of-distribution (OOD) as well as zero-shot (ZS) cross-lingual transfer datasets. We show across several NLU tasks that CL with training dynamics can result in better performance mostly on zero-shot cross-lingual transfer and OOD settings with improvements up by 8.5% in certain cases. Overall, experiments indicate that training dynamics can lead to better performing models with smoother training compared to other difficulty metrics while being 20% faster on average. In addition, through analysis we shed light on the correlations of task-specific versus task-agnostic metrics.",
                    "score": 0.3467761238237365,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.60498046875
                }
            ],
            "relevance_judgement": 0.72509765625,
            "relevance_judgment_input_expanded": "# Title: Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Fenia Christopoulou, Gerasimos Lampouras, Ignacio Iacobacci\n## Abstract\nCurriculum Learning (CL) is a technique of training models via ranking examples in a typically increasing difficulty trend with the aim of accelerating convergence and improving generalisability. Current approaches for Natural Language Understanding (NLU) tasks use CL to improve in-distribution data performance often via heuristic-oriented or task-agnostic difficulties. In this work, instead, we employ CL for NLU by taking advantage of training dynamics as difficulty metrics, i.e., statistics that measure the behavior of the model at hand on specific task-data instances during training and propose modifications of existing CL schedulers based on these statistics. Differently from existing works, we focus on evaluating models on in-distribution (ID), out-of-distribution (OOD) as well as zero-shot (ZS) cross-lingual transfer datasets. We show across several NLU tasks that CL with training dynamics can result in better performance mostly on zero-shot cross-lingual transfer and OOD settings with improvements up by 8.5% in certain cases. Overall, experiments indicate that training dynamics can lead to better performing models with smoother training compared to other difficulty metrics while being 20% faster on average. In addition, through analysis we shed light on the correlations of task-specific versus task-agnostic metrics.\n## Introduction\nTypically, CL requires a measure of difficulty for each example in the training set. Existing methods using CL in NLU tasks rely on heuristics such as sentence length, word rarity, depth of the dependency tree (Platanios et al., 2019;Tay et al., 2019), metrics based on item-response theory (Lalor and Yu, 2020) or task-agnostic model metrics such as perplexity (Zhou et al., 2020). Such metrics have been employed to either improve in-distribution performance on NLU or Machine Translation. However, their effect is still underexplored on other settings. \n\nIn this study instead, we propose to adopt training dynamics (Swayamdipta et al., 2020, TD) as difficulty measures for CL and fine-tune models with curricula on downstream tasks. TD were re-cently proposed as a set of statistics collected during the course of a model's training to automatically evaluate dataset quality, by identifying annotation artifacts. These statistics, offer a 3-dimensional view of a model's uncertainty towards each training example classifying them into distinct areas-easy, ambiguous and hard examples for a model to learn. \n\nWe test a series of easy-to-hard curricula using TD, namely TD-CL, with existing schedulers as well as novel modifications of those and experiment with other task-specific and task-agnostic metrics. We show performances and training times on three settings: in-distribution (ID), out-of-distribution (OOD) and zero-shot (ZS) transfer to languages different than English. To the best of our knowledge, no prior work on NLU considers the impact of CL on all these settings. To consolidate our findings, we evaluate models on different classification tasks, including Natural Language Inference, Paraphrase Identification, Commonsense Causal Reasoning and Document Classification. \n\nOur findings suggest that TD-CL provides better zero-shot cross-lingual transfer up to 1.2% over prior work and can gain an average speedup of 20%, up to 51% in certain cases. In ID settings CL has minimal to no impact, while in OOD settings models trained with TD-CL can boost performance up to 8.5% on a different domain.\n\n## Difficulty Metrics\nAs aforementioned, we use training dynamics (Swayamdipta et al., 2020), i.e. statistics originally introduced to analyse dataset quality, as difficulty metrics. The suitability of such statistics to serve as difficulty measures for CL is encapsulated in three core aspects. Firstly, training dynamics are straightforward. They can be easily obtained by training a single model on the target dataset and keeping statistics about its predictions on the training set. Secondly, training dynamics correlate well with model uncertainty and follow a similar trend to human (dis)agreement in terms of data annotation, essentially combining the view of both worlds. Finally, training dynamics manifest a clear pattern of separating instances into distinct areaseasy, ambiguous and hard examples for a model to learn-something that aligns well with the ideas behind Curriculum Learning. The difficulty of an example (x i , y i ) can be determined by a function f , where an example i is considered more difficult than example j if f (x i , y i ) > f (x j , y j ). We list three difficulty metrics that use statistics during the course of a model's training, as follows: CONFIDENCE (CONF) of an example x i is the average probability assigned to the gold label y i by a model with parameters \u03b8 across a number of epochs E. This is a continuous metric with higher values corresponding to easier examples. \n\nVARIABILITY (VAR) of an example x i is the standard deviation of the probabilities assigned to the gold label y i across E epochs. It is a continuous metric with higher values indicating greater uncertainty for a training example. \n\nConfidence and correctness are the primary metrics that we use in our curricula since low and high values correspond to hard and easy examples respectively. On the other hand, variability is used as an auxiliary metric since only high scores clearly represent uncertain examples while low scores offer no important information on their own.",
            "reference_string": "[247694098 | Christopoulou et al. | 2022 | Citations: 4]"
        },
        {
            "title": "Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 36,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2307.08859",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.08859, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2165570025",
                    "name": "Nidhi Vakil"
                },
                {
                    "authorId": "143656058",
                    "name": "Hadi Amiri"
                }
            ],
            "abstract": "A curriculum is a planned sequence of learning materials and an effective one can make learning efficient and effective for both humans and machines. Recent studies developed effective data-driven curriculum learning approaches for training graph neural networks in language applications. However, existing curriculum learning approaches often employ a single criterion of difficulty in their training paradigms. In this paper, we propose a new perspective on curriculum learning by introducing a novel approach that builds on graph complexity formalisms (as difficulty criteria) and model competence during training. The model consists of a scheduling scheme which derives effective curricula by accounting for different views of sample difficulty and model competence during training. The proposed solution advances existing research in curriculum learning for graph neural networks with the ability to incorporate a fine-grained spectrum of graph difficulty criteria in their training paradigms. Experimental results on real-world link prediction and node classification tasks illustrate the effectiveness of the proposed approach.",
            "corpus_id": 259370548,
            "sentences": [
                {
                    "corpus_id": "259370548",
                    "title": "Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach",
                    "text": "A curriculum is a planned sequence of learning materials and an effective one can make learning efficient and effective for both humans and machines. Recent studies developed effective data-driven curriculum learning approaches for training graph neural networks in language applications. However, existing curriculum learning approaches often employ a single criterion of difficulty in their training paradigms. In this paper, we propose a new perspective on curriculum learning by introducing a novel approach that builds on graph complexity formalisms (as difficulty criteria) and model competence during training. The model consists of a scheduling scheme which derives effective curricula by accounting for different views of sample difficulty and model competence during training. The proposed solution advances existing research in curriculum learning for graph neural networks with the ability to incorporate a fine-grained spectrum of graph difficulty criteria in their training paradigms. Experimental results on real-world link prediction and node classification tasks illustrate the effectiveness of the proposed approach.",
                    "score": 0.3078801513508629,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72314453125
                },
                {
                    "corpus_id": "259370548",
                    "title": "Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach",
                    "text": "Curriculum learning (Bengio et al., 2009) aims to improve the generalizability of a model by gradually training it with easy examples followed by hard ones. Castells et al. (2020) introduced a generic loss function called SuperLoss (SL) which can be added on top of any target-task loss function to dynamically weight the training samples according to their difficulty for the model using a batch-wise threshold. Zhou et al. (2020) proposed dynamic instance hardness to determine the difficulty of an instance with running average of the hardness metric over training history.\n\nCurriculum learning has been investigated in NLP (Elman, 1993;Sachan and Xing, 2016;Settles and Meeder, 2016;Amiri et al., 2017;Platanios et al., 2019;Amiri, 2019;Zhang et al., 2019;Lalor and Yu, 2020;Xu et al., 2020;Chu et al., 2021;Liu et al., 2021;Kreutzer et al., 2021;Agrawal and Carpuat, 2022;Maharana and Bansal, 2022). Specifically, Settles and Meeder (2016);Amiri et al. (2017) proposed spaced repetition-based curricula based on psycholinguistic theory where the training data is scheduled by increasing intervals of time between consecutive reviews of previously learned data samples. Zhang et al. (2019) investigated curriculum learning for domain adaptation in neural machine translation, where samples were grouped and ranked based on their similarity score such that more similar samples are seen earlier and more frequently during training. Platanios  2019) proposed an approach to use competency function using rarity of words or length of a sentence for neural machine translation and inspired Liu et al. (2021) to define a curriculum based on multi-modal (text and image) data to choose which modality should be used for training. The model uses sample perplexity at batch level to select the modality for training. Linguistic features such as word rarity or length of sentence in (Platanios et al., 2019) and sample perplexity in (Liu et al., 2021) were used as measures of difficulty. Xu et al. (2020) designed a curriculum learning approach for NLP tasks using cross-view of training data to",
                    "score": 0.3267790869213816,
                    "section_title": "Related Work",
                    "char_start_offset": 23435,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 20,
                            "end": 41,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 157,
                            "end": 179,
                            "matchedPaperCorpusId": "227275456"
                        },
                        {
                            "start": 413,
                            "end": 431,
                            "matchedPaperCorpusId": "227275560"
                        },
                        {
                            "start": 627,
                            "end": 640,
                            "matchedPaperCorpusId": "2105042"
                        },
                        {
                            "start": 640,
                            "end": 662,
                            "matchedPaperCorpusId": "16503693"
                        },
                        {
                            "start": 662,
                            "end": 687,
                            "matchedPaperCorpusId": "14155276"
                        },
                        {
                            "start": 687,
                            "end": 706,
                            "matchedPaperCorpusId": "1916665"
                        },
                        {
                            "start": 706,
                            "end": 729,
                            "matchedPaperCorpusId": "85498775"
                        },
                        {
                            "start": 729,
                            "end": 741,
                            "matchedPaperCorpusId": "174799918"
                        },
                        {
                            "start": 741,
                            "end": 760,
                            "matchedPaperCorpusId": "155089817"
                        },
                        {
                            "start": 760,
                            "end": 779,
                            "matchedPaperCorpusId": "226226711"
                        },
                        {
                            "start": 779,
                            "end": 795,
                            "matchedPaperCorpusId": "220045816"
                        },
                        {
                            "start": 795,
                            "end": 812,
                            "matchedPaperCorpusId": "235671564"
                        },
                        {
                            "start": 812,
                            "end": 829,
                            "matchedPaperCorpusId": "236460239"
                        },
                        {
                            "start": 829,
                            "end": 851,
                            "matchedPaperCorpusId": "238856825"
                        },
                        {
                            "start": 851,
                            "end": 877,
                            "matchedPaperCorpusId": "247518847"
                        },
                        {
                            "start": 877,
                            "end": 903,
                            "matchedPaperCorpusId": "250391006"
                        },
                        {
                            "start": 919,
                            "end": 945,
                            "matchedPaperCorpusId": "14155276"
                        },
                        {
                            "start": 945,
                            "end": 964,
                            "matchedPaperCorpusId": "1916665"
                        },
                        {
                            "start": 1174,
                            "end": 1193,
                            "matchedPaperCorpusId": "155089817"
                        },
                        {
                            "start": 1590,
                            "end": 1607,
                            "matchedPaperCorpusId": "236460239"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.666015625
                }
            ],
            "relevance_judgement": 0.72314453125,
            "relevance_judgment_input_expanded": "# Title: Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Nidhi Vakil, Hadi Amiri\n## Abstract\nA curriculum is a planned sequence of learning materials and an effective one can make learning efficient and effective for both humans and machines. Recent studies developed effective data-driven curriculum learning approaches for training graph neural networks in language applications. However, existing curriculum learning approaches often employ a single criterion of difficulty in their training paradigms. In this paper, we propose a new perspective on curriculum learning by introducing a novel approach that builds on graph complexity formalisms (as difficulty criteria) and model competence during training. The model consists of a scheduling scheme which derives effective curricula by accounting for different views of sample difficulty and model competence during training. The proposed solution advances existing research in curriculum learning for graph neural networks with the ability to incorporate a fine-grained spectrum of graph difficulty criteria in their training paradigms. Experimental results on real-world link prediction and node classification tasks illustrate the effectiveness of the proposed approach.\n## Related Work\nCurriculum learning (Bengio et al., 2009) aims to improve the generalizability of a model by gradually training it with easy examples followed by hard ones. Castells et al. (2020) introduced a generic loss function called SuperLoss (SL) which can be added on top of any target-task loss function to dynamically weight the training samples according to their difficulty for the model using a batch-wise threshold. Zhou et al. (2020) proposed dynamic instance hardness to determine the difficulty of an instance with running average of the hardness metric over training history.\n\nCurriculum learning has been investigated in NLP (Elman, 1993;Sachan and Xing, 2016;Settles and Meeder, 2016;Amiri et al., 2017;Platanios et al., 2019;Amiri, 2019;Zhang et al., 2019;Lalor and Yu, 2020;Xu et al., 2020;Chu et al., 2021;Liu et al., 2021;Kreutzer et al., 2021;Agrawal and Carpuat, 2022;Maharana and Bansal, 2022). Specifically, Settles and Meeder (2016);Amiri et al. (2017) proposed spaced repetition-based curricula based on psycholinguistic theory where the training data is scheduled by increasing intervals of time between consecutive reviews of previously learned data samples. Zhang et al. (2019) investigated curriculum learning for domain adaptation in neural machine translation, where samples were grouped and ranked based on their similarity score such that more similar samples are seen earlier and more frequently during training. Platanios  2019) proposed an approach to use competency function using rarity of words or length of a sentence for neural machine translation and inspired Liu et al. (2021) to define a curriculum based on multi-modal (text and image) data to choose which modality should be used for training. The model uses sample perplexity at batch level to select the modality for training. Linguistic features such as word rarity or length of sentence in (Platanios et al., 2019) and sample perplexity in (Liu et al., 2021) were used as measures of difficulty. Xu et al. (2020) designed a curriculum learning approach for NLP tasks using cross-view of training data to",
            "reference_string": "[259370548 | Vakil et al. | 2023 | Citations: 3]"
        },
        {
            "title": "HuCurl: Human-induced Curriculum Discovery",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 49,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.acl-long.104.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.07412, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1659451954",
                    "name": "Mohamed Elgaar"
                },
                {
                    "authorId": "143656058",
                    "name": "Hadi Amiri"
                }
            ],
            "abstract": "We introduce the problem of curriculum discovery and describe a curriculum learning framework capable of discovering effective curricula in a curriculum space based on prior knowledge about sample difficulty. Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as apposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy transition curricula are often at the risk of underperforming, and (iii): the curricula discovered for smaller datasets and models perform well on larger datasets and models respectively. The proposed framework encompasses some of the existing curriculum learning approaches and can discover curricula that outperform them across several NLP tasks.",
            "corpus_id": 259370648,
            "sentences": [
                {
                    "corpus_id": "259370648",
                    "title": "HuCurl: Human-induced Curriculum Discovery",
                    "text": "We introduce the problem of curriculum discovery and describe a curriculum learning framework capable of discovering effective curricula in a curriculum space based on prior knowledge about sample difficulty. Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as apposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy transition curricula are often at the risk of underperforming, and (iii): the curricula discovered for smaller datasets and models perform well on larger datasets and models respectively. The proposed framework encompasses some of the existing curriculum learning approaches and can discover curricula that outperform them across several NLP tasks.",
                    "score": 0.36721490268732726,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7119140625
                },
                {
                    "corpus_id": "259370648",
                    "title": "HuCurl: Human-induced Curriculum Discovery",
                    "text": "(Erhan et al., 2010;Wu et al., 2021), which limits the curriculum space that can be examined for finding effective curricula.\n\nThis paper advocates a re-imagining of CL paradigms by introducing and formalizing the task of curriculum discovery, which aims to find effective curricula for a given model and dataset over a curriculum space. The present work specifically focuses on determining when and in which difficulty order text data samples should be learned for effective training of NLP systems. We propose a framework that employs prior knowledge about sample difficulty, such as entropy in human annotations, to inform an effective and flexible sample weighting scheme for curriculum discovery. The framework is capable of discovering optimal curricula (within the space of its weight functions) for any given model and dataset by optimizing the weight functions and adjusting the difficulty group of data samples as training progresses. The discovered curricula provide useful insights about datasets and models, such as the relative importance of different groups of samples for models or knowledge dependency among samples. We illustrate that the proposed framework has the potential to encompass some of the existing CL approaches.\n\nExperimental results show that (a): the topperforming discovered curricula for the same model and dataset can be fundamentally dissimilar in their training strategies, indicating that effective training can be achieved in multiple ways; (b): the discovered curricula are often non-monotonic and greatly differ from the known strategies reported in existing literature, indicating that existing curricula, including easy-to-hard transition curricula, are at the risk of underperforming; and (c): the curricula discovered on small datasets and models perform exceptionally well on larger datasets and models respectively, illustrating the transferability of the discovered curricula. The paper presents a new curriculum learning approach that unlike existing approaches can discover multiple high-performing (and often diverse) curricula for each given NLP model and dataset, provide interpretable curricula in terms of sample difficulty, and encompass some of the existing curriculum learning approaches. 1",
                    "score": 0.4018965706009834,
                    "section_title": "Introduction",
                    "char_start_offset": 1796,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 20,
                            "end": 36,
                            "matchedPaperCorpusId": "227343966"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70361328125
                },
                {
                    "corpus_id": "259370648",
                    "title": "HuCurl: Human-induced Curriculum Discovery",
                    "text": "Annotation entropy is a natural measure of difficulty (for humans) and may serve as a reliable difficulty metric for models. Entropy of each sample x i is calculated as \u2212 l p c log p c (Shannon, 1948), where c is a class category and p c is the fraction of annotators who chose label c for the sample. The use of entropy is supported in (Nie et al., 2020a), reporting a consistent positive correlation between model accuracy and level of human agreement.\n\nFurthermore, moving average of a sample's instantaneous loss is a good metric for difficulty (Zhou et al., 2020). Using a baseline model trained with no curriculum and with default hyperparameters, we collect the loss values of all training instances at intervals of 0.5 epochs and use the average loss as prior knowledge about sample difficulty. We obtain twenty observations of the loss and compute the average for each sample. Figure 3 shows the distributions of entropy and loss, and examples of data partitions across four datasets. Most datasets are highly imbalanced across difficulty groups, often containing more easier samples than harder ones. Such data disparities would perhaps explain why computational models can achieve human-level performance on complex NLP tasks or recent results reporting neural models being largely invariant to random word order permutation of data (Sinha et al., 2021).\n\nWe acknowledge that while multiple annotations per sample may not be readily available for many NLP datasets, such annotations were collected for most NLP datasets at their dataset development time. Our work shows that such information can be used to find effective curricula for NLP models and encourages dataset creators to publish their full annotation information. In addition, our curriculum discovery framework is independent of annotation information. In fact, we evaluated our approach with both annotation entropy and loss as two choices for sample-level difficulty estimation.",
                    "score": 0.4870710704898112,
                    "section_title": "Prior Knowledge of Difficulty",
                    "char_start_offset": 10899,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 337,
                            "end": 356,
                            "matchedPaperCorpusId": "222177851"
                        },
                        {
                            "start": 549,
                            "end": 568,
                            "matchedPaperCorpusId": "227275560"
                        },
                        {
                            "start": 1344,
                            "end": 1364,
                            "matchedPaperCorpusId": "230435766"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69091796875
                },
                {
                    "corpus_id": "259370648",
                    "title": "HuCurl: Human-induced Curriculum Discovery",
                    "text": "We introduce an effective curriculum learning framework that employs prior knowledge about sample difficulty in its training paradigm for curriculum discovery. The proposed framework initially partitions its input data into several groups of increasing difficulty, defines parameterized func-  Table 2. tions to weight sample losses in each difficulty group, moves samples across difficulty groups based on their learning progress, and enables tuning the parameters of the weight function to discover novel curricula. We demonstrate that this framework is capable of representing several categories of curriculum learning approaches. The task of curriculum discovery alleviates the limitations imposed by selecting a single curriculum strategy, and instead, focuses on finding and analyzing different curricula that work equally-well for a given model and dataset. In addition, the discovered curricula provide insight into how different portions of the dataset contribute toward learning at different stages of training a model, which, in turn, provide knowledge about the learning dynamics of different models. The task of curriculum discovery could be costly on large datasets, in particular, when the goal is to find optimal curricula for different models and datasets. To mitigate the computational : Confidence assignment to samples in our datasets by three CL approaches. The x-axis is the epoch number, and y-axis is the average weight assigned to samples of each difficulty group. Blue (solid) is easy, orange (dashed) is medium, and green (dash-dot) is hard. The shaded area is the 95% CI over the datasets with five random seeds each. The curves are monotonic for most parts, and can be approximated by our framework.\n\ncost, we show that it is possible to rapidly discover a curriculum on a small subset of the dataset (or a smaller version of the model with significantly less number of parameters) and apply the resulting curriculum to the full dataset.\n\nThere are several promising areas for future work. These include approaches for learning new difficulty indicators from data (e.g., linguistic difficulty including lexical, syntactic and semantic difficulty), prioritizing medium level instances and those with greatest progress during training, and developing challenge datasets that contain diverse data samples with different levels of difficulty. Finally, investigating diverse curricula that are suitable for general use and across datasets through curriculum discovery and generalization is a promising area for research.",
                    "score": 0.37848916312898717,
                    "section_title": "Conclusion and Future Work",
                    "char_start_offset": 27435,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67626953125
                },
                {
                    "corpus_id": "259370648",
                    "title": "HuCurl: Human-induced Curriculum Discovery",
                    "text": "The present work investigates the use of two sample difficulty scoring functions, human-induced annotation entropy and model-induced loss, for NLP models and datasets. The former requires the availability of multiple annotations per sample and the latter requires training an auxiliary model to compute sample instantaneous loss during the course of training. Our work does not provide a general solution to the choice or availability of good difficulty scoring functions. However, once such a function is available, our work presents solutions to the problem of finding high-performing curricula in curriculum space. Our approach, although effective at finding such curricula, requires a Bayesian search of its hyperparameters. We reduce these costs by finding curricula on smaller datasets and smaller models that can then be applied to corresponding larger datasets and models. Finally, the proposed method lacks theoretical analysis of the dynamic interactions between data, downstream models, and discovered curricula.      Table 4: Number of trials for the best parameters found. The notation for configurations is the same as Figure 4.",
                    "score": 0.46121309436270014,
                    "section_title": "Limitations",
                    "char_start_offset": 29994,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.63330078125
                }
            ],
            "relevance_judgement": 0.7119140625,
            "relevance_judgment_input_expanded": "# Title: HuCurl: Human-induced Curriculum Discovery\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Mohamed Elgaar, Hadi Amiri\n## Abstract\nWe introduce the problem of curriculum discovery and describe a curriculum learning framework capable of discovering effective curricula in a curriculum space based on prior knowledge about sample difficulty. Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as apposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy transition curricula are often at the risk of underperforming, and (iii): the curricula discovered for smaller datasets and models perform well on larger datasets and models respectively. The proposed framework encompasses some of the existing curriculum learning approaches and can discover curricula that outperform them across several NLP tasks.\n## Introduction\n(Erhan et al., 2010;Wu et al., 2021), which limits the curriculum space that can be examined for finding effective curricula.\n\nThis paper advocates a re-imagining of CL paradigms by introducing and formalizing the task of curriculum discovery, which aims to find effective curricula for a given model and dataset over a curriculum space. The present work specifically focuses on determining when and in which difficulty order text data samples should be learned for effective training of NLP systems. We propose a framework that employs prior knowledge about sample difficulty, such as entropy in human annotations, to inform an effective and flexible sample weighting scheme for curriculum discovery. The framework is capable of discovering optimal curricula (within the space of its weight functions) for any given model and dataset by optimizing the weight functions and adjusting the difficulty group of data samples as training progresses. The discovered curricula provide useful insights about datasets and models, such as the relative importance of different groups of samples for models or knowledge dependency among samples. We illustrate that the proposed framework has the potential to encompass some of the existing CL approaches.\n\nExperimental results show that (a): the topperforming discovered curricula for the same model and dataset can be fundamentally dissimilar in their training strategies, indicating that effective training can be achieved in multiple ways; (b): the discovered curricula are often non-monotonic and greatly differ from the known strategies reported in existing literature, indicating that existing curricula, including easy-to-hard transition curricula, are at the risk of underperforming; and (c): the curricula discovered on small datasets and models perform exceptionally well on larger datasets and models respectively, illustrating the transferability of the discovered curricula. The paper presents a new curriculum learning approach that unlike existing approaches can discover multiple high-performing (and often diverse) curricula for each given NLP model and dataset, provide interpretable curricula in terms of sample difficulty, and encompass some of the existing curriculum learning approaches. 1\n\n## Prior Knowledge of Difficulty\nAnnotation entropy is a natural measure of difficulty (for humans) and may serve as a reliable difficulty metric for models. Entropy of each sample x i is calculated as \u2212 l p c log p c (Shannon, 1948), where c is a class category and p c is the fraction of annotators who chose label c for the sample. The use of entropy is supported in (Nie et al., 2020a), reporting a consistent positive correlation between model accuracy and level of human agreement.\n\nFurthermore, moving average of a sample's instantaneous loss is a good metric for difficulty (Zhou et al., 2020). Using a baseline model trained with no curriculum and with default hyperparameters, we collect the loss values of all training instances at intervals of 0.5 epochs and use the average loss as prior knowledge about sample difficulty. We obtain twenty observations of the loss and compute the average for each sample. Figure 3 shows the distributions of entropy and loss, and examples of data partitions across four datasets. Most datasets are highly imbalanced across difficulty groups, often containing more easier samples than harder ones. Such data disparities would perhaps explain why computational models can achieve human-level performance on complex NLP tasks or recent results reporting neural models being largely invariant to random word order permutation of data (Sinha et al., 2021).\n\nWe acknowledge that while multiple annotations per sample may not be readily available for many NLP datasets, such annotations were collected for most NLP datasets at their dataset development time. Our work shows that such information can be used to find effective curricula for NLP models and encourages dataset creators to publish their full annotation information. In addition, our curriculum discovery framework is independent of annotation information. In fact, we evaluated our approach with both annotation entropy and loss as two choices for sample-level difficulty estimation.\n\n## Conclusion and Future Work\nWe introduce an effective curriculum learning framework that employs prior knowledge about sample difficulty in its training paradigm for curriculum discovery. The proposed framework initially partitions its input data into several groups of increasing difficulty, defines parameterized func-  Table 2. tions to weight sample losses in each difficulty group, moves samples across difficulty groups based on their learning progress, and enables tuning the parameters of the weight function to discover novel curricula. We demonstrate that this framework is capable of representing several categories of curriculum learning approaches. The task of curriculum discovery alleviates the limitations imposed by selecting a single curriculum strategy, and instead, focuses on finding and analyzing different curricula that work equally-well for a given model and dataset. In addition, the discovered curricula provide insight into how different portions of the dataset contribute toward learning at different stages of training a model, which, in turn, provide knowledge about the learning dynamics of different models. The task of curriculum discovery could be costly on large datasets, in particular, when the goal is to find optimal curricula for different models and datasets. To mitigate the computational : Confidence assignment to samples in our datasets by three CL approaches. The x-axis is the epoch number, and y-axis is the average weight assigned to samples of each difficulty group. Blue (solid) is easy, orange (dashed) is medium, and green (dash-dot) is hard. The shaded area is the 95% CI over the datasets with five random seeds each. The curves are monotonic for most parts, and can be approximated by our framework.\n\ncost, we show that it is possible to rapidly discover a curriculum on a small subset of the dataset (or a smaller version of the model with significantly less number of parameters) and apply the resulting curriculum to the full dataset.\n\nThere are several promising areas for future work. These include approaches for learning new difficulty indicators from data (e.g., linguistic difficulty including lexical, syntactic and semantic difficulty), prioritizing medium level instances and those with greatest progress during training, and developing challenge datasets that contain diverse data samples with different levels of difficulty. Finally, investigating diverse curricula that are suitable for general use and across datasets through curriculum discovery and generalization is a promising area for research.\n\n## Limitations\nThe present work investigates the use of two sample difficulty scoring functions, human-induced annotation entropy and model-induced loss, for NLP models and datasets. The former requires the availability of multiple annotations per sample and the latter requires training an auxiliary model to compute sample instantaneous loss during the course of training. Our work does not provide a general solution to the choice or availability of good difficulty scoring functions. However, once such a function is available, our work presents solutions to the problem of finding high-performing curricula in curriculum space. Our approach, although effective at finding such curricula, requires a Bayesian search of its hyperparameters. We reduce these costs by finding curricula on smaller datasets and smaller models that can then be applied to corresponding larger datasets and models. Finally, the proposed method lacks theoretical analysis of the dynamic interactions between data, downstream models, and discovered curricula.      Table 4: Number of trials for the best parameters found. The notation for configurations is the same as Figure 4.",
            "reference_string": "[259370648 | Elgaar et al. | 2023 | Citations: 0]"
        },
        {
            "title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 60,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.06424, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116659852",
                    "name": "Joonho Lee"
                },
                {
                    "authorId": "2301016207",
                    "name": "Jae Oh Woo"
                },
                {
                    "authorId": "2301014929",
                    "name": "Juree Seok"
                },
                {
                    "authorId": "2301015562",
                    "name": "Parisa Hassanzadeh"
                },
                {
                    "authorId": "2301015224",
                    "name": "Wooseok Jang"
                },
                {
                    "authorId": "2301016428",
                    "name": "JuYoun Son"
                },
                {
                    "authorId": "91748824",
                    "name": "Sima Didari"
                },
                {
                    "authorId": "2301014892",
                    "name": "Baruch Gutow"
                },
                {
                    "authorId": "2065513368",
                    "name": "Heng Hao"
                },
                {
                    "authorId": "2301015932",
                    "name": "Hankyu Moon"
                },
                {
                    "authorId": "2301166595",
                    "name": "Wenjun Hu"
                },
                {
                    "authorId": "2301413463",
                    "name": "Yeong-Dae Kwon"
                },
                {
                    "authorId": "2301133161",
                    "name": "Taehee Lee"
                },
                {
                    "authorId": "2301015935",
                    "name": "Seungjai Min"
                }
            ],
            "abstract": "Assessing response quality to instructions in language models is vital but challenging due to the complexity of human language across different contexts. This complexity often results in ambiguous or inconsistent interpretations, making accurate assessment difficult. To address this issue, we propose a novel Uncertainty-aware Reward Model (URM) that introduces a robust uncertainty estimation for the quality of paired responses based on Bayesian approximation. Trained with preference datasets, our uncertainty-enabled proxy not only scores rewards for responses but also evaluates their inherent uncertainty. Empirical results demonstrate significant benefits of incorporating the proposed proxy into language model training. Our method boosts the instruction following capability of language models by refining data curation for training and improving policy optimization objectives, thereby surpassing existing methods by a large margin on benchmarks such as Vicuna and MT-bench. These findings highlight that our proposed approach substantially advances language model training and paves a new way of harnessing uncertainty within language models.",
            "corpus_id": 269741199,
            "sentences": [
                {
                    "corpus_id": "269741199",
                    "title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation",
                    "text": "We propose introducing uncertainty to both the query selection and the RL fine-tuning steps to address these challenges. Moreover, to mitigate the hallucination problem, trustworthy language models should refrain from answering questions when the answer is unknown, which can occur due to ambiguity in the question or the questioner's intent. Cole et al., 2023 proposed that using sampling-based confidence scores to quantify repetition in model outputs is crucial for addressing hallucination issues and helps improve accuracy, especially for ambiguous questions. \n\nCurriculum Learning It is observed that deep learning model training can benefit from the implementation of Curriculum Learning (CL), i.e., using data samples sorted based on a curriculum versus training on the randomly shuffled data (Soviany et al., 2022). Recently, CL methods have been developed and deployed for the LMs as well, at pre-training and post-training stages using a variety of linguistically motivated curricula such as sentence length or term frequency complexity measure based ranking (Liu et al., 2018;Zhang et al., 2021;Campos, 2021;Weber et al., 2023). \n\nStudies regarding the deployment of CL at the pre-training stage of LMs are focused on reducing the pre-training computational cost and the instability of the auto-regressive training emerging when increasing the models' size, batch size, sequence length, and learning rate. Li et al., 2021 implemented a CL at the pre-training of LMs using the sequence length as the difficulty metric with the curriculum of starting from the shorter sequence training data toward the longer sequence. They demonstrated that CL behaves as a regularization method and reduces the gradient variance, therefore enabling training auto-regressive models with much larger batch sizes and learning rates without training instability (for example, training GPT-2 models with 8x larger batch size and 4x larger learning rate). Ranaldi et al., 2023 proposed a new complexity measure based on the length, rarity, and comprehensibility of the samples and sorted the corpus according to the proposed complexity measure during the pre-training stage and showed that their CL approach led to better performance in downstream tasks. \n\nWang et al., 2022 used the frequency of words as the complexity metric for the curriculum-based pre-training of LMs.",
                    "score": 0.37065344764685015,
                    "section_title": "I. Related Work",
                    "char_start_offset": 80668,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 120
                        },
                        {
                            "start": 121,
                            "end": 342
                        },
                        {
                            "start": 343,
                            "end": 564
                        },
                        {
                            "start": 567,
                            "end": 824
                        },
                        {
                            "start": 825,
                            "end": 1140
                        },
                        {
                            "start": 1143,
                            "end": 1417
                        },
                        {
                            "start": 1418,
                            "end": 1628
                        },
                        {
                            "start": 1629,
                            "end": 1944
                        },
                        {
                            "start": 1945,
                            "end": 2243
                        },
                        {
                            "start": 2246,
                            "end": 2362
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 343,
                            "end": 360,
                            "matchedPaperCorpusId": "258866001"
                        },
                        {
                            "start": 801,
                            "end": 823,
                            "matchedPaperCorpusId": "231709290"
                        },
                        {
                            "start": 1070,
                            "end": 1088,
                            "matchedPaperCorpusId": "51606954"
                        },
                        {
                            "start": 1088,
                            "end": 1107,
                            "matchedPaperCorpusId": "243766208"
                        },
                        {
                            "start": 1945,
                            "end": 1965,
                            "matchedPaperCorpusId": "265068175"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6982421875
                }
            ],
            "relevance_judgement": 0.6982421875,
            "relevance_judgment_input_expanded": "# Title: Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation\n# Venue: International Conference on Machine Learning\n# Authors: Joonho Lee, Jae Oh Woo, Juree Seok, Parisa Hassanzadeh, Wooseok Jang, JuYoun Son, Sima Didari, Baruch Gutow, Heng Hao, Hankyu Moon, Wenjun Hu, Yeong-Dae Kwon, Taehee Lee, Seungjai Min\n## Abstract\nAssessing response quality to instructions in language models is vital but challenging due to the complexity of human language across different contexts. This complexity often results in ambiguous or inconsistent interpretations, making accurate assessment difficult. To address this issue, we propose a novel Uncertainty-aware Reward Model (URM) that introduces a robust uncertainty estimation for the quality of paired responses based on Bayesian approximation. Trained with preference datasets, our uncertainty-enabled proxy not only scores rewards for responses but also evaluates their inherent uncertainty. Empirical results demonstrate significant benefits of incorporating the proposed proxy into language model training. Our method boosts the instruction following capability of language models by refining data curation for training and improving policy optimization objectives, thereby surpassing existing methods by a large margin on benchmarks such as Vicuna and MT-bench. These findings highlight that our proposed approach substantially advances language model training and paves a new way of harnessing uncertainty within language models.\n## I. Related Work\nWe propose introducing uncertainty to both the query selection and the RL fine-tuning steps to address these challenges. Moreover, to mitigate the hallucination problem, trustworthy language models should refrain from answering questions when the answer is unknown, which can occur due to ambiguity in the question or the questioner's intent. Cole et al., 2023 proposed that using sampling-based confidence scores to quantify repetition in model outputs is crucial for addressing hallucination issues and helps improve accuracy, especially for ambiguous questions. \n\nCurriculum Learning It is observed that deep learning model training can benefit from the implementation of Curriculum Learning (CL), i.e., using data samples sorted based on a curriculum versus training on the randomly shuffled data (Soviany et al., 2022). Recently, CL methods have been developed and deployed for the LMs as well, at pre-training and post-training stages using a variety of linguistically motivated curricula such as sentence length or term frequency complexity measure based ranking (Liu et al., 2018;Zhang et al., 2021;Campos, 2021;Weber et al., 2023). \n\nStudies regarding the deployment of CL at the pre-training stage of LMs are focused on reducing the pre-training computational cost and the instability of the auto-regressive training emerging when increasing the models' size, batch size, sequence length, and learning rate. Li et al., 2021 implemented a CL at the pre-training of LMs using the sequence length as the difficulty metric with the curriculum of starting from the shorter sequence training data toward the longer sequence. They demonstrated that CL behaves as a regularization method and reduces the gradient variance, therefore enabling training auto-regressive models with much larger batch sizes and learning rates without training instability (for example, training GPT-2 models with 8x larger batch size and 4x larger learning rate). Ranaldi et al., 2023 proposed a new complexity measure based on the length, rarity, and comprehensibility of the samples and sorted the corpus according to the proposed complexity measure during the pre-training stage and showed that their CL approach led to better performance in downstream tasks. \n\nWang et al., 2022 used the frequency of words as the complexity metric for the curriculum-based pre-training of LMs.",
            "reference_string": "[269741199 | Lee et al. | 2024 | Citations: 2]"
        },
        {
            "title": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 42,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.17565, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2352756200",
                    "name": "Xiaoyu Tian"
                },
                {
                    "authorId": "2352024154",
                    "name": "Sitong Zhao"
                },
                {
                    "authorId": "2352011931",
                    "name": "Haotian Wang"
                },
                {
                    "authorId": "2351916957",
                    "name": "Shuaiting Chen"
                },
                {
                    "authorId": "2111015160",
                    "name": "Yiping Peng"
                },
                {
                    "authorId": "24780166",
                    "name": "Yunjie Ji"
                },
                {
                    "authorId": "2352622605",
                    "name": "Han Zhao"
                },
                {
                    "authorId": "2352034310",
                    "name": "Xiangang Li"
                }
            ],
            "abstract": "Although large language models (LLMs) have recently achieved remarkable performance on various complex reasoning benchmarks, the academic community still lacks an in-depth understanding of base model training processes and data quality. To address this, we construct a large-scale, difficulty-graded reasoning dataset containing approximately 3.34 million unique queries of varying difficulty levels and about 40 million distilled responses generated by multiple models over several passes. Leveraging pass rate and Coefficient of Variation (CV), we precisely select the most valuable training data to enhance reasoning capability. Notably, we observe a training pattern shift, indicating that reasoning-focused training based on base models requires higher learning rates for effective training. Using this carefully selected data, we significantly improve the reasoning capabilities of the base model, achieving a pass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This result surpasses most current distilled models and closely approaches state-of-the-art performance. We provide detailed descriptions of our data processing, difficulty assessment, and training methodology, and have publicly released all datasets and methods to promote rapid progress in open-source long-reasoning LLMs. The dataset is available at: \\href{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}",
            "corpus_id": 278033087,
            "sentences": [
                {
                    "corpus_id": "278033087",
                    "title": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training",
                    "text": "Although large language models (LLMs) have recently achieved remarkable performance on various complex reasoning benchmarks, the academic community still lacks an in-depth understanding of base model training processes and data quality. To address this, we construct a large-scale, difficulty-graded reasoning dataset containing approximately 3.34 million unique queries of varying difficulty levels and about 40 million distilled responses generated by multiple models over several passes. Leveraging pass rate and Coefficient of Variation (CV), we precisely select the most valuable training data to enhance reasoning capability. Notably, we observe a training pattern shift, indicating that reasoning-focused training based on base models requires higher learning rates for effective training. Using this carefully selected data, we significantly improve the reasoning capabilities of the base model, achieving a pass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This result surpasses most current distilled models and closely approaches state-of-the-art performance. We provide detailed descriptions of our data processing, difficulty assessment, and training methodology, and have publicly released all datasets and methods to promote rapid progress in open-source long-reasoning LLMs. The dataset is available at: \\href{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}",
                    "score": 0.3344649274716484,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69677734375
                }
            ],
            "relevance_judgement": 0.69677734375,
            "relevance_judgment_input_expanded": "# Title: DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training\n# Venue: arXiv.org\n# Authors: Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Yunjie Ji, Han Zhao, Xiangang Li\n## Abstract\nAlthough large language models (LLMs) have recently achieved remarkable performance on various complex reasoning benchmarks, the academic community still lacks an in-depth understanding of base model training processes and data quality. To address this, we construct a large-scale, difficulty-graded reasoning dataset containing approximately 3.34 million unique queries of varying difficulty levels and about 40 million distilled responses generated by multiple models over several passes. Leveraging pass rate and Coefficient of Variation (CV), we precisely select the most valuable training data to enhance reasoning capability. Notably, we observe a training pattern shift, indicating that reasoning-focused training based on base models requires higher learning rates for effective training. Using this carefully selected data, we significantly improve the reasoning capabilities of the base model, achieving a pass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This result surpasses most current distilled models and closely approaches state-of-the-art performance. We provide detailed descriptions of our data processing, difficulty assessment, and training methodology, and have publicly released all datasets and methods to promote rapid progress in open-source long-reasoning LLMs. The dataset is available at: \\href{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}\n",
            "reference_string": "[278033087 | Tian et al. | 2025 | Citations: 5]"
        },
        {
            "title": "CBM: Curriculum by Masking",
            "venue": "European Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 57,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.05193, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2310341007",
                    "name": "Andrei Jarca"
                },
                {
                    "authorId": "2154573729",
                    "name": "Florinel-Alin Croitoru"
                },
                {
                    "authorId": "2249763264",
                    "name": "R. Ionescu"
                }
            ],
            "abstract": "We propose Curriculum by Masking (CBM), a novel state-of-the-art curriculum learning strategy that effectively creates an easy-to-hard training schedule via patch (token) masking, offering significant accuracy improvements over the conventional training regime and previous curriculum learning (CL) methods. CBM leverages gradient magnitudes to prioritize the masking of salient image regions via a novel masking algorithm and a novel masking block. Our approach enables controlling sample difficulty via the patch masking ratio, generating an effective easy-to-hard curriculum by gradually introducing harder samples as training progresses. CBM operates with two easily configurable parameters, i.e. the number of patches and the curriculum schedule, making it a versatile curriculum learning approach for object recognition and detection. We conduct experiments with various neural architectures, ranging from convolutional networks to vision transformers, on five benchmark data sets (CIFAR-10, CIFAR-100, ImageNet, Food-101 and PASCAL VOC), to compare CBM with conventional as well as curriculum-based training regimes. Our results reveal the superiority of our strategy compared with the state-of-the-art curriculum learning regimes. We also observe improvements in transfer learning contexts, where CBM surpasses previous work by considerable margins in terms of accuracy. We release our code for free non-commercial use at https://github.com/CroitoruAlin/CBM.",
            "corpus_id": 271051051,
            "sentences": [
                {
                    "corpus_id": "271051051",
                    "title": "CBM: Curriculum by Masking",
                    "text": "Curriculum learning is a training technique introduced by Bengio et al. [3], which provides the training examples in a meaningful order, from easy to hard, to neural networks.The objective is to enhance the performance of neural models, while also improving the convergence speed of the training process.Since its introduction, curriculum learning has proven its effectiveness in various domains, such as computer vision [3,7,10,19,25,43,44,45], natural language processing [3,10,30,36,39,47], and signal processing [1,10,40].The method has been very successful and has undergone extensive development, as illustrated in some recent surveys [46,52].These developments range from strategies for measuring data difficulty [3,23,30,38,43,45,48,54,60] to methods focusing on other aspects of the training process [5,10,6,28,44].A wellknown method to apply curriculum learning is by defining a metric that evaluates the complexity of the data, and subsequently arranging the training examples from the simplest to the most challenging ones based on the respective metric.Researchers have made significant strides in finding improved metrics for various domains and tasks.For instance, images containing fewer and larger objects in computer vision are deemed easier than other images [43,45].In natural language processing, word frequency [3,36] and sequence length [8,30,48,60] are utilized to assess the sample difficulty.In some cases, researchers have also integrated human feedback into their metric design [26,38,54].\n\nThe aforementioned curriculum strategies have proven to be effective.However, they have been found to lack practicality due to their reliance on human expert input [26], which may not always be available.Moreover, these methods remain fixed during training and may not adapt the curriculum to the changing needs of the models.As a result, the research community developed new curriculum-based approaches to overcome these limitations.For in-stance, Kumar et al. [32] introduced self-paced learning, a method that measures the difficulty of the training samples based on the performance of the trained model.",
                    "score": 0.3820994630724619,
                    "section_title": "Related Work",
                    "char_start_offset": 4940,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 175,
                            "end": 304
                        },
                        {
                            "start": 304,
                            "end": 526
                        },
                        {
                            "start": 526,
                            "end": 649
                        },
                        {
                            "start": 649,
                            "end": 824
                        },
                        {
                            "start": 824,
                            "end": 1066
                        },
                        {
                            "start": 1066,
                            "end": 1166
                        },
                        {
                            "start": 1166,
                            "end": 1286
                        },
                        {
                            "start": 1286,
                            "end": 1418
                        },
                        {
                            "start": 1418,
                            "end": 1517
                        },
                        {
                            "start": 1519,
                            "end": 1588
                        },
                        {
                            "start": 1588,
                            "end": 1723
                        },
                        {
                            "start": 1723,
                            "end": 1845
                        },
                        {
                            "start": 1845,
                            "end": 1953
                        },
                        {
                            "start": 1953,
                            "end": 2126
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 72,
                            "end": 75,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 421,
                            "end": 424,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 424,
                            "end": 426,
                            "matchedPaperCorpusId": "5658192"
                        },
                        {
                            "start": 429,
                            "end": 432,
                            "matchedPaperCorpusId": "8105909"
                        },
                        {
                            "start": 432,
                            "end": 435,
                            "matchedPaperCorpusId": "51876228"
                        },
                        {
                            "start": 435,
                            "end": 438,
                            "matchedPaperCorpusId": "6954583"
                        },
                        {
                            "start": 438,
                            "end": 441,
                            "matchedPaperCorpusId": "220301592"
                        },
                        {
                            "start": 441,
                            "end": 444,
                            "matchedPaperCorpusId": "208138033"
                        },
                        {
                            "start": 474,
                            "end": 477,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 480,
                            "end": 483,
                            "matchedPaperCorpusId": "26468344"
                        },
                        {
                            "start": 483,
                            "end": 486,
                            "matchedPaperCorpusId": "51606954"
                        },
                        {
                            "start": 486,
                            "end": 489,
                            "matchedPaperCorpusId": "85498775"
                        },
                        {
                            "start": 489,
                            "end": 492,
                            "matchedPaperCorpusId": "1900277"
                        },
                        {
                            "start": 516,
                            "end": 519,
                            "matchedPaperCorpusId": "11590585"
                        },
                        {
                            "start": 522,
                            "end": 525,
                            "matchedPaperCorpusId": "19805513"
                        },
                        {
                            "start": 641,
                            "end": 645,
                            "matchedPaperCorpusId": "231709290"
                        },
                        {
                            "start": 645,
                            "end": 648,
                            "matchedPaperCorpusId": "232362223"
                        },
                        {
                            "start": 720,
                            "end": 723,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 723,
                            "end": 726,
                            "matchedPaperCorpusId": "879067"
                        },
                        {
                            "start": 726,
                            "end": 729,
                            "matchedPaperCorpusId": "26468344"
                        },
                        {
                            "start": 729,
                            "end": 732,
                            "matchedPaperCorpusId": "8502955"
                        },
                        {
                            "start": 732,
                            "end": 735,
                            "matchedPaperCorpusId": "6954583"
                        },
                        {
                            "start": 735,
                            "end": 738,
                            "matchedPaperCorpusId": "208138033"
                        },
                        {
                            "start": 738,
                            "end": 741,
                            "matchedPaperCorpusId": "166228313"
                        },
                        {
                            "start": 741,
                            "end": 744,
                            "matchedPaperCorpusId": "221995570"
                        },
                        {
                            "start": 744,
                            "end": 747,
                            "matchedPaperCorpusId": "233433844"
                        },
                        {
                            "start": 809,
                            "end": 812,
                            "matchedPaperCorpusId": "231986287"
                        },
                        {
                            "start": 815,
                            "end": 817,
                            "matchedPaperCorpusId": "190000064"
                        },
                        {
                            "start": 817,
                            "end": 820,
                            "matchedPaperCorpusId": "3568073"
                        },
                        {
                            "start": 820,
                            "end": 823,
                            "matchedPaperCorpusId": "220301592"
                        },
                        {
                            "start": 1278,
                            "end": 1282,
                            "matchedPaperCorpusId": "6954583"
                        },
                        {
                            "start": 1282,
                            "end": 1285,
                            "matchedPaperCorpusId": "208138033"
                        },
                        {
                            "start": 1333,
                            "end": 1336,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 1336,
                            "end": 1339,
                            "matchedPaperCorpusId": "51606954"
                        },
                        {
                            "start": 1363,
                            "end": 1366,
                            "matchedPaperCorpusId": "26468344"
                        },
                        {
                            "start": 1366,
                            "end": 1369,
                            "matchedPaperCorpusId": "166228313"
                        },
                        {
                            "start": 1369,
                            "end": 1372,
                            "matchedPaperCorpusId": "233433844"
                        },
                        {
                            "start": 1506,
                            "end": 1510,
                            "matchedPaperCorpusId": "204539326"
                        },
                        {
                            "start": 1510,
                            "end": 1513,
                            "matchedPaperCorpusId": "8502955"
                        },
                        {
                            "start": 1513,
                            "end": 1516,
                            "matchedPaperCorpusId": "221995570"
                        },
                        {
                            "start": 1683,
                            "end": 1687,
                            "matchedPaperCorpusId": "204539326"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69384765625
                }
            ],
            "relevance_judgement": 0.69384765625,
            "relevance_judgment_input_expanded": "# Title: CBM: Curriculum by Masking\n# Venue: European Conference on Artificial Intelligence\n# Authors: Andrei Jarca, Florinel-Alin Croitoru, R. Ionescu\n## Abstract\nWe propose Curriculum by Masking (CBM), a novel state-of-the-art curriculum learning strategy that effectively creates an easy-to-hard training schedule via patch (token) masking, offering significant accuracy improvements over the conventional training regime and previous curriculum learning (CL) methods. CBM leverages gradient magnitudes to prioritize the masking of salient image regions via a novel masking algorithm and a novel masking block. Our approach enables controlling sample difficulty via the patch masking ratio, generating an effective easy-to-hard curriculum by gradually introducing harder samples as training progresses. CBM operates with two easily configurable parameters, i.e. the number of patches and the curriculum schedule, making it a versatile curriculum learning approach for object recognition and detection. We conduct experiments with various neural architectures, ranging from convolutional networks to vision transformers, on five benchmark data sets (CIFAR-10, CIFAR-100, ImageNet, Food-101 and PASCAL VOC), to compare CBM with conventional as well as curriculum-based training regimes. Our results reveal the superiority of our strategy compared with the state-of-the-art curriculum learning regimes. We also observe improvements in transfer learning contexts, where CBM surpasses previous work by considerable margins in terms of accuracy. We release our code for free non-commercial use at https://github.com/CroitoruAlin/CBM.\n## Related Work\nCurriculum learning is a training technique introduced by Bengio et al. [3], which provides the training examples in a meaningful order, from easy to hard, to neural networks.The objective is to enhance the performance of neural models, while also improving the convergence speed of the training process.Since its introduction, curriculum learning has proven its effectiveness in various domains, such as computer vision [3,7,10,19,25,43,44,45], natural language processing [3,10,30,36,39,47], and signal processing [1,10,40].The method has been very successful and has undergone extensive development, as illustrated in some recent surveys [46,52].These developments range from strategies for measuring data difficulty [3,23,30,38,43,45,48,54,60] to methods focusing on other aspects of the training process [5,10,6,28,44].A wellknown method to apply curriculum learning is by defining a metric that evaluates the complexity of the data, and subsequently arranging the training examples from the simplest to the most challenging ones based on the respective metric.Researchers have made significant strides in finding improved metrics for various domains and tasks.For instance, images containing fewer and larger objects in computer vision are deemed easier than other images [43,45].In natural language processing, word frequency [3,36] and sequence length [8,30,48,60] are utilized to assess the sample difficulty.In some cases, researchers have also integrated human feedback into their metric design [26,38,54].\n\nThe aforementioned curriculum strategies have proven to be effective.However, they have been found to lack practicality due to their reliance on human expert input [26], which may not always be available.Moreover, these methods remain fixed during training and may not adapt the curriculum to the changing needs of the models.As a result, the research community developed new curriculum-based approaches to overcome these limitations.For in-stance, Kumar et al. [32] introduced self-paced learning, a method that measures the difficulty of the training samples based on the performance of the trained model.",
            "reference_string": "[271051051 | Jarca et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Distilling Wisdom: A Review on Optimizing Learning From Massive Language Models",
            "venue": "IEEE Access",
            "year": 2025,
            "reference_count": 205,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2025.3554586",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2025.3554586?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2025.3554586, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326763710",
                    "name": "Dingzong Zhang"
                },
                {
                    "authorId": "2345374431",
                    "name": "Devi Listiyani"
                },
                {
                    "authorId": "2302811064",
                    "name": "Priyanka Singh"
                },
                {
                    "authorId": "2309099465",
                    "name": "Manoranjan Mohanty"
                }
            ],
            "abstract": "In the era of Large Language Models (LLMs), Knowledge Distillation (KD) enables the transfer of capabilities from proprietary LLMs to open-source models. This survey provides a detailed discussion of the basic principles, algorithms, and implementation methods of knowledge distillation. It explores KD\u2019s impact on LLMs, emphasizing its utility in model compression, performance enhancement, and self-improvement. Through the analysis of practical examples such as DistilBERT, TinyBERT, and MobileBERT, the paper demonstrates how knowledge distillation can markedly enhance the efficiency and applicability of large language models in real-world scenarios. The discussion encompasses the varied applications of KD across multiple domains, including industrial systems, embedded systems, Natural Language Processing (NLP), multi-modal processing, and vertical domains, such as medicine, law, science, finance, and materials science. This survey outlines current KD methodologies and future research directions, highlighting its role in advancing AI technologies and fostering innovation across different sectors.",
            "corpus_id": 277398866,
            "sentences": [
                {
                    "corpus_id": "277398866",
                    "title": "Distilling Wisdom: A Review on Optimizing Learning From Massive Language Models",
                    "text": "For instance, if the student model performs well on easier samples but struggles with harder ones, the distillation process will emphasize these harder samples, providing more detailed and nuanced knowledge transfer from the teacher model. This targeted training helps in improving the student model's performance more efficiently compared to uniform distillation methods. Adaptive KD can be implemented using various techniques, such as adjusting the weight of each sample's loss based on its difficulty or using an adaptive temperature in the softmax function to control the smoothness of the teacher model's predictions. These methods ensure that the distillation process is more responsive to the learning needs of the student model, leading to more efficient and effective training outcomes. \n\nModel compression techniques like KD, pruning, quantization, and adaptive KD each have unique pros and cons. KD transfers knowledge from a larger teacher model to a smaller student model, significantly reducing size while maintaining accuracy [6], especially in natural language processing [16]. However, it relies on large labeled datasets and struggles with very small models [6]. Pruning removes redundant weights or neurons for better compression and lower computational demands, but often sacrifices accuracy and requires fine-tuning [48], [49]. Quantization reduces model precision, saving memory and speeding up inference, though it can degrade accuracy in complex tasks without specialized hardware [50]. Adaptive KD customizes the distillation process based on input difficulty or model alignment, improving performance but increasing training complexity [47], [51]. While KD and its adaptive forms balance performance and size, pruning and quantization excel in extreme compression scenarios, highlighting the complementary nature of these methods.",
                    "score": 0.3173388872112666,
                    "section_title": "1) ADDITIONAL TECHNIQUES AND APPLICATIONS",
                    "char_start_offset": 26928,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 239
                        },
                        {
                            "start": 240,
                            "end": 372
                        },
                        {
                            "start": 373,
                            "end": 623
                        },
                        {
                            "start": 624,
                            "end": 796
                        },
                        {
                            "start": 799,
                            "end": 907
                        },
                        {
                            "start": 908,
                            "end": 1094
                        },
                        {
                            "start": 1095,
                            "end": 1181
                        },
                        {
                            "start": 1182,
                            "end": 1349
                        },
                        {
                            "start": 1350,
                            "end": 1511
                        },
                        {
                            "start": 1512,
                            "end": 1674
                        },
                        {
                            "start": 1675,
                            "end": 1857
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1042,
                            "end": 1045,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 1177,
                            "end": 1180,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 1344,
                            "end": 1348,
                            "matchedPaperCorpusId": "212628335"
                        },
                        {
                            "start": 1506,
                            "end": 1510,
                            "matchedPaperCorpusId": "67855262"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69091796875
                }
            ],
            "relevance_judgement": 0.69091796875,
            "relevance_judgment_input_expanded": "# Title: Distilling Wisdom: A Review on Optimizing Learning From Massive Language Models\n# Venue: IEEE Access\n# Authors: Dingzong Zhang, Devi Listiyani, Priyanka Singh, Manoranjan Mohanty\n## Abstract\nIn the era of Large Language Models (LLMs), Knowledge Distillation (KD) enables the transfer of capabilities from proprietary LLMs to open-source models. This survey provides a detailed discussion of the basic principles, algorithms, and implementation methods of knowledge distillation. It explores KD\u2019s impact on LLMs, emphasizing its utility in model compression, performance enhancement, and self-improvement. Through the analysis of practical examples such as DistilBERT, TinyBERT, and MobileBERT, the paper demonstrates how knowledge distillation can markedly enhance the efficiency and applicability of large language models in real-world scenarios. The discussion encompasses the varied applications of KD across multiple domains, including industrial systems, embedded systems, Natural Language Processing (NLP), multi-modal processing, and vertical domains, such as medicine, law, science, finance, and materials science. This survey outlines current KD methodologies and future research directions, highlighting its role in advancing AI technologies and fostering innovation across different sectors.\n## 1) ADDITIONAL TECHNIQUES AND APPLICATIONS\nFor instance, if the student model performs well on easier samples but struggles with harder ones, the distillation process will emphasize these harder samples, providing more detailed and nuanced knowledge transfer from the teacher model. This targeted training helps in improving the student model's performance more efficiently compared to uniform distillation methods. Adaptive KD can be implemented using various techniques, such as adjusting the weight of each sample's loss based on its difficulty or using an adaptive temperature in the softmax function to control the smoothness of the teacher model's predictions. These methods ensure that the distillation process is more responsive to the learning needs of the student model, leading to more efficient and effective training outcomes. \n\nModel compression techniques like KD, pruning, quantization, and adaptive KD each have unique pros and cons. KD transfers knowledge from a larger teacher model to a smaller student model, significantly reducing size while maintaining accuracy [6], especially in natural language processing [16]. However, it relies on large labeled datasets and struggles with very small models [6]. Pruning removes redundant weights or neurons for better compression and lower computational demands, but often sacrifices accuracy and requires fine-tuning [48], [49]. Quantization reduces model precision, saving memory and speeding up inference, though it can degrade accuracy in complex tasks without specialized hardware [50]. Adaptive KD customizes the distillation process based on input difficulty or model alignment, improving performance but increasing training complexity [47], [51]. While KD and its adaptive forms balance performance and size, pruning and quantization excel in extreme compression scenarios, highlighting the complementary nature of these methods.",
            "reference_string": "[277398866 | Zhang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Non-compositional Expression Generation Based on Curriculum Learning and Continual Learning",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 60,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.findings-emnlp.286.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-emnlp.286?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-emnlp.286, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "102489044",
                    "name": "Jianing Zhou"
                },
                {
                    "authorId": "41048608",
                    "name": "Ziheng Zeng"
                },
                {
                    "authorId": "2008458",
                    "name": "Hongyu Gong"
                },
                {
                    "authorId": "2263637139",
                    "name": "Suma Bhat"
                }
            ],
            "abstract": "Non-compositional expressions, by virtue of their non-compositionality, are a classic \u2018pain in the neck\u2019 for NLP systems. Different from the general language modeling and generation tasks that are primarily compositional, generating non-compositional expressions is more challenging for current neural models, including large pre-trained language models. The main reasons are 1) their non-compositionality, and 2) the limited data resources. Therefore, to make the best use of available data for modeling non-compositionality, we pro-pose a dynamic curriculum learning framework, which learns training examples from easy ones to harder ones thus optimizing the learning step by step, but suffers from the forgetting problem. To alleviate the forgetting problem brought by the arrangement of training examples, we also apply a continual learning method into our curriculum learning framework. Our proposed method combined curriculum and continual learning, to gradually improve the model\u2019s performance on the task of non-compositional expression generation. Experiments on idiomatic expression generation and metaphor generation affirm the effectiveness of our proposed curriculum learning framework and the application of continual learning. Our codes are available at https: //github.com/zhjjn/CL2Gen.git .",
            "corpus_id": 266166227,
            "sentences": [
                {
                    "corpus_id": "266166227",
                    "title": "Non-compositional Expression Generation Based on Curriculum Learning and Continual Learning",
                    "text": "In this section, we briefly introduce our proposed curriculum learning method for non-compositional expression generation. Curriculum learning for efficiently leveraging available data resources consists of two main parts: a measure of difficulty of training instances, and an arrangement of the training examples using this measure. Accordingly, for non-compositional expression generation, we propose a data arrangement method for dynamically arranging the training examples according to a newly studied difficulty metric. In addition, due to the current large pre-trained language models' insufficiency in processing non-compositional expressions (Dankers et al., 2022), non-compositional expressions that are difficulty for LMs to understand would have a high perplexity score and the representations between non-compositional expressions and their constituent words would be large. Therefore, we use a combination of the representation distance and perplexity score as a measure of examples' difficulty. \n\nMoreover, in our experiments, we observe that following the curriculum learning principle of arranging the training examples based on their difficulty levels, the problem of forgetting arises due to the gradual shift of distribution in domain difficulty. Therefore, to alleviate this forgetting problem, we propose a simple yet effective continual learning method. Figure 1 demonstrates the workflow of our proposed curriculum learning framework and its details as follows.",
                    "score": 0.30674922134930377,
                    "section_title": "Framework",
                    "char_start_offset": 9802,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 122
                        },
                        {
                            "start": 123,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 524
                        },
                        {
                            "start": 525,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 1008
                        },
                        {
                            "start": 1011,
                            "end": 1265
                        },
                        {
                            "start": 1266,
                            "end": 1375
                        },
                        {
                            "start": 1376,
                            "end": 1484
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 650,
                            "end": 672,
                            "matchedPaperCorpusId": "248780588"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69091796875
                }
            ],
            "relevance_judgement": 0.69091796875,
            "relevance_judgment_input_expanded": "# Title: Non-compositional Expression Generation Based on Curriculum Learning and Continual Learning\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Jianing Zhou, Ziheng Zeng, Hongyu Gong, Suma Bhat\n## Abstract\nNon-compositional expressions, by virtue of their non-compositionality, are a classic \u2018pain in the neck\u2019 for NLP systems. Different from the general language modeling and generation tasks that are primarily compositional, generating non-compositional expressions is more challenging for current neural models, including large pre-trained language models. The main reasons are 1) their non-compositionality, and 2) the limited data resources. Therefore, to make the best use of available data for modeling non-compositionality, we pro-pose a dynamic curriculum learning framework, which learns training examples from easy ones to harder ones thus optimizing the learning step by step, but suffers from the forgetting problem. To alleviate the forgetting problem brought by the arrangement of training examples, we also apply a continual learning method into our curriculum learning framework. Our proposed method combined curriculum and continual learning, to gradually improve the model\u2019s performance on the task of non-compositional expression generation. Experiments on idiomatic expression generation and metaphor generation affirm the effectiveness of our proposed curriculum learning framework and the application of continual learning. Our codes are available at https: //github.com/zhjjn/CL2Gen.git .\n## Framework\nIn this section, we briefly introduce our proposed curriculum learning method for non-compositional expression generation. Curriculum learning for efficiently leveraging available data resources consists of two main parts: a measure of difficulty of training instances, and an arrangement of the training examples using this measure. Accordingly, for non-compositional expression generation, we propose a data arrangement method for dynamically arranging the training examples according to a newly studied difficulty metric. In addition, due to the current large pre-trained language models' insufficiency in processing non-compositional expressions (Dankers et al., 2022), non-compositional expressions that are difficulty for LMs to understand would have a high perplexity score and the representations between non-compositional expressions and their constituent words would be large. Therefore, we use a combination of the representation distance and perplexity score as a measure of examples' difficulty. \n\nMoreover, in our experiments, we observe that following the curriculum learning principle of arranging the training examples based on their difficulty levels, the problem of forgetting arises due to the gradual shift of distribution in domain difficulty. Therefore, to alleviate this forgetting problem, we propose a simple yet effective continual learning method. Figure 1 demonstrates the workflow of our proposed curriculum learning framework and its details as follows.",
            "reference_string": "[266166227 | Zhou et al. | 2023 | Citations: 2]"
        },
        {
            "title": "A Study on the Predictability of Sample Learning Consistency",
            "venue": "LatinX in AI at International Conference on Machine Learning 2022",
            "year": 2022,
            "reference_count": 19,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2207.03571",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2207.03571, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2110753585",
                    "name": "Alain Raymond-S\u00e1ez"
                },
                {
                    "authorId": "2064859104",
                    "name": "J. Hurtado"
                },
                {
                    "authorId": "2052429926",
                    "name": "\u00c1lvaro Soto"
                }
            ],
            "abstract": "Curriculum Learning is a powerful training method that allows for faster and better training in some settings. This method, however, requires having a notion of which examples are difficult and which are easy, which is not always trivial to provide. A recent metric called C-Score acts as a proxy for example difficulty by relating it to learning consistency. Unfortunately, this method is quite compute intensive which limits its applicability for alternative datasets. In this work, we train models through different methods to predict C-Score for CIFAR-100 and CIFAR-10. We find, however, that these models generalize poorly both within the same distribution as well as out of distribution. This suggests that C-Score is not defined by the individual characteristics of each sample but rather by other factors. We hypothesize that a sample's relation to its neighbours, in particular, how many of them share the same labels, can help in explaining C-Scores. We plan to explore this in future work.",
            "corpus_id": 250408174,
            "sentences": [
                {
                    "corpus_id": "250408174",
                    "title": "A Study on the Predictability of Sample Learning Consistency",
                    "text": "While Deep Learning has achieved significant milestones over the last decade, training strategies have remained relatively unchanged. Over the last years, strides have been made in improving these strategies. One such approach is Curriculum Learning (CL) (Bengio et al., 2009;Wu et al., 2021). CL strategies are based on providing a model with an increasingly difficult selection of examples, similar to how children learn in school. CL strategies are popular in Reinforcement Learning -where a curriculum of tasks is progressively learned -and Natural Language Processing applications -where commonly sentence length has been a proxy for difficulty. \n\nProceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s). \n\nThe problem, however, lies in how to determine the difficulty of samples. Typically, an ad hoc difficulty measure needs to be crafted by hand for a specific problem, which limits their application in practice. Even worse, for some applications like image classification, sometimes there's no natural difficulty ordering that can be created. This has led to the development of more general measures of difficulty: Self Paced Learning (Kumar et al., 2010) utilizes the current loss of an example as a proxy for difficulty; however, this requires extra computation and is prone to overfitting (Jiang et al., 2015). As an alternative, a metric called C-Score was proposed (Jiang et al., 2021). This metric measures how consistently a particular example is learned across various models, that is, it measures the proportion of iterations where a sample is successfully classified. The logic behind this metric is supported by the empirical observation that samples are generally learned in the same order (Toneva et al., 2019) -independent of the model's architecture-and that a significant number of examples are learned once and never forgotten. The drawback to this metric is that it requires inordinate amounts of computing for a single dataset, as it requires several models to be trained and averaged over, negating many of the practical benefits of applying CL in the first place. \n\nA reasonable strategy to alleviate the problems of computing C-Scores would be to learn a model that could extrapolate C-Scores to unseen datasets. This could be done by training a model on datasets where the score is already calculated.",
                    "score": 0.3544289123930477,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 133
                        },
                        {
                            "start": 134,
                            "end": 208
                        },
                        {
                            "start": 209,
                            "end": 293
                        },
                        {
                            "start": 294,
                            "end": 433
                        },
                        {
                            "start": 434,
                            "end": 650
                        },
                        {
                            "start": 653,
                            "end": 765
                        },
                        {
                            "start": 766,
                            "end": 798
                        },
                        {
                            "start": 801,
                            "end": 874
                        },
                        {
                            "start": 875,
                            "end": 1010
                        },
                        {
                            "start": 1011,
                            "end": 1141
                        },
                        {
                            "start": 1142,
                            "end": 1412
                        },
                        {
                            "start": 1413,
                            "end": 1490
                        },
                        {
                            "start": 1491,
                            "end": 1676
                        },
                        {
                            "start": 1677,
                            "end": 1943
                        },
                        {
                            "start": 1944,
                            "end": 2183
                        },
                        {
                            "start": 2186,
                            "end": 2333
                        },
                        {
                            "start": 2334,
                            "end": 2423
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 255,
                            "end": 276,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 276,
                            "end": 292,
                            "matchedPaperCorpusId": "227343966"
                        },
                        {
                            "start": 1391,
                            "end": 1411,
                            "matchedPaperCorpusId": "10891229"
                        },
                        {
                            "start": 1469,
                            "end": 1489,
                            "matchedPaperCorpusId": "219573311"
                        },
                        {
                            "start": 1801,
                            "end": 1822,
                            "matchedPaperCorpusId": "55481903"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6787109375
                }
            ],
            "relevance_judgement": 0.6787109375,
            "relevance_judgment_input_expanded": "# Title: A Study on the Predictability of Sample Learning Consistency\n# Venue: LatinX in AI at International Conference on Machine Learning 2022\n# Authors: Alain Raymond-S\u00e1ez, J. Hurtado, \u00c1lvaro Soto\n## Abstract\nCurriculum Learning is a powerful training method that allows for faster and better training in some settings. This method, however, requires having a notion of which examples are difficult and which are easy, which is not always trivial to provide. A recent metric called C-Score acts as a proxy for example difficulty by relating it to learning consistency. Unfortunately, this method is quite compute intensive which limits its applicability for alternative datasets. In this work, we train models through different methods to predict C-Score for CIFAR-100 and CIFAR-10. We find, however, that these models generalize poorly both within the same distribution as well as out of distribution. This suggests that C-Score is not defined by the individual characteristics of each sample but rather by other factors. We hypothesize that a sample's relation to its neighbours, in particular, how many of them share the same labels, can help in explaining C-Scores. We plan to explore this in future work.\n## Introduction\nWhile Deep Learning has achieved significant milestones over the last decade, training strategies have remained relatively unchanged. Over the last years, strides have been made in improving these strategies. One such approach is Curriculum Learning (CL) (Bengio et al., 2009;Wu et al., 2021). CL strategies are based on providing a model with an increasingly difficult selection of examples, similar to how children learn in school. CL strategies are popular in Reinforcement Learning -where a curriculum of tasks is progressively learned -and Natural Language Processing applications -where commonly sentence length has been a proxy for difficulty. \n\nProceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s). \n\nThe problem, however, lies in how to determine the difficulty of samples. Typically, an ad hoc difficulty measure needs to be crafted by hand for a specific problem, which limits their application in practice. Even worse, for some applications like image classification, sometimes there's no natural difficulty ordering that can be created. This has led to the development of more general measures of difficulty: Self Paced Learning (Kumar et al., 2010) utilizes the current loss of an example as a proxy for difficulty; however, this requires extra computation and is prone to overfitting (Jiang et al., 2015). As an alternative, a metric called C-Score was proposed (Jiang et al., 2021). This metric measures how consistently a particular example is learned across various models, that is, it measures the proportion of iterations where a sample is successfully classified. The logic behind this metric is supported by the empirical observation that samples are generally learned in the same order (Toneva et al., 2019) -independent of the model's architecture-and that a significant number of examples are learned once and never forgotten. The drawback to this metric is that it requires inordinate amounts of computing for a single dataset, as it requires several models to be trained and averaged over, negating many of the practical benefits of applying CL in the first place. \n\nA reasonable strategy to alleviate the problems of computing C-Scores would be to learn a model that could extrapolate C-Scores to unseen datasets. This could be done by training a model on datasets where the score is already calculated.",
            "reference_string": "[250408174 | Raymond-Saez et al. | 2022 | Citations: 0]"
        },
        {
            "title": "Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems",
            "venue": "ACM Transactions on Software Engineering and Methodology",
            "year": 2023,
            "reference_count": 72,
            "citation_count": 22,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.15995",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.15995, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2118221146",
                    "name": "Qinghua Xu"
                },
                {
                    "authorId": "47495965",
                    "name": "Sajid Ali"
                },
                {
                    "authorId": "145456500",
                    "name": "T. Yue"
                }
            ],
            "abstract": "Anomaly detection is critical to ensure the security of cyber-physical systems (CPS). However, due to the increasing complexity of attacks and CPS themselves, anomaly detection in CPS is becoming more and more challenging. In our previous work, we proposed a digital twin-based anomaly detection method, called ATTAIN, which takes advantage of both historical and real-time data of CPS. However, such data vary significantly in terms of difficulty. Therefore, similar to human learning processes, deep learning models (e.g., ATTAIN) can benefit from an easy-to-difficult curriculum. To this end, in this paper, we present a novel approach, named digitaL twin-based Anomaly deTecTion wIth Curriculum lEarning (LATTICE), which extends ATTAIN by introducing curriculum learning to optimize its learning paradigm. LATTICE attributes each sample with a difficulty score, before being fed into a training scheduler. The training scheduler samples batches of training data based on these difficulty scores such that learning from easy to difficult data can be performed. To evaluate LATTICE, we use five publicly available datasets collected from five real-world CPS testbeds. We compare LATTICE with ATTAIN and two other state-of-the-art anomaly detectors. Evaluation results show that LATTICE outperforms the three baselines and ATTAIN by 0.906%-2.367% in terms of the F1 score. LATTICE also, on average, reduces the training time of ATTAIN by 4.2% on the five datasets and is on par with the baselines in terms of detection delay time.",
            "corpus_id": 256460262,
            "sentences": [
                {
                    "corpus_id": "256460262",
                    "title": "Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems",
                    "text": "Despite the simplicity and effectiveness of predefined difficulty measurers, they have some essential limitations [57]. First, a predefined difficulty measurer remains unchanged during runtime, i.e., being unable to adapt to new data generated from CPS in operation. Second, a predefined difficulty measurer requires a good grasp of domain knowledge, which can be quite expensive and time-consuming in practice. Last but not least, the definition of difficulty for humans and machines can be quite different; what humans assume to be easy can be quite difficult for machines to comprehend. This discrepancy of decision boundaries between humans and machines causes challenges for experts to define difficulty scores manually.\n\nTo alleviate these problems, various automatic difficulty measurers have been developed and explored in the literature, including self-paced learning [31], transfer teacher [69], reinforcement learning teacher [19], and other automatic difficulty measurers [24,46,54]. Inspired by these methods, we modify difficulty scores automatically with prediction errors, which are critical indicators of CPS uncertainty. Substantial work has been conducted in the literature, demonstrating the importance of handling uncertainties in CPS security and safety [21,22,37,63,67,68]. In our context, we focus on prediction errors of DTM, which is pretrained on historical data. DTM in ATTAIN simulates corresponding CPS with high realism. Therefore, higher prediction errors of DTM indicate higher noise levels of labels produced by DTM. As mentioned in Section 5.1.1, training deep learning models with noisy data is more difficult. Therefore, we assume samples with higher prediction errors should be assigned with higher difficulty scores. Based on this assumption, we define the following two types of automatic difficulty measurers: Hamming Distance-based Measurer (HDM, Definition 5.1) and Cross Entropy-based Measurer (CEM, Definition 5.2). Hamming distance is commonly used to calculate the difference of two strings of equal length, while cross-entropy loss estimates uncertainty by comparing real distribution and prediction. We are aware that there are other distance-based and entropy-based metrics, we, however, argue that hamming distance and cross-entropy are commonly used and representative. In the future, we will explore other options.\n\nDefinition 5",
                    "score": 0.3685763966626367,
                    "section_title": "Automatic Difficulty Measurer.",
                    "char_start_offset": 22667,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 114,
                            "end": 118,
                            "matchedPaperCorpusId": "232362223"
                        },
                        {
                            "start": 877,
                            "end": 881,
                            "matchedPaperCorpusId": "1977996"
                        },
                        {
                            "start": 937,
                            "end": 941,
                            "matchedPaperCorpusId": "11137059"
                        },
                        {
                            "start": 984,
                            "end": 988,
                            "matchedPaperCorpusId": "51876228"
                        },
                        {
                            "start": 988,
                            "end": 991,
                            "matchedPaperCorpusId": "35187967"
                        },
                        {
                            "start": 1280,
                            "end": 1283,
                            "matchedPaperCorpusId": "253421545"
                        },
                        {
                            "start": 1283,
                            "end": 1286,
                            "matchedPaperCorpusId": "84185986"
                        },
                        {
                            "start": 1286,
                            "end": 1289,
                            "matchedPaperCorpusId": "253421605"
                        },
                        {
                            "start": 1289,
                            "end": 1292,
                            "matchedPaperCorpusId": "36524392"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.658203125
                }
            ],
            "relevance_judgement": 0.658203125,
            "relevance_judgment_input_expanded": "# Title: Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems\n# Venue: ACM Transactions on Software Engineering and Methodology\n# Authors: Qinghua Xu, Sajid Ali, T. Yue\n## Abstract\nAnomaly detection is critical to ensure the security of cyber-physical systems (CPS). However, due to the increasing complexity of attacks and CPS themselves, anomaly detection in CPS is becoming more and more challenging. In our previous work, we proposed a digital twin-based anomaly detection method, called ATTAIN, which takes advantage of both historical and real-time data of CPS. However, such data vary significantly in terms of difficulty. Therefore, similar to human learning processes, deep learning models (e.g., ATTAIN) can benefit from an easy-to-difficult curriculum. To this end, in this paper, we present a novel approach, named digitaL twin-based Anomaly deTecTion wIth Curriculum lEarning (LATTICE), which extends ATTAIN by introducing curriculum learning to optimize its learning paradigm. LATTICE attributes each sample with a difficulty score, before being fed into a training scheduler. The training scheduler samples batches of training data based on these difficulty scores such that learning from easy to difficult data can be performed. To evaluate LATTICE, we use five publicly available datasets collected from five real-world CPS testbeds. We compare LATTICE with ATTAIN and two other state-of-the-art anomaly detectors. Evaluation results show that LATTICE outperforms the three baselines and ATTAIN by 0.906%-2.367% in terms of the F1 score. LATTICE also, on average, reduces the training time of ATTAIN by 4.2% on the five datasets and is on par with the baselines in terms of detection delay time.\n## Automatic Difficulty Measurer.\nDespite the simplicity and effectiveness of predefined difficulty measurers, they have some essential limitations [57]. First, a predefined difficulty measurer remains unchanged during runtime, i.e., being unable to adapt to new data generated from CPS in operation. Second, a predefined difficulty measurer requires a good grasp of domain knowledge, which can be quite expensive and time-consuming in practice. Last but not least, the definition of difficulty for humans and machines can be quite different; what humans assume to be easy can be quite difficult for machines to comprehend. This discrepancy of decision boundaries between humans and machines causes challenges for experts to define difficulty scores manually.\n\nTo alleviate these problems, various automatic difficulty measurers have been developed and explored in the literature, including self-paced learning [31], transfer teacher [69], reinforcement learning teacher [19], and other automatic difficulty measurers [24,46,54]. Inspired by these methods, we modify difficulty scores automatically with prediction errors, which are critical indicators of CPS uncertainty. Substantial work has been conducted in the literature, demonstrating the importance of handling uncertainties in CPS security and safety [21,22,37,63,67,68]. In our context, we focus on prediction errors of DTM, which is pretrained on historical data. DTM in ATTAIN simulates corresponding CPS with high realism. Therefore, higher prediction errors of DTM indicate higher noise levels of labels produced by DTM. As mentioned in Section 5.1.1, training deep learning models with noisy data is more difficult. Therefore, we assume samples with higher prediction errors should be assigned with higher difficulty scores. Based on this assumption, we define the following two types of automatic difficulty measurers: Hamming Distance-based Measurer (HDM, Definition 5.1) and Cross Entropy-based Measurer (CEM, Definition 5.2). Hamming distance is commonly used to calculate the difference of two strings of equal length, while cross-entropy loss estimates uncertainty by comparing real distribution and prediction. We are aware that there are other distance-based and entropy-based metrics, we, however, argue that hamming distance and cross-entropy are commonly used and representative. In the future, we will explore other options.\n\nDefinition 5",
            "reference_string": "[256460262 | Xu et al. | 2023 | Citations: 22]"
        },
        {
            "title": "Your Model is Overconfident, and Other Lies We Tell Ourselves",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 54,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.01235, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "122281127",
                    "name": "Timothee Mickus"
                },
                {
                    "authorId": "2239526118",
                    "name": "Aman Sinha"
                },
                {
                    "authorId": "2257040447",
                    "name": "Ra'ul V'azquez"
                }
            ],
            "abstract": "The difficulty intrinsic to a given example, rooted in its inherent ambiguity, is a key yet often overlooked factor in evaluating neural NLP models. We investigate the interplay and divergence among various metrics for assessing intrinsic difficulty, including annotator dissensus, training dynamics, and model confidence. Through a comprehensive analysis using 29 models on three datasets, we reveal that while correlations exist among these metrics, their relationships are neither linear nor monotonic. By disentangling these dimensions of uncertainty, we aim to refine our understanding of data complexity and its implications for evaluating and improving NLP models.",
            "corpus_id": 276742147,
            "sentences": [
                {
                    "corpus_id": "276742147",
                    "title": "Your Model is Overconfident, and Other Lies We Tell Ourselves",
                    "text": "The difficulty intrinsic to a given example, rooted in its inherent ambiguity, is a key yet often overlooked factor in evaluating neural NLP models. We investigate the interplay and divergence among various metrics for assessing intrinsic difficulty, including annotator dissensus, training dynamics, and model confidence. Through a comprehensive analysis using 29 models on three datasets, we reveal that while correlations exist among these metrics, their relationships are neither linear nor monotonic. By disentangling these dimensions of uncertainty, we aim to refine our understanding of data complexity and its implications for evaluating and improving NLP models.",
                    "score": 0.3509754580961078,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65283203125
                }
            ],
            "relevance_judgement": 0.65283203125,
            "relevance_judgment_input_expanded": "# Title: Your Model is Overconfident, and Other Lies We Tell Ourselves\n# Venue: arXiv.org\n# Authors: Timothee Mickus, Aman Sinha, Ra'ul V'azquez\n## Abstract\nThe difficulty intrinsic to a given example, rooted in its inherent ambiguity, is a key yet often overlooked factor in evaluating neural NLP models. We investigate the interplay and divergence among various metrics for assessing intrinsic difficulty, including annotator dissensus, training dynamics, and model confidence. Through a comprehensive analysis using 29 models on three datasets, we reveal that while correlations exist among these metrics, their relationships are neither linear nor monotonic. By disentangling these dimensions of uncertainty, we aim to refine our understanding of data complexity and its implications for evaluating and improving NLP models.\n",
            "reference_string": "[276742147 | Mickus et al. | 2025 | Citations: 0]"
        },
        {
            "title": "2D-Curri-DPO: Two-Dimensional Curriculum Learning for Direct Preference Optimization",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 42,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.07856, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2354891644",
                    "name": "Mengyang Li"
                },
                {
                    "authorId": "2354954580",
                    "name": "Zhong Zhang"
                }
            ],
            "abstract": "Aligning large language models with human preferences is crucial for their safe deployment. While Direct Preference Optimization (DPO) offers an efficient alternative to reinforcement learning from human feedback, traditional DPO methods are limited by their reliance on single preference pairs. Recent work like Curriculum-DPO integrates multiple pairs using a one-dimensional difficulty curriculum based on pairwise distinguishability (PD), but overlooks the complexity of the input prompt itself. To address this, we propose 2D-Curri-DPO, a novel framework employing a two-dimensional curriculum that jointly models Prompt Complexity (PC) and Pairwise Distinguishability. This framework introduces dual difficulty metrics to quantify prompt semantic complexity and response preference clarity, defines a curriculum strategy space encompassing multiple selectable strategies for task adaptation, and incorporates a KL-divergence-based adaptive mechanism for dynamic reference model updates to enhance training stability. Comprehensive experiments demonstrate that 2D-Curri-DPO significantly outperforms standard DPO and prior curriculum methods across multiple benchmarks, including MT-Bench, Vicuna Bench, and WizardLM. Our approach achieves state-of-the-art performance on challenging test sets like UltraFeedback. Ablation studies confirm the benefits of the 2D structure and adaptive mechanisms, while analysis provides guidance for strategy selection. These findings demonstrate that effective alignment requires modeling both prompt complexity and pairwise distinguishability, establishing adaptive, multi-dimensional curriculum learning as a powerful and interpretable new paradigm for preference-based language model optimization.",
            "corpus_id": 277667729,
            "sentences": [
                {
                    "corpus_id": "277667729",
                    "title": "2D-Curri-DPO: Two-Dimensional Curriculum Learning for Direct Preference Optimization",
                    "text": "Curriculum Learning (CL) inherently relies on ordering data by difficulty [31], yet defining appropriate difficulty metrics remains crucial, especially for complex inputs like natural language prompts. While directly quantifying prompt difficulty for LLM alignment is relatively underexplored, insights can be drawn from methods measuring sample difficulty in broader machine learning contexts. These methods often inform adaptive training strategies. For instance, Zhu et al. [32] provided a formal definition of sample learning difficulty inspired by the bias-variance trade-off in statistical learning theory, proposing a variance-based measure reflecting learning stability. Their work suggests that samples leading to higher variance in model predictions or gradients across training perturbations might be considered more difficult. Other common heuristics for estimating sample difficulty include using training loss magnitude [33], gradient norms [34], or predictive uncertainty [35]. However, these often rely on the state of a single model during training or simple input features. \n\nParticularly inspired by the general idea in [32] of leveraging variance as a proxy for difficulty, our work develops a metric specifically for prompt complexity within the LLM alignment setting. Our approach measures the perplexity variance of responses generated by a single reference model for the input prompt. This prompt difficulty metric provides a solid foundation for our 2D curriculum.",
                    "score": 0.3908534054751986,
                    "section_title": "C. Quantifying Sample Difficulty for Curriculum Learning",
                    "char_start_offset": 8042,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 201
                        },
                        {
                            "start": 202,
                            "end": 394
                        },
                        {
                            "start": 395,
                            "end": 451
                        },
                        {
                            "start": 452,
                            "end": 678
                        },
                        {
                            "start": 679,
                            "end": 838
                        },
                        {
                            "start": 839,
                            "end": 992
                        },
                        {
                            "start": 993,
                            "end": 1091
                        },
                        {
                            "start": 1094,
                            "end": 1289
                        },
                        {
                            "start": 1290,
                            "end": 1408
                        },
                        {
                            "start": 1409,
                            "end": 1489
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 74,
                            "end": 78,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 477,
                            "end": 481,
                            "matchedPaperCorpusId": "248810776"
                        },
                        {
                            "start": 934,
                            "end": 938,
                            "matchedPaperCorpusId": "244958599"
                        },
                        {
                            "start": 987,
                            "end": 991,
                            "matchedPaperCorpusId": "252369193"
                        },
                        {
                            "start": 1139,
                            "end": 1143,
                            "matchedPaperCorpusId": "248810776"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6494140625
                }
            ],
            "relevance_judgement": 0.6494140625,
            "relevance_judgment_input_expanded": "# Title: 2D-Curri-DPO: Two-Dimensional Curriculum Learning for Direct Preference Optimization\n# Venue: arXiv.org\n# Authors: Mengyang Li, Zhong Zhang\n## Abstract\nAligning large language models with human preferences is crucial for their safe deployment. While Direct Preference Optimization (DPO) offers an efficient alternative to reinforcement learning from human feedback, traditional DPO methods are limited by their reliance on single preference pairs. Recent work like Curriculum-DPO integrates multiple pairs using a one-dimensional difficulty curriculum based on pairwise distinguishability (PD), but overlooks the complexity of the input prompt itself. To address this, we propose 2D-Curri-DPO, a novel framework employing a two-dimensional curriculum that jointly models Prompt Complexity (PC) and Pairwise Distinguishability. This framework introduces dual difficulty metrics to quantify prompt semantic complexity and response preference clarity, defines a curriculum strategy space encompassing multiple selectable strategies for task adaptation, and incorporates a KL-divergence-based adaptive mechanism for dynamic reference model updates to enhance training stability. Comprehensive experiments demonstrate that 2D-Curri-DPO significantly outperforms standard DPO and prior curriculum methods across multiple benchmarks, including MT-Bench, Vicuna Bench, and WizardLM. Our approach achieves state-of-the-art performance on challenging test sets like UltraFeedback. Ablation studies confirm the benefits of the 2D structure and adaptive mechanisms, while analysis provides guidance for strategy selection. These findings demonstrate that effective alignment requires modeling both prompt complexity and pairwise distinguishability, establishing adaptive, multi-dimensional curriculum learning as a powerful and interpretable new paradigm for preference-based language model optimization.\n## C. Quantifying Sample Difficulty for Curriculum Learning\nCurriculum Learning (CL) inherently relies on ordering data by difficulty [31], yet defining appropriate difficulty metrics remains crucial, especially for complex inputs like natural language prompts. While directly quantifying prompt difficulty for LLM alignment is relatively underexplored, insights can be drawn from methods measuring sample difficulty in broader machine learning contexts. These methods often inform adaptive training strategies. For instance, Zhu et al. [32] provided a formal definition of sample learning difficulty inspired by the bias-variance trade-off in statistical learning theory, proposing a variance-based measure reflecting learning stability. Their work suggests that samples leading to higher variance in model predictions or gradients across training perturbations might be considered more difficult. Other common heuristics for estimating sample difficulty include using training loss magnitude [33], gradient norms [34], or predictive uncertainty [35]. However, these often rely on the state of a single model during training or simple input features. \n\nParticularly inspired by the general idea in [32] of leveraging variance as a proxy for difficulty, our work develops a metric specifically for prompt complexity within the LLM alignment setting. Our approach measures the perplexity variance of responses generated by a single reference model for the input prompt. This prompt difficulty metric provides a solid foundation for our 2D curriculum.",
            "reference_string": "[277667729 | Li et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Angular Gap: Reducing the Uncertainty of Image Difficulty through Model Calibration",
            "venue": "ACM Multimedia",
            "year": 2022,
            "reference_count": 48,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://discovery.ucl.ac.uk/10161581/1/2207.08525.pdf",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2207.08525, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2147421343",
                    "name": "Bohua Peng"
                },
                {
                    "authorId": "145481136",
                    "name": "Mobarakol Islam"
                },
                {
                    "authorId": "2067051679",
                    "name": "Mei Tu"
                }
            ],
            "abstract": "Curriculum learning needs example difficulty to proceed from easy to hard. However, the credibility of image difficulty is rarely investigated, which can seriously affect the effectiveness of curricula. In this work, we propose Angular Gap, a measure of difficulty based on the difference in angular distance between feature embeddings and class-weight embeddings built by hyperspherical learning. To ascertain difficulty estimation, we introduce class-wise model calibration, as a post-training technique, to the learnt hyperbolic space. This bridges the gap between probabilistic model calibration and angular distance estimation of hyperspherical learning. We show the superiority of our calibrated Angular Gap over recent difficulty metrics on CIFAR10-H and ImageNetV2. We further propose a curriculum based on Angular Gap for unsupervised domain adaptation that can translate from learning easy samples to mining hard samples. We combine this curriculum with a state-of-the-art self-training method, Cycle Self Training (CST). The proposed Curricular CST learns robust representations and outperforms recent baselines on Office31 and VisDA 2017.",
            "corpus_id": 250626773,
            "sentences": [
                {
                    "corpus_id": "250626773",
                    "title": "Angular Gap: Reducing the Uncertainty of Image Difficulty through Model Calibration",
                    "text": "Ascertaining example difficulty is a critical problem to curriculum learning and self-paced learning, in that curricula rank training samples by difficulty and proceed from easy to hard. In the context of image classification, a natural idea is to quantify such difficulty with human selection frequency [34], i.e., the fraction of annotators selecting a sample for its target class. However, human labelling effort is not scalable to get fine-grained image difficulty. To measure the difficulty of 10,000 CIFAR10 images, CIFAR10-H [2] recruits 2570 annotators to perform 511,400 trials, with an average of 51 human decisions per image, not including a considerable amount of practice and attention checks. Hence, automating difficulty estimation is crucial to applying curriculum learning to large scale datasets. Probabilistic models are particularly compelling for this automatic estimation demand because of their consistency towards noisy image contents and uncertain labels regularly presented in large scale datasets. Early works have characterized image difficulty with maximum confidence or classification margin, the difference between the predicting probability of the correct class and the largest among others. However, difficulty measurers based on modern neural networks have a reputation of being poorly calibrated. Extensive research have shown that the negative log-likelihood can easily overfit training samples, pushing average predicting probability away from accuracy [13,21]. This suggests considerable uncertainty of softmax probabilities, and imprecise difficulty measurement undermines the performance of curriculum learning. While probability estimation deteriorates, final classification results actually improve [13]. Very recently, deep ensemble methods [1,18] measure example difficulty with agreement either from last layers' predictions or from intermediate layers' predictions. Reducing estimation uncertainty with ensembling requires selected treatments and controls such as architectures, number of submodels, and number of data splits. In this work, we show faithful image difficulty can be efficiently estimated by deep metric learning. Hyperspherical learning [26], a weakly supervised learning framework, groups instances of the same concept together and pushes instances of different concepts apart by enforcing angular discrimination during training. The framework allows for more robust similarity estimation and has improved representation learning in both computer vision [7] and natural language understanding [8].",
                    "score": 0.37335457867510946,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 186
                        },
                        {
                            "start": 187,
                            "end": 383
                        },
                        {
                            "start": 384,
                            "end": 469
                        },
                        {
                            "start": 470,
                            "end": 706
                        },
                        {
                            "start": 707,
                            "end": 814
                        },
                        {
                            "start": 815,
                            "end": 1024
                        },
                        {
                            "start": 1025,
                            "end": 1223
                        },
                        {
                            "start": 1224,
                            "end": 1331
                        },
                        {
                            "start": 1332,
                            "end": 1498
                        },
                        {
                            "start": 1499,
                            "end": 1651
                        },
                        {
                            "start": 1652,
                            "end": 1746
                        },
                        {
                            "start": 1747,
                            "end": 1911
                        },
                        {
                            "start": 1912,
                            "end": 2072
                        },
                        {
                            "start": 2073,
                            "end": 2174
                        },
                        {
                            "start": 2175,
                            "end": 2392
                        },
                        {
                            "start": 2393,
                            "end": 2560
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 304,
                            "end": 308,
                            "matchedPaperCorpusId": "67855879"
                        },
                        {
                            "start": 532,
                            "end": 535,
                            "matchedPaperCorpusId": "225098279"
                        },
                        {
                            "start": 1490,
                            "end": 1494,
                            "matchedPaperCorpusId": "28671436"
                        },
                        {
                            "start": 1494,
                            "end": 1497,
                            "matchedPaperCorpusId": "202773833"
                        },
                        {
                            "start": 1741,
                            "end": 1745,
                            "matchedPaperCorpusId": "28671436"
                        },
                        {
                            "start": 1784,
                            "end": 1787,
                            "matchedPaperCorpusId": "235458180"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6455078125
                }
            ],
            "relevance_judgement": 0.6455078125,
            "relevance_judgment_input_expanded": "# Title: Angular Gap: Reducing the Uncertainty of Image Difficulty through Model Calibration\n# Venue: ACM Multimedia\n# Authors: Bohua Peng, Mobarakol Islam, Mei Tu\n## Abstract\nCurriculum learning needs example difficulty to proceed from easy to hard. However, the credibility of image difficulty is rarely investigated, which can seriously affect the effectiveness of curricula. In this work, we propose Angular Gap, a measure of difficulty based on the difference in angular distance between feature embeddings and class-weight embeddings built by hyperspherical learning. To ascertain difficulty estimation, we introduce class-wise model calibration, as a post-training technique, to the learnt hyperbolic space. This bridges the gap between probabilistic model calibration and angular distance estimation of hyperspherical learning. We show the superiority of our calibrated Angular Gap over recent difficulty metrics on CIFAR10-H and ImageNetV2. We further propose a curriculum based on Angular Gap for unsupervised domain adaptation that can translate from learning easy samples to mining hard samples. We combine this curriculum with a state-of-the-art self-training method, Cycle Self Training (CST). The proposed Curricular CST learns robust representations and outperforms recent baselines on Office31 and VisDA 2017.\n## INTRODUCTION\nAscertaining example difficulty is a critical problem to curriculum learning and self-paced learning, in that curricula rank training samples by difficulty and proceed from easy to hard. In the context of image classification, a natural idea is to quantify such difficulty with human selection frequency [34], i.e., the fraction of annotators selecting a sample for its target class. However, human labelling effort is not scalable to get fine-grained image difficulty. To measure the difficulty of 10,000 CIFAR10 images, CIFAR10-H [2] recruits 2570 annotators to perform 511,400 trials, with an average of 51 human decisions per image, not including a considerable amount of practice and attention checks. Hence, automating difficulty estimation is crucial to applying curriculum learning to large scale datasets. Probabilistic models are particularly compelling for this automatic estimation demand because of their consistency towards noisy image contents and uncertain labels regularly presented in large scale datasets. Early works have characterized image difficulty with maximum confidence or classification margin, the difference between the predicting probability of the correct class and the largest among others. However, difficulty measurers based on modern neural networks have a reputation of being poorly calibrated. Extensive research have shown that the negative log-likelihood can easily overfit training samples, pushing average predicting probability away from accuracy [13,21]. This suggests considerable uncertainty of softmax probabilities, and imprecise difficulty measurement undermines the performance of curriculum learning. While probability estimation deteriorates, final classification results actually improve [13]. Very recently, deep ensemble methods [1,18] measure example difficulty with agreement either from last layers' predictions or from intermediate layers' predictions. Reducing estimation uncertainty with ensembling requires selected treatments and controls such as architectures, number of submodels, and number of data splits. In this work, we show faithful image difficulty can be efficiently estimated by deep metric learning. Hyperspherical learning [26], a weakly supervised learning framework, groups instances of the same concept together and pushes instances of different concepts apart by enforcing angular discrimination during training. The framework allows for more robust similarity estimation and has improved representation learning in both computer vision [7] and natural language understanding [8].",
            "reference_string": "[250626773 | Peng et al. | 2022 | Citations: 9]"
        },
        {
            "title": "Complexity-Guided Curriculum Learning for Text Graphs",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 40,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.13472, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2165570025",
                    "name": "Nidhi Vakil"
                },
                {
                    "authorId": "2267488904",
                    "name": "Hadi Amiri"
                }
            ],
            "abstract": "Curriculum learning provides a systematic approach to training. It refines training progressively, tailors training to task requirements, and improves generalization through exposure to diverse examples. We present a curriculum learning approach that builds on existing knowledge about text and graph complexity formalisms for training with text graph data. The core part of our approach is a novel data scheduler, which employs\"spaced repetition\"and complexity formalisms to guide the training process. We demonstrate the effectiveness of the proposed approach on several text graph tasks and graph neural network architectures. The proposed model gains more and uses less data; consistently prefers text over graph complexity indices throughout training, while the best curricula derived from text and graph complexity indices are equally effective; and it learns transferable curricula across GNN models and datasets. In addition, we find that both node-level (local) and graph-level (global) graph complexity indices, as well as shallow and traditional text complexity indices play a crucial role in effective curriculum learning.",
            "corpus_id": 265351926,
            "sentences": [
                {
                    "corpus_id": "265351926",
                    "title": "Complexity-Guided Curriculum Learning for Text Graphs",
                    "text": "In Curriculum learning (CL) (Bengio et al., 2009) data samples are scheduled in a meaningful difficulty order, typically from easy to hard, for iterative training. In graph machine learning, Wang et al. (2021) introduced CurGraph, a curriculum learning method designed for sub-graph classification. This X X X X X X X X  2019) proposed a dynamic curriculum approach that automatically assigns confidence scores to samples based on their estimated difficulty. However this model requires additional trainable parameters. To address this limitation, Castells et al. (2020) introduced the SuperLoss framework to calculate optimal confidence scores for each instance using a closed-form solution. In (Vakil and Amiri, 2022), we extended SuperLoss to incorporate trend information at the sample level. We utilized loss trajectories to estimate the emerging difficulty of subgraphs and employed weighted sample losses for data scheduling in order to create effective curricula for training GNNs and understanding their learning dynamics. \n\nCurrent curriculum learning methodologies in NLP rely on data properties, e.g., sentence length, word rarity, or syntactic features (Platanios et al., 2019;Liu et al., 2021), or annotation disagreement (Elgaar and Amiri, 2023); as well as model properties such as training loss and its variations (Graves et al., 2017;Amiri et al., 2017) to sequence data samples for training. Elgaar and Amiri (2023) developed a curriculum discovery framework based on prior knowledge of sample difficulty, utilized annotation entropy and loss values. They concluded that curricula based on easy-to-hard or hard-to-easy transition are often at the risk of under-performing, effective curricula are often non-monotonic, and curricula learned from smaller datasets perform well on larger datasets. \n\nOther instances of curriculum learning for textual data have primarily centered on machine translation and language comprehension. For instance, Agrawal and Carpuat (2022) introduced a framework for training non-autoregressive sequence-tosequence models for text editing.",
                    "score": 0.3649650428261123,
                    "section_title": "Related Work",
                    "char_start_offset": 21966,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 163
                        },
                        {
                            "start": 164,
                            "end": 298
                        },
                        {
                            "start": 299,
                            "end": 458
                        },
                        {
                            "start": 459,
                            "end": 519
                        },
                        {
                            "start": 520,
                            "end": 692
                        },
                        {
                            "start": 693,
                            "end": 796
                        },
                        {
                            "start": 797,
                            "end": 1031
                        },
                        {
                            "start": 1034,
                            "end": 1410
                        },
                        {
                            "start": 1411,
                            "end": 1569
                        },
                        {
                            "start": 1570,
                            "end": 1813
                        },
                        {
                            "start": 1816,
                            "end": 1946
                        },
                        {
                            "start": 1947,
                            "end": 2087
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 28,
                            "end": 49,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 191,
                            "end": 209,
                            "matchedPaperCorpusId": "235324744"
                        },
                        {
                            "start": 548,
                            "end": 570,
                            "matchedPaperCorpusId": "227275456"
                        },
                        {
                            "start": 696,
                            "end": 719,
                            "matchedPaperCorpusId": "248863268"
                        },
                        {
                            "start": 1166,
                            "end": 1190,
                            "matchedPaperCorpusId": "85498775"
                        },
                        {
                            "start": 1190,
                            "end": 1207,
                            "matchedPaperCorpusId": "236460239"
                        },
                        {
                            "start": 1236,
                            "end": 1260,
                            "matchedPaperCorpusId": "259370648"
                        },
                        {
                            "start": 1331,
                            "end": 1352,
                            "matchedPaperCorpusId": "11137059"
                        },
                        {
                            "start": 1352,
                            "end": 1371,
                            "matchedPaperCorpusId": "1916665"
                        },
                        {
                            "start": 1411,
                            "end": 1434,
                            "matchedPaperCorpusId": "259370648"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.63427734375
                }
            ],
            "relevance_judgement": 0.63427734375,
            "relevance_judgment_input_expanded": "# Title: Complexity-Guided Curriculum Learning for Text Graphs\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Nidhi Vakil, Hadi Amiri\n## Abstract\nCurriculum learning provides a systematic approach to training. It refines training progressively, tailors training to task requirements, and improves generalization through exposure to diverse examples. We present a curriculum learning approach that builds on existing knowledge about text and graph complexity formalisms for training with text graph data. The core part of our approach is a novel data scheduler, which employs\"spaced repetition\"and complexity formalisms to guide the training process. We demonstrate the effectiveness of the proposed approach on several text graph tasks and graph neural network architectures. The proposed model gains more and uses less data; consistently prefers text over graph complexity indices throughout training, while the best curricula derived from text and graph complexity indices are equally effective; and it learns transferable curricula across GNN models and datasets. In addition, we find that both node-level (local) and graph-level (global) graph complexity indices, as well as shallow and traditional text complexity indices play a crucial role in effective curriculum learning.\n## Related Work\nIn Curriculum learning (CL) (Bengio et al., 2009) data samples are scheduled in a meaningful difficulty order, typically from easy to hard, for iterative training. In graph machine learning, Wang et al. (2021) introduced CurGraph, a curriculum learning method designed for sub-graph classification. This X X X X X X X X  2019) proposed a dynamic curriculum approach that automatically assigns confidence scores to samples based on their estimated difficulty. However this model requires additional trainable parameters. To address this limitation, Castells et al. (2020) introduced the SuperLoss framework to calculate optimal confidence scores for each instance using a closed-form solution. In (Vakil and Amiri, 2022), we extended SuperLoss to incorporate trend information at the sample level. We utilized loss trajectories to estimate the emerging difficulty of subgraphs and employed weighted sample losses for data scheduling in order to create effective curricula for training GNNs and understanding their learning dynamics. \n\nCurrent curriculum learning methodologies in NLP rely on data properties, e.g., sentence length, word rarity, or syntactic features (Platanios et al., 2019;Liu et al., 2021), or annotation disagreement (Elgaar and Amiri, 2023); as well as model properties such as training loss and its variations (Graves et al., 2017;Amiri et al., 2017) to sequence data samples for training. Elgaar and Amiri (2023) developed a curriculum discovery framework based on prior knowledge of sample difficulty, utilized annotation entropy and loss values. They concluded that curricula based on easy-to-hard or hard-to-easy transition are often at the risk of under-performing, effective curricula are often non-monotonic, and curricula learned from smaller datasets perform well on larger datasets. \n\nOther instances of curriculum learning for textual data have primarily centered on machine translation and language comprehension. For instance, Agrawal and Carpuat (2022) introduced a framework for training non-autoregressive sequence-tosequence models for text editing.",
            "reference_string": "[265351926 | Vakil et al. | 2023 | Citations: 3]"
        },
        {
            "title": "Curriculum Consistency Learning for Conditional Sentence Generation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 53,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.768, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2287881971",
                    "name": "Liangxin Liu"
                },
                {
                    "authorId": "2256344322",
                    "name": "Xuebo Liu"
                },
                {
                    "authorId": "2324576203",
                    "name": "Lian Lian"
                },
                {
                    "authorId": "2324857773",
                    "name": "Shengjun Cheng"
                },
                {
                    "authorId": "2142545243",
                    "name": "Jun Rao"
                },
                {
                    "authorId": "2273704740",
                    "name": "Tengfei Yu"
                },
                {
                    "authorId": "2193388247",
                    "name": "Hexuan Deng"
                },
                {
                    "authorId": "2269805934",
                    "name": "Min Zhang"
                }
            ],
            "abstract": "Consistency learning (CL) has proven to be a valuable technique for improving the robustness of models in conditional sentence generation (CSG) tasks by ensuring stable predictions across various input data forms. However, models augmented with CL often face challenges in optimizing consistency features, which can detract from their efficiency and effectiveness. To address these challenges, we introduce Curriculum Consistency Learning (CCL), a novel strategy that guides models to learn consistency in alignment with their current capacity to differentiate between features. CCL is designed around the inherent aspects of CL-related losses, promoting task independence and simplifying implementation. Implemented across four representative CSG tasks, including instruction tuning (IT) for large language models and machine translation (MT) in three modalities (text, speech, and vision), CCL demonstrates marked improvements. Specifically, it delivers +2.0 average accuracy point improvement compared with vanilla IT and an average increase of +0.7 in COMET scores over traditional CL methods in MT tasks. Our comprehensive analysis further indicates that models utilizing CCL are particularly adept at managing complex instances, showcasing the effectiveness and efficiency of CCL in improving CSG models. Code and scripts are available at https://github.com/xinxinxing/Curriculum-Consistency-Learning.",
            "corpus_id": 273901573,
            "sentences": [
                {
                    "corpus_id": "273901573",
                    "title": "Curriculum Consistency Learning for Conditional Sentence Generation",
                    "text": "CCL integrates a difficulty measure and a proficiency estimator directly derived from the CL losses observed in both the training and validation phases. This innovation eliminates the dependency on task-specific metrics, streamlining the implementation process. CCL strategically guides the model to engage in CL with instances that match its current proficiency level, ensuring more effective and efficient optimization of CL-enhanced CSG models, leading to improved performance. \n\nExperimental evaluation in four CSG tasks, including instruction tuning (IT) tasks (Rao et al., 2024;Fan et al., 2024) for large language models (LLMs) and machine translation (MT) tasks in three distinct modalities (text, vision, and speech) establishes the superior performance of CCL over existing advanced baselines and conventional CL approaches. Specifically, it delivers +2.0 average accuracy point improvement compared with vanilla IT and an average increase of +0.7 in COMET scores over traditional CL methods in MT tasks. This advancement is supported by detailed analyses, which corroborate the effectiveness of CCL in improving the model's ability to handle complex instances, thus confirming the benefits of our method in enhancing the efficiency and efficacy of models in diverse CSG tasks. \n\nOur contributions are summarized as follows: \n\n\u2022 We introduce CCL, an innovative framework for the training of CSG models, which strategically enhances model consistency learning across differentiated training phases. \n\n\u2022 We propose a novel automatic metric for the difficulty of instances and the capability of models within the context of the CL paradigm. \n\n\u2022 We demonstrate that the notable performance improvements by CCL can be largely attributed to its superior handling of challenging samples, such as translating hard sentences. \n\n2 Background and Existing Challenges",
                    "score": 0.340376651473154,
                    "section_title": "Introduction",
                    "char_start_offset": 2089,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 152
                        },
                        {
                            "start": 153,
                            "end": 261
                        },
                        {
                            "start": 262,
                            "end": 480
                        },
                        {
                            "start": 483,
                            "end": 834
                        },
                        {
                            "start": 835,
                            "end": 1014
                        },
                        {
                            "start": 1015,
                            "end": 1287
                        },
                        {
                            "start": 1290,
                            "end": 1334
                        },
                        {
                            "start": 1337,
                            "end": 1507
                        },
                        {
                            "start": 1510,
                            "end": 1647
                        },
                        {
                            "start": 1650,
                            "end": 1826
                        },
                        {
                            "start": 1829,
                            "end": 1865
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 566,
                            "end": 584,
                            "matchedPaperCorpusId": "273162336"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.62451171875
                }
            ],
            "relevance_judgement": 0.62451171875,
            "relevance_judgment_input_expanded": "# Title: Curriculum Consistency Learning for Conditional Sentence Generation\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Liangxin Liu, Xuebo Liu, Lian Lian, Shengjun Cheng, Jun Rao, Tengfei Yu, Hexuan Deng, Min Zhang\n## Abstract\nConsistency learning (CL) has proven to be a valuable technique for improving the robustness of models in conditional sentence generation (CSG) tasks by ensuring stable predictions across various input data forms. However, models augmented with CL often face challenges in optimizing consistency features, which can detract from their efficiency and effectiveness. To address these challenges, we introduce Curriculum Consistency Learning (CCL), a novel strategy that guides models to learn consistency in alignment with their current capacity to differentiate between features. CCL is designed around the inherent aspects of CL-related losses, promoting task independence and simplifying implementation. Implemented across four representative CSG tasks, including instruction tuning (IT) for large language models and machine translation (MT) in three modalities (text, speech, and vision), CCL demonstrates marked improvements. Specifically, it delivers +2.0 average accuracy point improvement compared with vanilla IT and an average increase of +0.7 in COMET scores over traditional CL methods in MT tasks. Our comprehensive analysis further indicates that models utilizing CCL are particularly adept at managing complex instances, showcasing the effectiveness and efficiency of CCL in improving CSG models. Code and scripts are available at https://github.com/xinxinxing/Curriculum-Consistency-Learning.\n## Introduction\nCCL integrates a difficulty measure and a proficiency estimator directly derived from the CL losses observed in both the training and validation phases. This innovation eliminates the dependency on task-specific metrics, streamlining the implementation process. CCL strategically guides the model to engage in CL with instances that match its current proficiency level, ensuring more effective and efficient optimization of CL-enhanced CSG models, leading to improved performance. \n\nExperimental evaluation in four CSG tasks, including instruction tuning (IT) tasks (Rao et al., 2024;Fan et al., 2024) for large language models (LLMs) and machine translation (MT) tasks in three distinct modalities (text, vision, and speech) establishes the superior performance of CCL over existing advanced baselines and conventional CL approaches. Specifically, it delivers +2.0 average accuracy point improvement compared with vanilla IT and an average increase of +0.7 in COMET scores over traditional CL methods in MT tasks. This advancement is supported by detailed analyses, which corroborate the effectiveness of CCL in improving the model's ability to handle complex instances, thus confirming the benefits of our method in enhancing the efficiency and efficacy of models in diverse CSG tasks. \n\nOur contributions are summarized as follows: \n\n\u2022 We introduce CCL, an innovative framework for the training of CSG models, which strategically enhances model consistency learning across differentiated training phases. \n\n\u2022 We propose a novel automatic metric for the difficulty of instances and the capability of models within the context of the CL paradigm. \n\n\u2022 We demonstrate that the notable performance improvements by CCL can be largely attributed to its superior handling of challenging samples, such as translating hard sentences. \n\n2 Background and Existing Challenges",
            "reference_string": "[273901573 | Liu et al. | 2024 | Citations: 1]"
        },
        {
            "title": "On the Effectiveness of Curriculum Learning in Educational Text Scoring",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2023,
            "reference_count": 53,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/26707/26479",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v37i12.26707?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v37i12.26707, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "8647899",
                    "name": "Zijie Zeng"
                },
                {
                    "authorId": "65953975",
                    "name": "D. Ga\u0161evi\u0107"
                },
                {
                    "authorId": "49901492",
                    "name": "Guangliang Chen"
                }
            ],
            "abstract": "Automatic Text Scoring (ATS) is a widely-investigated task in education. Existing approaches often stressed the structure design of an ATS model and neglected the training process of the model. Considering the difficult nature of this task, we argued that the performance of an ATS model could be potentially boosted by carefully selecting data of varying complexities in the training process. Therefore, we aimed to investigate the effectiveness of curriculum learning (CL) in scoring educational text. Specifically, we designed two types of difficulty measurers: (i) pre-defined, calculated by measuring a sample's readability, length, the number of grammatical errors or unique words it contains; and (ii) automatic, calculated based on whether a model in a training epoch can accurately score the samples. These measurers were tested in both the easy-to-hard to hard-to-easy training paradigms. Through extensive evaluations on two widely-used datasets (one for short answer scoring and the other for long essay scoring), we demonstrated that (a) CL indeed could boost the performance of state-of-the-art ATS models, and the maximum improvement could be up to 4.5%, but most improvements were achieved when assessing short and easy answers; (b) the pre-defined measurer calculated based on the number of grammatical errors contained in a text sample tended to outperform the other difficulty measurers across different training paradigms.",
            "corpus_id": 259593617,
            "sentences": [
                {
                    "corpus_id": "259593617",
                    "title": "On the Effectiveness of Curriculum Learning in Educational Text Scoring",
                    "text": "Automatic. Though pre-defined strategies have been demonstrated effective in various application scenarios, they are often plagued by their strong reliance on human expertise to define an appropriate difficulty measurer and an extensive search for effective combinations of difficulty measurer and training scheduler. Therefore, in addition to the four pre-defined CL strategies described above, we further designed an automatic difficulty measurer to dynamically select data samples based on instance-wise training loss and enable a more flexible training process. Specifically, the automatic difficulty measurer used in this study characterizes data samples as either easy and difficult, which represents the samples whose ground truth scores are correctly or incorrectly predicted by a model in a training epoch. Let p t easy and p t dif f icult denote the probabilities of an individual easy sample and an individual difficult sample to be selected for model training at the current t-th epoch, we define r as the ratio between these two probabilities: \n\nBy choosing different values for r, we can enable the strategy to lay different emphasis on the easy and difficult samples. In particular, we explored two different ways to determine the value for r and consequently two variants of the automatic strategy: \n\n\u2022 Static, which sets r to the same value across different training epochs. During experiments, r was empirically determined by searching in the range of (0, 5] with an interval of 0.1. When r < 1 ( i.e., p t dif f icult < p t easy ), easy samples will be more likely to be selected for training; when r > 1, difficult samples will be more likely to be selected for training. \n\n\u2022 Adaptive, the value of r at the current t-th epoch is based on the number of easy and difficult samples in the previous epoch. We denote the set of easy and difficult samples as E and D, respectively, and formally define: \n\nNote that such a setting of r ensures that when there are relatively a larger portion of easy (or difficult) samples, the strategy tends to select difficult (or easy) samples more often for the subsequent training.",
                    "score": 0.3828208195931197,
                    "section_title": "Curriculum Learning Design",
                    "char_start_offset": 18939,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 10
                        },
                        {
                            "start": 11,
                            "end": 317
                        },
                        {
                            "start": 318,
                            "end": 565
                        },
                        {
                            "start": 566,
                            "end": 815
                        },
                        {
                            "start": 816,
                            "end": 1056
                        },
                        {
                            "start": 1059,
                            "end": 1182
                        },
                        {
                            "start": 1183,
                            "end": 1314
                        },
                        {
                            "start": 1317,
                            "end": 1391
                        },
                        {
                            "start": 1392,
                            "end": 1501
                        },
                        {
                            "start": 1502,
                            "end": 1691
                        },
                        {
                            "start": 1694,
                            "end": 1822
                        },
                        {
                            "start": 1823,
                            "end": 1917
                        },
                        {
                            "start": 1920,
                            "end": 2134
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.623046875
                }
            ],
            "relevance_judgement": 0.623046875,
            "relevance_judgment_input_expanded": "# Title: On the Effectiveness of Curriculum Learning in Educational Text Scoring\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Zijie Zeng, D. Ga\u0161evi\u0107, Guangliang Chen\n## Abstract\nAutomatic Text Scoring (ATS) is a widely-investigated task in education. Existing approaches often stressed the structure design of an ATS model and neglected the training process of the model. Considering the difficult nature of this task, we argued that the performance of an ATS model could be potentially boosted by carefully selecting data of varying complexities in the training process. Therefore, we aimed to investigate the effectiveness of curriculum learning (CL) in scoring educational text. Specifically, we designed two types of difficulty measurers: (i) pre-defined, calculated by measuring a sample's readability, length, the number of grammatical errors or unique words it contains; and (ii) automatic, calculated based on whether a model in a training epoch can accurately score the samples. These measurers were tested in both the easy-to-hard to hard-to-easy training paradigms. Through extensive evaluations on two widely-used datasets (one for short answer scoring and the other for long essay scoring), we demonstrated that (a) CL indeed could boost the performance of state-of-the-art ATS models, and the maximum improvement could be up to 4.5%, but most improvements were achieved when assessing short and easy answers; (b) the pre-defined measurer calculated based on the number of grammatical errors contained in a text sample tended to outperform the other difficulty measurers across different training paradigms.\n## Curriculum Learning Design\nAutomatic. Though pre-defined strategies have been demonstrated effective in various application scenarios, they are often plagued by their strong reliance on human expertise to define an appropriate difficulty measurer and an extensive search for effective combinations of difficulty measurer and training scheduler. Therefore, in addition to the four pre-defined CL strategies described above, we further designed an automatic difficulty measurer to dynamically select data samples based on instance-wise training loss and enable a more flexible training process. Specifically, the automatic difficulty measurer used in this study characterizes data samples as either easy and difficult, which represents the samples whose ground truth scores are correctly or incorrectly predicted by a model in a training epoch. Let p t easy and p t dif f icult denote the probabilities of an individual easy sample and an individual difficult sample to be selected for model training at the current t-th epoch, we define r as the ratio between these two probabilities: \n\nBy choosing different values for r, we can enable the strategy to lay different emphasis on the easy and difficult samples. In particular, we explored two different ways to determine the value for r and consequently two variants of the automatic strategy: \n\n\u2022 Static, which sets r to the same value across different training epochs. During experiments, r was empirically determined by searching in the range of (0, 5] with an interval of 0.1. When r < 1 ( i.e., p t dif f icult < p t easy ), easy samples will be more likely to be selected for training; when r > 1, difficult samples will be more likely to be selected for training. \n\n\u2022 Adaptive, the value of r at the current t-th epoch is based on the number of easy and difficult samples in the previous epoch. We denote the set of easy and difficult samples as E and D, respectively, and formally define: \n\nNote that such a setting of r ensures that when there are relatively a larger portion of easy (or difficult) samples, the strategy tends to select difficult (or easy) samples more often for the subsequent training.",
            "reference_string": "[259593617 | Zeng et al. | 2023 | Citations: 6]"
        },
        {
            "title": "Understanding Difficulty-based Sample Weighting with a Universal Difficulty Measure",
            "venue": "ECML/PKDD",
            "year": 2023,
            "reference_count": 37,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2301.04850",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.04850, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48667407",
                    "name": "Xiaoling Zhou"
                },
                {
                    "authorId": "2061463107",
                    "name": "Ou Wu"
                },
                {
                    "authorId": "2152350103",
                    "name": "Weiyao Zhu"
                },
                {
                    "authorId": "2200527528",
                    "name": "Ziyang Liang"
                }
            ],
            "abstract": "Sample weighting is widely used in deep learning. A large number of weighting methods essentially utilize the learning difficulty of training samples to calculate their weights. In this study, this scheme is called difficulty-based weighting. Two important issues arise when explaining this scheme. First, a unified difficulty measure that can be theoretically guaranteed for training samples does not exist. The learning difficulties of the samples are determined by multiple factors including noise level, imbalance degree, margin, and uncertainty. Nevertheless, existing measures only consider a single factor or in part, but not in their entirety. Second, a comprehensive theoretical explanation is lacking with respect to demonstrating why difficulty-based weighting schemes are effective in deep learning. In this study, we theoretically prove that the generalization error of a sample can be used as a universal difficulty measure. Furthermore, we provide formal theoretical justifications on the role of difficulty-based weighting for deep learning, consequently revealing its positive influences on both the optimization dynamics and generalization performance of deep models, which is instructive to existing weighting schemes.",
            "corpus_id": 255749213,
            "sentences": [
                {
                    "corpus_id": "255749213",
                    "title": "Understanding Difficulty-based Sample Weighting with a Universal Difficulty Measure",
                    "text": "The commonly used difficulty measures, such as loss [3] and gradient norm [9], are mainly related to the bias term. Shin et al. [27] emphasized that only using loss as the measurement cannot distinguish clean and noisy samples, especially for uniform label noise. There are also a few existing studies that use variance [28,29]. For instance, Agarwal et al. [30] applied the variance of gradient norms as the difficulty measure. Indeed, both the variance and bias terms should not be underestimated when measuring the samples' learning difficulty. Our theoretical analyses support that generalization error including both the two terms can capture four main factors influencing the samples' learning difficulty. Thus, the error can be leveraged as a universal measure that is more reasonable than existing measures. Existing studies generally apply the K-fold cross-validation method [26] to calculate the generalization error. More efficient error calculation algorithms are supposed to be proposed which will be our future work.",
                    "score": 0.31503165799208427,
                    "section_title": "Discussion about Generalization Error",
                    "char_start_offset": 19652,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 115
                        },
                        {
                            "start": 116,
                            "end": 263
                        },
                        {
                            "start": 264,
                            "end": 328
                        },
                        {
                            "start": 329,
                            "end": 428
                        },
                        {
                            "start": 429,
                            "end": 547
                        },
                        {
                            "start": 548,
                            "end": 711
                        },
                        {
                            "start": 712,
                            "end": 815
                        },
                        {
                            "start": 816,
                            "end": 927
                        },
                        {
                            "start": 928,
                            "end": 1030
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 52,
                            "end": 55,
                            "matchedPaperCorpusId": "1977996"
                        },
                        {
                            "start": 74,
                            "end": 77,
                            "matchedPaperCorpusId": "225499092"
                        },
                        {
                            "start": 320,
                            "end": 324,
                            "matchedPaperCorpusId": "20785832"
                        },
                        {
                            "start": 884,
                            "end": 888,
                            "matchedPaperCorpusId": "211505957"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.62255859375
                }
            ],
            "relevance_judgement": 0.62255859375,
            "relevance_judgment_input_expanded": "# Title: Understanding Difficulty-based Sample Weighting with a Universal Difficulty Measure\n# Venue: ECML/PKDD\n# Authors: Xiaoling Zhou, Ou Wu, Weiyao Zhu, Ziyang Liang\n## Abstract\nSample weighting is widely used in deep learning. A large number of weighting methods essentially utilize the learning difficulty of training samples to calculate their weights. In this study, this scheme is called difficulty-based weighting. Two important issues arise when explaining this scheme. First, a unified difficulty measure that can be theoretically guaranteed for training samples does not exist. The learning difficulties of the samples are determined by multiple factors including noise level, imbalance degree, margin, and uncertainty. Nevertheless, existing measures only consider a single factor or in part, but not in their entirety. Second, a comprehensive theoretical explanation is lacking with respect to demonstrating why difficulty-based weighting schemes are effective in deep learning. In this study, we theoretically prove that the generalization error of a sample can be used as a universal difficulty measure. Furthermore, we provide formal theoretical justifications on the role of difficulty-based weighting for deep learning, consequently revealing its positive influences on both the optimization dynamics and generalization performance of deep models, which is instructive to existing weighting schemes.\n## Discussion about Generalization Error\nThe commonly used difficulty measures, such as loss [3] and gradient norm [9], are mainly related to the bias term. Shin et al. [27] emphasized that only using loss as the measurement cannot distinguish clean and noisy samples, especially for uniform label noise. There are also a few existing studies that use variance [28,29]. For instance, Agarwal et al. [30] applied the variance of gradient norms as the difficulty measure. Indeed, both the variance and bias terms should not be underestimated when measuring the samples' learning difficulty. Our theoretical analyses support that generalization error including both the two terms can capture four main factors influencing the samples' learning difficulty. Thus, the error can be leveraged as a universal measure that is more reasonable than existing measures. Existing studies generally apply the K-fold cross-validation method [26] to calculate the generalization error. More efficient error calculation algorithms are supposed to be proposed which will be our future work.",
            "reference_string": "[255749213 | Zhou et al. | 2023 | Citations: 5]"
        },
        {
            "title": "Investigating the Performance of Data Complexity & Instance Hardness Measures as A Meta-Feature in Overlapping Classes Problem",
            "venue": "International Conference on Cloud and Big Data Computing",
            "year": 2023,
            "reference_count": 30,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3616131.3616132",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3616131.3616132?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3616131.3616132, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2249765658",
                    "name": "Omaimah Al Hosni"
                },
                {
                    "authorId": "2249765682",
                    "name": "Andrew Starkey"
                }
            ],
            "abstract": "Since the meta-learning recommendation's quality depends on the meta-features decision quality, a common problem in meta-learning is establishing a (good) collection of meta-features that best represent the dataset properties. Therefore, many meta-feature measures/methods have been proposed during the last decade to describe the characteristics of the data. However, little attention has been paid to validating the meta-feature decisions in reflecting the actual data properties. In particular, if the meta-feature analysis is negatively affected by complex data characteristics, such as class overlap due to the distortion imposed by the noisy features at the decision boundary of the classes and thereby produces biased meta-learning recommendations that do not match the actual data characteristics (either by overestimating or underestimating the complexity). Hence, this issue is crucial to ensure the success of the meta-learning model since the learning algorithm selection decision is based on meta-feature analysis. Based on that, in this work, we aim to investigate this by assessing the performance of Complexity Measures (global/data-level measures) & Instance Hardness Measures (local/instance-level measures) as a meta-feature in reflecting the actual data complexity associated with the high-class overlapping problem. The reason for focusing on the overlapping classes problem is that several studies have proven that this data issue significantly contributes to degrading prediction accuracy, with which most real-world datasets are associated. On the other hand, the motivation for using the above measures among different meta-feature methods proposed in the literature is that since this study aims to focus on the overlapping classes problem, the above measures are mainly proposed to estimate the data complexity according to the geometrical descriptions focusing on the class overlap imposed by feature values, in which match the data problem that the study interested to investigate.",
            "corpus_id": 263334839,
            "sentences": [
                {
                    "corpus_id": "263334839",
                    "title": "Investigating the Performance of Data Complexity & Instance Hardness Measures as A Meta-Feature in Overlapping Classes Problem",
                    "text": "According to their study outcomes, this meta-knowledge can be used as a descriptor for characterising a data set's hardness profile and provided an insight into the leading causes of the difficulties they represent in the dataset. Furthermore, a comprehensive survey done by [7] aims to review the applications of both Complexity Measures and Instance Hardness Measures in different areas, including the meta-learning field. Moreover, to overcome the limitation of the global Complexity Measures in characterising the overall complexity of the entire dataset, [12] have decomposed some of the global measures into instance/ local-level measures, so the analysis is conducted based on the individual contribution of each instance instead of global complexity of the entire dataset from the class prospective. Then, they compared the performance of the proposed decomposed instance/local-level measures against the global equivalents and concluded that the former provided better performance than the latter. \n\nDespite the advances shown in the recent work, an empirical comprehensive review of the ability of these measures (from both global and local perspectives) to give an estimation of the difficulty of a given data problem independent of the learning algorithm has not yet been undertaken. Most of the works undertaken in the literature are limited to examining the correlation between the values of the measures with the learning algorithms' prediction accuracy performance. From our study point of view, examining this correlation is not a good independent indicator to validate the complexity measure performance in estimating the actual data difficulty nor for showing the causes of the poor prediction of the learning algorithm's performance. Thus, in this work, the analysis of learning algorithm performance will be omitted from this study.",
                    "score": 0.3058360598962842,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 11609,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 230
                        },
                        {
                            "start": 231,
                            "end": 424
                        },
                        {
                            "start": 425,
                            "end": 807
                        },
                        {
                            "start": 808,
                            "end": 1006
                        },
                        {
                            "start": 1009,
                            "end": 1295
                        },
                        {
                            "start": 1296,
                            "end": 1481
                        },
                        {
                            "start": 1482,
                            "end": 1753
                        },
                        {
                            "start": 1754,
                            "end": 1853
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 560,
                            "end": 564,
                            "matchedPaperCorpusId": "224817007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5869140625
                }
            ],
            "relevance_judgement": 0.5869140625,
            "relevance_judgment_input_expanded": "# Title: Investigating the Performance of Data Complexity & Instance Hardness Measures as A Meta-Feature in Overlapping Classes Problem\n# Venue: International Conference on Cloud and Big Data Computing\n# Authors: Omaimah Al Hosni, Andrew Starkey\n## Abstract\nSince the meta-learning recommendation's quality depends on the meta-features decision quality, a common problem in meta-learning is establishing a (good) collection of meta-features that best represent the dataset properties. Therefore, many meta-feature measures/methods have been proposed during the last decade to describe the characteristics of the data. However, little attention has been paid to validating the meta-feature decisions in reflecting the actual data properties. In particular, if the meta-feature analysis is negatively affected by complex data characteristics, such as class overlap due to the distortion imposed by the noisy features at the decision boundary of the classes and thereby produces biased meta-learning recommendations that do not match the actual data characteristics (either by overestimating or underestimating the complexity). Hence, this issue is crucial to ensure the success of the meta-learning model since the learning algorithm selection decision is based on meta-feature analysis. Based on that, in this work, we aim to investigate this by assessing the performance of Complexity Measures (global/data-level measures) & Instance Hardness Measures (local/instance-level measures) as a meta-feature in reflecting the actual data complexity associated with the high-class overlapping problem. The reason for focusing on the overlapping classes problem is that several studies have proven that this data issue significantly contributes to degrading prediction accuracy, with which most real-world datasets are associated. On the other hand, the motivation for using the above measures among different meta-feature methods proposed in the literature is that since this study aims to focus on the overlapping classes problem, the above measures are mainly proposed to estimate the data complexity according to the geometrical descriptions focusing on the class overlap imposed by feature values, in which match the data problem that the study interested to investigate.\n## RELATED WORK\nAccording to their study outcomes, this meta-knowledge can be used as a descriptor for characterising a data set's hardness profile and provided an insight into the leading causes of the difficulties they represent in the dataset. Furthermore, a comprehensive survey done by [7] aims to review the applications of both Complexity Measures and Instance Hardness Measures in different areas, including the meta-learning field. Moreover, to overcome the limitation of the global Complexity Measures in characterising the overall complexity of the entire dataset, [12] have decomposed some of the global measures into instance/ local-level measures, so the analysis is conducted based on the individual contribution of each instance instead of global complexity of the entire dataset from the class prospective. Then, they compared the performance of the proposed decomposed instance/local-level measures against the global equivalents and concluded that the former provided better performance than the latter. \n\nDespite the advances shown in the recent work, an empirical comprehensive review of the ability of these measures (from both global and local perspectives) to give an estimation of the difficulty of a given data problem independent of the learning algorithm has not yet been undertaken. Most of the works undertaken in the literature are limited to examining the correlation between the values of the measures with the learning algorithms' prediction accuracy performance. From our study point of view, examining this correlation is not a good independent indicator to validate the complexity measure performance in estimating the actual data difficulty nor for showing the causes of the poor prediction of the learning algorithm's performance. Thus, in this work, the analysis of learning algorithm performance will be omitted from this study.",
            "reference_string": "[263334839 | Hosni et al. | 2023 | Citations: 2]"
        },
        {
            "title": "Epi-Curriculum: Episodic Curriculum Learning for Low-Resource Domain Adaptation in Neural Machine Translation",
            "venue": "IEEE Transactions on Artificial Intelligence",
            "year": 2023,
            "reference_count": 58,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.02640",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.02640, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2238164778",
                    "name": "Keyu Chen"
                },
                {
                    "authorId": "2237989461",
                    "name": "Zhuang Di"
                },
                {
                    "authorId": "2260916568",
                    "name": "Mingchen Li"
                },
                {
                    "authorId": "2334755606",
                    "name": "J. Morris Chang"
                }
            ],
            "abstract": "Neural machine translation (NMT) models have achieved comparable results to human translation with a large number of parallel corpora available. However, their performance remains poor when translating on new domains with a limited number of data. Recent studies either only show the model's robustness to domain shift or the superiority in adapting to new domains with a limited number of data. A solution for addressing both the model's robustness and adaptability is underexplored. In this article, we present a novel approach Epi-Curriculum to address low-resource domain adaptation (DA), which contains a new episodic training framework along with a denoised curriculum learning. Our episodic training framework enhances the model's robustness to domain shift by episodically exposing the encoder/decoder to an inexperienced decoder/encoder. The denoised curriculum learning filters the noised data and further improves the model's adaptability by gradually guiding the learning process from easy to more difficult tasks. Extensive experiments have been conducted on English\u2013German (En-De), English\u2013Romanian (En-Ro), and English\u2013French (En-Fr) translation tasks. Our results show that: 1) Epi-Curriculum outperforms the baseline on unseen and seen domains by 2.28 and 3.64 BLEU score on En-De task, and 3.32 and 2.23 on En-Ro task; and 2) our episodic training framework outperforms the recent popular metalearning framework in terms of robustness to domain shift and achieves comparable adaptability to new domains.",
            "corpus_id": 261557260,
            "sentences": [
                {
                    "corpus_id": "261557260",
                    "title": "Epi-Curriculum: Episodic Curriculum Learning for Low-Resource Domain Adaptation in Neural Machine Translation",
                    "text": "We follow the general curriculum learning framework of difficulty measure and training scheduler (Wang et al., 2021b), where difficulty measure determines the relative \"difficulty\" of each data sample and training scheduler decides the sequence of data subsets throughout the training process. Additionally, curriculum learning has been approved as an effective method for data cleaning, and many works have shown that the performance can be improved on denoised data (Wang et al., 2017(Wang et al., , 2019)). Thus, in order to let the model focus on the high-relevant in-domain corpus, curriculum learning is also applied to filter the data. \n\nWe evaluate our Epi-Curriculum on English-German (En-De) and English-Romanian (En-Ro) translation tasks with 10 and 9 different domains. There are 5 seen domains used for training a teacher model, and then individually fine-tuning on the 5 seen and the rest unseen domains. BLEU score (Papineni et al., 2002;Post, 2018) is reported and the experimental results show that Epi-Curriculum improves the model's robustness and adaptability on both seen and unseen domains. For instance, it outperforms the baselines by 1.37 -3.64 on the En-De task and 1.73 -3.32 on the En-Ro task. We further demonstrate the model's robustness to domain shift, where they improve the baseline by 2.55 and 2.59 BLEU scores, respectively. Our contributions mainly lie in three aspects: \n\n\u2022 We propose a novel episodic training framework to handle the model's robustness to domain shift. This is the first work that simulates the situation of unexpected statistics during training for domain adaptation in NMT. \n\n\u2022 Curriculum learning is applied to our training framework. Our curriculum learning not only guides the model from easy to difficult tasks but also denoises the training data. \n\n\u2022 We evaluate Epi-Curriculum on two language pairs with ten and nine different domains, empirically showing the strength of our proposed approach. \n\n2 Related Work",
                    "score": 0.3374813049941395,
                    "section_title": "Introduction",
                    "char_start_offset": 3566,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 293
                        },
                        {
                            "start": 294,
                            "end": 509
                        },
                        {
                            "start": 510,
                            "end": 642
                        },
                        {
                            "start": 645,
                            "end": 781
                        },
                        {
                            "start": 782,
                            "end": 918
                        },
                        {
                            "start": 919,
                            "end": 1112
                        },
                        {
                            "start": 1113,
                            "end": 1221
                        },
                        {
                            "start": 1222,
                            "end": 1360
                        },
                        {
                            "start": 1361,
                            "end": 1407
                        },
                        {
                            "start": 1410,
                            "end": 1508
                        },
                        {
                            "start": 1509,
                            "end": 1631
                        },
                        {
                            "start": 1634,
                            "end": 1693
                        },
                        {
                            "start": 1694,
                            "end": 1809
                        },
                        {
                            "start": 1812,
                            "end": 1958
                        },
                        {
                            "start": 1961,
                            "end": 1975
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 97,
                            "end": 117,
                            "matchedPaperCorpusId": "232362223"
                        },
                        {
                            "start": 468,
                            "end": 486,
                            "matchedPaperCorpusId": "1054586"
                        },
                        {
                            "start": 486,
                            "end": 508,
                            "matchedPaperCorpusId": "174798125"
                        },
                        {
                            "start": 930,
                            "end": 953,
                            "matchedPaperCorpusId": "11080756"
                        },
                        {
                            "start": 953,
                            "end": 964,
                            "matchedPaperCorpusId": "13751870"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.57958984375
                }
            ],
            "relevance_judgement": 0.57958984375,
            "relevance_judgment_input_expanded": "# Title: Epi-Curriculum: Episodic Curriculum Learning for Low-Resource Domain Adaptation in Neural Machine Translation\n# Venue: IEEE Transactions on Artificial Intelligence\n# Authors: Keyu Chen, Zhuang Di, Mingchen Li, J. Morris Chang\n## Abstract\nNeural machine translation (NMT) models have achieved comparable results to human translation with a large number of parallel corpora available. However, their performance remains poor when translating on new domains with a limited number of data. Recent studies either only show the model's robustness to domain shift or the superiority in adapting to new domains with a limited number of data. A solution for addressing both the model's robustness and adaptability is underexplored. In this article, we present a novel approach Epi-Curriculum to address low-resource domain adaptation (DA), which contains a new episodic training framework along with a denoised curriculum learning. Our episodic training framework enhances the model's robustness to domain shift by episodically exposing the encoder/decoder to an inexperienced decoder/encoder. The denoised curriculum learning filters the noised data and further improves the model's adaptability by gradually guiding the learning process from easy to more difficult tasks. Extensive experiments have been conducted on English\u2013German (En-De), English\u2013Romanian (En-Ro), and English\u2013French (En-Fr) translation tasks. Our results show that: 1) Epi-Curriculum outperforms the baseline on unseen and seen domains by 2.28 and 3.64 BLEU score on En-De task, and 3.32 and 2.23 on En-Ro task; and 2) our episodic training framework outperforms the recent popular metalearning framework in terms of robustness to domain shift and achieves comparable adaptability to new domains.\n## Introduction\nWe follow the general curriculum learning framework of difficulty measure and training scheduler (Wang et al., 2021b), where difficulty measure determines the relative \"difficulty\" of each data sample and training scheduler decides the sequence of data subsets throughout the training process. Additionally, curriculum learning has been approved as an effective method for data cleaning, and many works have shown that the performance can be improved on denoised data (Wang et al., 2017(Wang et al., , 2019)). Thus, in order to let the model focus on the high-relevant in-domain corpus, curriculum learning is also applied to filter the data. \n\nWe evaluate our Epi-Curriculum on English-German (En-De) and English-Romanian (En-Ro) translation tasks with 10 and 9 different domains. There are 5 seen domains used for training a teacher model, and then individually fine-tuning on the 5 seen and the rest unseen domains. BLEU score (Papineni et al., 2002;Post, 2018) is reported and the experimental results show that Epi-Curriculum improves the model's robustness and adaptability on both seen and unseen domains. For instance, it outperforms the baselines by 1.37 -3.64 on the En-De task and 1.73 -3.32 on the En-Ro task. We further demonstrate the model's robustness to domain shift, where they improve the baseline by 2.55 and 2.59 BLEU scores, respectively. Our contributions mainly lie in three aspects: \n\n\u2022 We propose a novel episodic training framework to handle the model's robustness to domain shift. This is the first work that simulates the situation of unexpected statistics during training for domain adaptation in NMT. \n\n\u2022 Curriculum learning is applied to our training framework. Our curriculum learning not only guides the model from easy to difficult tasks but also denoises the training data. \n\n\u2022 We evaluate Epi-Curriculum on two language pairs with ten and nine different domains, empirically showing the strength of our proposed approach. \n\n2 Related Work",
            "reference_string": "[261557260 | Chen et al. | 2023 | Citations: 3]"
        },
        {
            "title": "DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2022,
            "reference_count": 81,
            "citation_count": 25,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2212.03597",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.03597, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2609325",
                    "name": "Conglong Li"
                },
                {
                    "authorId": "9088433",
                    "name": "Z. Yao"
                },
                {
                    "authorId": "2129511744",
                    "name": "Xiaoxia Wu"
                },
                {
                    "authorId": "67016465",
                    "name": "Minjia Zhang"
                },
                {
                    "authorId": "2145020341",
                    "name": "Yuxiong He"
                }
            ],
            "abstract": "Recent advances on deep learning models come at the price of formidable training cost. The increasing model size is one of the root causes, but another less-emphasized fact is that data scale is actually increasing at a similar speed as model scale, and the training cost is proportional to both of them. Compared to the rapidly evolving model architecture, how to efficiently use the training data (especially for the expensive foundation model pretraining) is both less explored and difficult to realize due to the lack of a convenient framework that focus on data efficiency capabilities. To this end, we present DeepSpeed Data Efficiency, a framework that makes better use of data, increases training efficiency, and improves model quality. Specifically, we propose and combine two data efficiency techniques: efficient data sampling via a general curriculum learning library, and efficient data routing via a novel random layerwise token dropping technique. For GPT-3 1.3B language model pretraining, our work achieves 12.5x less data/time/cost ($3.7K if rent on Azure), while still maintaining 95% of model quality compared to baseline with full data and cost ($46.3K). For GPT-3 1.3B and BERT-large pretraining, our work can also achieve the same model quality with up to 2x less data/time/cost, or achieve better model quality under same data/time/cost. DeepSpeed Data Efficiency is easy to use and tune, enabling us to easily apply it and verify its benefit on additional tasks including GPT-3 MoE model pretraining and small-scale GPT-2/ViT finetuning.",
            "corpus_id": 254366532,
            "sentences": [
                {
                    "corpus_id": "254366532",
                    "title": "DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing",
                    "text": "These techniques improve data and training efficiency, but existing solutions have several limitations: \u2022 Techniques like curriculum learning (CL) improve data efficiency by indexing and sampling training data based on certain difficulty metric (Bengio et al. 2009), and it has recently proved effective on large-scale pretraining tasks (Li, Zhang, and He 2022). However, implementing different CL strategies for different user tasks can require a lot of code-refactoring, which is time-consuming and error-prone. In addition, existing implementations have less consideration on scalability, which makes it difficult to analyze and index large-scale training data based on different difficulty metrics. \u2022 Existing data routing techniques such as token drop/bypass/pruning were mostly designed for inference and inapplicable to training. TokenBypass (Hou et al. 2022), to our knowledge the only data routing technique for foundation model pretraining, skips the compute of part of the input tokens at some middle layers during BERT pretraining, reducing pretraining cost while maintaining model quality. However, it requires several special implementations that may only work for the tested BERT pretraining case, such as the importance score-based token dropping decisions and the whitelist for special tokens. This could limit the possibility and benefit of applying it to other cases. In the NLP area, curriculum learning has been applied on small-scale one-stage tasks and downstream finetuning tasks, such as neural machine translation (NMT) (Kocmi and Bojar 2017;Bojar et al. 2017;Zhang et al. 2018;Platanios et al. 2019;Zhang et al. 2019) and natural language understanding (NLU) (Sachan andXing 2016, 2018;Tay et al. 2019;Xu et al. 2020). There are also a few works that explore curriculum learning for language model pretraining (Press, Smith, and Lewis 2020; Zhang et al. 2021;Campos 2021;Li, Zhang, and He 2022). However, one common limitation among existing works is that there does not exist a scalable and customizable curriculum learning library, making it difficult to analyze large-scale data and explore custom difficulty metrics/pacing functions.",
                    "score": 0.39012472518113245,
                    "section_title": "Introduction",
                    "char_start_offset": 1863,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 362
                        },
                        {
                            "start": 363,
                            "end": 513
                        },
                        {
                            "start": 514,
                            "end": 702
                        },
                        {
                            "start": 703,
                            "end": 836
                        },
                        {
                            "start": 837,
                            "end": 1102
                        },
                        {
                            "start": 1103,
                            "end": 1310
                        },
                        {
                            "start": 1311,
                            "end": 1386
                        },
                        {
                            "start": 1387,
                            "end": 1745
                        },
                        {
                            "start": 1746,
                            "end": 1922
                        },
                        {
                            "start": 1923,
                            "end": 2164
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 245,
                            "end": 265,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 849,
                            "end": 866,
                            "matchedPaperCorpusId": "247628024"
                        },
                        {
                            "start": 1568,
                            "end": 1586,
                            "matchedPaperCorpusId": "9460719"
                        },
                        {
                            "start": 1604,
                            "end": 1626,
                            "matchedPaperCorpusId": "85498775"
                        },
                        {
                            "start": 1626,
                            "end": 1643,
                            "matchedPaperCorpusId": "155089817"
                        },
                        {
                            "start": 1686,
                            "end": 1697,
                            "matchedPaperCorpusId": "16503693"
                        },
                        {
                            "start": 1697,
                            "end": 1713,
                            "matchedPaperCorpusId": "16503693"
                        },
                        {
                            "start": 1713,
                            "end": 1729,
                            "matchedPaperCorpusId": "166228313"
                        },
                        {
                            "start": 1729,
                            "end": 1744,
                            "matchedPaperCorpusId": "220045816"
                        },
                        {
                            "start": 1868,
                            "end": 1886,
                            "matchedPaperCorpusId": "233433844"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.57861328125
                }
            ],
            "relevance_judgement": 0.57861328125,
            "relevance_judgment_input_expanded": "# Title: DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Conglong Li, Z. Yao, Xiaoxia Wu, Minjia Zhang, Yuxiong He\n## Abstract\nRecent advances on deep learning models come at the price of formidable training cost. The increasing model size is one of the root causes, but another less-emphasized fact is that data scale is actually increasing at a similar speed as model scale, and the training cost is proportional to both of them. Compared to the rapidly evolving model architecture, how to efficiently use the training data (especially for the expensive foundation model pretraining) is both less explored and difficult to realize due to the lack of a convenient framework that focus on data efficiency capabilities. To this end, we present DeepSpeed Data Efficiency, a framework that makes better use of data, increases training efficiency, and improves model quality. Specifically, we propose and combine two data efficiency techniques: efficient data sampling via a general curriculum learning library, and efficient data routing via a novel random layerwise token dropping technique. For GPT-3 1.3B language model pretraining, our work achieves 12.5x less data/time/cost ($3.7K if rent on Azure), while still maintaining 95% of model quality compared to baseline with full data and cost ($46.3K). For GPT-3 1.3B and BERT-large pretraining, our work can also achieve the same model quality with up to 2x less data/time/cost, or achieve better model quality under same data/time/cost. DeepSpeed Data Efficiency is easy to use and tune, enabling us to easily apply it and verify its benefit on additional tasks including GPT-3 MoE model pretraining and small-scale GPT-2/ViT finetuning.\n## Introduction\nThese techniques improve data and training efficiency, but existing solutions have several limitations: \u2022 Techniques like curriculum learning (CL) improve data efficiency by indexing and sampling training data based on certain difficulty metric (Bengio et al. 2009), and it has recently proved effective on large-scale pretraining tasks (Li, Zhang, and He 2022). However, implementing different CL strategies for different user tasks can require a lot of code-refactoring, which is time-consuming and error-prone. In addition, existing implementations have less consideration on scalability, which makes it difficult to analyze and index large-scale training data based on different difficulty metrics. \u2022 Existing data routing techniques such as token drop/bypass/pruning were mostly designed for inference and inapplicable to training. TokenBypass (Hou et al. 2022), to our knowledge the only data routing technique for foundation model pretraining, skips the compute of part of the input tokens at some middle layers during BERT pretraining, reducing pretraining cost while maintaining model quality. However, it requires several special implementations that may only work for the tested BERT pretraining case, such as the importance score-based token dropping decisions and the whitelist for special tokens. This could limit the possibility and benefit of applying it to other cases. In the NLP area, curriculum learning has been applied on small-scale one-stage tasks and downstream finetuning tasks, such as neural machine translation (NMT) (Kocmi and Bojar 2017;Bojar et al. 2017;Zhang et al. 2018;Platanios et al. 2019;Zhang et al. 2019) and natural language understanding (NLU) (Sachan andXing 2016, 2018;Tay et al. 2019;Xu et al. 2020). There are also a few works that explore curriculum learning for language model pretraining (Press, Smith, and Lewis 2020; Zhang et al. 2021;Campos 2021;Li, Zhang, and He 2022). However, one common limitation among existing works is that there does not exist a scalable and customizable curriculum learning library, making it difficult to analyze large-scale data and explore custom difficulty metrics/pacing functions.",
            "reference_string": "[254366532 | Li et al. | 2022 | Citations: 25]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "248810776",
            "title": "Exploring the Learning Difficulty of Data: Theory and Measure",
            "text": "Learning difficulty is considered as an intrinsic property of data in machine learning [46,75]. Existing measurements are usually based on heuristic cues or inspirations, and they can be divided into the following main categories: \n\n\u2022 Loss-based measurement. This category directly uses the loss as the measure. Most measures fall into this category because it is simple yet effective in various learning tasks. Some methods [68] directly utilize the loss in one epoch as the degree of difficulty. Accordingly, the degrees for the same samples vary in different epochs. Some others utilize the average loss [45] during the partial or whole training procedure for measurement. \u2022 Cross-validation-based measurement. This category adopts a crossvalidation strategy [65]. For example, five-fold cross-validation is performed, and the whole cross-validation is repeated ten times. Consequently, each ELDD training sample receives ten predictions. The value of error predictions is used as the indicator of difficulty. \u2022 Uncertainty-based measurement. This category uses the (model) uncertainty of a sample to measure the difficulty. Aguilar et al. [3] identified hard samples based on the epistemic uncertainty (also known as the model uncertainty). They leveraged the Bayesian Neural Network [67] to infer the epistemic uncertainty. \u2022 Margin-based measurement. This category uses the margin (distance) of a sample to the underlying decision surface as the measurement. The rationale is that a small margin denotes a large difficulty [42,68]. \u2022 Gradient-based measurement. This category uses the loss gradient of a sample to measure the difficulty. Agarwal and Hooker [2] proposed the variance of gradients (VOG) across different epochs to rank data from difficult to easy. They considered that samples with high VOG values are far more difficult for the model to learn. Santiago et al. [54] applied the norm of the gradients to measure the difficulty, and high norms indicate large difficulty for learning. \n\nThe above-mentioned categories are highly correlated. For example, margin-based measurement is indeed a loss-based one when margin-based loss (e.g., hinge loss) is used.",
            "score": 0.562215273814821,
            "section_title": "Learning Difficulty Measurement",
            "char_start_offset": 5000,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 230
                },
                {
                    "start": 233,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 2002
                },
                {
                    "start": 2005,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2174
                }
            ],
            "ref_mentions": [
                {
                    "start": 425,
                    "end": 429,
                    "matchedPaperCorpusId": "231807280"
                },
                {
                    "start": 762,
                    "end": 766,
                    "matchedPaperCorpusId": "202540591"
                },
                {
                    "start": 1143,
                    "end": 1146,
                    "matchedPaperCorpusId": "215414127"
                },
                {
                    "start": 1288,
                    "end": 1292,
                    "matchedPaperCorpusId": "53715584"
                },
                {
                    "start": 1533,
                    "end": 1536,
                    "matchedPaperCorpusId": "231807280"
                },
                {
                    "start": 1663,
                    "end": 1666,
                    "matchedPaperCorpusId": "221319579"
                },
                {
                    "start": 1882,
                    "end": 1886,
                    "matchedPaperCorpusId": "225499092"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89306640625
        },
        {
            "corpus_id": "264819795",
            "title": "Ling-CL: Understanding NLP Models through Linguistic Curricula",
            "text": "Second, existing approaches quantify the difficulty of data based on instantaneous training loss. However, training loss provides noisy estimates of sample difficulty due to data memorization (Zhang et al., 2017;Arpit et al., 2017) in neural models. We will address both issues as part of this research. \n\nThe contributions of this paper are: \n\n\u2022 incorporating human-verified linguistic complexity information to establish an effective scoring function for assessing the difficulty of text data with respect to NLP models, \u2022 deriving linguistic curricula for NLP models based on linguistic complexity of data and model behavior during training, and \u2022 identifying the core sets of linguistic complexity indices that most contribute to learning NLP tasks by models. \n\nWe evaluate our approach on several NLP tasks that require significant linguistic knowledge and reasoning to be addressed. Experimental results show that our approach can uncover latent linguistic knowledge that is most important for addressing NLP tasks. In addition, our approach obtains consistent performance gain over competing models. Source code and data is available at https://github.com/CLU-UML/Ling-CL. We present a framework for multiview curriculum learning using linguistic complexity indices. Our framework estimates the importance of various linguistic complexity indices, aggregates the resulting importance scores to determine the difficulty of samples for learning NLP tasks, and develops novel curricula for training models using complexity indices. The list of all indices used is in Appendix A.",
            "score": 0.5037594567063713,
            "section_title": "Introduction",
            "char_start_offset": 3511,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 303
                },
                {
                    "start": 306,
                    "end": 342
                },
                {
                    "start": 345,
                    "end": 763
                },
                {
                    "start": 766,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1582
                }
            ],
            "ref_mentions": [
                {
                    "start": 192,
                    "end": 212,
                    "matchedPaperCorpusId": "6212000"
                },
                {
                    "start": 212,
                    "end": 231,
                    "matchedPaperCorpusId": "11455421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76025390625
        },
        {
            "corpus_id": "259370648",
            "title": "HuCurl: Human-induced Curriculum Discovery",
            "text": "Annotation entropy is a natural measure of difficulty (for humans) and may serve as a reliable difficulty metric for models. Entropy of each sample x i is calculated as \u2212 l p c log p c (Shannon, 1948), where c is a class category and p c is the fraction of annotators who chose label c for the sample. The use of entropy is supported in (Nie et al., 2020a), reporting a consistent positive correlation between model accuracy and level of human agreement.\n\nFurthermore, moving average of a sample's instantaneous loss is a good metric for difficulty (Zhou et al., 2020). Using a baseline model trained with no curriculum and with default hyperparameters, we collect the loss values of all training instances at intervals of 0.5 epochs and use the average loss as prior knowledge about sample difficulty. We obtain twenty observations of the loss and compute the average for each sample. Figure 3 shows the distributions of entropy and loss, and examples of data partitions across four datasets. Most datasets are highly imbalanced across difficulty groups, often containing more easier samples than harder ones. Such data disparities would perhaps explain why computational models can achieve human-level performance on complex NLP tasks or recent results reporting neural models being largely invariant to random word order permutation of data (Sinha et al., 2021).\n\nWe acknowledge that while multiple annotations per sample may not be readily available for many NLP datasets, such annotations were collected for most NLP datasets at their dataset development time. Our work shows that such information can be used to find effective curricula for NLP models and encourages dataset creators to publish their full annotation information. In addition, our curriculum discovery framework is independent of annotation information. In fact, we evaluated our approach with both annotation entropy and loss as two choices for sample-level difficulty estimation.",
            "score": 0.4870710704898112,
            "section_title": "Prior Knowledge of Difficulty",
            "char_start_offset": 10899,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 337,
                    "end": 356,
                    "matchedPaperCorpusId": "222177851"
                },
                {
                    "start": 549,
                    "end": 568,
                    "matchedPaperCorpusId": "227275560"
                },
                {
                    "start": 1344,
                    "end": 1364,
                    "matchedPaperCorpusId": "230435766"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69091796875
        },
        {
            "corpus_id": "257937538",
            "title": "A Review of the Evaluation System for Curriculum Learning",
            "text": "Figure 4 illustrates the cross-review method. In the process of model training, samples with larger losses are harder to learn for the current stage of the model. Conversely, samples with smaller losses prove that the model has been able to correctly predict or classify that sample, which should reduce the sampling probability of that sample. Using sample loss [25,29] as a difficulty evaluation metric, such as Negative Log Likelihood loss [79], square loss [36], and cross-entropy loss [42,110,111] as difficulty scores, is widely used in self-paced learning [18] and its variant methods [22,112]. For example, self-paced learning [18] controls the model to start sampling from samples with smaller losses for training through the coordination of the SPregularizer and modulates the regularizer to keep decreasing during the training process, guiding the model to gradually sample samples with larger losses. Cross entropy is used as a measure of transferability [42], domain relevance [44], uncertainty [113], and representativeness [36,114], as in Shu et al. [42], where cross entropy loss is used as a measure of sample transferability for solving sample noises of the source domain and distribution shift across domains. In particular, cross entropy is used as a measure of domain relevance in neural machine translation, such as when using the model cross entropy as a sentence divergence score [44]. Where a higher divergence score indicates that the sentence has more in-domain features and is more likely to be different from samples in the generic domain, thus enabling learning from common to individual samples in different domains for better generalization. Zhang [110] and Wang et al. [115] used the cross-entropy of two models for measuring domain relevance and noise level, including the cross-entropy difference between two models trained using out-of-domain data and in-domain data (Moore-Lewis Method) and the degree of change in cross-entropy for selecting generaldomain data for model training (Cynical Data Selection) [110], such as assessing the domain relevance of sentences using the cross-entropy of in-domain and general-domain language models [115] (Equation ( 5)).",
            "score": 0.48686526903984245,
            "section_title": "Self-Scoring",
            "char_start_offset": 38321,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 2196
                }
            ],
            "ref_mentions": [
                {
                    "start": 363,
                    "end": 367,
                    "matchedPaperCorpusId": "243865335"
                },
                {
                    "start": 461,
                    "end": 465,
                    "matchedPaperCorpusId": "57784927"
                },
                {
                    "start": 490,
                    "end": 494,
                    "matchedPaperCorpusId": "69357784"
                },
                {
                    "start": 498,
                    "end": 502,
                    "matchedPaperCorpusId": "52952193"
                },
                {
                    "start": 563,
                    "end": 567,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 592,
                    "end": 596,
                    "matchedPaperCorpusId": "37032764"
                },
                {
                    "start": 596,
                    "end": 600,
                    "matchedPaperCorpusId": "9431293"
                },
                {
                    "start": 635,
                    "end": 639,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 967,
                    "end": 971,
                    "matchedPaperCorpusId": "69357784"
                },
                {
                    "start": 990,
                    "end": 994,
                    "matchedPaperCorpusId": "232104951"
                },
                {
                    "start": 1008,
                    "end": 1013,
                    "matchedPaperCorpusId": "220047761"
                },
                {
                    "start": 1038,
                    "end": 1042,
                    "matchedPaperCorpusId": "57784927"
                },
                {
                    "start": 1065,
                    "end": 1069,
                    "matchedPaperCorpusId": "69357784"
                },
                {
                    "start": 1404,
                    "end": 1408,
                    "matchedPaperCorpusId": "232104951"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79248046875
        },
        {
            "corpus_id": "257937538",
            "title": "A Review of the Evaluation System for Curriculum Learning",
            "text": "Zhang [110] and Wang et al. [115] used the cross-entropy of two models for measuring domain relevance and noise level, including the cross-entropy difference between two models trained using out-of-domain data and in-domain data (Moore-Lewis Method) and the degree of change in cross-entropy for selecting generaldomain data for model training (Cynical Data Selection) [110], such as assessing the domain relevance of sentences using the cross-entropy of in-domain and general-domain language models [115] (Equation ( 5)). \n\nIn particular, Mousavi et al. [116] proposed to use two parameters, entropy and mean alpha angle, to obtain direct scattering mechanism information for measuring the degree of complexity of each pixel, which is used to calculate the complexity of each PolSAR im- In the process of model training, samples with larger losses are harder to learn for the current stage of the model. Conversely, samples with smaller losses prove that the model has been able to correctly predict or classify that sample, which should reduce the sampling probability of that sample. Using sample loss [25,29] as a difficulty evaluation metric, such as Negative Log Likelihood loss [79], square loss [36], and cross-entropy loss [42,110,111] as difficulty scores, is widely used in self-paced learning [18] and its variant methods [22,112]. For example, self-paced learning [18] controls the model to start sampling from samples with smaller losses for training through the coordination of the SP-regularizer and modulates the regularizer to keep decreasing during the training process, guiding the model to gradually sample samples with larger losses. Cross entropy is used as a measure of transferability [42], domain relevance [44], uncertainty [113], and representativeness [36,114], as in Shu et al. [42], where cross entropy loss is used as a measure of sample transferability for solving sample noises of the source domain and distribution shift across domains. In particular, cross entropy is used as a measure of domain relevance in neural machine translation, such as when using the model cross entropy as a sentence divergence score [44].",
            "score": 0.474848022781657,
            "section_title": "Self-Scoring",
            "char_start_offset": 39995,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 522
                },
                {
                    "start": 525,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2152
                }
            ],
            "ref_mentions": [
                {
                    "start": 555,
                    "end": 560,
                    "matchedPaperCorpusId": "245502373"
                },
                {
                    "start": 1105,
                    "end": 1109,
                    "matchedPaperCorpusId": "243865335"
                },
                {
                    "start": 1203,
                    "end": 1207,
                    "matchedPaperCorpusId": "57784927"
                },
                {
                    "start": 1232,
                    "end": 1236,
                    "matchedPaperCorpusId": "69357784"
                },
                {
                    "start": 1240,
                    "end": 1244,
                    "matchedPaperCorpusId": "52952193"
                },
                {
                    "start": 1305,
                    "end": 1309,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 1334,
                    "end": 1338,
                    "matchedPaperCorpusId": "37032764"
                },
                {
                    "start": 1338,
                    "end": 1342,
                    "matchedPaperCorpusId": "9431293"
                },
                {
                    "start": 1377,
                    "end": 1381,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 1710,
                    "end": 1714,
                    "matchedPaperCorpusId": "69357784"
                },
                {
                    "start": 1733,
                    "end": 1737,
                    "matchedPaperCorpusId": "232104951"
                },
                {
                    "start": 1751,
                    "end": 1756,
                    "matchedPaperCorpusId": "220047761"
                },
                {
                    "start": 1781,
                    "end": 1785,
                    "matchedPaperCorpusId": "57784927"
                },
                {
                    "start": 1808,
                    "end": 1812,
                    "matchedPaperCorpusId": "69357784"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69189453125
        },
        {
            "corpus_id": "267740312",
            "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
            "text": "In this paper, we delve into the measurement of sample difficulty from the model's perspective. Drawing inspiration from the learning order metric in (Mekala et al., 2022a), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered. We then select the most difficult sample subsets based on this ranking and instruction-tune a language model. Our experiments involve two instruction-tuning datasets, Alpaca-Data (Taori et al., 2023), and Dolly (Conover et al., 2023), with performance measured using automated metrics such as Al-pacaEval (Li et al., 2023b) and human evaluation. \n\nOur main findings indicate that language models can autonomously select training data, achieving performance equal to or better than training on the entire dataset. Furthermore, this characteristic scales across different model sizes, ranging from smaller ones (1B) to larger ones (13B) \n\n1 in parameters. As the size of the language model increases, we observe a consistent reduction in the minimum amount of data needed to surpass the performance of a model trained on the entire dataset. Interestingly, we observe that the data hardness also transfers across models, meaning samples considered difficult by smaller models are similarly challenging for larger models. Moreover, we note that this transferability improves with the size of the smaller model, eventually achieving comparable quality, beyond a size threshold, to that attained by selfselection conducted by larger models. Our study employs open-sourced models such as OPT (Zhang et al., 2022) and Llama-2 (Touvron et al., 2023) to support these findings. The remainder of the paper is structured as follows: initially, we describe the experimental setup encompassing the language models, the datasets employed, and the evaluation metrics utilized (section 2). Subsequently, we present our learning percentage-based difficulty metric and analyze it in detail (section 3). Following this, we optimize the proposed metric and introduce an equally effective, approximate, and faster metric (section 4). Ultimately, we analyze the challenging data identified through this metric (section 5).",
            "score": 0.468397373739596,
            "section_title": "arXiv:2402.10430v1 [cs.CL] 16 Feb 2024",
            "char_start_offset": 1531,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 775
                },
                {
                    "start": 778,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1064
                },
                {
                    "start": 1067,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 2002
                },
                {
                    "start": 2003,
                    "end": 2113
                },
                {
                    "start": 2114,
                    "end": 2241
                },
                {
                    "start": 2242,
                    "end": 2329
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 172,
                    "matchedPaperCorpusId": "249060677"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8408203125
        },
        {
            "corpus_id": "259370648",
            "title": "HuCurl: Human-induced Curriculum Discovery",
            "text": "The present work investigates the use of two sample difficulty scoring functions, human-induced annotation entropy and model-induced loss, for NLP models and datasets. The former requires the availability of multiple annotations per sample and the latter requires training an auxiliary model to compute sample instantaneous loss during the course of training. Our work does not provide a general solution to the choice or availability of good difficulty scoring functions. However, once such a function is available, our work presents solutions to the problem of finding high-performing curricula in curriculum space. Our approach, although effective at finding such curricula, requires a Bayesian search of its hyperparameters. We reduce these costs by finding curricula on smaller datasets and smaller models that can then be applied to corresponding larger datasets and models. Finally, the proposed method lacks theoretical analysis of the dynamic interactions between data, downstream models, and discovered curricula.      Table 4: Number of trials for the best parameters found. The notation for configurations is the same as Figure 4.",
            "score": 0.46121309436270014,
            "section_title": "Limitations",
            "char_start_offset": 29994,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63330078125
        },
        {
            "corpus_id": "247694098",
            "title": "Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU",
            "text": "As aforementioned, we use training dynamics (Swayamdipta et al., 2020), i.e. statistics originally introduced to analyse dataset quality, as difficulty metrics. The suitability of such statistics to serve as difficulty measures for CL is encapsulated in three core aspects. Firstly, training dynamics are straightforward. They can be easily obtained by training a single model on the target dataset and keeping statistics about its predictions on the training set. Secondly, training dynamics correlate well with model uncertainty and follow a similar trend to human (dis)agreement in terms of data annotation, essentially combining the view of both worlds. Finally, training dynamics manifest a clear pattern of separating instances into distinct areaseasy, ambiguous and hard examples for a model to learn-something that aligns well with the ideas behind Curriculum Learning. The difficulty of an example (x i , y i ) can be determined by a function f , where an example i is considered more difficult than example j if f (x i , y i ) > f (x j , y j ). We list three difficulty metrics that use statistics during the course of a model's training, as follows: CONFIDENCE (CONF) of an example x i is the average probability assigned to the gold label y i by a model with parameters \u03b8 across a number of epochs E. This is a continuous metric with higher values corresponding to easier examples. \n\nVARIABILITY (VAR) of an example x i is the standard deviation of the probabilities assigned to the gold label y i across E epochs. It is a continuous metric with higher values indicating greater uncertainty for a training example. \n\nConfidence and correctness are the primary metrics that we use in our curricula since low and high values correspond to hard and easy examples respectively. On the other hand, variability is used as an auxiliary metric since only high scores clearly represent uncertain examples while low scores offer no important information on their own.",
            "score": 0.4572523256838916,
            "section_title": "Difficulty Metrics",
            "char_start_offset": 9233,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1393
                },
                {
                    "start": 1396,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1626
                },
                {
                    "start": 1629,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1969
                }
            ],
            "ref_mentions": [
                {
                    "start": 44,
                    "end": 70,
                    "matchedPaperCorpusId": "221856637"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6904296875
        },
        {
            "corpus_id": "248810776",
            "title": "Exploring the Learning Difficulty of Data: Theory and Measure",
            "text": "The partition of training data into different subsets according to their learning difficulties and adoption of separate learning schemes (e.g., weighting) are proven to be useful in many learning tasks [21,43,46,68]. The learning difficulty investigated in this study refers to the degrees of easy or hard to learn of training samples in a given learning task. Although learning difficulty has no formal and consensus definition, it has been widely discussed and utilized in previous machine learning literature, including noise-aware, curriculum, and metric learning. \n\nNumerous methods are proposed to measure the learning difficulty of a training sample. The most common practice is to leverage the training output (e.g., loss and the predicted value on the true category) of a sample to construct the measurements. In Self-paced Learning (SPL) [26,68], the training loss is used to determine whether a sample is easy or not, and easy samples are first learned. We assume that p i,yi is the prediction on the ground-truth category for a training sample x i . In object detection, the value of (1 \u2212 p i,yi ) is used to indicate the learning difficulty for x i [43] . Given that the training output in an epoch may be unreliable, some methods utilize the average training output of a sample during the training to measure the difficulty. Huang et al. [33] designed a cyclic training procedure, and the model is trained from underfitting to over-fitting in one cycle. The average training loss in the whole cyclic procedure is used as the noisy indicator for a training sample. Feng et al. [20] utilized the magnitude of the loss gradient to measure the learning difficulty of a training sample. A large gradient magnitude indicates a high degree of difficulty. \n\nDue to lack of a theoretical basis, different learning difficulty measures are based on different heuristic cues or empirical observations, resulting that each measure usually only suits specific application scenarios. A clearer understanding of the essence of a sample's learning difficulty can at least facilitate explaining difficulty-based weighting methods and designing more effective learning difficulty measures. However, we are still far from concluding that we have a comprehensive understanding of learning difficulty: \n\n(1) There is no formal definition of the learning difficulty of a sample. Different studies exhibit different understandings of learning difficulty.",
            "score": 0.45111663388989065,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 568
                },
                {
                    "start": 571,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1761
                },
                {
                    "start": 1764,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2184
                },
                {
                    "start": 2185,
                    "end": 2293
                },
                {
                    "start": 2296,
                    "end": 2369
                },
                {
                    "start": 2370,
                    "end": 2444
                }
            ],
            "ref_mentions": [
                {
                    "start": 202,
                    "end": 206,
                    "matchedPaperCorpusId": "6644398"
                },
                {
                    "start": 206,
                    "end": 209,
                    "matchedPaperCorpusId": "47252984"
                },
                {
                    "start": 212,
                    "end": 215,
                    "matchedPaperCorpusId": "231807280"
                },
                {
                    "start": 848,
                    "end": 852,
                    "matchedPaperCorpusId": "52902973"
                },
                {
                    "start": 852,
                    "end": 855,
                    "matchedPaperCorpusId": "231807280"
                },
                {
                    "start": 1162,
                    "end": 1166,
                    "matchedPaperCorpusId": "47252984"
                },
                {
                    "start": 1352,
                    "end": 1356,
                    "matchedPaperCorpusId": "207995300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73583984375
        },
        {
            "corpus_id": "257937538",
            "title": "A Review of the Evaluation System for Curriculum Learning",
            "text": "In particular, cross entropy is used as a measure of domain relevance in neural machine translation, such as when using the model cross entropy as a sentence divergence score [44]. Where a higher divergence score indicates that the sentence has more in-domain features and is more likely to be different from samples in the generic domain, thus enabling learning from common to individual samples in different domains for better generalization. Zhang [110] and Wang et al. [115] used the cross-entropy of two models for measuring domain relevance and noise level, including the cross-entropy difference between two models trained using out-of-domain data and in-domain data (Moore-Lewis Method) and the degree of change in cross-entropy for selecting general-domain data for model training (Cynical Data Selection) [110], such as assessing the domain relevance of sentences using the cross-entropy of in-domain and general-domain language models [115] (Equation ( 5)). \n\nIn particular, Mousavi et al. [116] proposed to use two parameters, entropy and mean alpha angle, to obtain direct scattering mechanism information for measuring the degree of complexity of each pixel, which is used to calculate the complexity of each PolSAR image patch. \n\nIn practice, research focusing on the instantaneous loss values of samples, as mentioned above, requires evaluating all samples before selecting them at each step, which involves additional inference on unselected samples, and that work is very costly in training. Rather than focusing on the instantaneous loss values of a sample, some studies have focused on its loss value during training, calculating the change in model loss over two consecutive training iterations [107] as a difficulty metric, proving that a sample is very difficult when its loss fluctuates between maximum and minimum values during the sequence. Zhou et al. [117] proposed the exponential moving average (EMA) method for the detection of clean and pseudo-labeled samples. When a sample's loss consistently maintains a low value during training, then its label has a higher probability of being correct, and when a sample's EMA consistency loss remains constant during training, then its pseudo-label is more reliable, achieving the selection of clean, correctly pseudo-labeled data for training and avoiding the inclusion of harmful noisy data.",
            "score": 0.44824992188435697,
            "section_title": "Self-Scoring",
            "char_start_offset": 41967,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 968
                },
                {
                    "start": 971,
                    "end": 1242
                },
                {
                    "start": 1245,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2365
                }
            ],
            "ref_mentions": [
                {
                    "start": 175,
                    "end": 179,
                    "matchedPaperCorpusId": "232104951"
                },
                {
                    "start": 1001,
                    "end": 1006,
                    "matchedPaperCorpusId": "245502373"
                },
                {
                    "start": 1716,
                    "end": 1721,
                    "matchedPaperCorpusId": "227275560"
                },
                {
                    "start": 1879,
                    "end": 1884,
                    "matchedPaperCorpusId": "233452059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6904296875
        },
        {
            "corpus_id": "264819795",
            "title": "Ling-CL: Understanding NLP Models through Linguistic Curricula",
            "text": "Effective curricula improve learning in humans (Tabibian et al., 2019;Nishimura, 2018) and machines (Bengio et al., 2009;Kumar et al., 2010;Zhou et al., 2020;Castells et al., 2020). Curriculum learning has been found effective in many NLP tasks (Settles and Meeder, 2016;Amiri et al., 2017;Platanios et al., 2019;Zhang et al., 2019;Amiri, 2019;Xu et al., 2020;Lalor and Yu, 2020;Jafarpour et al., 2021;Kreutzer et al., 2021;Agrawal and Carpuat, 2022;Maharana and Bansal, 2022). A multiview curriculum is a curriculum able to integrate multiple difficulty scores simultaneously and leverage their collective value (Vakil and Amiri, 2023). \n\nWe assume there exists a subset of linguistic complexity indices that are most influential to learning an NLP task by a particular model. To identify these indices for each model and NLP task, we derive a weight factor \u03c1 i \u2208 [\u22121, 1] for each linguistic index that quantifies how well the index estimates the true difficulty of data samples to the model, determined by model instantaneous loss against validation data. By learning these weight factors, we obtain precise estimations that shed light on the core linguistic complexity indices that each model needs at different stages of its training to learn an NLP task. In addition, these estimates can be readily used for linguistic curriculum development, e.g., by training models with linguistically easy samples (with respect to the model) and gradually introducing linguistically challenging samples. \n\nTo achieve the above goals, we should address two gaps in the existing literature: First, existing curricula are often limited to a single criterion of difficulty and are not applicable to multiview settings. This is while difficulty is a condition that can be realized from multiple perspectives, can vary across a continuum for different models, and can dynamically change as the model improves. Second, existing approaches quantify the difficulty of data based on instantaneous training loss.",
            "score": 0.443204836788762,
            "section_title": "Introduction",
            "char_start_offset": 1615,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1495
                },
                {
                    "start": 1498,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 1993
                }
            ],
            "ref_mentions": [
                {
                    "start": 47,
                    "end": 70,
                    "matchedPaperCorpusId": "58947495"
                },
                {
                    "start": 70,
                    "end": 86,
                    "matchedPaperCorpusId": "52157421"
                },
                {
                    "start": 100,
                    "end": 121,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 140,
                    "end": 158,
                    "matchedPaperCorpusId": "227275560"
                },
                {
                    "start": 158,
                    "end": 180,
                    "matchedPaperCorpusId": "227275456"
                },
                {
                    "start": 271,
                    "end": 290,
                    "matchedPaperCorpusId": "1916665"
                },
                {
                    "start": 290,
                    "end": 313,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 313,
                    "end": 332,
                    "matchedPaperCorpusId": "155089817"
                },
                {
                    "start": 332,
                    "end": 344,
                    "matchedPaperCorpusId": "174799918"
                },
                {
                    "start": 344,
                    "end": 360,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 360,
                    "end": 379,
                    "matchedPaperCorpusId": "226226711"
                },
                {
                    "start": 379,
                    "end": 402,
                    "matchedPaperCorpusId": "236486249"
                },
                {
                    "start": 402,
                    "end": 424,
                    "matchedPaperCorpusId": "238856825"
                },
                {
                    "start": 424,
                    "end": 450,
                    "matchedPaperCorpusId": "247518847"
                },
                {
                    "start": 450,
                    "end": 476,
                    "matchedPaperCorpusId": "250391006"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.477783203125
        },
        {
            "corpus_id": "269756933",
            "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
            "text": "Curriculum learning, introduced by Bengio et al. in 2009(Bengio et al., 2009), is a learning strategy where models are trained on data from simple to complex, mirroring human learning processes.This approach enhances model performance by establishing a foundational understanding before introducing more complex data.It has been widely applied in natural language processing (NLP) and computer vision.(Guo et al., 2018;Antonios Platanios et al., 2019;Tay et al., 2019).\n\nIn NLP, curriculum learning has proven to be effective by organizing the training sequence to begin with simpler texts and progressively transition to more complex ones.The complexity of these texts is assessed based on various criteria, including text length, the rarity of vocabulary, and edit distance.This structured approach has successfully demonstrated the efficacy of curriculum learning in the training of Pretrained Language Models (PLMs) (Chang et al., 2021;Nagatsuka et al., 2023).\n\nRecent advancements in large-scale language and multimodal models have highlighted the effective integration of curriculum learning with cutting-edge techniques, significantly boosting performance across various applications.Notably, projects such as WavLLM (Hu et al., 2024) and AutoWebGLM (Lai et al., 2024) showcase the potential of these integrations, tailoring the learning process to gradually introduce more complex tasks, thereby enhancing model robustness and application specificity.Alongside these innovations, further research has validated the effectiveness of curriculum learning in instruction tuning for language models.This approach not only systematically improves models' abilities to handle progressively challenging tasks but also optimizes their learning trajectory for better performance in specialized tasks.By methodically structuring the training process, these initiatives have set a new standard in the development and refinement of AI models, marking a significant step forward in their evolution.\n\nIn the development of training data for curricu-lum learning, accurately measuring data difficulty is crucial.Specifically, for LLMs (Large Language Models), determining the degree of data difficulty is challenging.Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021;Nagatsuka et al., 2023).",
            "score": 0.43745283469435003,
            "section_title": "Related Work",
            "char_start_offset": 3231,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 194,
                    "end": 317
                },
                {
                    "start": 317,
                    "end": 401
                },
                {
                    "start": 401,
                    "end": 469
                },
                {
                    "start": 471,
                    "end": 640
                },
                {
                    "start": 640,
                    "end": 776
                },
                {
                    "start": 776,
                    "end": 964
                },
                {
                    "start": 966,
                    "end": 1191
                },
                {
                    "start": 1191,
                    "end": 1459
                },
                {
                    "start": 1459,
                    "end": 1602
                },
                {
                    "start": 1602,
                    "end": 1798
                },
                {
                    "start": 1798,
                    "end": 1992
                },
                {
                    "start": 1994,
                    "end": 2104
                },
                {
                    "start": 2104,
                    "end": 2209
                },
                {
                    "start": 2209,
                    "end": 2373
                }
            ],
            "ref_mentions": [
                {
                    "start": 35,
                    "end": 56,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 56,
                    "end": 77,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 401,
                    "end": 419,
                    "matchedPaperCorpusId": "51920640"
                },
                {
                    "start": 940,
                    "end": 963,
                    "matchedPaperCorpusId": "255221201"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82275390625
        },
        {
            "corpus_id": "248887633",
            "title": "LeRaC: Learning Rate Curriculum",
            "text": "Curriculum learning was initially introduced by Bengio et al. [1] as a training strategy that helps machine learning models to generalize better when the training examples are presented in the ascending order of their difficulty. Extensive surveys on curriculum learning methods, including the most recent advancements on the topic, were conducted by Soviany et al. [2] and Wang et al. [4]. In the former survey, Soviany et al. [2] emphasized that curriculum learning is not only applied at the data level, but also with respect to the other components involved in a machine learning approach, namely at the model level, the task level and the objective (loss) level. Regardless of the component on which curriculum learning is applied, the technique has demonstrated its effectiveness on a broad range of machine learning tasks, from computer vision [1,7,[27][28][29][30][31][32][33][34] to natural language processing [1,[35][36][37][38] and audio processing [39,40]. \n\nThe main challenge for the methods that build the curriculum at the data level is measuring the difficulty of the data samples, which is required to order the samples from easy to hard. Most studies have addressed the problem with human input [41][42][43] or metrics based on domain-specific heuristics. For instance, the text length [36,[44][45][46] and the word frequency [1,38] have been employed in natural language processing. In computer vision, the samples containing fewer and larger objects have been considered to be easier in some works [32,33]. Other solutions employed difficulty estimators [47] or even the confidence level of the predictions made by the neural network [48,49] to approximate the complexity of the data samples. Other studies [27][28][29] used the error of a previously trained model to estimate the difficulty of each sample. Such solutions have shown their utility in specific application domains. Nonetheless, measuring the difficulty remains problematic when implementing standard (data-level) curriculum learning strategies, at least in some application domains. Therefore, several alternatives have emerged over time, handling the drawback and improving the conventional curriculum learning approach.",
            "score": 0.43564314479386546,
            "section_title": "Curriculum Learning",
            "char_start_offset": 6876,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 969
                },
                {
                    "start": 972,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2209
                }
            ],
            "ref_mentions": [
                {
                    "start": 366,
                    "end": 369,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 386,
                    "end": 389,
                    "matchedPaperCorpusId": "232362223"
                },
                {
                    "start": 428,
                    "end": 431,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 854,
                    "end": 856,
                    "matchedPaperCorpusId": "220301592"
                },
                {
                    "start": 856,
                    "end": 860,
                    "matchedPaperCorpusId": "266998701"
                },
                {
                    "start": 860,
                    "end": 864,
                    "matchedPaperCorpusId": "254246401"
                },
                {
                    "start": 864,
                    "end": 868,
                    "matchedPaperCorpusId": "256808576"
                },
                {
                    "start": 868,
                    "end": 872,
                    "matchedPaperCorpusId": "8105909"
                },
                {
                    "start": 872,
                    "end": 876,
                    "matchedPaperCorpusId": "51876228"
                },
                {
                    "start": 876,
                    "end": 880,
                    "matchedPaperCorpusId": "6954583"
                },
                {
                    "start": 884,
                    "end": 888,
                    "matchedPaperCorpusId": "5658192"
                },
                {
                    "start": 923,
                    "end": 927,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 927,
                    "end": 931,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 931,
                    "end": 935,
                    "matchedPaperCorpusId": "1900277"
                },
                {
                    "start": 935,
                    "end": 939,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "19805513"
                },
                {
                    "start": 965,
                    "end": 968,
                    "matchedPaperCorpusId": "11590585"
                },
                {
                    "start": 1215,
                    "end": 1219,
                    "matchedPaperCorpusId": "8502955"
                },
                {
                    "start": 1219,
                    "end": 1223,
                    "matchedPaperCorpusId": "204539326"
                },
                {
                    "start": 1223,
                    "end": 1227,
                    "matchedPaperCorpusId": "221995570"
                },
                {
                    "start": 1306,
                    "end": 1310,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 1314,
                    "end": 1318,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 1318,
                    "end": 1322,
                    "matchedPaperCorpusId": "233433844"
                },
                {
                    "start": 1349,
                    "end": 1352,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 1520,
                    "end": 1524,
                    "matchedPaperCorpusId": "6954583"
                },
                {
                    "start": 1576,
                    "end": 1580,
                    "matchedPaperCorpusId": "879067"
                },
                {
                    "start": 1656,
                    "end": 1660,
                    "matchedPaperCorpusId": "10364203"
                },
                {
                    "start": 1660,
                    "end": 1663,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 1729,
                    "end": 1733,
                    "matchedPaperCorpusId": "266998701"
                },
                {
                    "start": 1733,
                    "end": 1737,
                    "matchedPaperCorpusId": "254246401"
                },
                {
                    "start": 1737,
                    "end": 1741,
                    "matchedPaperCorpusId": "256808576"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.869140625
        },
        {
            "corpus_id": "256631050",
            "title": "Data Cartography for Low-Resource Neural Machine Translation",
            "text": "Metrics We compare the notion of sample difficulty from Data Maps against heuristic data characterizations from the literature. At each phase and for the overall Data Maps, we divide the data by regions and compute Pearson's correlation coefficient between the distance of a training example from the origin (using the variability and confidence values as a coordinate pair) and various measures of difficulty. There is no clear way to   define what makes a MT training sample difficult, however hypotheses about properties of easy vs. difficult samples emerge from prior work on data cleaning and curriculum learning. We include dual cross entropy (Junczys-Dowmunt, 2018) as a measurement of data noise, and sentence length, token frequency and word norm embeddings as sentence difficulty criteria from the curriculum learning literature (Zhang et al., 2019;Platanios et al., 2019;Liu et al., 2020). Sentence length and token frequency are computed at the word level; although we saw similar trends at the subword level. \n\nFindings We find that most metrics considered have a weak correlation with the distance from the origin according to Data Maps (Appendix Figures 4,5). Only dual cross-entropy achieves a moderate correlation, primarily in the hard regions, which is not surprising given that it is based on model scores just like Data Maps. We use a rule of thumb for cutoffs at 0.3 and 0.5 for weak and moderate correlation, as outlined by Hinkle et al. (2003). This might explain why no clear data difficulty metrics has emerged from the curriculum learning literature, where data ordering strategies that improve translation quality do not always align with meaningful intuitions about sample difficulty (Zhang et al., 2019), and raise the question of whether the difficulty captured by Data Maps is purely related to model uncertainty rather than intrinsic data uncertainty.",
            "score": 0.4310919816026011,
            "section_title": "Correlation with Data Difficulty Heuristics",
            "char_start_offset": 16360,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1021
                },
                {
                    "start": 1024,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1884
                }
            ],
            "ref_mentions": [
                {
                    "start": 839,
                    "end": 859,
                    "matchedPaperCorpusId": "155089817"
                },
                {
                    "start": 882,
                    "end": 899,
                    "matchedPaperCorpusId": "219260306"
                },
                {
                    "start": 1447,
                    "end": 1467,
                    "matchedPaperCorpusId": "121247505"
                },
                {
                    "start": 1713,
                    "end": 1733,
                    "matchedPaperCorpusId": "155089817"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5078125
        },
        {
            "corpus_id": "262064942",
            "title": "The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute",
            "text": "Language modelling is a central task in NLP where raw text is typically segmented into a sequence of words or subwords using a tokeniser, which operates based on a predefined vocabulary (Mikolov et al., 2010;Al-Rfou et al., 2019). These segmented units are commonly referred to as tokens. With this tokenised representation in place, the goal of a language model becomes the prediction of a subsequent token given its preceding sequence of tokens. This objective can be formally defined as maximising the probability of a sequence of tokens w 1 , w 2 , ..., w N : \n\nwhere p(w t |w 0 , ..., w t\u22121 ) is the probability of token w t given the sequence of previous tokens w 0 , ..., w t\u22121 . \n\nThe performance of a language model can be evaluated using the total cross-entropy loss, which for a given dataset is defined as: \n\nThe cross-entropy measures the negative log-likelihood of the observed data under the model. Lower loss indicates a better model, but comparing raw loss values can be unintuitive. Therefore, the loss is often transformed into perplexity, a more interpretable measure, defined as: \n\nwhere the cross entropy, or average loss, is equal to L N , and N is the number of tokens in the sequence. Consider a model predicting from vocabulary of M tokens that will predict p(w t |w 0 , w 1 , ..., w t\u22121 ) = 1 M for any t. Such a uniform model would have the following perplexity: \n\nThus, by exponentiating the average loss during training, perplexity can be interpreted as the effective vocabulary size of a uniform model. \n\nWhile perplexity is a standard measure in language modelling, it has limitations. The perplexity measures of two methods are only directly comparable when the same tokenisation is used. This is because its value is influenced by the granularity of tokenisation which depends on the tokenisation algorithm and vocabulary size. A larger vocabulary increases the difficulty of each individual prediction (higher loss) but may reduce the number of predictions in total (lower N ).",
            "score": 0.4296024464811785,
            "section_title": "Background: Language Modelling",
            "char_start_offset": 6592,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 563
                },
                {
                    "start": 566,
                    "end": 686
                },
                {
                    "start": 689,
                    "end": 818
                },
                {
                    "start": 821,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1100
                },
                {
                    "start": 1103,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1390
                },
                {
                    "start": 1393,
                    "end": 1533
                },
                {
                    "start": 1536,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 2012
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 208,
                    "matchedPaperCorpusId": "17048224"
                },
                {
                    "start": 208,
                    "end": 229,
                    "matchedPaperCorpusId": "52004855"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.476806640625
        },
        {
            "corpus_id": "246996522",
            "title": "Curriculum Optimization for Low-Resource Speech Recognition",
            "text": "[9] studied the dynamics of deep neural networks trained with dynamic instance hardness and showed that the model revisits harder samples more often due to higher variance in gradient values while easier examples tend to stay in the minima as soon as the minima are reached. However, [8] finds that in different empirical settings both learning harder and easy tasks first can benefit the model. \n\nCurriculum learning has been successfully used in natural language processing tasks such as language modelling [5,10] neural machine translation (NMT) [11,12,13], keyword spotting [14], and speech recognition [15,16]. Most commonly two complexity strategies are employed: model competencebased [11,12,9] and data-driven. Complexity measures for data-driven learning include sentence/utterance length [16], language model score, n-gram size [5,10], word frequency ranking, and sentence norm [13]. Speech processing systems can rely on speech to noise ratio (SNR) as a measure of difficulty by gradually blending more and more noise into clean speech signals [15,14] ditionally, we demonstrate that both an external teacher curriculum and learner's progress are important for low-resource ASR. The teacher curriculum acts as a reliable prior while the student can construct its own curriculum in an automated manner based on progress gains. We experiment with compression ratio and text-based difficulty measures to show that the signal-based prior leads to a more optimal solution. \n\nThe remainder of the paper as organized as follows: Section 2 describes the complexity measures and methods; Sections 3 and 4 discuss the experimental setting and the results; Section 5 summarizes our contributions.",
            "score": 0.4248008840882468,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2165,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 395
                },
                {
                    "start": 398,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1478
                },
                {
                    "start": 1481,
                    "end": 1696
                }
            ],
            "ref_mentions": [
                {
                    "start": 284,
                    "end": 287,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 509,
                    "end": 512,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 512,
                    "end": 515,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 549,
                    "end": 553,
                    "matchedPaperCorpusId": "20639213"
                },
                {
                    "start": 553,
                    "end": 556,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 578,
                    "end": 582,
                    "matchedPaperCorpusId": "231979234"
                },
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "14928979"
                },
                {
                    "start": 611,
                    "end": 614,
                    "matchedPaperCorpusId": "33957080"
                },
                {
                    "start": 692,
                    "end": 696,
                    "matchedPaperCorpusId": "20639213"
                },
                {
                    "start": 696,
                    "end": 699,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 699,
                    "end": 701,
                    "matchedPaperCorpusId": "227275560"
                },
                {
                    "start": 798,
                    "end": 802,
                    "matchedPaperCorpusId": "33957080"
                },
                {
                    "start": 838,
                    "end": 841,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 841,
                    "end": 844,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 1055,
                    "end": 1059,
                    "matchedPaperCorpusId": "14928979"
                },
                {
                    "start": 1059,
                    "end": 1062,
                    "matchedPaperCorpusId": "231979234"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.560546875
        },
        {
            "corpus_id": "251663102",
            "title": "Human-in-the-loop machine learning: a state of the art",
            "text": "Researchers have manually designed various difficulty measurers mainly based on the data characteristics of specific tasks and most of them designed for image in Computer Vision (CV) and text data in Natural Language Processing (NLP). Among data characteristics, complexity, diversity and noise estimation are considered. Complexity represents the structural complexity of a particular example, such that examples with higher complexity have more dimensions and are thus harder to be captured by the models (e.g. sentence length in NLP tasks). Diversity stands for the distributional diversity of a group of examples (e.g. regular or irregular shapes in CV tasks) where a larger value of diversity means the data is more diverse and is more difficult for model learning. Larger diversity sometimes also makes the data noisier. So, another characteristic to be studied is noise estimation which estimates the noise level of examples and defines cleaner data as easier. \n\nOther interesting difficulty measurers include human-annotation based Image Difficulty Scores (Soviany et al. 2020;Ionescu et al. 2016) which are proposed to measure the difficulty of an image by collecting the response times of human annotators answering \"Yes\" or \"No\" at identifying objects in images. Intuitively, longer response time corresponds to harder image examples. These measures can be considered separately but also correlated. For example, high complexity and high diversity bring more degrees of freedom to the data, which needs a model with larger capacity and bigger effort of training.",
            "score": 0.4234275792862424,
            "section_title": "Difficulty measurers",
            "char_start_offset": 75476,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 967
                },
                {
                    "start": 970,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1573
                }
            ],
            "ref_mentions": [
                {
                    "start": 1064,
                    "end": 1085,
                    "matchedPaperCorpusId": "204801162"
                },
                {
                    "start": 1085,
                    "end": 1105,
                    "matchedPaperCorpusId": "879067"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8388671875
        },
        {
            "corpus_id": "264819795",
            "title": "Ling-CL: Understanding NLP Models through Linguistic Curricula",
            "text": "Linguists devised effective approaches to determine the linguistic complexity of text data (Wolfe-Quintero et al., 1998;Bult\u00e9 and Housen, 2012;Housen et al., 2019). There is a spectrum of linguistic complexity indices for English, ranging from lexical diversity (Malvern et al., 2004;Yu, 2010) to word sophistication (O'Dell et al., 2000;Harley and King, 1989) to higher-level metrics such as readability, coherence, and information entropy (van der Sluis and van den Broek, 2010). These indices have not been fully leveraged in NLP. \n\nWe investigate the explicit incorporation of linguistic complexity of text data into the training process of NLP models, aiming to uncover the linguistic knowledge that models learn to address NLP tasks. Figure 1 shows data distribution and accuracy trend of Roberta-large (Liu et al., 2019) against the linguistic complexity index \"verb variation\" (ratio of distinct verbs). This analysis is conducted on ANLI (Nie et al., 2020)  for individual bins separately. The accuracy trend indicates that verb variation can describe the difficulty of ANLI samples to the model. In addition, the data distribution illustrates potential linguistic disparity in ANLI; see \u00a73.4 To reveal the linguistic knowledge NLP models learn during their training, we will employ known linguistic complexity indices to build multiview linguistic curricula for NLP tasks. A curriculum is a training paradigm that schedules data samples in a meaningful order for iterative training, e.g., by starting with easier samples and gradually introducing more difficult ones (Bengio et al., 2009). Effective curricula improve learning in humans (Tabibian et al., 2019;Nishimura, 2018) and machines (Bengio et al., 2009;Kumar et al., 2010;Zhou et al., 2020;Castells et al., 2020).",
            "score": 0.41946749992770826,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 533
                },
                {
                    "start": 536,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1781
                }
            ],
            "ref_mentions": [
                {
                    "start": 120,
                    "end": 143,
                    "matchedPaperCorpusId": "60511107"
                },
                {
                    "start": 143,
                    "end": 163,
                    "matchedPaperCorpusId": "150194108"
                },
                {
                    "start": 284,
                    "end": 293,
                    "matchedPaperCorpusId": "143479801"
                },
                {
                    "start": 947,
                    "end": 965,
                    "matchedPaperCorpusId": "207756753"
                },
                {
                    "start": 1577,
                    "end": 1598,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.321044921875
        },
        {
            "corpus_id": "269756933",
            "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
            "text": "Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021;Nagatsuka et al., 2023).However, these metrics may not fully reflect the complexity of a dataset.It is essential to assess data difficulty from the model's perspective, rather than relying solely on data-specific metrics.Our research proposes a new approach to calculate the degree of difficulty based on a model-centric perspective.By organizing the training dataset according to difficulty using our new metric, we aim to improve the model's performance compared to random shuffling.\n\n3 Methods for Quantitative Difficulty Measurement\n\nIn this section, we introduce a novel methodology for training models that begins with easier tasks and methodically progresses to more challenging ones.This approach requires organizing data by its level of complexity, for which we have established three principal criteria.By arranging the data in an order that goes from less to more difficult, this approach establishes a structured progression for learning.",
            "score": 0.4185848365485682,
            "section_title": "Related Work",
            "char_start_offset": 5440,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 164,
                    "end": 237
                },
                {
                    "start": 237,
                    "end": 361
                },
                {
                    "start": 361,
                    "end": 473
                },
                {
                    "start": 473,
                    "end": 625
                },
                {
                    "start": 627,
                    "end": 676
                },
                {
                    "start": 678,
                    "end": 831
                },
                {
                    "start": 831,
                    "end": 953
                },
                {
                    "start": 953,
                    "end": 1090
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 163,
                    "matchedPaperCorpusId": "255221201"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88037109375
        },
        {
            "corpus_id": "263816220",
            "title": "A Multi-Faceted Exploration Incorporating Question Difficulty in Knowledge Tracing for English Proficiency Assessment",
            "text": "The question difficulty that we focus on is an essential factor to model the human learning process, not only within KT tasks but also to the extent that other downstream tasks that solely predict the question difficulty exist [26][27][28]. Utilizing the difficulty levels associated with given questions and knowledge concepts plays a crucial role in tracking students' knowledge states. This is because questions in the assessment cannot be of the same difficulty level, and the proper question distribution based on the question difficulty is important during test design [29,30]. Furthermore, without consideration of difficulty during the learning process, KT-based application tasks may prove challenging. For instance, in the student learning curricula, receiving only excessively challenging problems relative to students' level can diminish the desire to achieve, whereas consistently encountering overly simple problems might lead to reduced interest [31,32]. \n\nAmong KT studies, there have been attempts to modify the model with the difficulty factor or deliberately exploit it as a training feature. For example, [6] designed a model structure for computing user knowledge acquisition, including the difficulty. In detail, the subjective difficulty that students perceive in the question is measured before the actual evaluation and used for initialization. After the evaluation, the student's knowledge states are updated based on the newly obtained difficulty level. Lee et al. [7] adopted a question replacement strategy according to the difficulty in terms of data augmentation. It is assumed that if a learner cannot solve a particular question, he or she will not be able to solve a more difficult question, and conversely, if they can solve it, they will be able to solve a more manageable problem. \n\nHowever, the existing studies either build a complex model structure only for considering the difficulty factor or use difficulty only as a secondary factor, which hinders application to pre-trained language models (PLMs) with general-purpose yet powerful performance. It is necessary to explore how to dissolve desired features properly, i.e., difficulty factors based on a general-purpose model such as the already existing Transformers structure. Therefore, this study focuses on exploring methods enabling self-attention-based models to leverage the difficulty factor effectively.",
            "score": 0.41394129050561246,
            "section_title": "Question Difficulty in Knowledge Tracing",
            "char_start_offset": 11945,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 969
                },
                {
                    "start": 972,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1817
                },
                {
                    "start": 1820,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2269
                },
                {
                    "start": 2270,
                    "end": 2404
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 231,
                    "matchedPaperCorpusId": "210695360"
                },
                {
                    "start": 231,
                    "end": 235,
                    "matchedPaperCorpusId": "227122854"
                },
                {
                    "start": 235,
                    "end": 239,
                    "matchedPaperCorpusId": "251711316"
                },
                {
                    "start": 575,
                    "end": 579,
                    "matchedPaperCorpusId": "155099535"
                },
                {
                    "start": 579,
                    "end": 582,
                    "matchedPaperCorpusId": "250625293"
                },
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "67827436"
                },
                {
                    "start": 965,
                    "end": 968,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1125,
                    "end": 1128,
                    "matchedPaperCorpusId": "250340202"
                },
                {
                    "start": 1492,
                    "end": 1495,
                    "matchedPaperCorpusId": "248367482"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16943359375
        },
        {
            "corpus_id": "247694098",
            "title": "Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU",
            "text": "Curriculum Learning was initially mentioned in the work of Elman (1993) who demonstrated the importance of feeding neural networks with small/easy inputs at the early stages of training. The concept was later formalised by Bengio et al. (2009) where training in an easy-to-hard ordering was shown to result in faster convergence and improved performance. In general, Curriculum Learning requires a difficulty metric (also known as the scoring function) used to rank training instances, and a scheduler (known as the pacing function) that decides when and how new examples-of different difficulty-should be introduced to the model. Example Difficulty was initially expressed via model loss, in self-paced learning (Kumar et al., 2010;Jiang et al., 2015), increasing the contribution of harder training instances over time. This setting posed a challenge due to the fast-changing pace of the loss during training, thus later approaches used human-intuitive difficulty metrics, such as sentence length or the existence of rare words (Platanios et al., 2019) to pre-compute difficulties of training instances. However, as such metrics do not express difficulty of the model, modelbased metrics have been proposed over the years, such as measuring the loss difference between two checkpoints (Xu et al., 2020b) or model translation variability (Wang et al., 2019b;Wan et al., 2020). In our curricula we use training dynamics to measure example difficulty, i.e. metrics that consider difficulty from the perspective of a model towards a certain task. Example difficulty can be also estimated either in a static (offline) or dynamic (online) manner, where in the latter training instances are evaluated and re-ordered at certain times during training, while in the former the difficulty of each example remains the same throughout. In our experiments we adopt the first setting and consider static example difficulties. \n\nTransfer Teacher CL is a particular family of such approaches that use an external model (namely the teacher) to measure the difficulty of training examples. Notable works incorporate a simpler model as the teacher (Zhang et al., 2018) or a larger-sized model (Hacohen and Weinshall, 2019), as well as using similar-sized learners trained on different subsets of the training data.",
            "score": 0.4127265053412392,
            "section_title": "Related Work",
            "char_start_offset": 3983,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1912
                },
                {
                    "start": 1915,
                    "end": 2072
                },
                {
                    "start": 2073,
                    "end": 2296
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 71,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 223,
                    "end": 243,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 713,
                    "end": 733,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 733,
                    "end": 752,
                    "matchedPaperCorpusId": "10891229"
                },
                {
                    "start": 1030,
                    "end": 1054,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 1287,
                    "end": 1305,
                    "matchedPaperCorpusId": "227227757"
                },
                {
                    "start": 1339,
                    "end": 1359,
                    "matchedPaperCorpusId": "202539965"
                },
                {
                    "start": 1359,
                    "end": 1376,
                    "matchedPaperCorpusId": "222272176"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53076171875
        },
        {
            "corpus_id": "259047693",
            "title": "Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review",
            "text": "Thus, for example, in a 3-gram (n = 3) model: \n\nThe probability of a word W i given its preceding word W i\u22121 can be estimated: \n\nwhere count(W i\u22121 , W i ) is the number of times the 3-gram (W i\u22121 , W i ) appears in the training corpus, and count(W i\u22121 ) is the number of times the word W i\u22121 appears in the training corpus. The models have achieved great success in recent years and have been a driving force behind recent advancements in NLP. The performance of the technique depends on the quality of the language model and the ability of the model to accurately reflect the patterns and structures of the target data. Therefore, much research effort has been devoted to improving the quality of language models for these tasks, including developing better training algorithms, larger training corpora, and better evaluation metrics. \n\nA representative corpus of repetitive and highly predictable programs is utilized to capture regularities within the corpus in order to evaluate the naturalness of software language models. By estimating the language model from this representative corpus, it can predict the contents of new programs with high confidence, thereby minimizing the surprise associated with the new program. In NLP, this idea is often measured using perplexity or cross-entropy (log-transformed version). Given a program p = {w 1 , w 2 , . . . , w n }, of length n, and a language model \u0398, it assumes that the probability of the programs estimated by the model is p \u0398 , and, thus, the cross-entropy H \u0398 (p) can be measured: \n\nand a formulation can be derived from Equation ( 2): \n\nThe entropy rate of a language model is utilized to assess the naturalness of the generated text [68]. It can be computed by taking the negative logarithm of the probability of each generated token. An effective model should have low entropy for the majority of programs, assigning higher probabilities (i.e., values closer to 1) to most words in the program, thereby resulting in lower absolute log values. In practice, this involves using techniques such as maximum likelihood estimation or neural networks to estimate the parameters. The final model can then be used to make predictions by calculating the probability of a given sequence of words.",
            "score": 0.40902242304054304,
            "section_title": "Measurement of Language Models with Entropy",
            "char_start_offset": 13437,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 48,
                    "end": 126
                },
                {
                    "start": 129,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 835
                },
                {
                    "start": 838,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1540
                },
                {
                    "start": 1543,
                    "end": 1595
                },
                {
                    "start": 1598,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2134
                },
                {
                    "start": 2135,
                    "end": 2248
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36279296875
        },
        {
            "corpus_id": "273811618",
            "title": "Does the Definition of Difficulty Matter? Scoring Functions and their Role for Curriculum Learning",
            "text": "Related literature offers several approaches to estimate the difficulty of each sample in the training set, primarily discussed in the context of CL. These can be conceptually split into two different categories: human-and model-based difficulty. Humanbased difficulty definitions fall under two main paradigms: \n\nHuman priors One straightforward strategy is to base the estimation on human priors of difficulty understanding, which are easy to define and implement. Such attempts may include the length of a sentence for natural language processing (NLP) tasks [19], [20], [21], or the signal-tonoise-ratio (SNR) in a signal [22], [23] for CA [4]. Despite their intuitive definitions, these difficulty estimations are often limited by the tendency to disregard the complexity of the data structures, inter-class relationships, and the fact that human-and model-based difficulty perceptions may differ. Labelling efforts A more direct approach incorporating a human understanding of difficulty into the SD estimation involves considering the human labelling behaviour for individual samples. This approach is particularly advantageous for datasets lacking ground truth labels due to the subjectivity of the task, such as in emotion recognition [24]. Here, gold standards are often formed for each sample based on a summary of the varying ratings assigned by different human annotators [25]. The interannotator agreement can then be interpreted as a measure of difficulty, with harder samples expected to result in greater variation among ratings [13]. Alternatively, humans can perform an explicit (direct or indirect) rating of the SD. As reported in [8], the minimum viewing time (MVT) necessary for a human to categorise an image can be interpreted as a good proxy for difficulty. In regression tasks, the targets -in terms of their distance to the mean of the value range -can be considered an estimate for difficulty [14]. \n\nThe concrete examples of SFs, however, that we primarily focus on in this work are model-based. These can be employed in the context of DNN-based classification tasks on any data type without requiring additional human annotations beyond the target labels.",
            "score": 0.40407604388718593,
            "section_title": "A. Scoring Functions",
            "char_start_offset": 5175,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 311
                },
                {
                    "start": 314,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1927
                },
                {
                    "start": 1930,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2186
                }
            ],
            "ref_mentions": [
                {
                    "start": 574,
                    "end": 578,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 626,
                    "end": 630,
                    "matchedPaperCorpusId": "14928979"
                },
                {
                    "start": 632,
                    "end": 636,
                    "matchedPaperCorpusId": "19805513"
                },
                {
                    "start": 1385,
                    "end": 1389,
                    "matchedPaperCorpusId": "198953418"
                },
                {
                    "start": 1546,
                    "end": 1550,
                    "matchedPaperCorpusId": "44092463"
                },
                {
                    "start": 1652,
                    "end": 1655,
                    "matchedPaperCorpusId": "266565244"
                },
                {
                    "start": 1922,
                    "end": 1926,
                    "matchedPaperCorpusId": "231684057"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37353515625
        },
        {
            "corpus_id": "259370648",
            "title": "HuCurl: Human-induced Curriculum Discovery",
            "text": "(Erhan et al., 2010;Wu et al., 2021), which limits the curriculum space that can be examined for finding effective curricula.\n\nThis paper advocates a re-imagining of CL paradigms by introducing and formalizing the task of curriculum discovery, which aims to find effective curricula for a given model and dataset over a curriculum space. The present work specifically focuses on determining when and in which difficulty order text data samples should be learned for effective training of NLP systems. We propose a framework that employs prior knowledge about sample difficulty, such as entropy in human annotations, to inform an effective and flexible sample weighting scheme for curriculum discovery. The framework is capable of discovering optimal curricula (within the space of its weight functions) for any given model and dataset by optimizing the weight functions and adjusting the difficulty group of data samples as training progresses. The discovered curricula provide useful insights about datasets and models, such as the relative importance of different groups of samples for models or knowledge dependency among samples. We illustrate that the proposed framework has the potential to encompass some of the existing CL approaches.\n\nExperimental results show that (a): the topperforming discovered curricula for the same model and dataset can be fundamentally dissimilar in their training strategies, indicating that effective training can be achieved in multiple ways; (b): the discovered curricula are often non-monotonic and greatly differ from the known strategies reported in existing literature, indicating that existing curricula, including easy-to-hard transition curricula, are at the risk of underperforming; and (c): the curricula discovered on small datasets and models perform exceptionally well on larger datasets and models respectively, illustrating the transferability of the discovered curricula. The paper presents a new curriculum learning approach that unlike existing approaches can discover multiple high-performing (and often diverse) curricula for each given NLP model and dataset, provide interpretable curricula in terms of sample difficulty, and encompass some of the existing curriculum learning approaches. 1",
            "score": 0.4018965706009834,
            "section_title": "Introduction",
            "char_start_offset": 1796,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 36,
                    "matchedPaperCorpusId": "227343966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70361328125
        },
        {
            "corpus_id": "261076116",
            "title": "Curriculum Learning with Adam: The Devil Is in the Wrong Details",
            "text": "cross-entropy-loss (2) as difficulty measures and employ the discrete schedule functions shown in Figure 6a. Ahead of training, we order the training data according to either their sequence length or the losses obtained by a RoBERTa BASE model that we finetuned on the respective task. The curriculum randomly samples from an incrementally larger portion of the ordered dataset. We determined the hyperparameters of the schedule functions by conducting grid-search, determining the best-performing setup on a subset of the validation data. We then finetune RoBERTa BASE (Liu et al. 2019) with both, an optimal and a slightly suboptimal learning rate, on the MRPC-task from the GLUE-dataset (Wang et al. 2018). \n\nTable 1 reports results for the hand-crafted curricula. If the learning rate is low, both of our improvised curricula let RoBERTa learn much faster compared to training without curriculum (as shown by the performance after i = 750 steps). However, as soon as we increase the learning rate to an optimal level vanilla Adam outperforms all other conditions. Analogously to our experiments with commentaries, we find the size of the parameter updates |\u2206\u03b8| to be increased during the time of the largest change in data distribution (Figure 6b). The gain in |\u2206\u03b8| for hand-crafted curricula is not as prolonged as for commentaries. This makes sense, as the shift in training distribution is especially large at the beginning of training, while in later steps the relative change is neglectable. Despite gains in |\u2206\u03b8| being relatively small and early in training, we observe that they are crucial for the performance gains of the curricula: If we eliminate the interaction with Adam by setting \u03b2 1 = \u03b2 2 the advantage of this simple curriculum vanishes (see Figure 12c in Appendix F). \n\nConclusions In summary, we find that interactions between curriculum structure and Adam can also occur in hand-crafted curricula. This is the case if the difficulty measures are correlated with the gradient norms that they produce (e.g. if long sequences produce small gradients and short sequences produce large gradients).",
            "score": 0.3972141312055675,
            "section_title": "Experiments",
            "char_start_offset": 21591,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 709
                },
                {
                    "start": 712,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1789
                },
                {
                    "start": 1792,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2116
                }
            ],
            "ref_mentions": [
                {
                    "start": 570,
                    "end": 586,
                    "matchedPaperCorpusId": "16384032"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.373046875
        },
        {
            "corpus_id": "268696658",
            "title": "Symmetric Self-Paced Learning for Domain Generalization",
            "text": "Current methods evaluate example difficulty through predefined metrics, such as sentence length, or dynamically update it based on training loss, such as cross-entropy loss. However, predefined difficulty measures do not integrate the model's feedback, and training loss, focusing only on the difference between predictions and ground truth, raises an inaccurate difficulty measurement issue when different examples yield identical training losses. \n\nTo address these limitations, we propose the Gradientbased Difficulty Measure (GDM), which evaluates example difficulty through dynamic measurement of the gradient magnitude with respect to the example itself. Unlike training loss, the gradient avoids inaccurate difficulty assessment by taking input features into account. As a result, even if examples yield the same loss, their gradients can differ. Additionally, loss landscapes can encompass plateaus or saddle points, where training loss remains relatively stable even with substantial shifts in model parameters. In these scenarios, utilizing training loss for evaluating example difficulty can be misleading. Conversely, the gradient provides finergrained insights into changes in model parameters, making the gradient magnitude a more informative approach for evaluating difficulty. The GDM is computed with Equation 8, where \u03be x denotes the difficulty of the example x. ColorJitter. An Empirical Risk Minimization (ERM) baseline is also included, which merges data from all source domains without utilizing domain generalization techniques. Evaluation Metrics. We adopt the leave-one-out-test evaluation strategy as the evaluation metric following the prior works (Li et al. 2017b;Carlucci et al. 2019;Li et al. 2019;Zhou et al. 2022). Specifically, we select one domain as the test domain at a time and use the remaining domains as the source domains for training. We report the accuracy for each separate domain. Performance measures are reported as top-1 classification accuracy (%) averaged over ten runs, along with their corresponding 95% confidence intervals. \n\nNetwork Structure. The network structure is chosen by following the previous work (Carlucci et al. 2019;Li et al. 2019;Zhou et al. 2020). In the Digits dataset, images are resized to 32 \u00d7 32 and converted to RGB by replicating channels. The backbone of the neural network is constructed by 3 \u00d7 3 Conv layers (64 kernels), each followed by a",
            "score": 0.39611444828693737,
            "section_title": "Gradient-based Difficulty Measure",
            "char_start_offset": 9632,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 448
                },
                {
                    "start": 451,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2077
                },
                {
                    "start": 2080,
                    "end": 2098
                },
                {
                    "start": 2099,
                    "end": 2217
                },
                {
                    "start": 2218,
                    "end": 2316
                },
                {
                    "start": 2317,
                    "end": 2420
                }
            ],
            "ref_mentions": [
                {
                    "start": 1675,
                    "end": 1692,
                    "matchedPaperCorpusId": "6037691"
                },
                {
                    "start": 1692,
                    "end": 1713,
                    "matchedPaperCorpusId": "81978372"
                },
                {
                    "start": 1713,
                    "end": 1728,
                    "matchedPaperCorpusId": "59553457"
                },
                {
                    "start": 2162,
                    "end": 2184,
                    "matchedPaperCorpusId": "81978372"
                },
                {
                    "start": 2184,
                    "end": 2199,
                    "matchedPaperCorpusId": "59553457"
                },
                {
                    "start": 2199,
                    "end": 2215,
                    "matchedPaperCorpusId": "212718063"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7685546875
        },
        {
            "corpus_id": "264819795",
            "title": "Ling-CL: Understanding NLP Models through Linguistic Curricula",
            "text": "We propose a new approach to linguistic curriculum learning. Our approach estimates the importance of multiple linguistic indices and aggregates them, provides effective difficulty estimates through correlation and optimization methods, and introduces novel curricula for using difficulty estimates, to uncover the underlying linguistic knowledge that NLP models learn during training. Furthermore, we present a method for a more accurate and fair evaluation of computational models for NLP tasks according to linguistic indices. Furthermore, the estimated importance factors present insights about each dataset and NLP task, the linguistic challenges contained within each task, and the factors that most contribute to model performance on the task. Further analysis of such learning dynamics for each NLP task will shed light on the linguistic capabilities of computational models at different stages of their training. \n\nOur framework and the corresponding tools serve as a guide for assessing linguistic complexity for various NLP tasks and uncover the learning dynamics of the corresponding NLP models during training. While we conducted our analysis on seven tasks and extracted insights on the key indices for each task, NLP researchers have the flexibility to either build on our results or apply our approach to other NLP tasks to extract relevant insights. Promising areas for future work include investigations on deriving optimal linguistic curriculum tailored for each NLP task; examining and enhancing linguistic capabilities of different computational models, particularly with respect to linguistically complex inputs; and developing challenge datasets that carry a fair distribution of linguistically complex examples for various NLP tasks. In addition, future work could study why specific indices are important, how they connect to the linguistic challenges of each task, and how different linguistic indices jointly contribute to learning a target task. We expect other aggregation functions, such as log-average, exponential-average, and probabilistic selection of maximum, to be effective approaches for difficulty estimation based on validation loss. Finally, other variations of the proposed Gaussian curriculum could be investigated for model improvement.",
            "score": 0.39426608062173396,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 26287,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 921
                },
                {
                    "start": 924,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2173
                },
                {
                    "start": 2174,
                    "end": 2280
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52587890625
        },
        {
            "corpus_id": "277667729",
            "title": "2D-Curri-DPO: Two-Dimensional Curriculum Learning for Direct Preference Optimization",
            "text": "Curriculum Learning (CL) inherently relies on ordering data by difficulty [31], yet defining appropriate difficulty metrics remains crucial, especially for complex inputs like natural language prompts. While directly quantifying prompt difficulty for LLM alignment is relatively underexplored, insights can be drawn from methods measuring sample difficulty in broader machine learning contexts. These methods often inform adaptive training strategies. For instance, Zhu et al. [32] provided a formal definition of sample learning difficulty inspired by the bias-variance trade-off in statistical learning theory, proposing a variance-based measure reflecting learning stability. Their work suggests that samples leading to higher variance in model predictions or gradients across training perturbations might be considered more difficult. Other common heuristics for estimating sample difficulty include using training loss magnitude [33], gradient norms [34], or predictive uncertainty [35]. However, these often rely on the state of a single model during training or simple input features. \n\nParticularly inspired by the general idea in [32] of leveraging variance as a proxy for difficulty, our work develops a metric specifically for prompt complexity within the LLM alignment setting. Our approach measures the perplexity variance of responses generated by a single reference model for the input prompt. This prompt difficulty metric provides a solid foundation for our 2D curriculum.",
            "score": 0.3908534054751986,
            "section_title": "C. Quantifying Sample Difficulty for Curriculum Learning",
            "char_start_offset": 8042,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1091
                },
                {
                    "start": 1094,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1489
                }
            ],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 78,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 477,
                    "end": 481,
                    "matchedPaperCorpusId": "248810776"
                },
                {
                    "start": 934,
                    "end": 938,
                    "matchedPaperCorpusId": "244958599"
                },
                {
                    "start": 987,
                    "end": 991,
                    "matchedPaperCorpusId": "252369193"
                },
                {
                    "start": 1139,
                    "end": 1143,
                    "matchedPaperCorpusId": "248810776"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6494140625
        },
        {
            "corpus_id": "256460262",
            "title": "Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems",
            "text": "Wang et al. [57] pointed out that it is difficult to find the best combination of difficulty scorer and training scheduler for a specific task except for performing an exhaustive search, which is often impossible to do for a complex problem. Thus, we did not perform an exhaustive search. However, we investigated various options of difficulty scorers and selected the best-performed ones. Particularly, we use two different types of difficulty measurers, namely predefined and automatic difficulty measurers. Predefined difficulty measurers are designed based on human prior knowledge with no data-driven models or algorithms involved, while automatic difficulty measures are learned by data-driven models or algorithms. Predefined difficulty measurers have been proven to be effective in various tasks and have been popularly used in multiple domains due to their simplicity [57] (also see Section 3.2). However, automatic difficulty measurers require less expert knowledge and can interact with current models to adapt to new data.",
            "score": 0.3904190460662787,
            "section_title": "Difficulty Measurer",
            "char_start_offset": 16268,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 16,
                    "matchedPaperCorpusId": "232362223"
                },
                {
                    "start": 877,
                    "end": 881,
                    "matchedPaperCorpusId": "232362223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55224609375
        },
        {
            "corpus_id": "254366532",
            "title": "DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing",
            "text": "These techniques improve data and training efficiency, but existing solutions have several limitations: \u2022 Techniques like curriculum learning (CL) improve data efficiency by indexing and sampling training data based on certain difficulty metric (Bengio et al. 2009), and it has recently proved effective on large-scale pretraining tasks (Li, Zhang, and He 2022). However, implementing different CL strategies for different user tasks can require a lot of code-refactoring, which is time-consuming and error-prone. In addition, existing implementations have less consideration on scalability, which makes it difficult to analyze and index large-scale training data based on different difficulty metrics. \u2022 Existing data routing techniques such as token drop/bypass/pruning were mostly designed for inference and inapplicable to training. TokenBypass (Hou et al. 2022), to our knowledge the only data routing technique for foundation model pretraining, skips the compute of part of the input tokens at some middle layers during BERT pretraining, reducing pretraining cost while maintaining model quality. However, it requires several special implementations that may only work for the tested BERT pretraining case, such as the importance score-based token dropping decisions and the whitelist for special tokens. This could limit the possibility and benefit of applying it to other cases. In the NLP area, curriculum learning has been applied on small-scale one-stage tasks and downstream finetuning tasks, such as neural machine translation (NMT) (Kocmi and Bojar 2017;Bojar et al. 2017;Zhang et al. 2018;Platanios et al. 2019;Zhang et al. 2019) and natural language understanding (NLU) (Sachan andXing 2016, 2018;Tay et al. 2019;Xu et al. 2020). There are also a few works that explore curriculum learning for language model pretraining (Press, Smith, and Lewis 2020; Zhang et al. 2021;Campos 2021;Li, Zhang, and He 2022). However, one common limitation among existing works is that there does not exist a scalable and customizable curriculum learning library, making it difficult to analyze large-scale data and explore custom difficulty metrics/pacing functions.",
            "score": 0.39012472518113245,
            "section_title": "Introduction",
            "char_start_offset": 1863,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2164
                }
            ],
            "ref_mentions": [
                {
                    "start": 245,
                    "end": 265,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 849,
                    "end": 866,
                    "matchedPaperCorpusId": "247628024"
                },
                {
                    "start": 1568,
                    "end": 1586,
                    "matchedPaperCorpusId": "9460719"
                },
                {
                    "start": 1604,
                    "end": 1626,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 1626,
                    "end": 1643,
                    "matchedPaperCorpusId": "155089817"
                },
                {
                    "start": 1686,
                    "end": 1697,
                    "matchedPaperCorpusId": "16503693"
                },
                {
                    "start": 1697,
                    "end": 1713,
                    "matchedPaperCorpusId": "16503693"
                },
                {
                    "start": 1713,
                    "end": 1729,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 1729,
                    "end": 1744,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 1868,
                    "end": 1886,
                    "matchedPaperCorpusId": "233433844"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57861328125
        },
        {
            "corpus_id": "246822409",
            "title": "Development and Comparison of Scoring Functions in Curriculum Learning",
            "text": "Training starts with the entire dataset and gradually decreases. Subsets are chosen probabilistically based on their current DIH values. In 11 image classification datasets, DIHCL outperformed the classical random stochastic gradient descent (SGD) and recent CL methods. \n\nIn NLP, difficulty determination can be made based on corpus characteristics, as well as using a different model. Sentence lengths can be used as difficulty values [8]. It can be argued that long sentences are more difficult because they usually consist of short sentences. The second difficulty determination approach is Word Rarity. According to this method, the sentences in which the words rarely used in the corpus are stated as more difficult. The results obtained with these strategies drastically cut the training time. They were more successful than existing methods in NLU tasks. \n\nAnother method used to determine the difficulty values of sentences is The Sentence Entropy: N-gram difficulty method [4]. With the assumption, if a rare word is discovered early in the training, the learned word embeddings will have a high variance and will most likely have a poor representation. It is reported that although CL may provide a benefit to small datasets, this benefit is lost in large datasets.",
            "score": 0.38704866882156796,
            "section_title": "Literature Review",
            "char_start_offset": 5893,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 270
                },
                {
                    "start": 273,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1276
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54931640625
        },
        {
            "corpus_id": "270514386",
            "title": "Metaphor Detection with Context Enhancement and Curriculum Learning",
            "text": "In CLCL (Zhou et al., 2023), curriculum learning is introduced and contrastive loss is used as the difficulty evaluation methods.However, because the model is trained according to the contrastive target and classification target, the impact of only considering contrastive loss is one-sided and cannot fully reflect the difficulty of a sentence.At the same time, because the method of manual evaluation of difficulty relies too much on expert knowledge, and even if the method is logically valid, it is not necessarily applicable to the machine.Certain sentences may be seen as \"easy\" or \"difficult\" for a human, but not necessarily the same for a machine.Therefore, instead of manual evaluation, we chose to adopt a framework that automatically measures the difficulty of the sentence.Specifically, we use the training loss of a pre-trained model as the measure of sentence difficulty.As a classification task, we use cross-entropy loss to measure the sentence difficulty:\n\nWhere M is the pre-trained model and d M (Y i ) is the difficulty of Y i .N is the number of training examples, y i and \u0177i represent the true label and predicted label distribution of the i th example, respectively, and w yi is the class weight, which is used to alleviate the data imbalance problem.\n\nAlgorithm 1: MiceCL   4) TroFi (Birke and Sarkar, 2006).The statistics of the datasets are shown in Table 1, which was summarized by Zhang and Liu (2022).",
            "score": 0.38704866882156796,
            "section_title": "Curriculum Learning",
            "char_start_offset": 14391,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 129,
                    "end": 345
                },
                {
                    "start": 345,
                    "end": 545
                },
                {
                    "start": 545,
                    "end": 656
                },
                {
                    "start": 656,
                    "end": 786
                },
                {
                    "start": 786,
                    "end": 886
                },
                {
                    "start": 886,
                    "end": 973
                },
                {
                    "start": 975,
                    "end": 1049
                },
                {
                    "start": 1049,
                    "end": 1275
                },
                {
                    "start": 1277,
                    "end": 1333
                },
                {
                    "start": 1333,
                    "end": 1431
                }
            ],
            "ref_mentions": [
                {
                    "start": 8,
                    "end": 27,
                    "matchedPaperCorpusId": "259370543"
                },
                {
                    "start": 1308,
                    "end": 1332,
                    "matchedPaperCorpusId": "11890804"
                },
                {
                    "start": 1410,
                    "end": 1430,
                    "matchedPaperCorpusId": "252819292"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3662109375
        },
        {
            "corpus_id": "259370648",
            "title": "HuCurl: Human-induced Curriculum Discovery",
            "text": "Annotation information has been extensively used by previous research in NLP to devise strategies for further data collection (Yang et al., 2019;Dligach et al., 2010), model improvement and annotation analysis (Zaidan and Eisner, 2008;Paun et al., 2018), pruning and weighting samples for better learning (Yang et al., 2019), or efficient use of monetary funds (Dligach et al., 2010). Recent studies show consistent positive correlation between difficulty of samples to the model and their level of human agreement (Nie et al., 2020a;Zaidan and Eisner, 2008;Yang et al., 2019). Building on these findings, we aim to utilize such prior knowledge about sample difficulty to develop a curriculum learning (CL) framework that is capable of discovering effective curricula for NLP tasks.\n\nA curriculum is a planned sequence of learning materials and an effective one can improve training of NLP systems (Settles and Meeder, 2016;Amiri et al., 2017;Zhang et al., 2019;Lalor and Yu, 2020;Kreutzer et al., 2021;Agrawal and Carpuat, 2022;Maharana and Bansal, 2022). CL seeks to improve model generalizability by ordering samples for training based on their latent difficulty (Bengio et al., 2009). Recent work reported efficiency and effectiveness gains through CL (Jiang et al., 2018;Castells et al., 2020;Zhou et al., 2020), especially in cases of harder tasks and limited or noisy data (Wu et al., 2021).\n\nExisting CL approaches are designed to learn a single curriculum that works best for a given model and dataset. However, effective training could be achieved in multiple ways. In addition, existing approaches quantify sample difficulty through model behavior during training. Although efficient and effective, model behavior can be affected by initialization and training dynamics (Erhan et al., 2010;Wu et al., 2021), which limits the curriculum space that can be examined for finding effective curricula.\n\nThis paper advocates a re-imagining of CL paradigms by introducing and formalizing the task of curriculum discovery, which aims to find effective curricula for",
            "score": 0.385825756717741,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 126,
                    "end": 145,
                    "matchedPaperCorpusId": "159040584"
                },
                {
                    "start": 210,
                    "end": 235,
                    "matchedPaperCorpusId": "635609"
                },
                {
                    "start": 235,
                    "end": 253,
                    "matchedPaperCorpusId": "58535743"
                },
                {
                    "start": 305,
                    "end": 324,
                    "matchedPaperCorpusId": "159040584"
                },
                {
                    "start": 515,
                    "end": 534,
                    "matchedPaperCorpusId": "222177851"
                },
                {
                    "start": 534,
                    "end": 558,
                    "matchedPaperCorpusId": "635609"
                },
                {
                    "start": 558,
                    "end": 576,
                    "matchedPaperCorpusId": "159040584"
                },
                {
                    "start": 898,
                    "end": 924,
                    "matchedPaperCorpusId": "14155276"
                },
                {
                    "start": 943,
                    "end": 962,
                    "matchedPaperCorpusId": "155089817"
                },
                {
                    "start": 962,
                    "end": 981,
                    "matchedPaperCorpusId": "226226711"
                },
                {
                    "start": 981,
                    "end": 1003,
                    "matchedPaperCorpusId": "238856825"
                },
                {
                    "start": 1029,
                    "end": 1055,
                    "matchedPaperCorpusId": "250391006"
                },
                {
                    "start": 1256,
                    "end": 1276,
                    "matchedPaperCorpusId": "51876228"
                },
                {
                    "start": 1298,
                    "end": 1316,
                    "matchedPaperCorpusId": "227275560"
                },
                {
                    "start": 1380,
                    "end": 1397,
                    "matchedPaperCorpusId": "227343966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5166015625
        },
        {
            "corpus_id": "259593617",
            "title": "On the Effectiveness of Curriculum Learning in Educational Text Scoring",
            "text": "Automatic. Though pre-defined strategies have been demonstrated effective in various application scenarios, they are often plagued by their strong reliance on human expertise to define an appropriate difficulty measurer and an extensive search for effective combinations of difficulty measurer and training scheduler. Therefore, in addition to the four pre-defined CL strategies described above, we further designed an automatic difficulty measurer to dynamically select data samples based on instance-wise training loss and enable a more flexible training process. Specifically, the automatic difficulty measurer used in this study characterizes data samples as either easy and difficult, which represents the samples whose ground truth scores are correctly or incorrectly predicted by a model in a training epoch. Let p t easy and p t dif f icult denote the probabilities of an individual easy sample and an individual difficult sample to be selected for model training at the current t-th epoch, we define r as the ratio between these two probabilities: \n\nBy choosing different values for r, we can enable the strategy to lay different emphasis on the easy and difficult samples. In particular, we explored two different ways to determine the value for r and consequently two variants of the automatic strategy: \n\n\u2022 Static, which sets r to the same value across different training epochs. During experiments, r was empirically determined by searching in the range of (0, 5] with an interval of 0.1. When r < 1 ( i.e., p t dif f icult < p t easy ), easy samples will be more likely to be selected for training; when r > 1, difficult samples will be more likely to be selected for training. \n\n\u2022 Adaptive, the value of r at the current t-th epoch is based on the number of easy and difficult samples in the previous epoch. We denote the set of easy and difficult samples as E and D, respectively, and formally define: \n\nNote that such a setting of r ensures that when there are relatively a larger portion of easy (or difficult) samples, the strategy tends to select difficult (or easy) samples more often for the subsequent training.",
            "score": 0.3828208195931197,
            "section_title": "Curriculum Learning Design",
            "char_start_offset": 18939,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 11,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 1056
                },
                {
                    "start": 1059,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1314
                },
                {
                    "start": 1317,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1691
                },
                {
                    "start": 1694,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 1917
                },
                {
                    "start": 1920,
                    "end": 2134
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.623046875
        },
        {
            "corpus_id": "271051051",
            "title": "CBM: Curriculum by Masking",
            "text": "Curriculum learning is a training technique introduced by Bengio et al. [3], which provides the training examples in a meaningful order, from easy to hard, to neural networks.The objective is to enhance the performance of neural models, while also improving the convergence speed of the training process.Since its introduction, curriculum learning has proven its effectiveness in various domains, such as computer vision [3,7,10,19,25,43,44,45], natural language processing [3,10,30,36,39,47], and signal processing [1,10,40].The method has been very successful and has undergone extensive development, as illustrated in some recent surveys [46,52].These developments range from strategies for measuring data difficulty [3,23,30,38,43,45,48,54,60] to methods focusing on other aspects of the training process [5,10,6,28,44].A wellknown method to apply curriculum learning is by defining a metric that evaluates the complexity of the data, and subsequently arranging the training examples from the simplest to the most challenging ones based on the respective metric.Researchers have made significant strides in finding improved metrics for various domains and tasks.For instance, images containing fewer and larger objects in computer vision are deemed easier than other images [43,45].In natural language processing, word frequency [3,36] and sequence length [8,30,48,60] are utilized to assess the sample difficulty.In some cases, researchers have also integrated human feedback into their metric design [26,38,54].\n\nThe aforementioned curriculum strategies have proven to be effective.However, they have been found to lack practicality due to their reliance on human expert input [26], which may not always be available.Moreover, these methods remain fixed during training and may not adapt the curriculum to the changing needs of the models.As a result, the research community developed new curriculum-based approaches to overcome these limitations.For in-stance, Kumar et al. [32] introduced self-paced learning, a method that measures the difficulty of the training samples based on the performance of the trained model.",
            "score": 0.3820994630724619,
            "section_title": "Related Work",
            "char_start_offset": 4940,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 175,
                    "end": 304
                },
                {
                    "start": 304,
                    "end": 526
                },
                {
                    "start": 526,
                    "end": 649
                },
                {
                    "start": 649,
                    "end": 824
                },
                {
                    "start": 824,
                    "end": 1066
                },
                {
                    "start": 1066,
                    "end": 1166
                },
                {
                    "start": 1166,
                    "end": 1286
                },
                {
                    "start": 1286,
                    "end": 1418
                },
                {
                    "start": 1418,
                    "end": 1517
                },
                {
                    "start": 1519,
                    "end": 1588
                },
                {
                    "start": 1588,
                    "end": 1723
                },
                {
                    "start": 1723,
                    "end": 1845
                },
                {
                    "start": 1845,
                    "end": 1953
                },
                {
                    "start": 1953,
                    "end": 2126
                }
            ],
            "ref_mentions": [
                {
                    "start": 72,
                    "end": 75,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 421,
                    "end": 424,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 424,
                    "end": 426,
                    "matchedPaperCorpusId": "5658192"
                },
                {
                    "start": 429,
                    "end": 432,
                    "matchedPaperCorpusId": "8105909"
                },
                {
                    "start": 432,
                    "end": 435,
                    "matchedPaperCorpusId": "51876228"
                },
                {
                    "start": 435,
                    "end": 438,
                    "matchedPaperCorpusId": "6954583"
                },
                {
                    "start": 438,
                    "end": 441,
                    "matchedPaperCorpusId": "220301592"
                },
                {
                    "start": 441,
                    "end": 444,
                    "matchedPaperCorpusId": "208138033"
                },
                {
                    "start": 474,
                    "end": 477,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 480,
                    "end": 483,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 483,
                    "end": 486,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 486,
                    "end": 489,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 489,
                    "end": 492,
                    "matchedPaperCorpusId": "1900277"
                },
                {
                    "start": 516,
                    "end": 519,
                    "matchedPaperCorpusId": "11590585"
                },
                {
                    "start": 522,
                    "end": 525,
                    "matchedPaperCorpusId": "19805513"
                },
                {
                    "start": 641,
                    "end": 645,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 645,
                    "end": 648,
                    "matchedPaperCorpusId": "232362223"
                },
                {
                    "start": 720,
                    "end": 723,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 723,
                    "end": 726,
                    "matchedPaperCorpusId": "879067"
                },
                {
                    "start": 726,
                    "end": 729,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 729,
                    "end": 732,
                    "matchedPaperCorpusId": "8502955"
                },
                {
                    "start": 732,
                    "end": 735,
                    "matchedPaperCorpusId": "6954583"
                },
                {
                    "start": 735,
                    "end": 738,
                    "matchedPaperCorpusId": "208138033"
                },
                {
                    "start": 738,
                    "end": 741,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 741,
                    "end": 744,
                    "matchedPaperCorpusId": "221995570"
                },
                {
                    "start": 744,
                    "end": 747,
                    "matchedPaperCorpusId": "233433844"
                },
                {
                    "start": 809,
                    "end": 812,
                    "matchedPaperCorpusId": "231986287"
                },
                {
                    "start": 815,
                    "end": 817,
                    "matchedPaperCorpusId": "190000064"
                },
                {
                    "start": 817,
                    "end": 820,
                    "matchedPaperCorpusId": "3568073"
                },
                {
                    "start": 820,
                    "end": 823,
                    "matchedPaperCorpusId": "220301592"
                },
                {
                    "start": 1278,
                    "end": 1282,
                    "matchedPaperCorpusId": "6954583"
                },
                {
                    "start": 1282,
                    "end": 1285,
                    "matchedPaperCorpusId": "208138033"
                },
                {
                    "start": 1333,
                    "end": 1336,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1336,
                    "end": 1339,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 1363,
                    "end": 1366,
                    "matchedPaperCorpusId": "26468344"
                },
                {
                    "start": 1366,
                    "end": 1369,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 1369,
                    "end": 1372,
                    "matchedPaperCorpusId": "233433844"
                },
                {
                    "start": 1506,
                    "end": 1510,
                    "matchedPaperCorpusId": "204539326"
                },
                {
                    "start": 1510,
                    "end": 1513,
                    "matchedPaperCorpusId": "8502955"
                },
                {
                    "start": 1513,
                    "end": 1516,
                    "matchedPaperCorpusId": "221995570"
                },
                {
                    "start": 1683,
                    "end": 1687,
                    "matchedPaperCorpusId": "204539326"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69384765625
        },
        {
            "corpus_id": "264303095",
            "title": "Classification of mathematical test questions using machine learning on datasets of learning management system questions",
            "text": "The scope of machine learning can be broken down into three main sub-fields, namely supervised learning, unsupervised learning, and reinforcement learning, each of which plays an important role in the field; therefore, when learning the basics of artificial intelligence, each should be thoroughly explored in equal measure. For our analysis, we focused on using the supervised learning approach. In supervised learning, a trained model is used to learn multiple features that can be classified into multiple classes or labels from the observed data; then, a new label value is predicted based on the prediction value that most accurately matches the actual target variable. \n\nIn most recent research, machine learning approaches have been used to solve multi-classification problems; these approaches have shown excellent performance in various domains. In a study by Petersen and Ostendorf [15], the difficulty level on a reading proficiency test for elementary school students was evaluated through a machine-learning approach. After experimenting with and fitting the data using various language models (i.e., lexical-only, skip-gram, and non-syntactic approaches), each language model was compared with the baseline SVM model. The comparison of model performances revealed that the best performance was shown when all the language models, instead of just a single SVM model, were used [15]. Sangodiah, Ahmad, and Ahmad [16] also conducted a study on automatically classifying question difficulty levels by applying the text mining technique of term-frequency-inverse document frequency (tf-idf) [17] and the SVM model. Additionally, Alammary [18] established an effective evaluation system called LOsMonitor, which uses a combination of machine learning and text mining to classify the cognitive level of a learner's evaluation questions and learning results. Although the supervised learning method has been used in many previous studies, there are few studies on the classification of mathematical questions according to their difficulty level. In this study, we intensively analyzed feature extraction and classified question difficulty levels through various machine learning techniques.",
            "score": 0.3807543604174508,
            "section_title": "Supervised learning",
            "char_start_offset": 8024,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 674
                },
                {
                    "start": 677,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 2051
                },
                {
                    "start": 2052,
                    "end": 2196
                }
            ],
            "ref_mentions": [
                {
                    "start": 892,
                    "end": 896,
                    "matchedPaperCorpusId": "719765"
                },
                {
                    "start": 1390,
                    "end": 1394,
                    "matchedPaperCorpusId": "719765"
                },
                {
                    "start": 1424,
                    "end": 1428,
                    "matchedPaperCorpusId": "18797335"
                },
                {
                    "start": 1600,
                    "end": 1604,
                    "matchedPaperCorpusId": "2996187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1708984375
        },
        {
            "corpus_id": "264451551",
            "title": "Data Optimization in Deep Learning: A Survey",
            "text": "These three levels can be used together to more comprehensively perceive the training data [67]. \n\n2) Perception on different types: The eight quantifying types are introduced as follows: \n\n\u2022 Distribution. This type aims to quantify the true data distribution for a learning task and the training data distribution. An effective quantification of these two distributions is significantly beneficial for training. Nevertheless, it is nearly impossible to obtain a clear picture of them. Therefore, the true distribution is usually assumed to conform to several some basic assumptions, such as Gaussian distribution for each category [39]. For the training data distribution, some studies [68], [69] apply clustering to deduce the intrinsic structure of the training data. These studies concern the global distribution of a category. Recently, researchers have investigated local distributions of training samples. One typical characteristic is about the neighborhood of each training sample. In deep learning on graphs, the distribution of neighborhood samples with heterogeneous labels negatively impacts the training or prediction for the sample. Wang et al. [70] defined a label difference index to quantify the difference between a node and its neighborhood in a graph as follows: \n\nwhere p xi and p Ni are the category distributions of x i and its neighborhood N i . \u2022 Cleanliness. This type aims to identify the degree of noise in each training sample. This study primarily focuses on label noise, as it garners more attention than sample noise. There are numerous metrics for noise measurement. \n\nAs illustrated in Fig. 7, typical measures include lossbased, gradient-based, uncertainty-based, margin-based, and multi-training-based techniques. Samples with large losses, gradient norms, or uncertainties are more likely to be noisy. In margin-based measures, a small margin indicates a high probability of being noise. Huang et al. [60] conducted multiple training procedures to identify noisy labels. \u2022 Difficulty. This type aims to infer the degree of learning difficulty for a training sample or a category. The accurate measurement of learning difficulty for each training sample is of great importance because several deep learning paradigms employ adaptive learning strategies based on the level of learning difficulty.",
            "score": 0.3784898635162564,
            "section_title": "A. Data perception",
            "char_start_offset": 29010,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 99,
                    "end": 187
                },
                {
                    "start": 190,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1283
                },
                {
                    "start": 1286,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1600
                },
                {
                    "start": 1603,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2117
                },
                {
                    "start": 2118,
                    "end": 2332
                }
            ],
            "ref_mentions": [
                {
                    "start": 687,
                    "end": 691,
                    "matchedPaperCorpusId": "250699224"
                },
                {
                    "start": 693,
                    "end": 697,
                    "matchedPaperCorpusId": "261049552"
                },
                {
                    "start": 1939,
                    "end": 1943,
                    "matchedPaperCorpusId": "207995300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46142578125
        },
        {
            "corpus_id": "259370648",
            "title": "HuCurl: Human-induced Curriculum Discovery",
            "text": "We introduce an effective curriculum learning framework that employs prior knowledge about sample difficulty in its training paradigm for curriculum discovery. The proposed framework initially partitions its input data into several groups of increasing difficulty, defines parameterized func-  Table 2. tions to weight sample losses in each difficulty group, moves samples across difficulty groups based on their learning progress, and enables tuning the parameters of the weight function to discover novel curricula. We demonstrate that this framework is capable of representing several categories of curriculum learning approaches. The task of curriculum discovery alleviates the limitations imposed by selecting a single curriculum strategy, and instead, focuses on finding and analyzing different curricula that work equally-well for a given model and dataset. In addition, the discovered curricula provide insight into how different portions of the dataset contribute toward learning at different stages of training a model, which, in turn, provide knowledge about the learning dynamics of different models. The task of curriculum discovery could be costly on large datasets, in particular, when the goal is to find optimal curricula for different models and datasets. To mitigate the computational : Confidence assignment to samples in our datasets by three CL approaches. The x-axis is the epoch number, and y-axis is the average weight assigned to samples of each difficulty group. Blue (solid) is easy, orange (dashed) is medium, and green (dash-dot) is hard. The shaded area is the 95% CI over the datasets with five random seeds each. The curves are monotonic for most parts, and can be approximated by our framework.\n\ncost, we show that it is possible to rapidly discover a curriculum on a small subset of the dataset (or a smaller version of the model with significantly less number of parameters) and apply the resulting curriculum to the full dataset.\n\nThere are several promising areas for future work. These include approaches for learning new difficulty indicators from data (e.g., linguistic difficulty including lexical, syntactic and semantic difficulty), prioritizing medium level instances and those with greatest progress during training, and developing challenge datasets that contain diverse data samples with different levels of difficulty. Finally, investigating diverse curricula that are suitable for general use and across datasets through curriculum discovery and generalization is a promising area for research.",
            "score": 0.37848916312898717,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 27435,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67626953125
        },
        {
            "corpus_id": "250626773",
            "title": "Angular Gap: Reducing the Uncertainty of Image Difficulty through Model Calibration",
            "text": "Ascertaining example difficulty is a critical problem to curriculum learning and self-paced learning, in that curricula rank training samples by difficulty and proceed from easy to hard. In the context of image classification, a natural idea is to quantify such difficulty with human selection frequency [34], i.e., the fraction of annotators selecting a sample for its target class. However, human labelling effort is not scalable to get fine-grained image difficulty. To measure the difficulty of 10,000 CIFAR10 images, CIFAR10-H [2] recruits 2570 annotators to perform 511,400 trials, with an average of 51 human decisions per image, not including a considerable amount of practice and attention checks. Hence, automating difficulty estimation is crucial to applying curriculum learning to large scale datasets. Probabilistic models are particularly compelling for this automatic estimation demand because of their consistency towards noisy image contents and uncertain labels regularly presented in large scale datasets. Early works have characterized image difficulty with maximum confidence or classification margin, the difference between the predicting probability of the correct class and the largest among others. However, difficulty measurers based on modern neural networks have a reputation of being poorly calibrated. Extensive research have shown that the negative log-likelihood can easily overfit training samples, pushing average predicting probability away from accuracy [13,21]. This suggests considerable uncertainty of softmax probabilities, and imprecise difficulty measurement undermines the performance of curriculum learning. While probability estimation deteriorates, final classification results actually improve [13]. Very recently, deep ensemble methods [1,18] measure example difficulty with agreement either from last layers' predictions or from intermediate layers' predictions. Reducing estimation uncertainty with ensembling requires selected treatments and controls such as architectures, number of submodels, and number of data splits. In this work, we show faithful image difficulty can be efficiently estimated by deep metric learning. Hyperspherical learning [26], a weakly supervised learning framework, groups instances of the same concept together and pushes instances of different concepts apart by enforcing angular discrimination during training. The framework allows for more robust similarity estimation and has improved representation learning in both computer vision [7] and natural language understanding [8].",
            "score": 0.37335457867510946,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2072
                },
                {
                    "start": 2073,
                    "end": 2174
                },
                {
                    "start": 2175,
                    "end": 2392
                },
                {
                    "start": 2393,
                    "end": 2560
                }
            ],
            "ref_mentions": [
                {
                    "start": 304,
                    "end": 308,
                    "matchedPaperCorpusId": "67855879"
                },
                {
                    "start": 532,
                    "end": 535,
                    "matchedPaperCorpusId": "225098279"
                },
                {
                    "start": 1490,
                    "end": 1494,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1494,
                    "end": 1497,
                    "matchedPaperCorpusId": "202773833"
                },
                {
                    "start": 1741,
                    "end": 1745,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1784,
                    "end": 1787,
                    "matchedPaperCorpusId": "235458180"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6455078125
        },
        {
            "corpus_id": "264819795",
            "title": "Ling-CL: Understanding NLP Models through Linguistic Curricula",
            "text": "The curriculum learning approaches in \u00a72.2 use difficulty scores or compute confidence to quantify sample difficulty in order to rank sentences. We use as difficulty scores: aggregate linguistic complexity Ling, see Section 2.1.3, and Loss (Xu et al., 2020;Wu et al., 2021;Zhou et al., 2020). We take the loss from a proxy model (No-CL in \u00a73.3) by recording all samples losses two times per epoch during training and computing the sample-wise average.",
            "score": 0.3721991141151649,
            "section_title": "Difficulty Scoring Functions",
            "char_start_offset": 14856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 451
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 257,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 257,
                    "end": 273,
                    "matchedPaperCorpusId": "227343966"
                },
                {
                    "start": 273,
                    "end": 291,
                    "matchedPaperCorpusId": "227275560"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2138671875
        },
        {
            "corpus_id": "256460262",
            "title": "Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems",
            "text": "WADI, and BATADAL are sufficient to support our hypothesis that the diversity is related to the attack labels. Advantage of the automatic difficulty measurers. We successfully adapted CL to time series data by introducing the contextualized difficulty measurer. Most existing CL approaches use context-free difficulty measurers, implying that the difficulty score of each sample is assigned based on its generic characteristics such as diversity, noise level, and intensity. These context-free difficulty measurers are sufficient for classification tasks in the natural language processing and image recognition domains, while the classification of time series data requires consecutive data for reserving chronological characteristics [27,35]. In this paper, we take advantage of DTM and propose two automatic difficulty measurers: CEM and HDM. CEM and HDM successfully incorporate context information into the difficulty scores of each sample. The ablation study in Section 7.2 shows the improvement brought by these two contextualized difficulty measurers. Figure 6 shows a significant performance drop between LATTICE and LATTICE-CEM-HDM. Advantage of CL's optimization principle. Another reason for the improvement is the optimization principle of CL. Bengio et al. [6] pointed out that CL can be seen as an optimization strategy for non-convex functions. Such a strategy first optimizes a smoother version of the problem to reveal the global picture and then gradually considers less smoothing versions until the target objective of interest is reached. In our case, the introduction of CL potentially prevents our method from getting stuck at a local optimum. As we can observe in the example (Table 1), the first attack starts at 10:29:14 after 29 minutes of normal operation. Consequently, we acquire far more normal data than anomaly data, inducing machine learning methods to a local optimum. CL, however, alleviates this problem by re-ordering batches based on difficulty scores. Particularly, difficult samples are fed into our model gradually, presenting a smoother optimization problem. CL enables LATTICE to learn the global picture instead of being stuck at a local optimum while speeding up the whole training process.",
            "score": 0.37189185153803367,
            "section_title": "OVERALL DISCUSSION",
            "char_start_offset": 61438,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 736,
                    "end": 740,
                    "matchedPaperCorpusId": "140295406"
                },
                {
                    "start": 740,
                    "end": 743,
                    "matchedPaperCorpusId": "214713827"
                },
                {
                    "start": 1271,
                    "end": 1274,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5341796875
        },
        {
            "corpus_id": "269741199",
            "title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation",
            "text": "We propose introducing uncertainty to both the query selection and the RL fine-tuning steps to address these challenges. Moreover, to mitigate the hallucination problem, trustworthy language models should refrain from answering questions when the answer is unknown, which can occur due to ambiguity in the question or the questioner's intent. Cole et al., 2023 proposed that using sampling-based confidence scores to quantify repetition in model outputs is crucial for addressing hallucination issues and helps improve accuracy, especially for ambiguous questions. \n\nCurriculum Learning It is observed that deep learning model training can benefit from the implementation of Curriculum Learning (CL), i.e., using data samples sorted based on a curriculum versus training on the randomly shuffled data (Soviany et al., 2022). Recently, CL methods have been developed and deployed for the LMs as well, at pre-training and post-training stages using a variety of linguistically motivated curricula such as sentence length or term frequency complexity measure based ranking (Liu et al., 2018;Zhang et al., 2021;Campos, 2021;Weber et al., 2023). \n\nStudies regarding the deployment of CL at the pre-training stage of LMs are focused on reducing the pre-training computational cost and the instability of the auto-regressive training emerging when increasing the models' size, batch size, sequence length, and learning rate. Li et al., 2021 implemented a CL at the pre-training of LMs using the sequence length as the difficulty metric with the curriculum of starting from the shorter sequence training data toward the longer sequence. They demonstrated that CL behaves as a regularization method and reduces the gradient variance, therefore enabling training auto-regressive models with much larger batch sizes and learning rates without training instability (for example, training GPT-2 models with 8x larger batch size and 4x larger learning rate). Ranaldi et al., 2023 proposed a new complexity measure based on the length, rarity, and comprehensibility of the samples and sorted the corpus according to the proposed complexity measure during the pre-training stage and showed that their CL approach led to better performance in downstream tasks. \n\nWang et al., 2022 used the frequency of words as the complexity metric for the curriculum-based pre-training of LMs.",
            "score": 0.37065344764685015,
            "section_title": "I. Related Work",
            "char_start_offset": 80668,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 564
                },
                {
                    "start": 567,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1140
                },
                {
                    "start": 1143,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2243
                },
                {
                    "start": 2246,
                    "end": 2362
                }
            ],
            "ref_mentions": [
                {
                    "start": 343,
                    "end": 360,
                    "matchedPaperCorpusId": "258866001"
                },
                {
                    "start": 801,
                    "end": 823,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 1070,
                    "end": 1088,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 1088,
                    "end": 1107,
                    "matchedPaperCorpusId": "243766208"
                },
                {
                    "start": 1945,
                    "end": 1965,
                    "matchedPaperCorpusId": "265068175"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6982421875
        },
        {
            "corpus_id": "266741778",
            "title": "Answering from Sure to Uncertain: Uncertainty-Aware Curriculum Learning for Video Question Answering",
            "text": "Curriculum learning (CL) [18] emulates the human learning process by starting with easier tasks and gradually progressing to more challenging ones. Two central components of CL are the difficulty measure and the training scheduler. In the case of self-paced CL (SPL, where difficulty is measured during training), the loss function is often used as the difficulty measure [19]- [22]. Initially, during training, samples with higher losses are excluded from optimization. As training advances, the threshold is gradually increased to incorporate more complex data into the optimization process. However, relying solely on loss might not accurately represent the inherent difficulty of data, as difficulty is an intrinsic attribute of samples and should be independent of ground truth labels. To overcome this limitation, we propose employing uncertainty as the difficulty measure for SPL. To the best of our knowledge, [44] is the only work that also uses uncertainty for CL. However, our method is essentially different from it: 1) We derive uncertainty by probabilistic modeling, while it obtains data uncertainty by a pretrained language model (predefined CL); 2) We perform CL by re-weighting the data, while it adopts baby step [18] to arrange data; 3) We focus on VideoQA, while it addresses neural machine translation. The pretrained model and the training scheduler based on baby step make [44] more complex to implement than ours.",
            "score": 0.3685763966626367,
            "section_title": "B. Curriculum Learning",
            "char_start_offset": 6196,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1438
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 29,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 372,
                    "end": 376,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 378,
                    "end": 382,
                    "matchedPaperCorpusId": "94785766"
                },
                {
                    "start": 1232,
                    "end": 1236,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7392578125
        },
        {
            "corpus_id": "256460262",
            "title": "Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems",
            "text": "Despite the simplicity and effectiveness of predefined difficulty measurers, they have some essential limitations [57]. First, a predefined difficulty measurer remains unchanged during runtime, i.e., being unable to adapt to new data generated from CPS in operation. Second, a predefined difficulty measurer requires a good grasp of domain knowledge, which can be quite expensive and time-consuming in practice. Last but not least, the definition of difficulty for humans and machines can be quite different; what humans assume to be easy can be quite difficult for machines to comprehend. This discrepancy of decision boundaries between humans and machines causes challenges for experts to define difficulty scores manually.\n\nTo alleviate these problems, various automatic difficulty measurers have been developed and explored in the literature, including self-paced learning [31], transfer teacher [69], reinforcement learning teacher [19], and other automatic difficulty measurers [24,46,54]. Inspired by these methods, we modify difficulty scores automatically with prediction errors, which are critical indicators of CPS uncertainty. Substantial work has been conducted in the literature, demonstrating the importance of handling uncertainties in CPS security and safety [21,22,37,63,67,68]. In our context, we focus on prediction errors of DTM, which is pretrained on historical data. DTM in ATTAIN simulates corresponding CPS with high realism. Therefore, higher prediction errors of DTM indicate higher noise levels of labels produced by DTM. As mentioned in Section 5.1.1, training deep learning models with noisy data is more difficult. Therefore, we assume samples with higher prediction errors should be assigned with higher difficulty scores. Based on this assumption, we define the following two types of automatic difficulty measurers: Hamming Distance-based Measurer (HDM, Definition 5.1) and Cross Entropy-based Measurer (CEM, Definition 5.2). Hamming distance is commonly used to calculate the difference of two strings of equal length, while cross-entropy loss estimates uncertainty by comparing real distribution and prediction. We are aware that there are other distance-based and entropy-based metrics, we, however, argue that hamming distance and cross-entropy are commonly used and representative. In the future, we will explore other options.\n\nDefinition 5",
            "score": 0.3685763966626367,
            "section_title": "Automatic Difficulty Measurer.",
            "char_start_offset": 22667,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 118,
                    "matchedPaperCorpusId": "232362223"
                },
                {
                    "start": 877,
                    "end": 881,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 937,
                    "end": 941,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 984,
                    "end": 988,
                    "matchedPaperCorpusId": "51876228"
                },
                {
                    "start": 988,
                    "end": 991,
                    "matchedPaperCorpusId": "35187967"
                },
                {
                    "start": 1280,
                    "end": 1283,
                    "matchedPaperCorpusId": "253421545"
                },
                {
                    "start": 1283,
                    "end": 1286,
                    "matchedPaperCorpusId": "84185986"
                },
                {
                    "start": 1286,
                    "end": 1289,
                    "matchedPaperCorpusId": "253421605"
                },
                {
                    "start": 1289,
                    "end": 1292,
                    "matchedPaperCorpusId": "36524392"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.658203125
        },
        {
            "corpus_id": "259370648",
            "title": "HuCurl: Human-induced Curriculum Discovery",
            "text": "We introduce the problem of curriculum discovery and describe a curriculum learning framework capable of discovering effective curricula in a curriculum space based on prior knowledge about sample difficulty. Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as apposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy transition curricula are often at the risk of underperforming, and (iii): the curricula discovered for smaller datasets and models perform well on larger datasets and models respectively. The proposed framework encompasses some of the existing curriculum learning approaches and can discover curricula that outperform them across several NLP tasks.",
            "score": 0.36721490268732726,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7119140625
        },
        {
            "corpus_id": "265351926",
            "title": "Complexity-Guided Curriculum Learning for Text Graphs",
            "text": "In Curriculum learning (CL) (Bengio et al., 2009) data samples are scheduled in a meaningful difficulty order, typically from easy to hard, for iterative training. In graph machine learning, Wang et al. (2021) introduced CurGraph, a curriculum learning method designed for sub-graph classification. This X X X X X X X X  2019) proposed a dynamic curriculum approach that automatically assigns confidence scores to samples based on their estimated difficulty. However this model requires additional trainable parameters. To address this limitation, Castells et al. (2020) introduced the SuperLoss framework to calculate optimal confidence scores for each instance using a closed-form solution. In (Vakil and Amiri, 2022), we extended SuperLoss to incorporate trend information at the sample level. We utilized loss trajectories to estimate the emerging difficulty of subgraphs and employed weighted sample losses for data scheduling in order to create effective curricula for training GNNs and understanding their learning dynamics. \n\nCurrent curriculum learning methodologies in NLP rely on data properties, e.g., sentence length, word rarity, or syntactic features (Platanios et al., 2019;Liu et al., 2021), or annotation disagreement (Elgaar and Amiri, 2023); as well as model properties such as training loss and its variations (Graves et al., 2017;Amiri et al., 2017) to sequence data samples for training. Elgaar and Amiri (2023) developed a curriculum discovery framework based on prior knowledge of sample difficulty, utilized annotation entropy and loss values. They concluded that curricula based on easy-to-hard or hard-to-easy transition are often at the risk of under-performing, effective curricula are often non-monotonic, and curricula learned from smaller datasets perform well on larger datasets. \n\nOther instances of curriculum learning for textual data have primarily centered on machine translation and language comprehension. For instance, Agrawal and Carpuat (2022) introduced a framework for training non-autoregressive sequence-tosequence models for text editing.",
            "score": 0.3649650428261123,
            "section_title": "Related Work",
            "char_start_offset": 21966,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 1031
                },
                {
                    "start": 1034,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1813
                },
                {
                    "start": 1816,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2087
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 49,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 191,
                    "end": 209,
                    "matchedPaperCorpusId": "235324744"
                },
                {
                    "start": 548,
                    "end": 570,
                    "matchedPaperCorpusId": "227275456"
                },
                {
                    "start": 696,
                    "end": 719,
                    "matchedPaperCorpusId": "248863268"
                },
                {
                    "start": 1166,
                    "end": 1190,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 1190,
                    "end": 1207,
                    "matchedPaperCorpusId": "236460239"
                },
                {
                    "start": 1236,
                    "end": 1260,
                    "matchedPaperCorpusId": "259370648"
                },
                {
                    "start": 1331,
                    "end": 1352,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 1352,
                    "end": 1371,
                    "matchedPaperCorpusId": "1916665"
                },
                {
                    "start": 1411,
                    "end": 1434,
                    "matchedPaperCorpusId": "259370648"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63427734375
        },
        {
            "corpus_id": "277780682",
            "title": "Estimating Item Difficulty Using Large Language Models and Tree-Based Machine Learning Algorithms",
            "text": "This aligns with recent innovative uses of LLMs in educational measurement, where models are treated as \"artificial students.\" For example, Maeda (2025) fine-tuned a large transformer model to behave as examinees of varying ability levels, generating synthetic responses to estimate item difficulties in lieu of human field-testing. \n\nSuch studies exploring the feasibility of LLM-driven item analysis highlight the capacity of these models to engage with assessment content in human-like ways. \n\nGiven these developments, it is natural to ask whether LLMs like GPT can directly predict item difficulty with useful accuracy. Several recent works have started to apply transformer models to item difficulty estimation. For example, Zhou and Tao (2020) trained a BERT-based model to classify programming questions by difficulty and achieved about 67% accuracy above prior baseline models. Overall, the state of the art has rapidly advanced: transformer models pre-trained on large corpora, when appropriately fine-tuned or adapted, now typically outperform models using only handcrafted features (AlKhuzaey et al., 2024). However, most such studies still rely on supervised learning with difficulty labels from student data. An open question is whether a single-shot approach with LLMs (i.e. harnessing the model's own internal knowledge without task-specific training) could yield reasonable difficulty estimates. \n\nGPT models, especially in their instructible forms (e.g. ChatGPT), can be prompted in natural language to evaluate or rate a piece of text. This raises an intriguing possibility: could we simply ask an LLM to judge how difficult a test item would be for students? If feasible, this direct approach might provide a fast and cost-effective way to get difficulty estimates, complementing or even expediting the traditional calibration process.",
            "score": 0.3643539988308766,
            "section_title": "Learning Algorithms",
            "char_start_offset": 4501,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 332
                },
                {
                    "start": 335,
                    "end": 494
                },
                {
                    "start": 497,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1412
                },
                {
                    "start": 1415,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1855
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 152,
                    "matchedPaperCorpusId": "273128550"
                },
                {
                    "start": 1094,
                    "end": 1118,
                    "matchedPaperCorpusId": "261625264"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.294677734375
        },
        {
            "corpus_id": "260063181",
            "title": "Generative Data Augmentation for Aspect Sentiment Quad Prediction",
            "text": "In addition to the existence of low-quality data, another problem we observe is that more than half of the generated texts are simple expressions. These generated texts are far simpler than most texts in the ASTE dataset. A text can be divided into three different parts, aspect terms, opinion terms, and context. Even if being given different quads as inputs, the Q2T model usually generates text with relatively similar context, such as 'The at is ot'. When most augmented training data are too simple, the model may not learn the complex patterns required to make accurate predictions on unseen data. Therefore, it is necessary to balance the distribution of the sample difficulty of the augmented dataset. \n\nTo assess the sample difficulty, we propose a new measurement factor, called the Average Context Inverse Document Frequency (AC-IDF). The difficulty of a text can be defined as the level of language proficiency required to understand the text (Fulcher, 1997). A text that uses many uncommon words is considered more difficult than one that uses simple and common language. Therefore, one way to measure the difficulty of a text is to calculate the average IDF score of the words in the text. Furthermore, because aspects and opinions are directly copied from the input of the model, it is critical to evaluate the difficulty of the context part of the text. Therefore, we propose using the context difficulty to measure the learning difficulty of the sample for our model. Specifically, given a text collection X from the dataset, we remove all aspects and opinions terms to obtain only the context words. We denote the preprocessed text collection by X. Then, for each text xi after preprocessing, we calculate the AC-IDF i of the text as follows: \n\nwhere t ij is the j-th word in xi , n i is the number of words in xi , | X| denotes the size of X, and |{x \u2208 X : t ij \u2208 x}| represents the total number of texts where t ij appears. \n\nWe build a subset according to the AC-IDF of the generated texts so that the difficulty of the selected data follows a uniform distribution. Specifically, we set several intervals according to the sample difficulty and randomly sample similar amounts of data from the entire augmented dataset for each interval.",
            "score": 0.3639105881940031,
            "section_title": "Difficulty Balancing",
            "char_start_offset": 13784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 709
                },
                {
                    "start": 712,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1760
                },
                {
                    "start": 1763,
                    "end": 1943
                },
                {
                    "start": 1946,
                    "end": 2086
                },
                {
                    "start": 2087,
                    "end": 2257
                }
            ],
            "ref_mentions": [
                {
                    "start": 955,
                    "end": 970,
                    "matchedPaperCorpusId": "62554706"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30224609375
        },
        {
            "corpus_id": "265506572",
            "title": "Can training neural language models on a curriculum with developmentally plausible data improve alignment with human reading behavior?",
            "text": "Curriculum learning (Bengio et al., 2009) refers to training models through a difficulty-based ordering of training examples (i.e. a curriculum), most often \"starting small\" (Elman, 1993) from easy examples before progressing to increasingly difficult sentences. In NLP, curriculum learning has been widely used for Machine Translation (e.g., Platanios et al., 2019), but has also been applied more recently to other natural language understanding tasks (Xu et al., 2020). For a survey see Soviany et al. (2022); Wang et al. (2021). \n\nThere are two steps involved in designing a curriculum: assigning a difficulty score to every training example (\"difficulty measurer\") and using these difficulty scores to determine the order in which training examples are presented to the model (\"training scheduler\") (Wang et al., 2021).",
            "score": 0.3624959134650269,
            "section_title": "Background",
            "char_start_offset": 3761,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 532
                },
                {
                    "start": 535,
                    "end": 824
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 41,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 454,
                    "end": 471,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 490,
                    "end": 511,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 513,
                    "end": 531,
                    "matchedPaperCorpusId": "232362223"
                },
                {
                    "start": 804,
                    "end": 823,
                    "matchedPaperCorpusId": "232362223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55859375
        },
        {
            "corpus_id": "250391006",
            "title": "On Curriculum Learning for Commonsense Reasoning",
            "text": "The initial data order imposed by the teacher model is updated at regular intervals during training by taking the learner model's current state into account (Kong et al., 2021). Importantly, we propose to reverse the ranking to a difficulty-to-easy curriculum in ACL, in order to reinforce feedback from the hardto-learn data points, which has been shown to be beneficial for generalization (Swayamdipta et al., 2020). In order to measure difficulty, we explore three different data-sample informativeness scoring methods i.e. Question Answering Probability (QAP) (Zhang and Bansal, 2019), Energy-based Out-of-Distribution Score (Liu et al., 2020) and Cartography-based Variability (Swayamdipta et al., 2020). Our work is most related to Xu et al. (2020) which splits the training data into N meta-datasets, trains N models for computing the curriculum and follows a heuristically designed training regimen. In contrast, we train a single model for computing the curriculum and use Bayesian optimization (Snoek et al., 2012) to find the best pacing of the curriculum for the target dataset, which is more effective than Xu et al. (2020) as we show in Sec. 5.4, besides being computationally efficient. \n\nWe analyze these methods on five commonsense reasoning datasets dealing with various tasks such as reasoning about social interactions (SocialIQA; Sap et al. (2019)), reading comprehension (Cos-mosQA; Huang et al. ( 2019)), natural language inference (HellaSwag; Zellers et al. (2019)), pronoun resolution (WinoGrande; Sakaguchi et al. (2020)) and adversarial commonsense (CODAH; Chen et al. (2019)). We explore curriculum learning in full-model finetuning as parameter-efficient tuning and show significant improvements using curriculum learning on each of these datasets. We also demonstrate that curriculum learning prevents the learner model from over-fitting on the training set, which leads to improved generalization to in-domain and out-of-domain data.",
            "score": 0.3621854388488994,
            "section_title": "Introduction",
            "char_start_offset": 2157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1201
                },
                {
                    "start": 1204,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1964
                }
            ],
            "ref_mentions": [
                {
                    "start": 391,
                    "end": 417,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 564,
                    "end": 588,
                    "matchedPaperCorpusId": "202572810"
                },
                {
                    "start": 629,
                    "end": 647,
                    "matchedPaperCorpusId": "222208700"
                },
                {
                    "start": 682,
                    "end": 708,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 738,
                    "end": 754,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 1004,
                    "end": 1024,
                    "matchedPaperCorpusId": "632197"
                },
                {
                    "start": 1120,
                    "end": 1136,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 1351,
                    "end": 1368,
                    "matchedPaperCorpusId": "128296356"
                },
                {
                    "start": 1467,
                    "end": 1488,
                    "matchedPaperCorpusId": "159041722"
                },
                {
                    "start": 1523,
                    "end": 1546,
                    "matchedPaperCorpusId": "199370376"
                },
                {
                    "start": 1584,
                    "end": 1602,
                    "matchedPaperCorpusId": "104292422"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50537109375
        },
        {
            "corpus_id": "270068172",
            "title": "Symmetric Reinforcement Learning Loss for Robust Learning on Diverse Tasks and Model Scales",
            "text": "We briefly introduce robust loss functions studied in the presence of noise in supervised learning classification tasks.Ghosh et al. [2017] prove that when a noisy dataset is present, the mean absolute error (MAE) learning speed is slower than that of the cross entropy loss (CE), but the model is learned more robustly.Zhang and Sabuncu [2018] propose a generalized cross entropy loss L q , L q becomes CE when q \u2192 0, and becomes MAE when q \u2192 1.By adjusting this parameter 0 \u2264 q \u2264 1, robust learning is achieved in noisy datasets.The symmetric cross entropy (SCE) [Wang et al., 2019] we mainly refer to uses a symmetric cross entropy loss, which not only considers the flow of information from the true distribution, but also includes information in the reverse direction.SCE works better than GCE in general, especially for data with high noise rates.Ma et al. [2020] introduce various loss functions and classify them into types: Active Loss and Passive Loss functions.They also demonstrate that normalizing the loss can help improve robustness.They use a combination of one active loss and one passive loss like SCE.We define a loss function that considers reverse information to match the RL version and use it to improve the RL learning procedure.\n\nReinforcement Learning from Human Feedback (RLHF) Ouyang et al. [2022], Lee et al. [2023b] and Reinforcement Learning from AI Feedback (RLAIF) [Lee et al., 2023a, Bai et al., 2022] have contributed to the success of large language models (LLMs) by aligning them with user preferences.However, these methods require training a reward model and a value function.Each of these components has prediction errors, and finding appropriate hyperparameters for training requires significant effort.Direct Preference Optimization (DPO) [Rafailov et al., 2023] eliminates the cost associated with the reward model by rearranging PPO loss for ranking-based feedback (e.g., sample A is preferred over sample B).Ethayarajh et al. [2024] remove the requirement ranking-based feedback by modifying DPO loss further, allowing a model to be trained with bad or good labels.",
            "score": 0.36191782553471435,
            "section_title": "Related Work",
            "char_start_offset": 6436,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 120,
                    "end": 320
                },
                {
                    "start": 320,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 531
                },
                {
                    "start": 531,
                    "end": 773
                },
                {
                    "start": 773,
                    "end": 853
                },
                {
                    "start": 853,
                    "end": 972
                },
                {
                    "start": 972,
                    "end": 1048
                },
                {
                    "start": 1048,
                    "end": 1120
                },
                {
                    "start": 1120,
                    "end": 1253
                },
                {
                    "start": 1255,
                    "end": 1539
                },
                {
                    "start": 1539,
                    "end": 1615
                },
                {
                    "start": 1615,
                    "end": 1744
                },
                {
                    "start": 1744,
                    "end": 1953
                },
                {
                    "start": 1953,
                    "end": 2110
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15966796875
        },
        {
            "corpus_id": "259370648",
            "title": "HuCurl: Human-induced Curriculum Discovery",
            "text": "the computational expenses of using full datasets. Fourth, as noted previously, instances of the Reddit dataset consist of long paragraphs, causing high variance in models trained using the dataset. Consequently, the curricula obtained using the Reddit and loss as measure of difficulty are of lower quality and perform poorly. Appendix D reports the results of all configurations. Table 2 shows the transferability of discovered curricula across model sizes. We consider three models with increasing sizes applied to ChaosNLI: distilroberta-base with 82M parameters, roberta-base with 125M parameters, and bart-large with 406M parameters. The results show that the curricula discovered for small models are transferable to larger models, with significant improvement over No-CL and other CL baselines. In particular, we observe greater transferability for smaller model sizes, which indicates curriculum discovery is more beneficial to smaller models than larger (more robust) models. In some cases, the curricula discovered for smaller models perform better than those discovered for larger models, see Ent(sp) 82M and 125M. This is because curriculum discovery is less expensive on smaller models, allowing better exploration of curriculum space to find better curricula. Figure 6 shows the curricula obtained using models of different sizes. The three curricula are similar in their relative treatment of difficulty groups: samples from the easy class are assigned higher weights than those from the medium class, and medium samples receive higher weights than hard samples. In addition, hard samples are considerably down-weighted, which indicates deemphasizing hard samples during training can lead to better results on the test data of ChaosNLi.  Figure 5: Notation is the same as Figure 4: {ChaosNLI, SNLI, Twitter, Reddit}, followed by the type of the dataset {Difficulty-balanced or Full}, and the difficulty score used {Entropy, Loss}. The x-axis lists curricula discovered using a particular dataset and scoring function, and the increasing curriculum inc (Figure 2b). The y-axis lists models that are trained using each curriculum. For example, the cell at the intersection of row \"S-F-L\" and column \"T-F-E\" represents a model trained on SNLI full partitioned by loss, using",
            "score": 0.36061126836723967,
            "section_title": "Discovered Curricula Are Generalizable",
            "char_start_offset": 22886,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2113037109375
        },
        {
            "corpus_id": "253384678",
            "title": "Changes from Classical Statistics to Modern Statistics and Data Science",
            "text": "There are clear evidence that the cross-entropy loss of the language generative models such as generative imaging modeling, video modeling, mathematical problem solving that requires some level of reasoning, decreases as the number of parameters, the sizes of available training data and compute budget increase (Henighan et al. 2020). The loss function can be empirically approximated by \n\nwhere () represents an entropy of the true model, and is often called irreducible loss and   represents the Kullback-Leibler (KL) distance between the ground truth and model distribution and is often called the reducible loss. The entropy here is defined for the curves of performance of language model (Balestrin et al 2009). The entropy of the curves measures the complexity of the curves and dynamic system. If the curve is a straight line, then its entropy is 1, or the dynamic system is linear, then again, the entropy of the linear dynamic system is 1. The entropy grows with nonlinearity and irregularity of the curve. The performance curve of the large language model can be modeled as a piecewise stationary dynamic system. Abrupt changes in the performance curve can be detected by the symmetrized KL distance (Last and Shumway 2008). \n\nRecently, machine learning based model free, data driven methods for predicting abrupt changes or catastrophic collapse due to parameter drift without relying on a model have been developed (Kong et al. 2021;Patel and Ott, 2022). These model-free machine learning methods can be used to predict tipping points and abrupt changes of the performance of large language models and investigate how the continuously quantitative changes lead to discontinuously qualitative changes of the performance of large language models. Unlike classical probability theory where the values random variables take are in Euclidean space, modern probability theory also considers manifold-valued random variables and constructs a probabilized space with manifold-valued outcomes (Pennec 2004). Probability theory on Riemannian manifolds is divided into two intrinsic and extrinsic methodology. Pennec (2006) gives a general definition of probability measures on Riemannian manifolds.",
            "score": 0.35884355824302766,
            "section_title": "Abrupt changes in the performance curve can be detected by the symmetrized KL distance",
            "char_start_offset": 18865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 388
                },
                {
                    "start": 391,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1235
                },
                {
                    "start": 1238,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 2011
                },
                {
                    "start": 2012,
                    "end": 2111
                },
                {
                    "start": 2112,
                    "end": 2201
                }
            ],
            "ref_mentions": [
                {
                    "start": 1211,
                    "end": 1234,
                    "matchedPaperCorpusId": "30348010"
                },
                {
                    "start": 1428,
                    "end": 1446,
                    "matchedPaperCorpusId": "227254410"
                },
                {
                    "start": 2112,
                    "end": 2125,
                    "matchedPaperCorpusId": "653972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.127685546875
        },
        {
            "corpus_id": "273098749",
            "title": "Structural-Entropy-Based Sample Selection for Efficient and Effective Learning",
            "text": "Existing sample selection methods primarily utilize local information. They can be classified into two categories based on the information utilized: attribute-based methods and connection-based methods. \n\nAttribute-based methods rely on the attributes of individual samples. A commonly used attribute is the training difficulty, which is typically assessed from two perspectives: confidence and error. Metrics that measure model confidence include the entropy of the prediction vector (Coleman et al., 2020) and the variance of the predicted probabilities across training epochs (Swayamdipta et al., 2020). Metrics that measure model error include EL2N (Paul et al., 2021), which calculates the L 2 norm of the error vector, and the Forgetting score (Toneva et al., 2019), which tracks the frequency of misclassifications after initial correct classifications. AUM (Pleiss et al., 2020) combines both perspectives by measuring the confidence for correct classifications and the error for misclassifications. Based on these metrics, several sample selection methods have been developed. One simple yet effective method is selecting the most difficult samples, as they have a larger impact on the model performance (Paul et al., 2021). However, this method overlooks easy samples, which are crucial for model training when data budgets are limited (Sorscher et al., 2022). To address this issue, CCS (Zheng et al., 2022) divides the dataset into strata based on training difficulty and performs random sampling within each stratum. InfoBatch (Qin et al., 2023) retains some easy samples and enhances their influence by upscaling their gradient. Another line of work uses the gradient as the attribute and aims to match the average gradient of the selected samples with that of all samples (Mirzasoleiman et al., 2019;Killamsetty et al., 2021). However, these gradients depend on the model's current state during training, limiting the applicability of the selected samples to other models. \n\nConnection-based methods utilize local connections within the sample graph to optimize sample diversity and coverage. GraphCut (Iyer et al., 2021) selects samples with weak connections among them to promote diversity while maintaining strong connections to unselected samples for better coverage.",
            "score": 0.3577321202318,
            "section_title": "RELATED WORK",
            "char_start_offset": 5602,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 202
                },
                {
                    "start": 205,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 1987
                },
                {
                    "start": 1990,
                    "end": 2107
                },
                {
                    "start": 2108,
                    "end": 2286
                }
            ],
            "ref_mentions": [
                {
                    "start": 485,
                    "end": 507,
                    "matchedPaperCorpusId": "195750622"
                },
                {
                    "start": 579,
                    "end": 605,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 653,
                    "end": 672,
                    "matchedPaperCorpusId": "235898952"
                },
                {
                    "start": 750,
                    "end": 771,
                    "matchedPaperCorpusId": "55481903"
                },
                {
                    "start": 865,
                    "end": 886,
                    "matchedPaperCorpusId": "210932316"
                },
                {
                    "start": 1213,
                    "end": 1232,
                    "matchedPaperCorpusId": "235898952"
                },
                {
                    "start": 1346,
                    "end": 1369,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 1398,
                    "end": 1418,
                    "matchedPaperCorpusId": "253224188"
                },
                {
                    "start": 1540,
                    "end": 1558,
                    "matchedPaperCorpusId": "257427625"
                },
                {
                    "start": 1787,
                    "end": 1815,
                    "matchedPaperCorpusId": "211259075"
                },
                {
                    "start": 1815,
                    "end": 1840,
                    "matchedPaperCorpusId": "235421747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75048828125
        },
        {
            "corpus_id": "236460239",
            "title": "Competence-based Multimodal Curriculum Learning for Medical Report Generation",
            "text": "The training instances of medical report generation task are pairs of medical images and corresponding reports which is a multi-modal data. It's hard to estimate the difficulty with only one metric. In addition, the experimental results (see Table 4) show that directly fusing multiple difficulty metrics as one (d 1 + d 2 + d 3 + d 4 ) is obviously inappropriate, which is also verified in Platanios et al. (2019). To this end, we extend the single difficulty-based curriculum learning into the multiple difficulty-based curriculum learning, where we provide the medical report generation models with four different difficulty metrics, i.e., d 1 , d 2 , d 3 , d 4 (see Section 4). A simple and natural way is to randomly or sequentially choose a curricula to train the model, i.e., 1\u21922\u21923\u21924\u21921. However, a better approach is to adaptively select the most appropriate curricula for each training step, which follows the common practice of human learning behavior: When we have learned some curricula well, we tend to choose the under-learned curricula to learn. Algorithm 2 summarizes the overall learning process of the proposed framework and Figure 3 illustrates the process of Algorithm 2. In implementations, similarly, we first sort the training dataset based on the four difficulty metrics and acquire four sorted training datasets in line 1-2. Then, based on the model competence, we acquire the training samples for each curricula, in line 4. In line 5, we further estimate the perplexity (PPL) of model on different training samples B i (t i ) corresponding to different curricula, defined as:\n\nThe perplexity (PPL) measures how many bits on average would be needed to encode each word of the report given the model, so the current curricula with higher PPL means that the model is not well-learned for this curricula and need to be improved. Therefore, the PPL can be used to determine the curricula at each training step dynamically. Specifically, in line 8-9, we select the under-learned curricula, i.e., the curricula with maximum PPL, to train the current model. After that",
            "score": 0.3572687399229406,
            "section_title": "Multiple Difficulty-based Curriculum Learning",
            "char_start_offset": 17680,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.501953125
        },
        {
            "corpus_id": "252089870",
            "title": "A Review of Sparse Expert Models in Deep Learning",
            "text": "Sparse expert models have excelled when trained on large datasets. A common paradigm in natural language processing is to perform upstream training (e.g. pre-training) which is then followed by downstream training (e.g. fine-tuning) on data distributions of specific interest. Sparse expert models have consistently yielded high gains over dense counterparts during the upstream phase. Shazeer et al. (2017) presented scaling curves with respect to model parameters and the computational budget on the 1-Billion-Word Language-Modeling Benchmark (Chelba et al., 2013), achieving significant gains over dense versions. Lepikhin et al. (2020) presented translation improvements as a function of model scale, and obtained a 13.5 BLEU score gain on their largest 600B parameter sparse model. Switch Transformers (Fedus et al., 2021) measured 4-7\u00d7 speed-ups in wall-time using the same compute resources over T5 models. The work also studied the cross entropy loss scaling as a function of parameter count, but observed the gains diminished with 256+ experts. \n\nFurthering our understanding, Artetxe et al. (2021) distinguished upstream scaling behavior of MoE models on in-domain and out-of-domain data and found significantly better scaling for in-domain language modeling compared to dense models, corroborating the difficulties of transfer from Fedus et al. (2021). After these early empirical successes, Clark et al. (2022) conducted the first large-scale effort to mathematically characterize the scaling properties of sparse expert models. This work considered three classes of sparse models and derived a notion of effective parameter count (EPC). The EPC estimates the dense-parameter equivalent for a sparse expert models, based on the FLOPs and the number of experts. It was derived by conjecturing that sparse expert models followed a bilinear loss and it was shown empirically that the cross entropy loss scales as a power law in this variable. Figure 3 presents the cross entropy scaling of Switch Transformers on the left and the three sparse variants of Clark et al. (2022) on the right.",
            "score": 0.35698921816450485,
            "section_title": "UPSTREAM SCALING",
            "char_start_offset": 11939,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1053
                },
                {
                    "start": 1056,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2097
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22119140625
        },
        {
            "corpus_id": "266180578",
            "title": "Mmi01 at The BabyLM Challenge: Linguistically Motivated Curriculum Learning for Pretraining in Low-Resource Settings",
            "text": "Currently, pretraining language models (LMs) involve training models on large, diverse datasets before fine-tuning them on specific downstream tasks. As a byproduct of this procedure, datasets have grown substantially beyond developmentally plausible amounts. For instance, the recently released large variant of LLAMA-2 has 70 billion parameters and it was pre-trained with 2 trillion tokens (Touvron et al., 2023). This amount of data is well over the amount of exposure a child would have. Gilkerson et al. (2017) find that on average, a child aged 48-mo would be exposed to 12,128 tokens, from solely their parents. Calculations show LLAMA2's pretraining data is 165,000 times more than this developmental-plausible quantity. \n\nTherefore, the goal of this task is to use humandevelopment plausible methods for pretraining smaller-sized language models. In particular, we combine intuitions from linguistics and curriculum learning to explore whether different curricula designs affect models' performance. To do this, we investigate two strands of complexity measures, namely, structural complexity and lexical complexity. \n\nOur research questions (RQs) are as follows: \n\n1. Do pre-training LMs using CL produce better performance? If so: \n\n2. Are linguistic complexity measures helpful in designing curricula for CL? \n\n3. Which linguistic metric is advantageous and which is less? Is one strand of complexity measure inherently better than the other? \n\nTo answer RQ1, we aim to compare a baseline non-CL model to the results of CL-pretrained models. For RQ2, we make a similar comparison but this time using the results of a model that is trained on a random curriculum. For the last RQ, we make inter-model comparisons. \n\nWe provide an analysis of curriculum designs and the novel aspects of our work ( \u00a7 2). Following this, we explain the linguistic metrics in detail and provide details of our approach ( \u00a7 3). In \u00a7 4, we present our findings and discussions, before finally summarising the paper in \u00a7 5.",
            "score": 0.3560253460197497,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1126
                },
                {
                    "start": 1129,
                    "end": 1173
                },
                {
                    "start": 1176,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1242
                },
                {
                    "start": 1245,
                    "end": 1321
                },
                {
                    "start": 1324,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1455
                },
                {
                    "start": 1458,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1725
                },
                {
                    "start": 1728,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2012
                }
            ],
            "ref_mentions": [
                {
                    "start": 493,
                    "end": 516,
                    "matchedPaperCorpusId": "4447742"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39306640625
        },
        {
            "corpus_id": "277501711",
            "title": "Enlightenment Period Improving DNN Performance",
            "text": "Curriculum Learning (CL) is a strategy that mimics the sequencing observed in human learning processes to train machine learning models. The core principle of CL is \"from easy to difficult,\" [Zhou et al., 2024]. A representative approach \"baby steps\" [Bengio et al., 2009], implements automated curriculum scheduling by ordering training data based on loss values. Most current CL approaches are designed based on a 'difficulty measurer + training scheduler' framework [Wang et al., 2021]. However, existing studies reveal neither easy-to-hard nor hard-to-easy curricula can improve model performance.",
            "score": 0.35591180406030776,
            "section_title": "Curriculum Learning",
            "char_start_offset": 6385,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 601
                }
            ],
            "ref_mentions": [
                {
                    "start": 191,
                    "end": 210,
                    "matchedPaperCorpusId": "272330351"
                },
                {
                    "start": 251,
                    "end": 272,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 469,
                    "end": 488,
                    "matchedPaperCorpusId": "232362223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2578125
        },
        {
            "corpus_id": "274192425",
            "title": "The Impossible Test: A 2024 Unsolvable Dataset and A Chance for an AGI Quiz",
            "text": "This research introduces a novel evaluation framework designed to assess large language models' (LLMs) ability to acknowledge uncertainty on 675 fundamentally unsolvable problems. Using a curated dataset of graduate-level grand challenge questions with intentionally unknowable answers, we evaluated twelve state-of-the-art LLMs, including both open and closed-source models, on their propensity to admit ignorance rather than generate plausible but incorrect responses. The best models scored in 62-68% accuracy ranges for admitting the problem solution was unknown in fields ranging from biology to philosophy and mathematics. We observed an inverse relationship between problem difficulty and model accuracy, with GPT-4 demonstrating higher rates of uncertainty acknowledgment on more challenging problems (35.8%) compared to simpler ones (20.0%). This pattern indicates that models may be more prone to generate speculative answers when problems appear more tractable. The study also revealed significant variations across problem categories, with models showing difficulty in acknowledging uncertainty in invention and NP-hard problems while performing relatively better on philosophical and psychological challenges. These results contribute to the growing body of research on artificial general intelligence (AGI) assessment by highlighting the importance of uncertainty recognition as a critical component of future machine intelligence evaluation. This impossibility test thus extends previous theoretical frameworks for universal intelligence testing by providing empirical evidence of current limitations in LLMs' ability to recognize their own knowledge boundaries, suggesting new directions for improving model training architectures and evaluation approaches.",
            "score": 0.355786966369069,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.19287109375
        },
        {
            "corpus_id": "260500164",
            "title": "Test Accuracy vs. Generalization Gap: Model Selection in NLP without Accessing Training or Testing Data",
            "text": "Selecting the optimal hyperparameters, such as those for training or model size, is a critical phase in the ML pipeline. Motivated by the importance of model selection, recent years have seen a wide array of large-scale empirical studies on the various metrics used to predict the test-time performance of ML models [9,17,26,27]. These generalization metrics have been applied in a wide variety of data science tasks, including predicting the quality of pretrained learning models [23,27], designing effective training procedures [11,14], improving network efficiency [5,8], quantifying model robustness [41,47], improving ensemble learning techniques [12,13], analyzing and improving large-scale machine learning contests [26], and so on. They are typically studied using correlational analysis, measuring how strongly each metric correlates with (and therefore, can predict) model performance. In this regard, several recent works point out the deficiencies of existing generalization metrics, including a lack of \"robustness\" to the changes of environmental hyperparameters [9,17] (such as data, neural network architecture and training schemes), or the Simpson's paradox that generalization metrics perform differently (i.e., predict opposite trends) when applied to each sub-part of a collection of learning models or to the holistic study [26]. Another drawback is the over-reliance on CV models, which are relatively well-explored, and are not always representative of other types of tasks. With few exceptions [27,31,46], systematic studies in other fields, such as NLP, are largely missing. Generalization metrics for model selection in NLP. The objective of this work is to provide a systematic study of generalization metrics in NLP, addressing several deficiencies in prior studies [9,17,27]. Compared to CV, model selection in NLP has several important differences that require careful consideration. For example, the training data from standard CV benchmarks can often be easily obtained, while large language model datasets are typically web-scale and are challenging to access. Therefore, generalization metrics that can assess the quality of learning models without access to data are ideal for NLP. In this paper, we focus on",
            "score": 0.35514177827755633,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 316,
                    "end": 319,
                    "matchedPaperCorpusId": "225040271"
                },
                {
                    "start": 319,
                    "end": 322,
                    "matchedPaperCorpusId": "204893960"
                },
                {
                    "start": 325,
                    "end": 328,
                    "matchedPaperCorpusId": "211132500"
                },
                {
                    "start": 481,
                    "end": 485,
                    "matchedPaperCorpusId": "59222778"
                },
                {
                    "start": 485,
                    "end": 488,
                    "matchedPaperCorpusId": "211132500"
                },
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "222134093"
                },
                {
                    "start": 534,
                    "end": 537,
                    "matchedPaperCorpusId": "3833416"
                },
                {
                    "start": 568,
                    "end": 571,
                    "matchedPaperCorpusId": "232013680"
                },
                {
                    "start": 571,
                    "end": 573,
                    "matchedPaperCorpusId": "148571720"
                },
                {
                    "start": 608,
                    "end": 611,
                    "matchedPaperCorpusId": "220486747"
                },
                {
                    "start": 656,
                    "end": 659,
                    "matchedPaperCorpusId": "4055784"
                },
                {
                    "start": 1077,
                    "end": 1080,
                    "matchedPaperCorpusId": "225040271"
                },
                {
                    "start": 1080,
                    "end": 1083,
                    "matchedPaperCorpusId": "204893960"
                },
                {
                    "start": 1518,
                    "end": 1522,
                    "matchedPaperCorpusId": "211132500"
                },
                {
                    "start": 1522,
                    "end": 1525,
                    "matchedPaperCorpusId": "207808916"
                },
                {
                    "start": 1525,
                    "end": 1528,
                    "matchedPaperCorpusId": "236318524"
                },
                {
                    "start": 1794,
                    "end": 1797,
                    "matchedPaperCorpusId": "225040271"
                },
                {
                    "start": 1797,
                    "end": 1800,
                    "matchedPaperCorpusId": "204893960"
                },
                {
                    "start": 1800,
                    "end": 1803,
                    "matchedPaperCorpusId": "211132500"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.354736328125
        },
        {
            "corpus_id": "270285974",
            "title": "A Survey on Medical Large Language Models: Technology, Application, Trustworthiness, and Future Directions",
            "text": "One of the possible future research directions of Med-LLMs is to explore the construction of effective adversarial test samples in the medical field, including the construction of synthetic anomaly cases [267] and boundary stress testing [268] to evaluate the robustness of large language models in the medical field.Due to the limited research in this area, especially as Alberts pointed out in the 2023 study [68], there are many challenges in constructing medically relevant adversarial test samples, so future research should focus on overcoming these challenges and developing adversarial test methods suitable for the medical field.\n\nThere have been many studies that use uncertainty quantification methods to improve robustness.For example, Ke Shen et al. [269] proposed a formalism and method to improve the robustness of large language models by using risk-adjusted confidence scores.Future research can continue to explore how to apply uncertainty quantification techniques to large language models in the medical field to improve the robustness of the model and reduce risks.At the same time, considering the difficulties of large language models in processing outof-distribution inputs, future research can also focus on how to use techniques similar to LLM-TTA [270] to improve the robustness of the model to unknown or rare cases in the medical field.By effectively using the augmentation data generated by the model as a means of augmentation at test time, we can reduce the dependence of the model on expensive augmentation while maintaining the performance of the model, which will have important implications for advancing the application of large language models in the medical field.",
            "score": 0.35462948539301414,
            "section_title": "D. Robustness",
            "char_start_offset": 81960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 317
                },
                {
                    "start": 317,
                    "end": 638
                },
                {
                    "start": 640,
                    "end": 735
                },
                {
                    "start": 735,
                    "end": 893
                },
                {
                    "start": 893,
                    "end": 1086
                },
                {
                    "start": 1086,
                    "end": 1365
                },
                {
                    "start": 1365,
                    "end": 1703
                }
            ],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 209,
                    "matchedPaperCorpusId": "259096157"
                },
                {
                    "start": 411,
                    "end": 415,
                    "matchedPaperCorpusId": "257425466"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16455078125
        },
        {
            "corpus_id": "264492472",
            "title": "Intra- and Inter-Modal Curriculum for Multimodal Learning",
            "text": "Curriculum Design. The first and key step to deal with the under-optimized modality problem is to optimize them further based on their intra-modal data. We design an intra-modal curriculum by introducing offline distillation with feature-based knowledge, from which the distillation loss acts as the data difficulty measurer. It does not depend on extra labels or unimodal performances but only needs one forward propagation in each training step, ensuring the generality and efficiency of our method. \n\nDifficulty Measurer. We employ a pretrained teacher model   parameterized by   and adopt the  2 norm distance as the optimization objective to distill knowledge for the  \u210e modality: min \n\nWe denote the distillation loss from Equation ( 7) as L  , and regard the distillation loss   of the data instance   such that L  = 1   =1   as the difficulty measurer of   for the modal encoder   . For the sake of comprehensiveness, we treat multimodality as a special modality, denote the task loss from Equation (2) as L 0 and view the task loss  0 of the data pair   as the difficulty measurer of   for the whole model. Within each modality, we split all data into two parts, the hard ones and the easy ones, by comparing their losses to the moving average   of losses L  : \n\nwhere   \u2208 [0, 1] is a discount factor and  refers to training steps. A relatively large   satisfying  \n\n means that the encoder has not been able to represent   well in the current training step, so we can treat   as hard data and decrease its weight, while a small loss value  \n\n represents easy data worthy of an increased weight. Based on the losses as the data difficulty measurers, we can realize specific curricula for all modalities and teach them from easy to hard with the learning scheduler described in the next paragraph. \n\nLearning Scheduler. The scheduler that guides the learning of each modality is implemented through data reweighting. The weight   assigned to data   impacts the learning process by scaling the loss:     , where   should at least satisfies the following conditions to be consistent of the core idea of curriculum learning, i.e., from easy to hard:",
            "score": 0.35452525834238935,
            "section_title": "Intra-Modal Curriculum",
            "char_start_offset": 13936,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 501
                },
                {
                    "start": 504,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 689
                },
                {
                    "start": 692,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1269
                },
                {
                    "start": 1272,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1549
                },
                {
                    "start": 1552,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1805
                },
                {
                    "start": 1808,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2154
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56005859375
        },
        {
            "corpus_id": "250408174",
            "title": "A Study on the Predictability of Sample Learning Consistency",
            "text": "While Deep Learning has achieved significant milestones over the last decade, training strategies have remained relatively unchanged. Over the last years, strides have been made in improving these strategies. One such approach is Curriculum Learning (CL) (Bengio et al., 2009;Wu et al., 2021). CL strategies are based on providing a model with an increasingly difficult selection of examples, similar to how children learn in school. CL strategies are popular in Reinforcement Learning -where a curriculum of tasks is progressively learned -and Natural Language Processing applications -where commonly sentence length has been a proxy for difficulty. \n\nProceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s). \n\nThe problem, however, lies in how to determine the difficulty of samples. Typically, an ad hoc difficulty measure needs to be crafted by hand for a specific problem, which limits their application in practice. Even worse, for some applications like image classification, sometimes there's no natural difficulty ordering that can be created. This has led to the development of more general measures of difficulty: Self Paced Learning (Kumar et al., 2010) utilizes the current loss of an example as a proxy for difficulty; however, this requires extra computation and is prone to overfitting (Jiang et al., 2015). As an alternative, a metric called C-Score was proposed (Jiang et al., 2021). This metric measures how consistently a particular example is learned across various models, that is, it measures the proportion of iterations where a sample is successfully classified. The logic behind this metric is supported by the empirical observation that samples are generally learned in the same order (Toneva et al., 2019) -independent of the model's architecture-and that a significant number of examples are learned once and never forgotten. The drawback to this metric is that it requires inordinate amounts of computing for a single dataset, as it requires several models to be trained and averaged over, negating many of the practical benefits of applying CL in the first place. \n\nA reasonable strategy to alleviate the problems of computing C-Scores would be to learn a model that could extrapolate C-Scores to unseen datasets. This could be done by training a model on datasets where the score is already calculated.",
            "score": 0.3544289123930477,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 650
                },
                {
                    "start": 653,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 798
                },
                {
                    "start": 801,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2183
                },
                {
                    "start": 2186,
                    "end": 2333
                },
                {
                    "start": 2334,
                    "end": 2423
                }
            ],
            "ref_mentions": [
                {
                    "start": 255,
                    "end": 276,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 276,
                    "end": 292,
                    "matchedPaperCorpusId": "227343966"
                },
                {
                    "start": 1391,
                    "end": 1411,
                    "matchedPaperCorpusId": "10891229"
                },
                {
                    "start": 1469,
                    "end": 1489,
                    "matchedPaperCorpusId": "219573311"
                },
                {
                    "start": 1801,
                    "end": 1822,
                    "matchedPaperCorpusId": "55481903"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6787109375
        },
        {
            "corpus_id": "266180578",
            "title": "Mmi01 at The BabyLM Challenge: Linguistically Motivated Curriculum Learning for Pretraining in Low-Resource Settings",
            "text": "Our submission considers GPT-2 models (Radford et al., 2019) pretrained using curricula formed by various linguistic measures detailed in \u00a7 2. The pretraining approach involves sequentially training the model using ten different curriculum levels of the dataset, with each level building upon the previous one in terms of difficulty. Each model is pretrained three times, with a random seed used each time.",
            "score": 0.3527445332254625,
            "section_title": "Methodology",
            "char_start_offset": 5009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 406
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.291015625
        },
        {
            "corpus_id": "270257882",
            "title": "CSS: Contrastive Semantic Similarity for Uncertainty Quantification of LLMs",
            "text": "Despite recent breakthroughs in a wide range of natural language generation (NLG) tasks [Hoffmann et al., 2022, Touvron et al., 2023, Chowdhery et al., 2023], the uncertainty quantification (UQ) of large language models (LLMs) remains an open challenge.Without reliable measures of uncertainty, it is implausible to apply LLMs in critical tasks such as medical [Singhal et al., 2023] or legal questionanswering [Louis et al., 2023], or medical diagnosing [Wang et al., 2023].A reliable measure of uncertainty helps to decide when to trust a model, which is also the key problem in building safer AI systems [Hendrycks et al., 2021].Recently, LLMs have been deployed in the industry as powerful tools to assist professional or personal work, with well-known interfaces such as ChatGPT1 , Gemini2 and Perplexity AI3 .However, with the enhanced capabilities of LLMs, concerns are simultaneously raised about their trustworthiness.\n\nThe study of UQ in LLMs has gained significant attention recently.Most existing methods are white-box, relying on either calculating entropy from predicted probabilities [Malinin andGales, 2020, Kuhn et al., 2023] or querying models for their prediction confidence [Lin et al., 2022b, Kadavath et al., 2022].However, these techniques often require task-specific labels, additional training data, or white-box access to the internal model information.Black-box UQ strategies address this by analyzing the consistency of information across model generations.Techniques like n-gram overlap [Fomicheva et al., 2020] assess surface-level similarity, while more recent approaches explore semantic equivalence [Kuhn et al., 2023, Lin et al., 2023].These methods cluster sentences based on meaning to estimate uncertainty, with a higher number of clusters indicating greater semantic diversity and thus higher LLM uncertainty.However, a key limitation lies in using Natural Language Inference (NLI) classifier logits to measure semantic equivalence.Logits represent class probabilities, not the semantic features needed for accurate clustering.",
            "score": 0.352020852203769,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 253,
                    "end": 475
                },
                {
                    "start": 475,
                    "end": 632
                },
                {
                    "start": 632,
                    "end": 815
                },
                {
                    "start": 815,
                    "end": 927
                },
                {
                    "start": 929,
                    "end": 995
                },
                {
                    "start": 995,
                    "end": 1237
                },
                {
                    "start": 1237,
                    "end": 1379
                },
                {
                    "start": 1379,
                    "end": 1485
                },
                {
                    "start": 1485,
                    "end": 1670
                },
                {
                    "start": 1670,
                    "end": 1847
                },
                {
                    "start": 1847,
                    "end": 1970
                },
                {
                    "start": 1970,
                    "end": 2065
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 157,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 1194,
                    "end": 1212,
                    "matchedPaperCorpusId": "248780252"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1607666015625
        },
        {
            "corpus_id": "276742147",
            "title": "Your Model is Overconfident, and Other Lies We Tell Ourselves",
            "text": "The difficulty intrinsic to a given example, rooted in its inherent ambiguity, is a key yet often overlooked factor in evaluating neural NLP models. We investigate the interplay and divergence among various metrics for assessing intrinsic difficulty, including annotator dissensus, training dynamics, and model confidence. Through a comprehensive analysis using 29 models on three datasets, we reveal that while correlations exist among these metrics, their relationships are neither linear nor monotonic. By disentangling these dimensions of uncertainty, we aim to refine our understanding of data complexity and its implications for evaluating and improving NLP models.",
            "score": 0.3509754580961078,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65283203125
        },
        {
            "corpus_id": "271865496",
            "title": "Leveraging Perceptual Scores for Dataset Pruning in Computer Vision Tasks",
            "text": "Deep learning has made tremendous progress in the past few years exploiting the scale of large training sets, among other factors. Recently data centric methods, such as training on pruned dataset [21], or using non uniform mixing strategies [24] have become standard practice for training large scale models. In these methods a score is attached to each data instance, and an instance is selected (or not) for training using the ordered scores of available instances. In this paper we focus on data pruning for computer vision tasks where a subset of the instances available is used for training with minimal loss of performance. \n\nIn previous data pruning approaches for computer vision, mostly on the classification task, scores naturally reflect the learning task at hand. That is, they are based on the distribution of input and its label(s). This implicitly pays less attention to the input itself. Also modeling this distribution accurately is expensive -usually some steps of training have to be done before the scores can be calculated [7,15,21,22]. Thus one of the key questions, raised in literature, is how early in training can the instances to be pruned identified [15]? \n\nWe start with four observations. First, the vision perception literature recognizes that images with less clutter, simpler background, iconic objects, are processed faster by the human visual system [18]. Many measures for characterising human perception of scene complexity have been proposed [10], among which the information theoretic measure of entropy is not only the simplest but also captures the clutter in an image well [18]. \n\nSecond, images with less clutter and a plain background, also lead to better labeling of pictures by young children [14]. In general child development literature shows that the nature of picture books for young children leads to different transfer learning experience. Similar studies for deep learning can yield interesting insights. \n\nThird, in natural language processing scores such as length of sentences, or word rarity, have been proposed as difficulty scores for machine translation [17]. Sentences that are shorter are easier to translate than sentences that are longer; here difficulty is a measure of only the input sentence, not of the input, output sentences. Machine translation is similar to the semantic segmentation task in computer vision.",
            "score": 0.3506732420799208,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 630
                },
                {
                    "start": 633,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1184
                },
                {
                    "start": 1187,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1621
                },
                {
                    "start": 1624,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 1958
                },
                {
                    "start": 1961,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2296
                },
                {
                    "start": 2297,
                    "end": 2381
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 201,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 1048,
                    "end": 1051,
                    "matchedPaperCorpusId": "235898952"
                },
                {
                    "start": 1051,
                    "end": 1054,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 1179,
                    "end": 1183,
                    "matchedPaperCorpusId": "235898952"
                },
                {
                    "start": 1386,
                    "end": 1390,
                    "matchedPaperCorpusId": "10847280"
                },
                {
                    "start": 1481,
                    "end": 1485,
                    "matchedPaperCorpusId": "253518465"
                },
                {
                    "start": 1616,
                    "end": 1620,
                    "matchedPaperCorpusId": "10847280"
                },
                {
                    "start": 1740,
                    "end": 1744,
                    "matchedPaperCorpusId": "55786227"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4189453125
        },
        {
            "corpus_id": "263829494",
            "title": "Constructive Large Language Models Alignment with Diverse Feedback",
            "text": "Initially, in our experiment, we use the Vicuna-7B model as our base model. To evaluate the difficulty level of each problem in the dataset D, we first generate an answer a for each problem q in D using a greedy search algorithm. Then, we calculate the perplexity P P L(a, q) for each answer a using the following formula: \n\nwhere l represents the length of the answer a. We adopt perplexity as the metric for evaluating problem difficulty since it is a representative measure of semantic uncertainty. This concept has been widely utilized in previous studies that address uncertainty in language generation tasks. Similar to how humans may express uncertainty when dealing with challenging questions during exams, a high perplexity score typically indicates that the model is also experiencing a hard question with a high degree of uncertainty while generating its answer. \n\nGrouping Strategy. After computing the perplexity score for each problem in D, we first organize the problems in ascending order of perplexity. We then evenly distribute them into three groups: the easy group E comprising problems with the lowest perplexity; the medium group M containing problems with moderate perplexity; and the hard group H including problems with the highest perplexity scores.",
            "score": 0.3488551214263195,
            "section_title": "DIFFICULTY CALCULATION AND GROUPING",
            "char_start_offset": 8906,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 322
                },
                {
                    "start": 325,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 873
                },
                {
                    "start": 876,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1275
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.494140625
        },
        {
            "corpus_id": "254685579",
            "title": "Efficient Pre-training of Masked Language Model via Concept-based Curriculum Masking",
            "text": "CoLA Recent NLP studies have shown that curriculum learning (CL), presenting examples in an easyto-difficult order rather than presenting them randomly, can accelerate the model convergence and improve the generalization performance (Zhang et al., 2018;Tay et al., 2019;Zhan et al., 2021). There mainly exist two criteria for assessing the difficulty of examples, 1) model-based criteria  and 2) linguisticinspired criteria (Sachan and Xing, 2016;Tay et al., 2019;Nagatsuka et al., 2021;Campos, 2021). Model-based criteria measure the difficulty of each example using task-specific models. However, these criteria are unsuitable for reducing the computation cost of pre-training, given that they require calculating the loss of every example in a large pretraining corpus using language models. In contrast, linguistic-inspired criteria can efficiently assess the difficulty of examples based on prior knowledge and rules. Therefore, we adopt CL with linguistic difficulty criteria into MLM to improve the efficiency of pre-training.\n\nHowever, we argue that existing linguisticsinspired criteria, such as length, rarity, and masking ratio of a sequence, do not effectively reflect the nature of MLM, as verified empirically in Table 1.\n\nThe difficulty associated with MLM is significantly affected by the choice of tokens to be masked in the given sequence, rather than by the given sequence itself. For example, given \"The man is a Stanford <mask> student\", we can easily predict that the masked token would be University, whereas given \"The man is a <mask> University student\", it would be relatively difficult to predict the original token due to the insufficient clues in the context. Then, how can we measure the MLM difficulty? MLM can be viewed as predicting masked tokens based on other contextual tokens related to masked tokens. Therefore, if a word is related to many other words and phrases, it is likely that it has several clues in the context that make MLM easier.\n\nIn this paper, we propose a novel concept-based curriculum masking (CCM) for improving pretraining efficiency by considering the nature of MLM. We consider words and phrases that are related to several other concepts as easy ones and define them as the initial concepts to be masked first. To identify them,",
            "score": 0.34778850335050526,
            "section_title": "Methods",
            "char_start_offset": 784,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 253,
                    "end": 270,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 270,
                    "end": 288,
                    "matchedPaperCorpusId": "232104951"
                },
                {
                    "start": 424,
                    "end": 447,
                    "matchedPaperCorpusId": "16503693"
                },
                {
                    "start": 447,
                    "end": 464,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 464,
                    "end": 487,
                    "matchedPaperCorpusId": "244048238"
                },
                {
                    "start": 487,
                    "end": 500,
                    "matchedPaperCorpusId": "236912520"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.394287109375
        },
        {
            "corpus_id": "254974529",
            "title": "Confidence-Aware Paced-Curriculum Learning by Label Smoothing for Surgical Scene Understanding",
            "text": "Inspired by human and animal learning principles, curriculum learning introduces samples from easier to the complex during training. It is observed that the learning process from easy to difficult tasks helps achieve better performance by avoiding the local minima and obtaining better generalization results [22], [23]. Curriculum learning and self-paced learning also improve the robustness and reliability of noisy samples [9], [11]. There are several ways to measure the sample difficulty in designing this learning technique. Most of the works adopt the confidence score to sort the samples. In this way, first, feed the high confidence/easy samples and subsequently introduce low confidence/difficult ones into the learning. Previous studies also utilized handcrafted features [15], multi-raters disagreement [16] and similarity scores [17]. A cutting-edge strategy [24] is introduced to use curricula to pinpoint the fundamentals of how a system learns. Domainaware Curriculum Learning [25] identifies curriculum learning as one crucial element that can reduce the multiple domain shifts in the multi-target domain adaptation. It adapts to the easier target domains first, then moves on to the more difficult ones. A curriculum based on human visual acuity [26] lessens the texture biases in models for gallbladder cancer. Most recently, the Curriculum By Smoothing (CBS) [27] employs the Gaussian filter of feature maps from a higher variance to lower across the training epochs. The higher variance smoothes the feature map heavily and limits the model to learn less information at the beginning. However, designing a curriculum by smoothing label probability is still unexplored in this domain.",
            "score": 0.34775420082335906,
            "section_title": "A. Curriculum learning / self-paced learning",
            "char_start_offset": 5014,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1704
                }
            ],
            "ref_mentions": [
                {
                    "start": 309,
                    "end": 313,
                    "matchedPaperCorpusId": "14892153"
                },
                {
                    "start": 315,
                    "end": 319,
                    "matchedPaperCorpusId": "2603360"
                },
                {
                    "start": 426,
                    "end": 429,
                    "matchedPaperCorpusId": "58981386"
                },
                {
                    "start": 431,
                    "end": 435,
                    "matchedPaperCorpusId": "33880888"
                },
                {
                    "start": 783,
                    "end": 787,
                    "matchedPaperCorpusId": "244119160"
                },
                {
                    "start": 815,
                    "end": 819,
                    "matchedPaperCorpusId": "221995570"
                },
                {
                    "start": 872,
                    "end": 876,
                    "matchedPaperCorpusId": "251647193"
                },
                {
                    "start": 993,
                    "end": 997,
                    "matchedPaperCorpusId": "233004251"
                },
                {
                    "start": 1264,
                    "end": 1268,
                    "matchedPaperCorpusId": "248376979"
                },
                {
                    "start": 1379,
                    "end": 1383,
                    "matchedPaperCorpusId": "220301592"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4296875
        },
        {
            "corpus_id": "265506572",
            "title": "Can training neural language models on a curriculum with developmentally plausible data improve alignment with human reading behavior?",
            "text": "Prior work exploring the efficacy of curriculum learning for NLU tasks has used a wide range of properties to compute sentence difficulty such as sentence length, word frequency (or rarity), tree depth, diversity and understandability (for a review, see Soviany et al., 2022). None of these properties by themselves can comprehensively capture what makes one sentence more difficult to process or acquire than another. For example, while long sentences are in general more difficult than short sentences, a shorter ambiguous sentence (\"the horse raced past the barn fell\") is more difficult to process than a longer unambiguous one (\"the horse which was raced past the barn is the same horse that fell\"). Given the complex ways in which all of the individual properties can interact, a holistic way of combining these properties is likely necessary to generate good measures of sentence difficulty. \n\nA natural way of combining these properties to compute a difficulty measure is to use a \"teacher\" language model to compute the predictability of words in a sentence: given some context, a good language model will assign lower probabilities to words that result in long continuations with infrequent words and structures and/or continuations that describe improbable or hard-to-understand events. Concretely, in this work we define difficulty of a sentence as the mean surprisal of words in the sentence, as given in equation 1, where D is difficulty, L is the model being used to compute difficulty, s k is the k-th sentence, and n is the number of words in s k . \n\nThere are two issues with estimating sentence difficulty in this manner. First, the difficulty estimates can be inaccurate if the teacher language model is trained on the same data for which difficulty scores are being computed. Second, the difficulty estimates can be affected by noisy idiosyncrasies if they are computed from just one teacher language model. To avoid these two issues, we use the Cross-Review method proposed by Xu et al. (2020). In this method each teacher is trained on a subset of the data, and then evaluated on all subsets other than the one it was trained on.",
            "score": 0.3475544306000783,
            "section_title": "Difficulty measurer",
            "char_start_offset": 4609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 898
                },
                {
                    "start": 901,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1565
                },
                {
                    "start": 1568,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2152
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 275,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 1999,
                    "end": 2015,
                    "matchedPaperCorpusId": "220045816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3251953125
        },
        {
            "corpus_id": "272345158",
            "title": "Weighted Cross-entropy for Low-Resource Languages in Multilingual Speech Recognition",
            "text": "By applying weighted cross-entropy, we assign greater importance to low-resource languages during training, and we explore evolving weight approaches in cross-entropy calculations to identify the most effective strategy. In addition, the simultaneous application of data augmentation on the target language emerges as a technique to smooth the application of weighted cross-entropy and prevent degradation in the other languages. \n\nWe conduct our experiments using the well-known Whisper model [7], a weakly supervised pre-trained multilingual ASR model that uses a large amount of labelled data. We finetune the model on five high-resource languages -Spanish, Portuguese, French, German, and English -and one low-resource language -Galician. For training, we use the Common Voice dataset [8], employing all available resources for Galician, and ensuring balanced data for the other languages. Since the test split is used for training in Galician, and in order to test the robustness of the multilingual model for a domain different from the training domain and against a larger and more diverse dataset, we use the FalAI dataset [9] for testing Galician. \n\nThe results show significant improvements in recognition accuracy for the low-resource language, as evidenced by a remarkable 6.69% reduction in Word Error Rate (WER) compared to the fine-tuned model without weighted cross-entropy, and a 48.86% relative reduction in WER compared to the original Whisper model. On average, our approach achieves a 3.29% WER relative reduction across the six languages under study when compared to the simple fine-tuned model, with no degradation for the high-resource languages. When compared to the original Whisper model, our approach showcases a substantial average WER reduction of 32.5%. \n\nThe rest of the paper is structured as follows: Section 2 outlines related work on low-resource languages in ASR. Section 3 details our approaches to weighted cross-entropy computation. Sections 4 and 5 cover the experimental setup and results. Conclusions and future directions are given in Section 6.",
            "score": 0.3470933866398058,
            "section_title": "Introduction",
            "char_start_offset": 2360,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 429
                },
                {
                    "start": 432,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1156
                },
                {
                    "start": 1159,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1784
                },
                {
                    "start": 1787,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2089
                }
            ],
            "ref_mentions": [
                {
                    "start": 494,
                    "end": 497,
                    "matchedPaperCorpusId": "252923993"
                },
                {
                    "start": 789,
                    "end": 792,
                    "matchedPaperCorpusId": "209376338"
                },
                {
                    "start": 1131,
                    "end": 1134,
                    "matchedPaperCorpusId": "269803965"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.578125
        },
        {
            "corpus_id": "247694098",
            "title": "Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU",
            "text": "Curriculum Learning (CL) is a technique of training models via ranking examples in a typically increasing difficulty trend with the aim of accelerating convergence and improving generalisability. Current approaches for Natural Language Understanding (NLU) tasks use CL to improve in-distribution data performance often via heuristic-oriented or task-agnostic difficulties. In this work, instead, we employ CL for NLU by taking advantage of training dynamics as difficulty metrics, i.e., statistics that measure the behavior of the model at hand on specific task-data instances during training and propose modifications of existing CL schedulers based on these statistics. Differently from existing works, we focus on evaluating models on in-distribution (ID), out-of-distribution (OOD) as well as zero-shot (ZS) cross-lingual transfer datasets. We show across several NLU tasks that CL with training dynamics can result in better performance mostly on zero-shot cross-lingual transfer and OOD settings with improvements up by 8.5% in certain cases. Overall, experiments indicate that training dynamics can lead to better performing models with smoother training compared to other difficulty metrics while being 20% faster on average. In addition, through analysis we shed light on the correlations of task-specific versus task-agnostic metrics.",
            "score": 0.3467761238237365,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60498046875
        },
        {
            "corpus_id": "266166801",
            "title": "Optimizing GPT-2 Pretraining on BabyLM Corpus with Difficulty-based Sentence Reordering",
            "text": "Using word frequencies as a difficulty heuristic can be helpful when training language models with limited data. Training examples with rare words need repeated exposure for effective learning, making them difficult to learn (Platanios et al., 2019). Moreover, limited data can lead to high variance in gradients for rare word embedding due to insufficient contextual information. This suggests that word frequencies can be an effective difficulty heuristic. \n\nGiven a corpus of M sentences, {s i } M i=1 , where each sentence is a sequence of words, s i = {w i 1 , ..., w i N i }, word frequencies are defined as: \n\nwhere j = 1, ...,, #{unique words in corpus} and 1 condition is the indicator function which is equal to 1 if its condition is satisfied and 0 otherwise. Here, we argue that using the product of the unigram probabilities of word frequency counts, which is employed to compute SR, is not an appropriate strategy for aggregating word frequencies into a singular difficulty score for sentences in the BabyLM Corpus. This approach implicitly incorporates sentence length into the difficulty score, resulting in shorter sentences being classified as easy and subsequently overrepresented in the training dataset when sampling from the difficulty-sorted datasets with the competence function. Instead, we propose using the average of the word frequencies as the singular score for sentence difficulty. This ensures that the difficulty metric is independent of sentence length. We thus propose the average sentence rarity difficulty heuristic: \n\nFor the easier sentences to receive a higher score using this metric, we incorporate the \u22121 factor in our difficulty metric. Implementing this difficulty metric along with the competence function to construct the training corpus led to a performance increase of over 3% on BLiMP, reaching 74.51% (Table 3).",
            "score": 0.34557070845542814,
            "section_title": "Average Sentence Rarity",
            "char_start_offset": 14776,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 458
                },
                {
                    "start": 461,
                    "end": 614
                },
                {
                    "start": 617,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1553
                },
                {
                    "start": 1556,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1862
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26025390625
        },
        {
            "corpus_id": "271334531",
            "title": "Quantifying the Role of Textual Predictability in Automatic Speech Recognition",
            "text": "Textual predictability in ASR is typically measured using perplexity as measured by some language model (LM). For the distribution Q induced by an LM, perplexity is the exponent of the negative log likelihood (NLL) Hy of a token sequence y = y1, y2, . . . , yL, formally: \n\nEquation ( 1) is an estimate of the cross-entropy rate Ey[Hy] of Q relative to the population distribution P which generates y [6]. Since a lower Hy implies a higher Q(y), the NLL measures how well Q predicts y, and, if we average Hy over a corpus drawn from P , how well Q predicts P . We expect NLL calculated with respect to Q to be correlated with ASR accuracy: whether the ASR system uses an explicit language model following Q or not, assuming that the system is trained on data following P , the system has an implicit marginal textual distribution Qsys: for a transcription y, and where x \u2208 X is the set of all possible utterances: \n\nBecause of the shared training data, we expect Qsys to be fairly close to both P and to some LM distribution Q. Indeed, NLL was proposed as a measure of the intrinsic difficulty of transcribing an utterance [7], with some attempts at modelling the relationship between ASR error rates ey, 0 \u2264 ey \u2264 1, and Hy [8,9,10]. Klakow and Peters [9] suggest the following power law relationship with fit coefficients a, b \u2208 R, that is, log error rates being proportional to Hy: ey = b exp(aHy). \n\n(  Peters [9], the coefficient a decreases (while b grows) as acoustic conditions become more \"challenging.\" Thus, Eq. ( 3) is unlikely to generalize across corpora. Rather than attempt to link NLL directly to performance, we propose to work using ratios, relating relative predictability to relative performance. Furthermore, we construct a measure which is aggregated over acoustic conditions of increasing difficulty, in an attempt to further factor out the role of acoustics.",
            "score": 0.3446687035125956,
            "section_title": "ASR and textual predictability",
            "char_start_offset": 2235,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 271
                },
                {
                    "start": 274,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 913
                },
                {
                    "start": 916,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1400
                },
                {
                    "start": 1403,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1882
                }
            ],
            "ref_mentions": [
                {
                    "start": 1123,
                    "end": 1126,
                    "matchedPaperCorpusId": "14789841"
                },
                {
                    "start": 1224,
                    "end": 1227,
                    "matchedPaperCorpusId": "5647296"
                },
                {
                    "start": 1227,
                    "end": 1229,
                    "matchedPaperCorpusId": "38297589"
                },
                {
                    "start": 1252,
                    "end": 1255,
                    "matchedPaperCorpusId": "38297589"
                },
                {
                    "start": 1413,
                    "end": 1416,
                    "matchedPaperCorpusId": "38297589"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29296875
        },
        {
            "corpus_id": "264819795",
            "title": "Ling-CL: Understanding NLP Models through Linguistic Curricula",
            "text": "Our work requires the availability of linguistic indices, which in turn requires expert knowledge. Such availability requirements may not be fulfilled in many languages. Nevertheless, some linguistic complexity indices are language independent, such as the commonly-used \"word rarity\" measure, which facilitates extending our approach to other languages. Moreover, our approach relies on the effectiveness of specific linguistic complexity indices for target tasks and datasets employed for evaluation; different linguistic complexity indices may not capture all aspects of linguistic complexity and may yield different results for the same task or dataset. In addition, the incorporation of linguistic complexity indices and the generation of data-driven curricula can introduce additional computational overhead during the training process. Finally, our approach does not provide insights into the the interactions between linguistic indices during training. indices, despite them not being correlated with loss. \n\nFigure 5 illustrates the process of clustering together linguistic indices based on their matching \u03c1 curves. We cluster the indices using hierarchical clustering with complete linkage using the flat clustering method 23 .",
            "score": 0.34314053202294054,
            "section_title": "Limitations",
            "char_start_offset": 28583,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1014
                },
                {
                    "start": 1017,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1238
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09844970703125
        },
        {
            "corpus_id": "278502460",
            "title": "DMRL: Data- and Model-aware Reward Learning for Data Extraction",
            "text": "In this part, we propose Data and Model Hardness Measurement (DMHM) in Algorithm 2, which is a hardness-aware technique that measures data hardness and model hardness to facilitate the process of policy optimization. \n\nAlgorithm 2 Data and Model Hardness Measurement (DMHM) \n\nStep 1: Data Hardness Measurement 6: for (q ji , o ji , o ji ) \u2208 P \u03b8 j do 7: \n\nCalculate similarity score for (S ji , S ji ) according to ( 5) -( 7) \n\n9: \n\nCalculate data hardness \u03b1 D ji based on (8) 10: end for \n\nStep 2: Model Hardness Measurement \n\nCalculate reward gap R ji according to (9) \n\n13: \n\nCalculate the filtered reward gap RP \u03b8 j according to ( 10) -( 11) \n\n14: \n\nCalculate data hardness \u03b1 M j based on (12) 15: end for 16: \n\nText Splitting For each harmful instruction and its corresponding CoD reasoning sequence, we first use the current LLM policy to generate response and construct the the pair-wise response. Formally, given LLM policy \u03c0 \u03b8 , for category j \u2208 [N ] and (q ji , o ji ) \u2208 D j , we obtain o ji \u223c \u03c0 \u03b8 (\u2022|q ji ). Then we get the pair-wise response (q ji , o ji , o ji ) for each question. Similar to the definition of demonstration dataset D j , we define the pair-wise response dataset P \u03b8 j as follows \n\nThen we break down the complex responses into simple, self-contained sub-sentences for preparation of the following text similarity measurement. Specifically, we prompt an open-source large language model, such as LLaMA-3 (Grattafiori et al., 2024), to split \n\n, where K, L denote the the number of the sub-sentences for S ji , S ji . \n\nData Hardness Measurement Then we want to capture the similarity between the response pair via CLIP classifier (Radford et al., 2021). Given each sub-sentence pair (S k ji , S \u2113 ji ), we apply CLIP classifier \u0393 CLIP to calculate similarity score, \n\nFor each sub-sentence S k ji \u2208 S ji , we define the maximal similarity score as follows \n\nThen we will calculate the following overall similarity score for sub-sentence set pair (S ji , S ji ),",
            "score": 0.3429514668378136,
            "section_title": "Data and Model Hardness Measurement",
            "char_start_offset": 8627,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 219,
                    "end": 273
                },
                {
                    "start": 276,
                    "end": 352
                },
                {
                    "start": 355,
                    "end": 424
                },
                {
                    "start": 427,
                    "end": 429
                },
                {
                    "start": 432,
                    "end": 487
                },
                {
                    "start": 490,
                    "end": 524
                },
                {
                    "start": 527,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 644
                },
                {
                    "start": 647,
                    "end": 650
                },
                {
                    "start": 653,
                    "end": 712
                },
                {
                    "start": 715,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1208
                },
                {
                    "start": 1211,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1469
                },
                {
                    "start": 1472,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1794
                },
                {
                    "start": 1797,
                    "end": 1884
                },
                {
                    "start": 1887,
                    "end": 1990
                }
            ],
            "ref_mentions": [
                {
                    "start": 1659,
                    "end": 1681,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1251220703125
        },
        {
            "corpus_id": "265051074",
            "title": "Data Factors for Better Compositional Generalization",
            "text": "We show how models behave differently with different distributions of example difficulties. \n\nSimpler examples make generalization easier on SCAN*. The results are shown in Fig. 5. All the models are tested on the same testing set similar to the original SCAN Jump testing set. For both settings, we can see a steadily decreasing trend when the examples become harder. When the maximum length is reduced from 500 to 62, the performance is increased from 16.65% to 47.31%.9 With only 2 unique primitives per example, the performance is also increased to 49.14%. These results demonstrate that easier examples can make the correct composition easier to learn. \n\nMix of simple and hard examples needed on real language datasets. We next examine the impact of example difficulty on more complex larger-scale natural language datasets. Due to the flexible and diverse nature of natural language in real datasets, models now not only need to understand the correct composition but also need to capture other language uses through potentially non-compositional ways. Therefore, the trends in natural language datasets can be different from the previous observation. For this study, we conduct experiments on ATIS (Price, 1990;Dahl et al., 1994), SM-CalFlow (Andreas et al., 2020) and the compositional version SMCalFlow-CS (Yin et al., 2021). For all three datasets, we train models on multiple subsets with different difficulties but all con- tain 25% of the training examples. Since on these datasets, we no longer have access to the ground truth example complexity, so we present results with prototype-based difficulty in Figure 4. We observe similar but more complicated trends on larger-scale natural language datasets. First, we observe that difficult examples become important on these datasets. On all three datasets, using only the easiest examples leads to the worst results. We suspect that only using the simplest examples does not provide enough coverage for all the diverse linguistic phenomena in the dataset. However, the best performance is also not achieved with the most difficult examples, but at the medium level. Additionally, we notice that even in largerscale natural language datasets, simpler examples are still important for compositional generalization.",
            "score": 0.342691843891771,
            "section_title": "Experiments",
            "char_start_offset": 17954,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 94,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 657
                },
                {
                    "start": 660,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2128
                },
                {
                    "start": 2129,
                    "end": 2275
                }
            ],
            "ref_mentions": [
                {
                    "start": 1206,
                    "end": 1219,
                    "matchedPaperCorpusId": "3047811"
                },
                {
                    "start": 1219,
                    "end": 1237,
                    "matchedPaperCorpusId": "8180378"
                },
                {
                    "start": 1316,
                    "end": 1334,
                    "matchedPaperCorpusId": "235097473"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2333984375
        },
        {
            "corpus_id": "273822116",
            "title": "Training on the Test Model: Contamination in Ranking Distillation",
            "text": "Neural approaches to ranking based on pre-trained language models are highly effective in ad-hoc search. However, the computational expense of these models can limit their application. As such, a process known as knowledge distillation is frequently applied to allow a smaller, efficient model to learn from an effective but expensive model. A key example of this is the distillation of expensive API-based commercial Large Language Models into smaller production-ready models. However, due to the opacity of training data and processes of most commercial models, one cannot ensure that a chosen test collection has not been observed previously, creating the potential for inadvertent data contamination. We, therefore, investigate the effect of a contaminated teacher model in a distillation setting. We evaluate several distillation techniques to assess the degree to which contamination occurs during distillation. By simulating a ``worst-case'' setting where the degree of contamination is known, we find that contamination occurs even when the test data represents a small fraction of the teacher's training samples. We, therefore, encourage caution when training using black-box teacher models where data provenance is ambiguous.",
            "score": 0.3421237703721308,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.155517578125
        },
        {
            "corpus_id": "248810977",
            "title": "Near-Negative Distinction: Giving a Second Life to Human Evaluation Datasets",
            "text": "et al. (2020) propose Masked-Language Model Scoring to repurpose BERT-style NLU models into producing pseudo-log likelihoods shown to measure textual fluency. Although large external language models can be useful at measuring gen-eral language quality, it is challenging for a single model to assess task-specific quality of generated text. In NND, test pairs are targeted at evaluating model performance on specific task skills.\n\nContrastive Learning. The use of negative (or near-negative) candidates in NLG has been explored with recent interest in applying contrastive learning (Chopra et al., 2005) methods to NLG training (He and Glass, 2020; Cao and Wang, 2021). In contrastive learning, a model being trained receives both positive and negative candidates, and has a two-sided objective of increasing likelihood of positive candidates, while reducing the likelihood of negative candidates.\n\nSimilarly, Self-Critical Sequence Training (Rennie et al., 2017;Laban et al., 2021a) is an RL training method in which a model generates several candidates which are scored and contrasted. NND relies on pairs of candidates of differing quality as well, however the framework is focused on evaluation and not model training. Furthermore, training methods rely on automatic metrics to generate or score negative candidates, whereas NND is based on human annotations. When a large number of NND tests available, NND could be compatible with contrastive learning: a portion of the tests can be for model training, while a portion is reserved for evaluation.\n\nLanguage Model Behavioral Analysis. Recent work has built behavioral analysis corpora (Isabelle et al., 2017;Naik et al., 2018;Vig et al., 2020) to evaluate model behavior and bias. For example, in the prompt: \"The nurse said that _ is fine\", a biased model assigns higher likelihood to a stereotypical \"she\" pronoun than an anti-stereotypical pronoun (\"he\", \"it\"). Behavioral analysis corpora rely on a corpus of unit tests, and models are evaluated by the percentage of passed tests. Unlike NND, behavioral analysis often relies on rules or a lexicon to construct the corpus, and is often focused on measuring the effect of a single word or phrase, whereas NND relies on model-generated candidates",
            "score": 0.3418202488959131,
            "section_title": "Related Work",
            "char_start_offset": 21663,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 942,
                    "end": 963,
                    "matchedPaperCorpusId": "206594923"
                },
                {
                    "start": 963,
                    "end": 983,
                    "matchedPaperCorpusId": "234688073"
                },
                {
                    "start": 1663,
                    "end": 1681,
                    "matchedPaperCorpusId": "46932607"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1739501953125
        },
        {
            "corpus_id": "272310219",
            "title": "An Empirical Study of Scaling Laws for Transfer",
            "text": "On first approximation, the difficulty of training a model to perform well on a task appears to be primarily determined by the task's intrinsic complexity. However, the field of natural language processing presents a compelling counterexample. Despite the vast, inherent complexity of natural language, significant developments have been made in automating classic NLP tasks, such as translation and classification, through the development and application of large language models. We think this progress can largely be explained by the vast amount of natural language data freely available on the internet, which has enabled the training of these models on an unprecedented scale. The success in automating NLP tasks suggests that the availability of abundant, relevant training data can help overcome the challenges posed by intrinsically complex tasks. \n\nIn domains where data is less abundant, such as general-purpose robotics, we speculate that the difficulty of achieving high-performance in these domains may be significantly influenced by the transfer gap from a cheap, abundant pre-training distribution to the downstream task of interest. This is because, although collecting finetuning data can be expensive, it may be feasible to leverage transfer learning from much cheaper pre-training distributions with additional model scaling. Indeed, roboticists have identified the hardness of effective sim2real transfer as a key difficulty in making progress in robotics (see, for example, Weng 2019). \n\nGiven that our framework enables the direct measurement of the transfer gap, we believe it can be a valuable tool for estimating the difficulty of achieving high performance on tasks in various domains. By quantifying the transfer gap between cheap, abundant pre-training data (such as internet text data) and the target task data, researchers can gain insights into the potential challenges and feasibility of achieving high performance on specific tasks using machine learning. This information can help guide decisions on resource allocation, such as whether to focus on collecting more task-specific data or investing in larger-scale pre-training. Ultimately, understanding the transfer gap may be key to unlocking progress in domains where data scarcity has been a significant barrier to performance.",
            "score": 0.34181672996581247,
            "section_title": "The transfer gap can set the difficulty of achieving high performance in novel domains",
            "char_start_offset": 25356,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1506
                },
                {
                    "start": 1509,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2160
                },
                {
                    "start": 2161,
                    "end": 2314
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48388671875
        },
        {
            "corpus_id": "271039566",
            "title": "Relative Difficulty Distillation for Semantic Segmentation",
            "text": "Curriculum Learning (CL) [58] is a training strategy that trains a machine learning model from simple samples to difficult samples by mimicking the meaningful learning sequence in a human curriculum.CL can be extended to other methods.For example, Self-paced Learning [59] refers explicitly to a training strategy in which the student network acts as a teacher network and measures the difficulty of training samples based on its losses.The teacher network is pre-trained in the current dataset or other large-scale datasets, and its knowledge is transferred to the curriculum design of the student network.Compared with Self-paced Learning [59], Transfer Teacher [60] selects a mature teacher network to evaluate the difficulty of the training samples.This method addresses the limitation that \"the machine learning network itself may not be mature enough in the initial stage of training to measure sample difficulty accurately.\"Although these CL methods have shown effectiveness and ease of use, they often come with increased training costs.Specifically for semantic segmentation tasks, designing courses or curricula with samples sorted from easy to difficult can lead to additional computational overhead.Additionally, it has been observed in previous studies [61] that CL may lead to the loss of correlation between some samples when applied alongside knowledge distillation methods based on sample relationships or graphs.This correlation loss can result in model degradation.",
            "score": 0.3412602429954208,
            "section_title": "Training strategies based on curriculum learning",
            "char_start_offset": 12467,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 199,
                    "end": 235
                },
                {
                    "start": 235,
                    "end": 437
                },
                {
                    "start": 437,
                    "end": 607
                },
                {
                    "start": 607,
                    "end": 753
                },
                {
                    "start": 753,
                    "end": 931
                },
                {
                    "start": 931,
                    "end": 1045
                },
                {
                    "start": 1045,
                    "end": 1211
                },
                {
                    "start": 1211,
                    "end": 1430
                },
                {
                    "start": 1430,
                    "end": 1484
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 29,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 268,
                    "end": 272,
                    "matchedPaperCorpusId": "9686483"
                },
                {
                    "start": 641,
                    "end": 645,
                    "matchedPaperCorpusId": "9686483"
                },
                {
                    "start": 664,
                    "end": 668,
                    "matchedPaperCorpusId": "51869212"
                },
                {
                    "start": 1266,
                    "end": 1270,
                    "matchedPaperCorpusId": "250631682"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33740234375
        },
        {
            "corpus_id": "265444951",
            "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation",
            "text": "In information theory, entropy is a measure of the information or uncertainty associated with a random variable [28]. In this study, we employed it as an indicator for assessing the difficulty of each sample. Specifically, the entropy of the teacher's softened probability predictions offers insights into how challenging or ambiguous each sample is, which can be calculated by \n\nwhere H T n is the entropy for the n-th sample logits predicted by the teacher T , and p T n,i (T \u2032 ) represents the probability of the i-th class for sample n, softened with an alternative temperature T \u2032 . This T \u2032 is a hyperparameter to finetune the entropy values, ensuring they accurately reflect the difficulty perceived by the teacher for each sample.",
            "score": 0.340807846142792,
            "section_title": "Entropy-Reweighted Knowledge Distillation",
            "char_start_offset": 11066,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 377
                },
                {
                    "start": 380,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 738
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.330810546875
        },
        {
            "corpus_id": "273901573",
            "title": "Curriculum Consistency Learning for Conditional Sentence Generation",
            "text": "CCL integrates a difficulty measure and a proficiency estimator directly derived from the CL losses observed in both the training and validation phases. This innovation eliminates the dependency on task-specific metrics, streamlining the implementation process. CCL strategically guides the model to engage in CL with instances that match its current proficiency level, ensuring more effective and efficient optimization of CL-enhanced CSG models, leading to improved performance. \n\nExperimental evaluation in four CSG tasks, including instruction tuning (IT) tasks (Rao et al., 2024;Fan et al., 2024) for large language models (LLMs) and machine translation (MT) tasks in three distinct modalities (text, vision, and speech) establishes the superior performance of CCL over existing advanced baselines and conventional CL approaches. Specifically, it delivers +2.0 average accuracy point improvement compared with vanilla IT and an average increase of +0.7 in COMET scores over traditional CL methods in MT tasks. This advancement is supported by detailed analyses, which corroborate the effectiveness of CCL in improving the model's ability to handle complex instances, thus confirming the benefits of our method in enhancing the efficiency and efficacy of models in diverse CSG tasks. \n\nOur contributions are summarized as follows: \n\n\u2022 We introduce CCL, an innovative framework for the training of CSG models, which strategically enhances model consistency learning across differentiated training phases. \n\n\u2022 We propose a novel automatic metric for the difficulty of instances and the capability of models within the context of the CL paradigm. \n\n\u2022 We demonstrate that the notable performance improvements by CCL can be largely attributed to its superior handling of challenging samples, such as translating hard sentences. \n\n2 Background and Existing Challenges",
            "score": 0.340376651473154,
            "section_title": "Introduction",
            "char_start_offset": 2089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 480
                },
                {
                    "start": 483,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1287
                },
                {
                    "start": 1290,
                    "end": 1334
                },
                {
                    "start": 1337,
                    "end": 1507
                },
                {
                    "start": 1510,
                    "end": 1647
                },
                {
                    "start": 1650,
                    "end": 1826
                },
                {
                    "start": 1829,
                    "end": 1865
                }
            ],
            "ref_mentions": [
                {
                    "start": 566,
                    "end": 584,
                    "matchedPaperCorpusId": "273162336"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62451171875
        },
        {
            "corpus_id": "267627161",
            "title": "Through the Lens of Split Vote: Exploring Disagreement, Difficulty and Calibration in Legal Case Outcome Classification",
            "text": "Our study is constrained by the datasets, models, and selective prediction techniques under consideration, primarily relying on the ECtHR dataset. \n\nExpanding the investigation to encompass diverse datasets and legal jurisdictions would enhance our understanding of disagreement in judge decision votes and the alignment of perceived difficulty between judges and models. Additionally, due to computational limitations, we are constrained from pre-training language models from scratch or fine-tuning large language models (LLMs). Our study relies on existing pretrained BERT-based models, focusing solely on fine-tuning. We refrain from exploring LLM models, as no widely agreed-upon method for measur-ing calibration for LLMs has emerged at the time of submission. Furthermore, with respect to variations introduced by prompts and data contamination during pretraining, exploring the use of LLMs for difficulty perception and calibration on a smallscale, specialized legal dataset is a distinct research question deserving a separate paper.",
            "score": 0.34025151012835186,
            "section_title": "Limitations",
            "char_start_offset": 31364,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 149,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 1042
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09588623046875
        },
        {
            "corpus_id": "247222685",
            "title": "A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation",
            "text": "imbalanced. To alleviate this issue, we adopt over-sampling for classes with fewer instances.\n\nOur experimental results are shown in Figure 2, from which we find that: (1) For the task of estimating sentence-level difficulty, the shallow neural models perform as well as simple majority model. Only the BERT model can slightly outperform the majority model. (2) For token-level difficulty, these neural models perform slightly better than the majority model. The insignificant improvement over the majority model demonstrate that, the performance of the neural models mainly come from the learning of prior distribution of label instead of extracting difficulty-related features from instances. In the case of label imbalance, the accuracy can not well measure model performance. Besides, in the context of early exiting, we are more interested in cases that the model performs a false exit for an unsolved instance. Thus, we also report the precision, recall, and F1 score on the negative class. As shown in Table 1, all the evaluated models perform poorly on recognizing the incorrectly predicted instances and tokens.\n\nThough, it can not be concluded that the instance difficulty can not be learned since there are still a variety of machine learning models and training techniques that are under explored. Our preliminary experiments demonstrate that, at least, instance difficulty, whether human-defined or modeldefined, is hard to learn for modern neural networks. In fact, our evaluated learn-to-exit models are upper baselines than that used in previous work because: (1) we also adopt more powerful deep models instead of simple linear models in previous methods (Elbayad et al., 2020;Xin et al., 2021), and (2) Different from our method that trains learnto-exit module on development set, previous methods jointly train their learn-to-exit module on the training set where few instances are incorrectly predicted, leading to more serious label imbalance. To facilitate future research, our constructed difficulty datasets will be publicly available.",
            "score": 0.3394020296783013,
            "section_title": "Model-defined Difficulty",
            "char_start_offset": 10584,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1672,
                    "end": 1694,
                    "matchedPaperCorpusId": "204824061"
                },
                {
                    "start": 1694,
                    "end": 1711,
                    "matchedPaperCorpusId": "233189542"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10394287109375
        },
        {
            "corpus_id": "273901292",
            "title": "McGill NLP Group Submission to the MRL 2024 Shared Task: Ensembling Enhances Effectiveness of Multilingual Small LMs",
            "text": "For instance, consider the passage: \"Tom went to the supermarket and bought two apples.\" If the question is \"How many apples did Tom buy?\" and the four options are \"A. 1\", \"B. 2\", \"C. 3\", and \"D. 4\", the system should return \"B\". \n\nCurriculum Learning Curriculum learning (Bengio et al., 2009) is a machine learning strategy that gradually introduces a model to progressively more challenging data pieces over multiple training iterations. This method can often produce better results compared to using a randomly shuffled training set. This approach is effective in the sense that, the model begins by learning general concepts through simpler examples, and then incrementally incorporates more detailed and complex information as more difficult examples are introduced. For our systems, we define \"difficulty\" by the length of the input text, where longer text equates to greater complexity and comes later in the epoch, as shown in Figure 1. Since curriculum learning is a paradigm that focuses solely on the selection and ordering of training data, it can be integrated with various other machine learning techniques, like Interleaving Multilingual Data Pieces which we will introduce later in this section. \n\nKnowledge Transfer Knowledge transfer in multilingual LLMs refers to the model's ability to leverage information, patterns, or representations learned in one language to enhance its performance or understanding in another. This happens because multilingual LLMs develop shared representations of concepts that can be applied across different languages. To facilitate the knowledge transfer for our base models, we fine-tuned the base models on diverse multilingual data. This includes a relatively small amount of data for the target languages, additional data for languages closely related to the target languages, and a large amount of data from high-resource languages like English. \n\nInterleaving Multilingual Data Pieces Interleaving Multilingual Data Pieces is a machine learning technique used to train multilingual models by interleaving data from various languages during training. This approach promotes cross-lingual knowledge transfer by encouraging the model to develop shared linguistic representations and structures, which improves its ability to generalize across languages. It is especially effective in crosslingual information retrieval scenarios, allowing the model to utilize common features across languages and enhance performance in low-resource language settings.",
            "score": 0.33900458285524215,
            "section_title": "Background",
            "char_start_offset": 5382,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 229
                },
                {
                    "start": 232,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1211
                },
                {
                    "start": 1214,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1899
                },
                {
                    "start": 1902,
                    "end": 2104
                },
                {
                    "start": 2105,
                    "end": 2305
                },
                {
                    "start": 2306,
                    "end": 2503
                }
            ],
            "ref_mentions": [
                {
                    "start": 272,
                    "end": 293,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34912109375
        },
        {
            "corpus_id": "276580301",
            "title": "Scaling LLM Pre-training with Vocabulary Curriculum",
            "text": "Curriculum learning [13] progressively increases task difficulty during training to improve model performance. While successful in LLM post-training [15,21], effective curriculum strategies for pretraining remain challenging [16]. Previous attempts at vocabulary-based curricula for decoder-only models found no improvements [19], highlighting the difficulty of designing effective curricula for language model pre-training. Our work addresses these limitations with a novel adaptive approach to vocabulary curriculum.",
            "score": 0.3381699870038287,
            "section_title": "Curriculum Learning",
            "char_start_offset": 4277,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 518
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 24,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39599609375
        },
        {
            "corpus_id": "248810776",
            "title": "Exploring the Learning Difficulty of Data: Theory and Measure",
            "text": "Different studies exhibit different understandings of learning difficulty. A one-sided understanding usually results in a biased measure. (2) There is no formal definition of the easy and hard samples in a learning task. \n\nIn most existing studies, easy and hard samples are heuristically judged. Consequently, it is nearly impossible to conduct a theoretical analysis for difficulty-based weighting strategies with existing heuristic considerations. (3) There has been few experimental studies particularly on the learning difficulty measure. Most studies only refer to the noisy learning or uncertainty settings. An extensive empirical evaluation under different settings is useful for the understanding the learning difficulty. \n\nThis study attempts to establish a preliminary theoretical definition for learning difficulty on the basis of the basic machine learning theory, namely bias-variance trade-off theory, for generalization errors of samples. The proposed definition leverages the optimal model complexity of a sample to capture its learning difficulty, inspired by previous observations that easy samples are modeled by simple patterns [6]. The definitions of easy, medium and hard samples are subsequently proposed based on our theoretical definition. As it is nearly infeasible to calculate the optimal model complexity, a practical approach is proposed to approximate the theoretical definition. Then, the properties for difficulty-based weighting learning methods are explored to support the rationality of the theoretical definitions. Several typical weighting strategies, including SPL and Focal loss, are theoretically analyzed and explained. The results of the extensive experiments verify the superiority of the proposed measure. \n\nOur contributions are summarized as follows: \n\n\u2022 An attempt in theoretical definition of learning difficulty is made based on the basic machine learning theory, namely, bias-variance trade-off. To our knowledge, this work is the first step on the formal description of learning difficulty. \u2022 Formal definitions of easy and hard samples are established. As far as we are aware, this is the first attempt on this formalization. \u2022 A measure of learning difficulty is proposed and is attested to be rational and exceptional. \u2022 The theoretical properties of difficulty-aware weighting strategies in machine learning are explored, and the theoretical explanations based on definition of learning difficulty are presented for several typical weighting methods, which enhance understanding over effective strategies. \n\n2 Related Work",
            "score": 0.33748140519510234,
            "section_title": "Introduction",
            "char_start_offset": 2385,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 220
                },
                {
                    "start": 223,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 730
                },
                {
                    "start": 733,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1751
                },
                {
                    "start": 1754,
                    "end": 1798
                },
                {
                    "start": 1801,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2106
                },
                {
                    "start": 2107,
                    "end": 2179
                },
                {
                    "start": 2180,
                    "end": 2274
                },
                {
                    "start": 2275,
                    "end": 2562
                },
                {
                    "start": 2565,
                    "end": 2579
                }
            ],
            "ref_mentions": [
                {
                    "start": 138,
                    "end": 141,
                    "matchedPaperCorpusId": "221319579"
                },
                {
                    "start": 1149,
                    "end": 1152,
                    "matchedPaperCorpusId": "11455421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.505859375
        },
        {
            "corpus_id": "261557260",
            "title": "Epi-Curriculum: Episodic Curriculum Learning for Low-Resource Domain Adaptation in Neural Machine Translation",
            "text": "We follow the general curriculum learning framework of difficulty measure and training scheduler (Wang et al., 2021b), where difficulty measure determines the relative \"difficulty\" of each data sample and training scheduler decides the sequence of data subsets throughout the training process. Additionally, curriculum learning has been approved as an effective method for data cleaning, and many works have shown that the performance can be improved on denoised data (Wang et al., 2017(Wang et al., , 2019)). Thus, in order to let the model focus on the high-relevant in-domain corpus, curriculum learning is also applied to filter the data. \n\nWe evaluate our Epi-Curriculum on English-German (En-De) and English-Romanian (En-Ro) translation tasks with 10 and 9 different domains. There are 5 seen domains used for training a teacher model, and then individually fine-tuning on the 5 seen and the rest unseen domains. BLEU score (Papineni et al., 2002;Post, 2018) is reported and the experimental results show that Epi-Curriculum improves the model's robustness and adaptability on both seen and unseen domains. For instance, it outperforms the baselines by 1.37 -3.64 on the En-De task and 1.73 -3.32 on the En-Ro task. We further demonstrate the model's robustness to domain shift, where they improve the baseline by 2.55 and 2.59 BLEU scores, respectively. Our contributions mainly lie in three aspects: \n\n\u2022 We propose a novel episodic training framework to handle the model's robustness to domain shift. This is the first work that simulates the situation of unexpected statistics during training for domain adaptation in NMT. \n\n\u2022 Curriculum learning is applied to our training framework. Our curriculum learning not only guides the model from easy to difficult tasks but also denoises the training data. \n\n\u2022 We evaluate Epi-Curriculum on two language pairs with ten and nine different domains, empirically showing the strength of our proposed approach. \n\n2 Related Work",
            "score": 0.3374813049941395,
            "section_title": "Introduction",
            "char_start_offset": 3566,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1407
                },
                {
                    "start": 1410,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1631
                },
                {
                    "start": 1634,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1809
                },
                {
                    "start": 1812,
                    "end": 1958
                },
                {
                    "start": 1961,
                    "end": 1975
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 117,
                    "matchedPaperCorpusId": "232362223"
                },
                {
                    "start": 468,
                    "end": 486,
                    "matchedPaperCorpusId": "1054586"
                },
                {
                    "start": 486,
                    "end": 508,
                    "matchedPaperCorpusId": "174798125"
                },
                {
                    "start": 930,
                    "end": 953,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 953,
                    "end": 964,
                    "matchedPaperCorpusId": "13751870"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57958984375
        },
        {
            "corpus_id": "251622541",
            "title": "PCC: Paraphrasing with Bottom-k Sampling and Cyclic Learning for Curriculum Data Augmentation",
            "text": "Data augmentation techniques create artificial data mixed with the original data for improved performance. Traditional data augmentation techniques in the language community include word-level perturbation such as synonym replacement, random insertion, random swap, and random deletion (Wei and Zou, 2019). Sentence-level techniques such as Round-trip Translation (Sennrich et al., 2016b) exploits the use of machine translation models to * The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200719). translate the input sentence to another language before translating back to the source language which can be essentially treated as a form of paraphrasing.\n\nCurriculum learning presents training instances in a meaningful order with increasing difficulties to neural models for a boost in performance. Traditional curriculum learning (Bengio et al., 2009;Liu et al., 2018Liu et al., , 2020Platanios et al., 2019;Xu et al., 2020a,b;Su et al., 2021) categorizes the original training instances into different levels of difficulties to be gradually presented to the model where a core component called difficulty measure, which is usually defined as a numerical number where a bigger number indicates a more difficult sample.\n\nCombining the merits of the above two mentioned techniques, Curriculum Data Augmentation (CDA) creates synthetic data with increasing levels of difficulties to be presented to our neural models. Existing CDA defines the ratio of the words perturbation as the difficulty measure for curriculums and a gradual course which increases the difficulty of curriculums when the training loss plateaus (Wei et al., 2021), which then ends when the most challenging curriculum ends. Although existing CDA is effective, yet there are several disadvantages. First, it employs word-level perturbation. This superficial operation keeps the augmentation to have a similar sentence structure as the original one. Next, it employs random insertion, random swap, and random deletion for augmentation. Although this can be durable as for text classification (Wei et al., 2021), this is not suitable for generation tasks, particularly when many words are perturbed, which can even easily break the sentence grammar. Third, it uses a gradual course",
            "score": 0.33724394599038077,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 957,
                    "end": 978,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2447509765625
        },
        {
            "corpus_id": "246652398",
            "title": "Transferable Student Performance Modeling for Intelligent Tutoring Systems",
            "text": "Student performance models estimate a student's ability to solve different questions based on sequential log data that describes their prior interactions with the system. The student proficiency estimates produced by such performance models are a key component, which allows the ITS to adapt to each student's personal needs as they go through the curriculum [12]. In the literature there are three major categories of student performance modeling techniques: (i) Markov process based inference, (ii) logistic regression and (iii) deep learning based approaches. Markov process based techniques, such as Bayesian Knowledge Tracing (BKT) [11] and BKT+ [21], are well established and can for example be found in the Cognitive Tutor [25]. Most probabilistic approaches determine a student's proficiency level by performing inference in a two state Hidden Markov Model -one state to represent mastery and one for non-mastery. Logistic regression models rely on a set of manually specified features which summarizes the student's interaction sequence. Given an input vector with feature values, the regression based performance model estimates the probability that the student is proficient in a certain question or KC. Some approaches in this class are IRT [40], PFA [36], DAS3H [9], Best-LR [16] and its recent extension Best-LR+ [42]. Deep learning based approaches take as input the same interaction sequence data, but unlike logistic regression techniques can learn suitable features on their own without requiring human feature engineering. Deep learning models benefit from large-scale training data and might in the future also include additional video and text data into their performance predictions. As of today, BKT and logistic regression models are still competitive with deep learning based approaches in multiple domains [16,21,42]. Two comprehensive surveys on recent deep learning based performance models are provided by Liu et al. [26] and Sarsa et al. [41]. \n\nImportantly, all of the above mentioned student performance modeling approaches rely on course-specific parameters (e.g., parameters that represent the difficulty of individual questions and KCs in the target course) which need to be learned from target course data. This makes these models inapplicable in our coldstart setting where a new course is first introduced and there is no training data available.",
            "score": 0.3371291277775321,
            "section_title": "Student Performance Modeling",
            "char_start_offset": 8573,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1973
                },
                {
                    "start": 1976,
                    "end": 2242
                },
                {
                    "start": 2243,
                    "end": 2384
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 363,
                    "matchedPaperCorpusId": "14826104"
                },
                {
                    "start": 637,
                    "end": 641,
                    "matchedPaperCorpusId": "19228797"
                },
                {
                    "start": 651,
                    "end": 655,
                    "matchedPaperCorpusId": "3587872"
                },
                {
                    "start": 1263,
                    "end": 1267,
                    "matchedPaperCorpusId": "16644912"
                },
                {
                    "start": 1275,
                    "end": 1278,
                    "matchedPaperCorpusId": "155099780"
                },
                {
                    "start": 1288,
                    "end": 1292,
                    "matchedPaperCorpusId": "226253566"
                },
                {
                    "start": 1327,
                    "end": 1331,
                    "matchedPaperCorpusId": "237420476"
                },
                {
                    "start": 1832,
                    "end": 1836,
                    "matchedPaperCorpusId": "226253566"
                },
                {
                    "start": 1836,
                    "end": 1839,
                    "matchedPaperCorpusId": "3587872"
                },
                {
                    "start": 1839,
                    "end": 1842,
                    "matchedPaperCorpusId": "237420476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1954345703125
        },
        {
            "corpus_id": "265696189",
            "title": "Teaching Specific Scientific Knowledge into Large Language Models through Additional Training",
            "text": "Most models in Fig. 3a that showed a score of zero had experienced divergence in the training loss, indicating a breakdown during learning. These models tended to exhibit internal variable overflow errors or produce meaningless text strings (e.g., \"oooooo\") during inference. The implications of these results are significant for fine-tuning LLMs in complex learning environments, pointing to the importance of carefully calibrating learning rates in response to the volume and relevance of training data.",
            "score": 0.33684288277671404,
            "section_title": "Knowledge addition via LoRA-type training",
            "char_start_offset": 26167,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 505
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33349609375
        },
        {
            "corpus_id": "263829596",
            "title": "FTFT: efficient and robust Fine-Tuning by transFerring Training dynamics",
            "text": "Note that a training instance can be categorized as both hard-tolearn and ambiguous (a low mean but high variance for p true ). Finally, the main model is fine-tuned only on the ambiguous or hard-to-learn subset. Swayamdipta et al. (2020) show that, with a slight loss of In-Distribution (ID) performance, dataset cartography improves Out-Of-Distribution (OOD) performance of models. In this paper, we train main models with ambiguous data, since Swayamdipta et al. (2020) reported better performance using this data than hard-to-learn data. Swayamdipta et al. (2020) use the same PLM as both the reference and the main model. In contrast, Sar-Shalom and Schwartz (2023) show that a DM constructed by ELECTRA Large (Clark et al., 2020) can be used to improve the robustness of DeBERTaV3 Large (He et al., 2023). However, instead of using only the ambiguous subset, they added k copies of this subset to the original training set to train the main model. Moreover, they did not investigate either DM transfer across model sizes and pretraining methods, or how such transferability can be exploited to improve efficiency. \n\nModel-Based Data Selection/Reweighing Our work is also connected to studies that have selected or reweighed data using reference models to improve in-distribution (ID) performance. Chang et al. (2017) use p true variance and proximity to the classification threshold from a reference model to reweigh training instances; Toneva et al. (2019) calculate the frequency of forgetting events (i.e. from a correct to incorrect prediction), and remove the least forgettable instances; Paul et al. (2021) and Baldock et al. (2021) instead use error vector norm and effective prediction depth to estimate the contribution of a training instance. \n\nPrevious studies have also explored the use of a smaller reference model to improve efficiency. Coleman et al. (2020) use a small model for active learning and core-set selection. Xie et al. (2023) reweigh domains for language model pretraining, by training a small reference model to estimate the difficulty of each domain.",
            "score": 0.33560239656203195,
            "section_title": "Background",
            "char_start_offset": 3799,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1119
                },
                {
                    "start": 1122,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1758
                },
                {
                    "start": 1761,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2085
                }
            ],
            "ref_mentions": [
                {
                    "start": 213,
                    "end": 238,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 447,
                    "end": 472,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 542,
                    "end": 567,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 640,
                    "end": 670,
                    "matchedPaperCorpusId": "259859071"
                },
                {
                    "start": 715,
                    "end": 735,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 793,
                    "end": 810,
                    "matchedPaperCorpusId": "244346093"
                },
                {
                    "start": 1303,
                    "end": 1322,
                    "matchedPaperCorpusId": "20785832"
                },
                {
                    "start": 1443,
                    "end": 1463,
                    "matchedPaperCorpusId": "55481903"
                },
                {
                    "start": 1600,
                    "end": 1618,
                    "matchedPaperCorpusId": "235898952"
                },
                {
                    "start": 1623,
                    "end": 1644,
                    "matchedPaperCorpusId": "235458180"
                },
                {
                    "start": 1857,
                    "end": 1878,
                    "matchedPaperCorpusId": "195750622"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.407958984375
        },
        {
            "corpus_id": "271855813",
            "title": "Building Decision Making Models Through Language Model Regime",
            "text": "The conventional method for aligning language models with downstream tasks, such as decision making, employs supervised fine-tuning (SFT). We introduce a training paradigm named Learning then Using(LTU), with separate learning and using phase. \n\nLearning in LTU. Our approach involves an initial learning phase with continued pre-training on large language models (LLMs). Continue pre-training(CT) is widely used to enhence abilities of different domains or languages based on already well-trained large language models (Cui et al., 2023;Rozi\u00e8re et al., 2023). The basic idea of LTU method is LLMs are able to learn inherent patterns and statistics correlations via decision making knowledge formulated as (s, a, r). Through continued pre-training, we can integrate this collective decision making intelligence into a LLM , transforming it into a comprehensive foundation decision making model which is suitable for various downstream tasks. We construct the data format in (s, a, r) pairs as we mentioned in 3.2. We use this part of data to do an auto-regression training following the Eq 1 based on a LLM. After the learning phase, we get our foundation decision making model. \n\nUsing in LTU. The using phase is a classic supervised fine-tuning phase. Supervised fine-tuning(SFT) has emerged as a powerful and effective technique to adapt pre-trained LLMs to specific downstream tasks through supervised learning. In this part, we leverage the foundation decision making model trained in learning phase and learn to predict P (r|s, a) to solve certain decision making tasks.",
            "score": 0.3350178839788255,
            "section_title": "Training Paradigm",
            "char_start_offset": 11138,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 243
                },
                {
                    "start": 246,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1178
                },
                {
                    "start": 1181,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1576
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.310546875
        },
        {
            "corpus_id": "272653794",
            "title": "AutoIRT: Calibrating Item Response Theory Models with Automated Machine Learning",
            "text": "Explanatory IRT models that use item features as in (2) have been used for decades. Notably, the Log Linear Trait Model (LLTM) of (Fischer 1973) is an extension of the Rasch model, where the item features predict the item difficulty in a linear model (see also (De Boeck and Wilson 2004)). \n\nRecent extensions of this feature based approach utilize pretrained language models such as BERT (Devlin et al. 2019) to calibrate items from limited response data. (Benedetto et al. 2021) predicted item difficulty using a finetuned BERT model from student responses to multiple-choice questions. (Byrd and Srivastava 2022) took a similar approach to predicting difficulty and discrimination for 2PL IRT models from natural language questions. In that work, they constructed linguistically motivated features, such as semanticambiguity, as well as contextual embeddings via BERT. In (Reyes et al. 2023), they use RNNs to predict difficulty that is fitted in advance from a Rasch model. Most of these approaches predict item parameters that have been fit on an existing item bank with many responses as training data. To train such a model, one needs to already have a large item bank, with hundreds of responses per item, to obtain the necessary training data. For this reason, we do not include them in our comparisons. \n\nAn alternative approach is to train an explanatory IRT model by fitting the non-parametric terms, e.g., f d , using a NNet. This approach trains a NNet to predict grades from features and the ability \u03b8 while constraining the model to take the IRT form. BERT-LLTM was proposed in (McCarthy et al. 2021) to train LLTM models by using BERT embeddings as features along with hand-crafted linguistic features. More recently, (Yancey et al. 2024) proposed BERT-IRT which fits a 2PL model by constraining the NNet to take the form of ( 1) and ( 2). In that work, they show that BERT-IRT can accurately train explanatory IRT models with only a handful of test taker responses per item, and apply it to English language proficiency testing.",
            "score": 0.33499412270937157,
            "section_title": "Feature-based IRT Models and BERT-IRT",
            "char_start_offset": 8244,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 289
                },
                {
                    "start": 292,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1312
                },
                {
                    "start": 1315,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 389,
                    "end": 409,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 457,
                    "end": 480,
                    "matchedPaperCorpusId": "233365134"
                },
                {
                    "start": 589,
                    "end": 615,
                    "matchedPaperCorpusId": "248780313"
                },
                {
                    "start": 875,
                    "end": 894,
                    "matchedPaperCorpusId": "261529772"
                },
                {
                    "start": 1594,
                    "end": 1616,
                    "matchedPaperCorpusId": "243865410"
                },
                {
                    "start": 1735,
                    "end": 1755,
                    "matchedPaperCorpusId": "270765517"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10125732421875
        },
        {
            "corpus_id": "278033087",
            "title": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training",
            "text": "Although large language models (LLMs) have recently achieved remarkable performance on various complex reasoning benchmarks, the academic community still lacks an in-depth understanding of base model training processes and data quality. To address this, we construct a large-scale, difficulty-graded reasoning dataset containing approximately 3.34 million unique queries of varying difficulty levels and about 40 million distilled responses generated by multiple models over several passes. Leveraging pass rate and Coefficient of Variation (CV), we precisely select the most valuable training data to enhance reasoning capability. Notably, we observe a training pattern shift, indicating that reasoning-focused training based on base models requires higher learning rates for effective training. Using this carefully selected data, we significantly improve the reasoning capabilities of the base model, achieving a pass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This result surpasses most current distilled models and closely approaches state-of-the-art performance. We provide detailed descriptions of our data processing, difficulty assessment, and training methodology, and have publicly released all datasets and methods to promote rapid progress in open-source long-reasoning LLMs. The dataset is available at: \\href{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}",
            "score": 0.3344649274716484,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69677734375
        },
        {
            "corpus_id": "273351402",
            "title": "3DS: Decomposed Difficulty Data Selection's Case Study on LLM Medical Domain Adaptation",
            "text": "To validate the effectiveness of each difficulty metric in our decomposed difficulties, we conduct ablation studies by removing each of the three metrics-Instruction Understanding Difficulty, Response Confidence Difficulty, and Response Correctness Difficulty. As shown in Table 3 and Table 4, in general, removing any single component result in noticeable performance drops on some evaluation metrics for all three models, indicating a decline in certain aspects of the model's medical domain abilities. For instance, the exclusion of Response Confidence Difficulty leads to a noticeable decrease in the performance of both Baichuan2-7B-Chat and Baichuan2-13B-Chat across all evaluation metrics. Similarly, Qwen-1.5-7B-chat's performance drops on CMB-Clin. These observations validate the necessity of each difficulty metric in identifying beneficial data samples for enhancing LLM's domain abilities. Overall, the combination of these difficulty metrics contributes to a more accurate data difficulty measurement, ensuring that selected data matches the model's learning capacity and optimally enhances its domain performance.",
            "score": 0.33418562268932683,
            "section_title": "ABLATION STUDIES",
            "char_start_offset": 23393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1128
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.393798828125
        },
        {
            "corpus_id": "247450945",
            "title": "Uncertainty Estimation for Language Reward Models",
            "text": "Our work seeks to develop techniques for uncertainty estimation and active learning for fine-tuning pre-trained language models. Uncertainty estimation [13] and active learning [37] are both wellestablished fields, while fine-tuning language models is a new but popular area of research [5]. However, to the best of our knowledge the only prior work for uncertainty in fine-tuned language models is limited to calibrating the total uncertainty of these models [10,19,21,20]. \n\nWhile calibrated total uncertainty is helpful, many applications (such as active learning) depend on separately estimating aleatoric (or data) uncertainty and epistemic (or model) uncertainty [18]. Aleatoric uncertainty is due to irreducible randomness in the data generating process. By contrast, epistemic uncertainty is due to a lack of knowledge and could be reduced by seeing more data points. \n\nUncertainty estimation for deep learning [13] seeks to provide well-calibrated estimates of epistemic and aleatoric uncertainty for neural networks. Ensembles are a simple and popular approach [23]: training multiple models, with their disagreement estimating model uncertainty. They have been found to be a strong baseline in a variety of applications [3,47,30,15], although they have been criticized on both a theoretical [28] and empirical [29] basis. Hypermodels are a related approach [16,11], generating the weights of another network as a function of some noise variable, effectively simulating a very large ensemble. Bayesian neural networks are an alternative class of approaches that learn a distribution over the weights of a neural network [4,12]. \n\nActive learning [37] seeks to achieve greater sample efficiency by having the algorithm select which data points to ask a human annotator to label. One effective heuristic selects the point with maximal epistemic uncertainty, i.e. the point the model is most unsure about, making active learning and uncertainty estimation intricately connected. This is related to the notion of query-by-committee [40]: selecting points where a \"committee\" of models most disagree, whether a literal ensemble of models or a generative model class such as a hypermodel or Bayesian neural network.",
            "score": 0.3337607829143476,
            "section_title": "Related Work",
            "char_start_offset": 5039,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 474
                },
                {
                    "start": 477,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 875
                },
                {
                    "start": 878,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1637
                },
                {
                    "start": 1640,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2219
                }
            ],
            "ref_mentions": [
                {
                    "start": 669,
                    "end": 673,
                    "matchedPaperCorpusId": "216465307"
                },
                {
                    "start": 1240,
                    "end": 1243,
                    "matchedPaperCorpusId": "174798361"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.308837890625
        },
        {
            "corpus_id": "261076515",
            "title": "From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning",
            "text": "In our study, we introduce a novel approach for autonomously identifying the most impactful training samples, which we refer to as \"cherry data\", from extensive open-source datasets. These data samples are particularly effective in enhancing LLM instruction tuning. Central to our hypothesis is the idea that LLMs, through initial training with a small amount of instruction data, can inherently learn to discern and follow instructions, allowing them to estimate the difficulty of instruction data. \n\nOur method involves a self-guided process that begins with familiarizing the model with a small subset of the dataset during the \"Learning from Brief Experience\" phase. This phase lays the groundwork for the subsequent \"Evaluating Based on Experience\" phase, where we introduce the Instruction-Following Difficulty (IFD) score. This metric evaluates how much help the instruction provides to the generation of the corresponding response, by comparing the loss in model responses with and without instructional context. The higher IFD score, indicating less instructional help, suggests a greater difficulty with instructions. On the contrary, the lower IFD score represents that the given instruction can directly benefit the language model largely even without further training, representing the easiness and necessity of the instruction. Thus in the final \"Retraining from Self-Guided Experience\" phase, we use data with relatively large IFD scores as the cherry data to train our model, resulting in what we term \"cherry models\". This methodology, which emphasizes data quality over quantity, differs markedly from existing techniques that rely on external models for data curation. \n\nExtensive experimental results validate the efficacy of our method. By applying our methodology to the Alpaca and WizardLM instruction tuning datasets, our model outperforms the official Alpaca model with only approximately 5% data selected and outperforms the reimplemented WizardLM model with approximately 10% data selected. The key contributions of this paper: \n\n\u2022 We propose a self-guided approach enabling models to autonomously select the \"cherry data\" from vast open-source datasets. This innovation minimizes manual curation and optimizes the use of existing data resources, reducing costs and streamlining training.",
            "score": 0.3329024473704189,
            "section_title": "Introduction",
            "char_start_offset": 1752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 499
                },
                {
                    "start": 502,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1687
                },
                {
                    "start": 1690,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2054
                },
                {
                    "start": 2057,
                    "end": 2181
                },
                {
                    "start": 2182,
                    "end": 2315
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48095703125
        },
        {
            "corpus_id": "261076116",
            "title": "Curriculum Learning with Adam: The Devil Is in the Wrong Details",
            "text": "Common difficulty measures in NLP include the sequence lengths of an input (or the closely related depth of the parse tree) (Tay et al. 2019;Mart\u00ednez Alonso et al. 2017;Platanios et al. 2019), the number of coordinating conjunctions (Kocmi and Bojar 2017) or the diversity of the used vocabulary (Platanios et al. 2019). Schedule functions typically expand the data distribution towards more difficult examples monotonically, either as a step-function (Bengio et al. 2009;Spitkovsky, Alshawi, and Jurafsky 2010;Kocmi and Bojar 2017) or continuously (Hacohen and Weinshall 2019;Platanios et al. 2019;Penha and Hauff 2020;Liu et al. 2018). Examples of step functions can be seen in Figure 6a. Hand-crafted curricula have the advantage of being cheap and easy to implement. On the other hand, the choice of the correct setup requires experience or expert domain knowledge, idiosyncracies of data and tasks make them potentially difficult to generalise and the method is 'coarse', such that it is limited to the predefined structure and cannot dynamically adapt to the current state of the learner. \n\nAutomated curricula There are different approaches to addressing the shortcomings of hand-crafted curricula. We coarsely bin them into (1) non-parametric and (2) parametric solutions. Fan et al. 2018). The predicted utility is then used to optimise the learning process. As they require no manual work, end-to-end approaches are convenient. However, they come oftentimes with the high computational cost of optimising 'teacher' models, making them too expensive to optimise with large target models. \n\nTheoretical underpinnings Theoretical explanations of the efficiency of curriculum learning remain relatively sparse. \n\nThe two most referred-to explanations can be found in Bengio et al. (2009) which state that CL helps 1) with denoising the dataset and 2) by smoothening of the non-convex optimisation landscape (as a form of continuation method; compare Allgower and Georg 1980).",
            "score": 0.33288592142048956,
            "section_title": "Introduction",
            "char_start_offset": 3828,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 1094
                },
                {
                    "start": 1097,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1596
                },
                {
                    "start": 1599,
                    "end": 1716
                },
                {
                    "start": 1719,
                    "end": 1981
                }
            ],
            "ref_mentions": [
                {
                    "start": 124,
                    "end": 141,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 141,
                    "end": 169,
                    "matchedPaperCorpusId": "16384032"
                },
                {
                    "start": 169,
                    "end": 191,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 296,
                    "end": 319,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 452,
                    "end": 472,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 472,
                    "end": 511,
                    "matchedPaperCorpusId": "1363892"
                },
                {
                    "start": 549,
                    "end": 577,
                    "matchedPaperCorpusId": "51876228"
                },
                {
                    "start": 577,
                    "end": 599,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 599,
                    "end": 620,
                    "matchedPaperCorpusId": "215746703"
                },
                {
                    "start": 620,
                    "end": 636,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 1281,
                    "end": 1297,
                    "matchedPaperCorpusId": "5933530"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.256591796875
        },
        {
            "corpus_id": "266182457",
            "title": "Findings of the BabyLM Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora",
            "text": "CLIMB (Martinez et al., 2023). This submission presents a thorough comparison of different approaches to curriculum learning in the Strict-Small setting. They consider three main criteria for curriculum design: the size of the input vocabulary, the difficulty of the training sample, and the size of the output space for MLM prediction. They conduct experiments exploring eight different curricula sorted into these three main approaches. While there are many small differences in performance among these settings, curricula provide no consistent improvements over more naive training algorithms. \n\nAcquiring Linguistic Knowledge from Multimodal Input (Amariucai and Warstadt, 2023). The authors explored whether vision-language co-training helps the learning of linguistic knowledge. They trained models on Wiki texts with images using the state-of-the-art multi-modality model (FLAVA). After varying the amount of training data and how many images are used, the authors found that visual input only provides a slight improvement on grammar benchmarks for 10M-word training, but not for 100M-word training. \n\nGPT-like Models are Bad Babies (Steuer et al., 2023). This paper trains a decoder-only model, trying different hyperparameters, including reordering the training data by different orders (based on cues which did not improve over regular shuffling), different sizes, layer widths, among other features. The main focus of the paper is to test if models that perform better on BabyLM evaluation tasks are also better at modeling reading difficulty in humans. Surprisingly, models performing better on BabyLM tasks performed less well in modeling reading difficulty. \n\nBaby's CoThought (Zhang et al., 2023). This system leverages a large language model, GPT-3.5-Turbo, to reformat semantically unrelated sentences into cohesive paragraphs. In low-data settings, this approach can form better training examples for language models; the proposed approach results in improvements across BLiMP tasks, though performance is not significantly different on (Super)GLUE or MSGS. Note that the LLM is trained on far more than 100M words, so this submission technically does not qualify under any track.",
            "score": 0.33287060495351173,
            "section_title": "F Summary of Each Submission",
            "char_start_offset": 56718,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 596
                },
                {
                    "start": 599,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1672
                },
                {
                    "start": 1675,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2199
                }
            ],
            "ref_mentions": [
                {
                    "start": 6,
                    "end": 29,
                    "matchedPaperCorpusId": "265213396"
                },
                {
                    "start": 1692,
                    "end": 1712,
                    "matchedPaperCorpusId": "259188012"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1436767578125
        },
        {
            "corpus_id": "258378286",
            "title": "Linguistic Constructs Represent the Domain Model in Intelligent Language Tutoring",
            "text": "All learner answers and all requested hints to each exercise are recorded. A learner may attempt to answer each exercise multiple times. For each attempt, Revita analyzes the answers and the requested hints to calculate credits and penalties for the corresponding language constructs. Partial correctness of answers is taken into account, e.g., if the answer used the correct tense but wrong number, only number will be penalized, and tense will receive credit. \n\nThe collected information on performance with constructs is used to model learner skills and the difficulty of the constructs. To model learner skills and exercise difficulty, we use Item Response Theory (IRT) (Embretson and Reise, 2013;van der Linden and Hambleton, 2013). IRT comes from psychometrics and is widely used in education (Klinkenberg et al., 2011). The Item in IRT is a task that the learner should solve. Most IRT applications have a clear definition of an item, and a clear credit standard. The classic example of an item is a test question in mathematics: it is unambiguous and there is a clear judgment of the answer-correct or wrong. Our major challenge is that language constructs are not directly judged, as test items in other learning domains. It is challenging to determine the credit and penalty for each construct based on the student's answer, because the link from exercise to constructs is one-to-many. \n\nWe leave the details of modeling difficulty with IRT outside the scope of this paper. To date, we have collected 570K answers for Russian exercises. Experiments with this data show a strong correlation between students' proficiency estimated by IRT vs. by their teachers. This suggests that with IRT we are able to reliably model learner proficiency. Interestingly, the estimates of exercise difficulty do not correlate with teachers' judgments, which agrees with the findings of other researchers (Abbakumov and Lebedeva, 2016). \n\nAt present, we assume that the difficulty of an exercise depends on the hardest construct linked to it. Thus, exercises are sampled for practice based on the difficulty of the hardest construct linked to each exercise. The difficulty of constructs is modeled by IRT. We aim to provide exercises that are best suited to each student's proficiency level.",
            "score": 0.3312302983682669,
            "section_title": "Learner Modeling and Exercise Sampling",
            "char_start_offset": 18593,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 461
                },
                {
                    "start": 464,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1395
                },
                {
                    "start": 1398,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1927
                },
                {
                    "start": 1930,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2148
                },
                {
                    "start": 2149,
                    "end": 2196
                },
                {
                    "start": 2197,
                    "end": 2282
                }
            ],
            "ref_mentions": [
                {
                    "start": 799,
                    "end": 825,
                    "matchedPaperCorpusId": "12579925"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0880126953125
        },
        {
            "corpus_id": "273532238",
            "title": "Beware of Calibration Data for Pruning Large Language Models",
            "text": "Recently, Large Language Models (LLMs) have exhibited remarkable performance and enormous potential in Natural Language Processing (NLP) and Artificial Intelligence (AI) (OpenAI, 2022;2023;Bubeck et al., 2023;Yang et al., 2023). The success of LLMs is closely tied to scaling laws (Kaplan et al., 2020;Hoffmann et al., 2022): training language models with more parameters, using more data and greater computational resources leads to more powerful capabilities. However, LLMs with more parameters increase the difficulty and cost of deployment and inference. Therefore, much work has been devoted to compressing LLMs to achieve a trade-off between efficiency and performance, such as pruning (Frantar & Alistarh, 2023;Ma et al., 2023;Xia et al., 2024) and quantization (Frantar et al., 2023;Huang et al., 2024;Shao et al., 2024). \n\nPruning is a model compression technique that has evolved over many years (LeCun et al., 1989) and remains full of potential and challenges. Based on the over-parameterization of neural networks, it aims to remove redundant parameters while minimizing the degradation of model performance. Pruning has been successfully applied to compress small to medium-sized neural networks. Through sparse training (Lee et al., 2019;Frankle & Carbin, 2019;Yuan et al., 2021;Lasby et al., 2024) or pruning-aware training (Sanh et al., 2020;Lagunas et al., 2021;Jiang et al., 2023) methods, it can achieve performance comparable to dense models with a high sparsity ratio (\u226570%). However, these methods require iterative training, which is costly and time-consuming for LLMs with billions of parameters. As a result, post-training pruning that does not require iterative training has become the preferred approach for pruning LLMs. \n\nThe challenge of post-training pruning is how to perform training-free parameter importance estimation.",
            "score": 0.33098859787291274,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 829
                },
                {
                    "start": 832,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1749
                },
                {
                    "start": 1752,
                    "end": 1855
                }
            ],
            "ref_mentions": [
                {
                    "start": 718,
                    "end": 734,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 734,
                    "end": 751,
                    "matchedPaperCorpusId": "263830786"
                },
                {
                    "start": 769,
                    "end": 791,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 791,
                    "end": 810,
                    "matchedPaperCorpusId": "267523201"
                },
                {
                    "start": 810,
                    "end": 828,
                    "matchedPaperCorpusId": "261214575"
                },
                {
                    "start": 906,
                    "end": 925,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 1235,
                    "end": 1253,
                    "matchedPaperCorpusId": "52920837"
                },
                {
                    "start": 1253,
                    "end": 1276,
                    "matchedPaperCorpusId": "53388625"
                },
                {
                    "start": 1276,
                    "end": 1294,
                    "matchedPaperCorpusId": "239998338"
                },
                {
                    "start": 1294,
                    "end": 1313,
                    "matchedPaperCorpusId": "258461498"
                },
                {
                    "start": 1340,
                    "end": 1359,
                    "matchedPaperCorpusId": "218665313"
                },
                {
                    "start": 1359,
                    "end": 1380,
                    "matchedPaperCorpusId": "237485472"
                },
                {
                    "start": 1380,
                    "end": 1399,
                    "matchedPaperCorpusId": "252846209"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2421875
        },
        {
            "corpus_id": "265552007",
            "title": "The Efficiency Spectrum of Large Language Models: An Algorithmic Survey",
            "text": "Curriculum learning [18,75] is a strategy that aims to improve the model training efficiency by carefully designing the feeding order of the instances in the training data. The principle of this approach is to initiate training with simpler samples or subtasks and progressively escalate to more challenging ones. Two critical components are integral to the design of a curriculum learning method. (i) The difficulty metric (or difficulty criterion), responsible for ranking the training samples based on their complexity. This metric serves as a guide to categorize training instances from the simplest to the most complex. (ii) The pacing function (also known as curriculum scheduler or arrangement), which determines the rate at which these ranked samples are fed to the model training. This function modulates the learning curve, ensuring that the model is not overwhelmed by the complexity of the tasks too early. Difficulty Metric. In the realm of curriculum learning for natural language processing, the most widely used difficulty metric is perhaps the sequence length [84,157,158]. The underlying assumption is that processing longer sentences poses greater challenges than shorter ones. Another prevalent metric is vocabulary rarity [285,334], based on the intuition that sentences with less frequently used words in the training set are inherently more complex to comprehend. In addition, this metric could be measured by the uncertainty sampling principle in active learning, where the uncertainty indicated by other pre-trained models could serves as a gauge of difficulty as well. \n\nFor fine-tuning in specific downstream tasks, researchers have innovated task-specific difficulty metrics. A notable example is in paraphrase generation, where Kadotani et al. [124] proposed using the edit distance between paraphrased sentence pairs as a metric, approximating the extent of required transformations. These custom metrics, tailored to the nuances of specific tasks like paraphrase generation, often outperform general metrics such as sentence length or word rarity. In the context of neural machine translation (NMT), defining sample difficulty is more complex. In addition to sentence length and word rarity, some studies [139] have incorporated linguistically-motivated features. For instance, the number of coordinating conjunctions in a target sentence has been used to estimate sentence complexity.",
            "score": 0.33092007672030155,
            "section_title": "Curriculum Learning",
            "char_start_offset": 27786,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1594
                },
                {
                    "start": 1597,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2078
                },
                {
                    "start": 2079,
                    "end": 2174
                },
                {
                    "start": 2175,
                    "end": 2294
                },
                {
                    "start": 2295,
                    "end": 2416
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 24,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 24,
                    "end": 27,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 1077,
                    "end": 1081,
                    "matchedPaperCorpusId": "255185900"
                },
                {
                    "start": 1085,
                    "end": 1089,
                    "matchedPaperCorpusId": "252917575"
                },
                {
                    "start": 1248,
                    "end": 1252,
                    "matchedPaperCorpusId": "211102338"
                },
                {
                    "start": 1773,
                    "end": 1778,
                    "matchedPaperCorpusId": "237329957"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80517578125
        },
        {
            "corpus_id": "274777442",
            "title": "Are You Doubtful? Oh, It Might Be Difficult Then! Exploring the Use of Model Uncertainty for Question Difficulty Estimation",
            "text": "Lastly, this choice is also motivated by dataset availability, as explained in Section 3. \n\nIt is worthwhile explicitly mentioning that in the current work, the term \"uncertainty\" is used to encompass both 1st token probability and choice-order probability metrics (see Section 4.2 for details.) These measures are taken to broadly represent the inverse of model confidence. While accurately determining the uncertainty of an LLM is an open field of research, previous research suggests that both 1st token probability [22] and choice order probability [40] correlate well with model correctness in the MCQ setup. These findings also hold in the current experimental setup, as shown in Appendix A. \n\nOur Contribution. The contribution of our work is twofold. \n\nFirst, thanks to extensive experiments with a variety of LLMs and feature analysis using a regressor model (Random Forest Regressor), we showcase that model uncertainty is a useful proxy for item difficulty estimation on three different question sets assessing both factual knowledge and reading comprehension. Second, as a byproduct of our experiments investigating model uncertainty we yield a model which achieves best results to date on the BEA 2024 Shared Task dataset as well as the CMCQRD dataset. This model, together with all experimental code, is made available to the community for replicability and future extensions. We believe that our conceptual insight (model uncertainty as a useful signal for item difficulty), as well as our practical contribution in terms of an existing modular system, will foster further improvements in the task of MCQ automatic difficulty estimation, which is core in the educational setting. 1",
            "score": 0.33092007672030155,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2145,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 92,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 758
                },
                {
                    "start": 761,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1696
                }
            ],
            "ref_mentions": [
                {
                    "start": 553,
                    "end": 557,
                    "matchedPaperCorpusId": "271051443"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.459716796875
        },
        {
            "corpus_id": "264451551",
            "title": "Data Optimization in Deep Learning: A Survey",
            "text": "The accurate measurement of learning difficulty for each training sample is of great importance because several deep learning paradigms employ adaptive learning strategies based on the level of learning difficulty. For instance, curriculum learning [71] holds the perspective that easy samples should receive more focus in the early training stages, while hard samples should be given more attention in the later stages of training. Some other studies [72] hold the opposite perspective that hard samples should be prioritized throughout the training procedure. \n\nAs shown in Fig. 7, there are five major manners to measure learning difficulty of samples, namely, lossbased, gradient-based, uncertainty-based, multi-trainingbased, and distance-based. Obviously, the measures for learning difficulty are quite similar to those for cleanliness. In fact, some studies consider that noisy samples are those quite difficult to learn and divide samples into easy/medium/hard/noisy. Paul et al. [73] proposed the error l2-norm score to measure difficulty. Zhu et al. [74] established a formal definition for learning difficulty of samples inspired by the bias-variance trade-off theorem and proposed a new learning difficulty measures. Sorscher et al. [75] defined the cosine distance of a sample to its nearest cluster center as the sample's difficulty measure and applied it in sample selection. \u2022 Uncertainty. This type contains two sub-types, namely, aleatory uncertainty and epistemic uncertainty [76]. The former is also called data uncertainty and occurs when training samples are imperfect, e.g., noisy. Therefore, the cleanliness degree can be used as a measure of data uncertainty [77]. Epistemic uncertainty is also called model uncertainty. It appears when the learning strategy is imperfect. Model uncertainty can be calculated based on information entropy of the DNN prediction or the variance of multiple predictions output by a DNN with the dropout trick [78]. \u2022 Diversity. This type aims to identify the diversity of a subset of training samples. The subset is usually a category. The measurement for subset diversity is useful in the design of data augmentation strategy for the subset [79] and data selection [59]. Friedman and Dieng [80] leveraged the exponential of the Shannon entropy of the eigenvalues of a similarity matrix, namely, vendi score to measure diversity.",
            "score": 0.33092007672030155,
            "section_title": "A. Data perception",
            "char_start_offset": 31128,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 561
                },
                {
                    "start": 564,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2056
                },
                {
                    "start": 2057,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2226
                },
                {
                    "start": 2227,
                    "end": 2384
                }
            ],
            "ref_mentions": [
                {
                    "start": 249,
                    "end": 253,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 452,
                    "end": 456,
                    "matchedPaperCorpusId": "47252984"
                },
                {
                    "start": 1495,
                    "end": 1499,
                    "matchedPaperCorpusId": "226307260"
                },
                {
                    "start": 1964,
                    "end": 1968,
                    "matchedPaperCorpusId": "71134"
                },
                {
                    "start": 2197,
                    "end": 2201,
                    "matchedPaperCorpusId": "174799713"
                },
                {
                    "start": 2221,
                    "end": 2225,
                    "matchedPaperCorpusId": "252494342"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85693359375
        },
        {
            "corpus_id": "247222685",
            "title": "A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation",
            "text": "Early exiting is a widely used technique to accelerate inference of deep neural networks. With the rising of pre-trained language models (PLMs) (Devlin et al., 2019;Yang et al., 2019;Raffel et al., 2020;, early exiting is drawing increasing attention in the NLP community. At its core, early exiting allows simple instances to exit early while allowing hard instances to exit late. Thus, how to measure instance difficulty is a crucial problem. * * Corresponding author (xpqiu@fudan.edu.cn) Most existing early exiting methods attach multiple internal classifiers to the PLM and adopt some heuristic metrics, such as entropy Liu et al., 2020a) or maximum softmax score (Schwartz et al., 2020) of internal outputs, to measure instance difficulty. However, these methods can not easily generalize to new tasks. On the one hand, these metrics are not accessible on some tasks such as regression. On the other hand, In order for these methods to perform well, one usually needs to fine-tune the threshold, which varies widely across different tasks and datasets.\n\nAnother way to measure instance difficulty is to directly learn it. Recent studies (Elbayad et al., 2020;Xin et al., 2021) that use the idea of \"learn-toexit\" have achieved promising results. They jointly train a neural model to predict for each instance the exiting layer. At their core, the learn-to-exit module is to estimate the difficulty for each instance. Compared with previous heuristically designed metrics for difficulty, learn-to-exit is task-agnostic and does not require threshold-tuning, therefore is a more promising way.\n\nDespite their success, it is still unknown whether or how well the instance difficulty can be learned. As a response, in this work, we construct datasets for two kinds of instance difficulty: (a) Humandefined difficulty, and (b) Model-defined difficulty. The dataset for human-defined difficulty has two labels, 0 for instances that can be annotated by human and 1 for instances that cannot. For modeldefined difficulty, we train a multi-exit BERT (Devlin et al., 2019), which is attached with an internal classifier at each",
            "score": 0.3307838894353855,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 165,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 165,
                    "end": 183,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 183,
                    "end": 203,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 625,
                    "end": 643,
                    "matchedPaperCorpusId": "214802887"
                },
                {
                    "start": 669,
                    "end": 692,
                    "matchedPaperCorpusId": "215785895"
                },
                {
                    "start": 1143,
                    "end": 1165,
                    "matchedPaperCorpusId": "204824061"
                },
                {
                    "start": 1165,
                    "end": 1181,
                    "matchedPaperCorpusId": "233189542"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43701171875
        },
        {
            "corpus_id": "247694098",
            "title": "Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU",
            "text": "Transformer-based language models (Vaswani et al., 2017;Devlin et al., 2019, LMs) have recently achieved great success in a variety of NLP tasks (Wang et al., 2018(Wang et al., , 2019a)). However, generalisation to out-of-distribution (OOD) data and zero-shot cross-lingual transfer still remain a challenge (Linzen, 2020;Hu et al., 2020). Among existing techniques, improving OOD performance has been addressed by training with adversarial data (Yi et al., 2021), while better transfer across languages has been achieved by selecting appropriate languages to transfer from (Lin et al., 2019;Turc et al., 2021), employing meta-learning (Nooralahzadeh et al., 2020) or data alignment (Fang et al., 2020). \n\nContrastive to such approaches that take advantage of additional training data is Curriculum Learning (Bengio et al., 2009, CL), a technique that aims to train models using a specific ordering of the original training examples. This ordering typically follows an increasing difficulty trend where easy examples are fed to the model first, moving towards harder instances. The intuition behind CL stems from human learning, as humans focus on simpler concepts before learning more complex ones, a procedure that is called shaping (Krueger and Dayan, 2009). Although curricula have been primarily used for Computer Vision (Hacohen and Weinshall, 2019;Wu et al., 2021) and Machine Translation (Zhang et al., 2019a;Platanios et al., 2019), there are only a handful of approaches that incorporate CL into Natural Language Understanding tasks (Sachan and Xing, 2016;Tay et al., 2019;Lalor and Yu, 2020;Xu et al., 2020a). \n\nTypically, CL requires a measure of difficulty for each example in the training set.",
            "score": 0.3302951307665567,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 703
                },
                {
                    "start": 706,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1620
                },
                {
                    "start": 1623,
                    "end": 1707
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 56,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 145,
                    "end": 163,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 163,
                    "end": 186,
                    "matchedPaperCorpusId": "143424870"
                },
                {
                    "start": 308,
                    "end": 322,
                    "matchedPaperCorpusId": "218487293"
                },
                {
                    "start": 322,
                    "end": 338,
                    "matchedPaperCorpusId": "214641214"
                },
                {
                    "start": 446,
                    "end": 463,
                    "matchedPaperCorpusId": "235166908"
                },
                {
                    "start": 636,
                    "end": 664,
                    "matchedPaperCorpusId": "212415221"
                },
                {
                    "start": 683,
                    "end": 702,
                    "matchedPaperCorpusId": "221641003"
                },
                {
                    "start": 1235,
                    "end": 1260,
                    "matchedPaperCorpusId": "206863670"
                },
                {
                    "start": 1326,
                    "end": 1355,
                    "matchedPaperCorpusId": "102350936"
                },
                {
                    "start": 1355,
                    "end": 1371,
                    "matchedPaperCorpusId": "227343966"
                },
                {
                    "start": 1396,
                    "end": 1417,
                    "matchedPaperCorpusId": "155089817"
                },
                {
                    "start": 1417,
                    "end": 1440,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 1543,
                    "end": 1566,
                    "matchedPaperCorpusId": "16503693"
                },
                {
                    "start": 1566,
                    "end": 1583,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 1583,
                    "end": 1602,
                    "matchedPaperCorpusId": "226226711"
                },
                {
                    "start": 1602,
                    "end": 1619,
                    "matchedPaperCorpusId": "220045816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35791015625
        },
        {
            "corpus_id": "269762685",
            "title": "Curriculum Learning: Theories, Approaches, Applications, Tools, and Future Directions in the Era of Large Language Models",
            "text": "A general framework for curriculum design consists of two core components: Difficulty Measurer and Training Scheduler, which decide two things respectively: 1) What kind of training data is supposed to be easier than other data?2) When should we present harder data for training, and how much more?Thus, we can divide existing CL methods into two types: when both the Difficulty Measurer and Training Scheduler are designed by human prior knowledge with no data-driven algorithms involved, we call the CL method predefined CL.If any (or both) of the two components are learned by data-driven models or algorithms, then we denote the CL method as automatic CL.\n\nIn the early stages, predefined CL takes the mainstream.However, this type of predefined approach is not flexible and general enough for widespread applications.In 2010, Kumar et al. propose self-paced learning (SPL), enabling automatic curriculum scheduling by ordering data according to their training loss.Subsequently, a variety of automatic curriculum learning methods have continued to emerge.For example, transfer learning methods employ teacher models to offer student models curricula.Reinforcement learning methods allow teacher models to adapt curriculum based on the feedback from student models.In addition, there are other ones based on Bayesian optimization, meta-learning, and adversarial learning for implementing automatic curriculum learning.All representative approaches of both categories will be reviewed and discussed in this tutorial.",
            "score": 0.32973882748331895,
            "section_title": "Approaches",
            "char_start_offset": 5464,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 228,
                    "end": 298
                },
                {
                    "start": 298,
                    "end": 526
                },
                {
                    "start": 526,
                    "end": 659
                },
                {
                    "start": 661,
                    "end": 717
                },
                {
                    "start": 717,
                    "end": 822
                },
                {
                    "start": 822,
                    "end": 970
                },
                {
                    "start": 970,
                    "end": 1060
                },
                {
                    "start": 1060,
                    "end": 1155
                },
                {
                    "start": 1155,
                    "end": 1269
                },
                {
                    "start": 1269,
                    "end": 1422
                },
                {
                    "start": 1422,
                    "end": 1519
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.736328125
        },
        {
            "corpus_id": "268819980",
            "title": "Benchmark Transparency: Measuring the Impact of Data on Evaluation",
            "text": "The large multi-task benchmarks (Wang et al., 2019;Srivastava et al., 2023) rely on testing a single model across multiple tasks.Datasets designed to test one or more concrete phenomena (Kovatchev et al., 2018;Hossain et al., 2020;Saha et al., 2020;Kovatchev and Taul\u00e9, 2022) can be used for diagnostics, and Ribeiro et al. (2020) propose an approach for unit-testing NLP models based on predefined capabilities.Kiela et al. (2021) suggest the use of \"beat the machine\" human-in-the-loop approach to gather datasets with increasing difficulty (Kovatchev et al., 2022).\n\nMore recently, approaches for automatic dataset analysis grow in popularity.Swayamdipta et al. (2020) analyze the process of model learning and identify patterns in the training set.Rodriguez et al. (2021) use evaluation approaches borrowed from the educational domain to improve relative model ranking.Ethayarajh et al. (2022) propose a measure for dataset \"difficulty\" based on information theory.\n\nWhile promising, many of the existing approaches for dataset analysis have limited scope and scalability.Some of them are not directly applicable for measuring absolute or relative model performance.We combine and improve existing data-centric techniques and propose new ones with the goal of designing a framework for data-centric and data-informed evaluation of NLP models.",
            "score": 0.32909601411774464,
            "section_title": "Related Work",
            "char_start_offset": 6852,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 129,
                    "end": 412
                },
                {
                    "start": 412,
                    "end": 568
                },
                {
                    "start": 570,
                    "end": 646
                },
                {
                    "start": 646,
                    "end": 752
                },
                {
                    "start": 752,
                    "end": 873
                },
                {
                    "start": 873,
                    "end": 969
                },
                {
                    "start": 971,
                    "end": 1076
                },
                {
                    "start": 1076,
                    "end": 1170
                },
                {
                    "start": 1170,
                    "end": 1346
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 210,
                    "matchedPaperCorpusId": "21732517"
                },
                {
                    "start": 210,
                    "end": 231,
                    "matchedPaperCorpusId": "226262357"
                },
                {
                    "start": 231,
                    "end": 249,
                    "matchedPaperCorpusId": "224803276"
                },
                {
                    "start": 249,
                    "end": 275,
                    "matchedPaperCorpusId": "252735141"
                },
                {
                    "start": 309,
                    "end": 330,
                    "matchedPaperCorpusId": "218551201"
                },
                {
                    "start": 412,
                    "end": 431,
                    "matchedPaperCorpusId": "233444226"
                },
                {
                    "start": 646,
                    "end": 671,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 752,
                    "end": 775,
                    "matchedPaperCorpusId": "235703772"
                },
                {
                    "start": 873,
                    "end": 897,
                    "matchedPaperCorpusId": "250340652"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.374267578125
        },
        {
            "corpus_id": "255941901",
            "title": "Dynamic data-free knowledge distillation by easy-to-hard learning strategy",
            "text": "Curriculum learning (CL) has been widely used to train models by an easy-to-hard strategy [3,38,32]. Bengio [3] provides a clear illustration of its convergence of it. \n\nAutomatic CL is a dynamic strategy to adjust the difficulty by the feedback of training process. Self-paced learning (SPL) [19] is the most widely used automatic CL method assigning data with different difficulties based on the training losses at each timestamp (or epoch). Several theoretical studies [24,26] provide a deep understanding of SPL and are categorized into majorization minimization (MM) [6] algorithm and concave optimization. They use transfer learning [32] and uncertainty [50] as specific representative techniques to transfer from teacher. Recently, SPL is also applied to the field of unsupervised learning [22], clustering [47], anomaly detection [44] and graph [15]. \n\nBesides, some data-driven KD methods also use CL to enhance student learning from the teacher. For example, Xiang et al. [42] use SPL for instance selection in longtailed datasets, and Li et al. [21] use a similar uncertainty curriculum to distill models from large pretrained language models. In this work, CL provides an adaptive training target for the generation process of DFKD. Additionally, it provides a theoretical understanding of accelerating the convergence of DFKD methods and contributes to the usage of CL in knowledge distillation.",
            "score": 0.3289642509311528,
            "section_title": "Curriculum Learning",
            "char_start_offset": 8283,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 167
                },
                {
                    "start": 170,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 858
                },
                {
                    "start": 861,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1408
                }
            ],
            "ref_mentions": [
                {
                    "start": 90,
                    "end": 93,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 93,
                    "end": 96,
                    "matchedPaperCorpusId": "232362223"
                },
                {
                    "start": 96,
                    "end": 99,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 108,
                    "end": 111,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 472,
                    "end": 476,
                    "matchedPaperCorpusId": "263862224"
                },
                {
                    "start": 476,
                    "end": 479,
                    "matchedPaperCorpusId": "533152"
                },
                {
                    "start": 572,
                    "end": 575,
                    "matchedPaperCorpusId": "5708790"
                },
                {
                    "start": 639,
                    "end": 643,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 660,
                    "end": 664,
                    "matchedPaperCorpusId": "220047761"
                },
                {
                    "start": 797,
                    "end": 801,
                    "matchedPaperCorpusId": "245131303"
                },
                {
                    "start": 838,
                    "end": 842,
                    "matchedPaperCorpusId": "249917174"
                },
                {
                    "start": 853,
                    "end": 857,
                    "matchedPaperCorpusId": "247598732"
                },
                {
                    "start": 982,
                    "end": 986,
                    "matchedPaperCorpusId": "209862398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.309326171875
        },
        {
            "corpus_id": "267682083",
            "title": "How to Train Data-Efficient LLMs",
            "text": "Large language model (LLM) pre-training is perhaps the most data-and compute-intensive task attempted by the machine learning community to date, with impressive capabilities primarily being accomplished by training massive transformer architectures on trillions of tokens of text (Ope-nAI, 2023;Gemini et al., 2023;Touvron et al., 2023b). \n\nBut even these incredibly capable LLMs are subject to empirical scaling laws, which predict sharply diminishing returns from a linear increase in model-or data-size (Hoffmann et al., 2022;Kaplan et al., 2020). Power-law scaling therefore acts as a soft limit on model quality, beyond which it is prohibitively expensive to drive performance by scaling up the data or model. At the same time, Sorscher et al. (2022)-in the context of vision pre-training-show that we can significantly improve the power law constants in the aforementioned scaling laws if we prioritize important training examples using some robust notion of data quality or impact. \n\nA similar call for data-curation is also apparent in the context of training LLMs, where our largest models are quickly approaching their capacity and data thresholds. LIMA (Zhou et al., 2023) showed that LLaMA-65B (Touvron et al., 2023a) can be better aligned with human preferences when trained on a set of 1,000 carefully selected fine-tuning prompts, compared to training on as much as 52,000 unfiltered examples. Tirumala et al. (2023) recently conducted a large-scale data-efficient pre-training evaluation, showing that a 6.7B OPT model (Zhang et al., 2022) can converge up to 20% faster on data curated by a technique based on stratified cluster sampling. The Phi-2 experiments also suggest that when data curation is performed at a human-expert level (e.g., by textbook editors), models can outperform baselines that are up to 25x larger (Javaheripi et al., 2023). \n\nData curation routines can be fundamentally characterized as selecting training samples for quality, coverage, or some mixture of both (Figure 2).",
            "score": 0.32853243884320366,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 338
                },
                {
                    "start": 341,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 988
                },
                {
                    "start": 991,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1864
                },
                {
                    "start": 1867,
                    "end": 2013
                }
            ],
            "ref_mentions": [
                {
                    "start": 506,
                    "end": 529,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 733,
                    "end": 755,
                    "matchedPaperCorpusId": "250113273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.306884765625
        },
        {
            "corpus_id": "259593617",
            "title": "On the Effectiveness of Curriculum Learning in Educational Text Scoring",
            "text": "Automatic Text Scoring (ATS) is a widely-investigated task in education. Existing approaches often stressed the structure design of an ATS model and neglected the training process of the model. Considering the difficult nature of this task, we argued that the performance of an ATS model could be potentially boosted by carefully selecting data of varying complexities in the training process. Therefore, we aimed to investigate the effectiveness of curriculum learning (CL) in scoring educational text. Specifically, we designed two types of difficulty measurers: (i) pre-defined, calculated by measuring a sample's readability, length, the number of grammatical errors or unique words it contains; and (ii) automatic, calculated based on whether a model in a training epoch can accurately score the samples. These measurers were tested in both the easy-to-hard to hard-to-easy training paradigms. Through extensive evaluations on two widely-used datasets (one for short answer scoring and the other for long essay scoring), we demonstrated that (a) CL indeed could boost the performance of state-of-the-art ATS models, and the maximum improvement could be up to 4.5%, but most improvements were achieved when assessing short and easy answers; (b) the pre-defined measurer calculated based on the number of grammatical errors contained in a text sample tended to outperform the other difficulty measurers across different training paradigms.",
            "score": 0.3282303926058635,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.330078125
        },
        {
            "corpus_id": "250334600",
            "title": "An Exploration of How Training Set Composition Bias in Machine Learning Affects Identifying Rare Objects",
            "text": "We demonstrate training set balance bias and whether deweighting is an effective method to compensate for it using two techniques. First, Figure 6 shows each model's performance overshoot in the standard machine learning algorithm performance metrics as a function of training set size. Second, Figure 7 shows a measure of distance between the various machine learning algorithm model probabilities and the analytic probability as a function of training set size. Finally, we also did a short assessment of the half-EM method for constructing a PCP. \n\nFigure 6 contains the overshoot in the performance metrics on a test data set for: when the machine learning algorithm's model is applied to a representative data set without weighting (\"base\"), when the data was weighted to simulate each class being equally likely (\"weighted\"), when the probabilities from the weighted fit model were deweighted according to Equation 12(\"deweighted\"), when the training data were of biased equal composition (\"biased\"), and when the probabilities from the biased fit model were debiased using the same process as the deweighted ones (\"debiased\"). Each column in Figure 6 is dedicated to one source class (0, 1, and 2, respectively). Each line has a shaded 1-\u03c3 band around it, but most are not visible. \n\nFigure 6 shows that the deweighted and debiased models mostly follow the behavior of the base model, and the model fit to the biased training set behaves similarly to the weighted fit one. It also shows that the weighted and biased fits' performance metrics indicate inaccurate probabilities in all metrics. They also show, specifically, that the weighted and biased models applied to the rare class (2, right hand column) are over-complete and under-reliable, as predicted. \n\nThe comparisons in Figure 6 are not particularly sensitive measures of how accurately the models are computing the probabilities at each data point: a lot of information is discarded when classes are assigned. Having the probabilities that the models should produce in hand permits us to compute the distance between the model probabilities and true ones. For the distance metric we selected the Kullback-Leibler divergence",
            "score": 0.3278619202350847,
            "section_title": "Simulation Results",
            "char_start_offset": 59040,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 549
                },
                {
                    "start": 552,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1765
                },
                {
                    "start": 1768,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2191
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.038116455078125
        },
        {
            "corpus_id": "272882110",
            "title": "Larger and more instructable language models become less reliable",
            "text": "In both OpenBook and GPQA, human ratings of difficulty are incorporated, enriching the understanding of model performance relative to human standards. \n\nThey are described in the introduction, fully in sections 4.1 and 4.2, and summarised in Table 2, with examples of instances and their raw and human-calibrated difficulties. The 73 tasks in transforms are listed in Table A. 13. We have applied the same methodology to these new tasks and the results for these benchmarks complement the key figures in the paper or are covered in new figures. These results totally support our previous findings and make them more generally applicable. \n\nOriginality and significance R1.2. The paper is most original in its systematic analysis of some of the most well-known LLMs. The relationship between difficulty, accuracy, and confidence is shown more clearly here than in prior work. However, there is considerable prior work on many parts of the paper: \n\n- In addition, Chen et al (https://arxiv.org/abs/2307.09009), compare the performance of two versions of GPT-3.5 and GPT-4 in March and June 2023, corresponding almost exactly to the GPT-4v1 and GPT-4v2 in this submission. This submission goes further in assessing a broader family of models, and focusing more directly on the relationship between accuracy and coverage (i.e. what fractions of questions the system tries to answer). \n\nAnswer. Thank you for all these suggestions. We have included all these references in the paper. In addition, in the new section A.14 in the appendix, we have further discussed how our approach aligns with and differs from the state of the art, using these and many other recent papers on these topics. Here, we also discuss those references suggested in R1.6: \n\nOn the relationship between difficulty and performance, our research extends the concept of difficulty to very different domains, also providing a more granular analysis of model performance across a spectrum of difficulty levels. In our case, difficulty is instrumental for understanding user expectations and seeing the distribution of errors (and avoidance) as a function of difficulty.",
            "score": 0.3276843530888929,
            "section_title": "Key results",
            "char_start_offset": 35806,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 153,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 944
                },
                {
                    "start": 947,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1379
                },
                {
                    "start": 1382,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1742
                },
                {
                    "start": 1745,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2134
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2281494140625
        },
        {
            "corpus_id": "248392007",
            "title": "Evaluating Automatic Difficulty Estimation of Logic Formalization Exercises",
            "text": "contribution of the NL features. A general downward trend for FAC and an upward trend for AA (more difficult sentences are solved correctly the first time less often and take more attempts to solve) is apparent but the spread, especially around the difficulty classes 2 and 4, is quite large. \n\nThese results suggest that, at least on the Gradegrinder data4 , the difficulty estimation criteria proposed by Perikos et al. (2016). capture some important sources of difficulty. However, it also serves to make the point that expert evaluation does not always provide an accurate prediction of student performance. \n\nWe were interested to know which of the features in Perikos et al. ( 2016)'s algorithm have the biggest weight in determining difficulty. We trained a machine learning Random Forest-based regressor and used it to predict FAC and AA based on all 11 features in Perikos et al. (2016). We then extracted feature importances, and observed that number of quantifiers was the feature that the regressor 5 assigned by far the most weight to, 0.34 and 0.58 out of 1 for FAC and AA respectively. Word order matching was second in importance, with the weights of 0.23 and 0.14 respectively for the two measures. The rest of the features were assigned quite small weights. We then correlated the (average) number of quantifiers (grouped into 5 levels using 1-dimensional k-means clustering in order to be comparable with",
            "score": 0.3276843530888929,
            "section_title": "Evaluation using the Grade Grinder Corpus",
            "char_start_offset": 20858,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 292
                },
                {
                    "start": 295,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 611
                },
                {
                    "start": 614,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1423
                }
            ],
            "ref_mentions": [
                {
                    "start": 407,
                    "end": 428,
                    "matchedPaperCorpusId": "37207629"
                },
                {
                    "start": 874,
                    "end": 895,
                    "matchedPaperCorpusId": "37207629"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1309814453125
        },
        {
            "corpus_id": "266202354",
            "title": "A surprisal oracle for active curriculum language modeling",
            "text": "We describe our submission to the BabyLM Challenge (Warstadt et al., 2023), a shared-task about language models trained from scratch on a developmentally plausible corpus.Inspired by expectationbased theories of sentence processing (Hale, 2001;Levy, 2008) and active curriculum learning (ACL) (Jafarpour et al., 2021), our approach relies on surprisal to select informative samples and streamline them into the model during training.We henceforth refer to our strategy as active curriculum learning modeling (ACLM).\n\nThere is a large volume of published studies describing how the processing difficulty of a sentence is correlated with its incremental probability in context (Linzen and Jaeger, 2016;Futrell and Levy, 2017;Hahn et al., 2019, among others).In other words, as people process sentences, they generate predictions about what is coming next and this can be measured using surprisal (Demberg et al., 2012).Here, we test to what extent this principle of syntactic predictability can also be used to guide the learning of a language model.ACL, on the other hand, combines the strengths from Active Learning (AL) and Curriculum Learning (CL).AL is a classic paradigm for small data supervised scenarios, whereby an oracle labels infor-mative examples selected by the model itself based (most often) on a uncertainty heuristic.The uncertainty metrics, however, tend to bias the model towards eccentric examples (Zhang et al., 2022b).To counteract this, Jafarpour et al. (2021) use CL, a technique that mimics how humans learn by regulating the training according to some schedule criterion, e.g., easy to difficult or short to long examples (Bengio et al., 2009).\n\nIn our approach, we use surprisal as sampling heuristic.A sample is formed from the sentence with the highest surprisal value s from an initial pool, along with the n most similar sentences to s from the rest of the training data.At each iteration, a new sample is added to the pool until convergence.",
            "score": 0.3276109678952012,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 171,
                    "end": 433
                },
                {
                    "start": 433,
                    "end": 515
                },
                {
                    "start": 517,
                    "end": 756
                },
                {
                    "start": 756,
                    "end": 917
                },
                {
                    "start": 917,
                    "end": 1048
                },
                {
                    "start": 1048,
                    "end": 1150
                },
                {
                    "start": 1150,
                    "end": 1334
                },
                {
                    "start": 1334,
                    "end": 1440
                },
                {
                    "start": 1440,
                    "end": 1670
                },
                {
                    "start": 1672,
                    "end": 1728
                },
                {
                    "start": 1728,
                    "end": 1902
                },
                {
                    "start": 1902,
                    "end": 1973
                }
            ],
            "ref_mentions": [
                {
                    "start": 51,
                    "end": 74,
                    "matchedPaperCorpusId": "266182457"
                },
                {
                    "start": 232,
                    "end": 244,
                    "matchedPaperCorpusId": "5490143"
                },
                {
                    "start": 244,
                    "end": 255,
                    "matchedPaperCorpusId": "6211281"
                },
                {
                    "start": 293,
                    "end": 317,
                    "matchedPaperCorpusId": "236486249"
                },
                {
                    "start": 675,
                    "end": 700,
                    "matchedPaperCorpusId": "16858335"
                },
                {
                    "start": 700,
                    "end": 723,
                    "matchedPaperCorpusId": "5395686"
                },
                {
                    "start": 894,
                    "end": 916,
                    "matchedPaperCorpusId": "7234265"
                },
                {
                    "start": 1418,
                    "end": 1439,
                    "matchedPaperCorpusId": "252992688"
                },
                {
                    "start": 1460,
                    "end": 1483,
                    "matchedPaperCorpusId": "236486249"
                },
                {
                    "start": 1648,
                    "end": 1669,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2371826171875
        },
        {
            "corpus_id": "271874686",
            "title": "Evaluating Fine-Tuning Efficiency of Human-Inspired Learning Strategies in Medical Question Answering",
            "text": "Level of improvements The highest accuracy gains in our study (1.81% and 1.44%) align with previous findings on curriculum learning. Xu et al. [37] reported a 1.3% improvement in natural language understanding tasks by using multiple teacher models to define question difficulty for BERT fine-tuning, while Maharana and Bansal [24] showed a 2% gain in common sense reasoning with RoBERTa fine-tuned on fixed and adaptive curricula. Our results are lower than Lee et al. [21], who reported gains of 3.28% and 1.73% on World Knowledge and Commonsense Reasoning benchmarks for Llama-13B using Interleaved Curriculum. \n\nTwo key factors may explain the difference. First, Lee et al. [21] used a broader curriculum, covering subjects from secondary to graduate level, while we focused on a specialised graduate-level medical curriculum, with a narrower scope of content. Second, their approach used Bloom's taxonomy to classify questions by distinct difficulty levels, whereas our dataset, with more semantically similar medical questions, exhibits subtler difficulty variations. These differences in curriculum scope and difficulty categorisation may account for the variation in performance gains. \n\nLimited generalisation Most previous studies demonstrated the effectiveness of curriculum learning using a single model, showing consistent improvements across several benchmarks [37,24,21]. However, these results are insufficient to extrapolate that curriculum learning generalises well to other models, even though it is often assumed in these studies. Our findings suggest that a strategy effective for one model does not generalise to others, emphasising the need for caution when generalising results and drawing conclusions from limited model-specific evidence. This limited generalisation across models and datasets may be due to variations in how different LLMs perceive question difficulty and inconsistencies in question categories across medical datasets. \n\nBenefits of LLM-defined labels Our results show that using LLM-generated difficulty modestly improves the performance of curriculum-based learning strategies. This aligns with previous studies, which found that language-model-ranked difficulty led to consistent accuracy gains in curriculum learning across benchmarks [24,37].",
            "score": 0.32749421252458344,
            "section_title": "Discussion",
            "char_start_offset": 12904,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 613
                },
                {
                    "start": 616,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1193
                },
                {
                    "start": 1196,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1962
                },
                {
                    "start": 1965,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2291
                }
            ],
            "ref_mentions": [
                {
                    "start": 143,
                    "end": 147,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 327,
                    "end": 331,
                    "matchedPaperCorpusId": "250391006"
                },
                {
                    "start": 1375,
                    "end": 1379,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 1379,
                    "end": 1382,
                    "matchedPaperCorpusId": "250391006"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27197265625
        },
        {
            "corpus_id": "266348815",
            "title": "Paloma: A Benchmark for Evaluating Language Model Fit",
            "text": "Progress in AI is often catalyzed by benchmarks that define new ways of measuring progress (Deng et al., 2009, Wang et al., 2018, and Wang et al., 2019, inter alia). Language models (LMs) often report LM fit in the form of perplexity (Jelinek et al., 1977) or its logarithm, cross-entropy loss, on held out data from a model's training distribution or a small number of traditional test sets (Chelba et al., 2013, andMerity et al., 2016, inter alia). Such measures of LM fit have been shown to improve predictably with increases in scale from more parameters, training data, and compute (Kaplan et al., 2020;Hoffmann et al., 2022). However, increasingly large training data also aggregates increasing numbers of distinct communities (Diaz & Madaio, 2023) with differing distributions of language, i.e., domains, that LMs implicitly learn to model (Aharoni & Goldberg, 2020). Does rising performance lift all data? Or do some domains capture most improvement in LM fit? How do we evaluate decisions, such as how to compose pretraining data, that determine which distributions of language are modeled? We contend that, rather than extrapolating trends from a prescriptive mix of domains, benchmarks ought to measure LM fit to many domains and inspect where fit differs. \n\nIn this work we introduce PALOMA, a benchmark to study LM fit on many domains. We measure perplexity on different distributions of language that we surface by sampling from 18 sources, such as C4 (Raffel et al., 2019;Dodge et al., 2021), that have metadata marking 585 textual domains, such as URL domains or academic disciplines. Beyond evaluation data, we aim to enable and enrich fair comparisons for scientific research on language modeling with the following artifacts: guidelines for experiments on LM fit, 6 baseline 1B parameter models pretrained on popular corpora, standardized inference code, and a submission process for coordinating comparable results across the research community.",
            "score": 0.3268930805636165,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1267
                },
                {
                    "start": 1270,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1965
                }
            ],
            "ref_mentions": [
                {
                    "start": 91,
                    "end": 109,
                    "matchedPaperCorpusId": "57246310"
                },
                {
                    "start": 234,
                    "end": 256,
                    "matchedPaperCorpusId": "121680873"
                },
                {
                    "start": 847,
                    "end": 873,
                    "matchedPaperCorpusId": "214802856"
                },
                {
                    "start": 1487,
                    "end": 1506,
                    "matchedPaperCorpusId": "237568724"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.360107421875
        },
        {
            "corpus_id": "259370548",
            "title": "Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach",
            "text": "Curriculum learning (Bengio et al., 2009) aims to improve the generalizability of a model by gradually training it with easy examples followed by hard ones. Castells et al. (2020) introduced a generic loss function called SuperLoss (SL) which can be added on top of any target-task loss function to dynamically weight the training samples according to their difficulty for the model using a batch-wise threshold. Zhou et al. (2020) proposed dynamic instance hardness to determine the difficulty of an instance with running average of the hardness metric over training history.\n\nCurriculum learning has been investigated in NLP (Elman, 1993;Sachan and Xing, 2016;Settles and Meeder, 2016;Amiri et al., 2017;Platanios et al., 2019;Amiri, 2019;Zhang et al., 2019;Lalor and Yu, 2020;Xu et al., 2020;Chu et al., 2021;Liu et al., 2021;Kreutzer et al., 2021;Agrawal and Carpuat, 2022;Maharana and Bansal, 2022). Specifically, Settles and Meeder (2016);Amiri et al. (2017) proposed spaced repetition-based curricula based on psycholinguistic theory where the training data is scheduled by increasing intervals of time between consecutive reviews of previously learned data samples. Zhang et al. (2019) investigated curriculum learning for domain adaptation in neural machine translation, where samples were grouped and ranked based on their similarity score such that more similar samples are seen earlier and more frequently during training. Platanios  2019) proposed an approach to use competency function using rarity of words or length of a sentence for neural machine translation and inspired Liu et al. (2021) to define a curriculum based on multi-modal (text and image) data to choose which modality should be used for training. The model uses sample perplexity at batch level to select the modality for training. Linguistic features such as word rarity or length of sentence in (Platanios et al., 2019) and sample perplexity in (Liu et al., 2021) were used as measures of difficulty. Xu et al. (2020) designed a curriculum learning approach for NLP tasks using cross-view of training data to",
            "score": 0.3267790869213816,
            "section_title": "Related Work",
            "char_start_offset": 23435,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 41,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 157,
                    "end": 179,
                    "matchedPaperCorpusId": "227275456"
                },
                {
                    "start": 413,
                    "end": 431,
                    "matchedPaperCorpusId": "227275560"
                },
                {
                    "start": 627,
                    "end": 640,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 640,
                    "end": 662,
                    "matchedPaperCorpusId": "16503693"
                },
                {
                    "start": 662,
                    "end": 687,
                    "matchedPaperCorpusId": "14155276"
                },
                {
                    "start": 687,
                    "end": 706,
                    "matchedPaperCorpusId": "1916665"
                },
                {
                    "start": 706,
                    "end": 729,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 729,
                    "end": 741,
                    "matchedPaperCorpusId": "174799918"
                },
                {
                    "start": 741,
                    "end": 760,
                    "matchedPaperCorpusId": "155089817"
                },
                {
                    "start": 760,
                    "end": 779,
                    "matchedPaperCorpusId": "226226711"
                },
                {
                    "start": 779,
                    "end": 795,
                    "matchedPaperCorpusId": "220045816"
                },
                {
                    "start": 795,
                    "end": 812,
                    "matchedPaperCorpusId": "235671564"
                },
                {
                    "start": 812,
                    "end": 829,
                    "matchedPaperCorpusId": "236460239"
                },
                {
                    "start": 829,
                    "end": 851,
                    "matchedPaperCorpusId": "238856825"
                },
                {
                    "start": 851,
                    "end": 877,
                    "matchedPaperCorpusId": "247518847"
                },
                {
                    "start": 877,
                    "end": 903,
                    "matchedPaperCorpusId": "250391006"
                },
                {
                    "start": 919,
                    "end": 945,
                    "matchedPaperCorpusId": "14155276"
                },
                {
                    "start": 945,
                    "end": 964,
                    "matchedPaperCorpusId": "1916665"
                },
                {
                    "start": 1174,
                    "end": 1193,
                    "matchedPaperCorpusId": "155089817"
                },
                {
                    "start": 1590,
                    "end": 1607,
                    "matchedPaperCorpusId": "236460239"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.666015625
        },
        {
            "corpus_id": "273654253",
            "title": "Rethinking the Uncertainty: A Critical Review and Analysis in the Era of Large Language Models",
            "text": "As discussed, the literature on uncertainty estimation in LLMs primarily interprets confidence scores as measures of uncertainty. Such a prevalent method oversimplifies the nuanced and complex nature of uncertainty inherent in model predictions, which is crucial for accurate interpretation and reliability of model outputs. The limitations inherent in current methodologies necessitate the development of a more advanced framework for categorizing uncertainty estimation in large language models that surpasses the reliance on simple confidence scores. \n\nLack of Explainability: Current methods of confidence estimation provide certainty predictions without elucidating the underlying causes of potential uncertainties. While these scores may seem reasonable to human observers, the absence of insight into the sources of uncertainty complicates trust in the model's outputs, particularly in safety-critical contexts where explainability is essential. Current confidence quantification techniques struggle to pinpoint specific weaknesses or improvement areas in the model. Additionally, these methods lack the necessary transparency to clarify the reasons for model uncertainty, whether due to input ambiguity from users, insufficient knowledge, or conflicting information in the training data. \n\nLack of Ground Truth for Uncertainty Estimation: Current methods for estimating uncertainty are empirically evaluated and assess how accurately they predict the correctness of an answer. However, there is generally no established ground truth for validation, particularly regarding the sources of uncertainty, meaning currently there is no metric and method to determine the contribution of different uncertainties for specific models and tasks. \n\nLack of Transferability of Uncertainty Estimation Methods Across Different Applications and Datasets: Current uncertainty estimation methods often struggle with adaptability and generalizability when applied to new applications or datasets. Although effective within their specific domains, these methods frequently fail to yield reliable results in different settings due to factors like data distribution differences and unique domain-specific requirements. To overcome these limitations, it is crucial to develop more robust and flexible uncertainty estimation techniques that can adjust to the varied conditions and demands of diverse applications.",
            "score": 0.32642157925898274,
            "section_title": "Go beyond Confidence Estimation:",
            "char_start_offset": 33476,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 553
                },
                {
                    "start": 556,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1295
                },
                {
                    "start": 1298,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1743
                },
                {
                    "start": 1746,
                    "end": 1986
                },
                {
                    "start": 1987,
                    "end": 2205
                },
                {
                    "start": 2206,
                    "end": 2398
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25341796875
        },
        {
            "corpus_id": "264451551",
            "title": "Data Optimization in Deep Learning: A Survey",
            "text": "There are some common theoretical issues, analyses, and conclusions heavily influencing most data optimization techniques. They are the natural connections among different techniques. Several examples are listed as follows: \n\n\u2022 Theoretical issues in data perception. A solid theoretical basis for data perception in data optimization is lacking, even though most data optimization methods implicitly or explicitly rely on the perception for the training data. For instance, many methods from resampling, weighting, and perturbation are based on dividing samples into easy and hard. Nevertheless, there is not yet a widely accepted learning difficulty measure with a rigorous theoretical basis in the literature. More than ten types of learning difficulty measures are utilized to distinguish easy from hard samples in previous literature. A theoretical formulation for data perception is of great importance. \u2022 Probabilistic density (ratio) estimation. Many data optimization methods, especially data resampling and weighting, heavily rely on the probabilistic density (ratio) estimation. The most representative method is the importance sampling. In learning difficulty-based weighting, the probabilistic density ratio, in terms of learning difficulty, is revealed to determine the priority mode [217], namely, easy/medium/hard-first. \u2022 Regularization-based explanation. Many data optimization methods are considered as a type of regularization, including data augmentation and perturbation. In these methods, data optimization performs implicit model regularization other than explicit regularization that directly works on model parameters. Regularization is not always beneficial as over-regularization may occur. Li et al. [335] pointed out that large amount of augmented noisy data could lead to over-regularization and proposed an adaptive augmentation method. Adversarial training may result in robust overfitting [336]. \u2022 Generalization bound for data optimization. Many studies choose to deduce a mathematical bound for the generalization risk in terms of the empirical risk and variables related to the data optimization. This manner can theoretically explain the utility of the involved data optimization. Xiao et al. [337] derived stability-based generalization bounds for stochastic gradient descent (SGD) on the loss with adversarial perturbations.",
            "score": 0.32551044429506637,
            "section_title": "D. Connections via theory",
            "char_start_offset": 91300,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 223
                },
                {
                    "start": 226,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2217
                },
                {
                    "start": 2218,
                    "end": 2363
                }
            ],
            "ref_mentions": [
                {
                    "start": 1297,
                    "end": 1302,
                    "matchedPaperCorpusId": "238634649"
                },
                {
                    "start": 1922,
                    "end": 1927,
                    "matchedPaperCorpusId": "249848242"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35888671875
        },
        {
            "corpus_id": "248780086",
            "title": "On the Robustness of Question Rewriting Systems to Questions of Varying Hardness",
            "text": "Analysis of Rewriting Difficulty Measures. In our work, we use BLEU to measure the discrepancy between a question and its rewrite. We further experiment with other methods to assess their effectiveness for difficulty measurement. CANARD is evaluated here. As shown in Table 8, we first use the length of a question (|q|), its rewrite (|q \u2032 |), and their ratio (|q|/|q \u2032 |) to calculate a difficulty score. After re-ranking the questions with a difficulty score, we divide the ranked questions equally into three classes. Interestingly, we find that |q| works well. After analysis, we find that rewriting short questions requires finding much missing information, which makes short questions hard questions. The |q|/|q \u2032 | metric is not very useful, since |q|/|q \u2032 | can only measure the discrepancy in question lengths, but does not necessarily measure their semantic difference. |q \u2032 | does not work for difficulty measurement. Not surprisingly, the ROUGE score is also useful in measuring discrepancy just like BLEU. Analysis of Learning Data Distribution. Tables 6 and 7 show that learning private models can enhance performance on each class. We further divide the data into eleven classes (z \u2208 [0, 0.1], (0.1, 0.2], \u00b7 \u00b7 \u00b7 , (0.9, 1), 1) and learn a private model for each class. We build the private models using LSTM-S, in which we first train a shared model on the full training data, then fine-tune the shared model on each class to obtain the private models. Table 9 shows the BLEU scores where the score in the (i, j) entry is obtained by training on class   i and testing on class j. On the whole, learning private models can enhance the performance of the corresponding class. With these private models, we can better model the data distributions, but how to combine a large number of private models is a challenge, since it is hard to train a classifier to correctly predict so many class labels, which will have some negative effects on the model ensemble.",
            "score": 0.32471611760327296,
            "section_title": "Further Analysis",
            "char_start_offset": 24153,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.275146484375
        },
        {
            "corpus_id": "268358466",
            "title": "Algorithmic progress in language models",
            "text": "A standard metric for LLM performance is the measured test perplexity on standard datasets.For a language model, this is defined in as the exponential of the cross-entropy L between the model predictions and the test set, i.e.Perplexity = e L .",
            "score": 0.3244780451268607,
            "section_title": "E.2 Perplexity",
            "char_start_offset": 45408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 91,
                    "end": 226
                },
                {
                    "start": 226,
                    "end": 244
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.382568359375
        },
        {
            "corpus_id": "254685579",
            "title": "Efficient Pre-training of Masked Language Model via Concept-based Curriculum Masking",
            "text": "In the domain of machine learning, CL is a training strategy that gradually learns the complex examples after learning the easy ones, instead of learning all data simultaneously, which has been firstly proposed by (Bengio et al., 2009). The concept of CL can be traced back to Elman (1993) that has attempted to train machines from easy to difficult Figure 1: Illustration of CCM. In the first stage, CCM masks the initial concepts. Then it progressively includes the concepts within k-hops from the concepts used in the previous stages. Note that, \"S\" represents a set of concepts to be masked, \"O\" represents the original token sequence, and \"M\" represents a masked token sequence.\n\ntasks. Krueger and Dayan (2009) have tried to divide complex tasks into easy sub-tasks, and then trained machines on these sub-tasks. Recent studies have shown that a curriculumbased approach can improve the convergence speed and generalization performance of NLP systems (Sachan and Xing, 2016;Zhang et al., 2018;Tay et al., 2019;Zhan et al., 2021). The identification of easy samples in a given training dataset is important in applying CL to a target task (Kumar et al., 2010). There are two main strategies for assessing the data difficulty; 1) using human prior knowledge regarding specific tasks (Zhang et al., 2018;Tay et al., 2019;Zhan et al., 2021), and 2) using trained models to measure the difficulty of examples . However, existing difficulty criteria do not work well with CL for MLM, as they have overlooked that the MLM difficulty of examples can be changed according to which tokens are masked. To better assess the difficulty of MLM, we propose a novel curriculum that intervenes in the masking process based on the relation-based difficulty criteria.",
            "score": 0.3244780451268607,
            "section_title": "Curriculum Learning",
            "char_start_offset": 6082,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 235,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 277,
                    "end": 289,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 692,
                    "end": 716,
                    "matchedPaperCorpusId": "206863670"
                },
                {
                    "start": 957,
                    "end": 980,
                    "matchedPaperCorpusId": "16503693"
                },
                {
                    "start": 999,
                    "end": 1016,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 1016,
                    "end": 1034,
                    "matchedPaperCorpusId": "232104951"
                },
                {
                    "start": 1144,
                    "end": 1164,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 1307,
                    "end": 1324,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 1324,
                    "end": 1342,
                    "matchedPaperCorpusId": "232104951"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.541015625
        },
        {
            "corpus_id": "265466496",
            "title": "Comprehensive Benchmarking of Entropy and Margin Based Scoring Metrics for Data Selection",
            "text": "While data selection methods have been studied extensively in active learning, data pruning, and data augmentation settings, there is little evidence for the efficacy of these methods in industry scale settings, particularly in low-resource languages. Our work presents ways of assessing prospective training examples in those settings for their\"usefulness\"or\"difficulty\". We also demonstrate how these measures can be used in selecting important examples for training supervised machine learning models. We primarily experiment with entropy and Error L2-Norm (EL2N) scores. We use these metrics to curate high quality datasets from a large pool of \\textit{Weak Signal Labeled} data, which assigns no-defect high confidence hypotheses during inference as ground truth labels. We then conduct training data augmentation experiments using these de-identified datasets and demonstrate that score-based selection can result in a 2% decrease in semantic error rate and 4%-7% decrease in domain classification error rate when compared to the baseline technique of random selection.",
            "score": 0.32297905362763957,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75537109375
        },
        {
            "corpus_id": "266163943",
            "title": "Towards Building More Robust NER datasets: An Empirical Study on NER Dataset Bias from a Dataset Difficulty View",
            "text": "We summarize the limitations of this work as follows: \n\n(1) We regard our work as an exploratory study on the robustness problem of NER from the dataset view. Nevertheless, the method to measure the dataset difficulty is not quite efficient since it requires first training two individual models on an i.i.d. training set. There might be more efficient ways to measure the dataset and instance difficulty, thus further improving the efficiency and practicality of the data reconstruction methods. \n\n(2) \n\nIn this work, we consider the robustness problem of NER models with only small pre-trained models (model size less than 1B). As the large language models have shown powerful ability in information extraction tasks, it is in doubt that if the same conclusion can also generalize to large language models. Nevertheless, we believe our study is worthful since building more robust datasets is always important for building a NER system required for real-world usage. We expect our work can prompt more valuable future works also on large language models.",
            "score": 0.3222975479922755,
            "section_title": "Limitations",
            "char_start_offset": 23908,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 56,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 496
                },
                {
                    "start": 499,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1056
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4150390625
        },
        {
            "corpus_id": "271134718",
            "title": "Unveiling the robustness of machine learning families",
            "text": "The evaluation of machine learning systems has typically been limited to performance measures on clean and curated datasets, which may not accurately reflect their robustness in real-world situations where data distribution can vary from learning to deployment, and where truthfully predict some instances could be more difficult than others. Therefore, a key aspect in understanding robustness is instance difficulty, which refers to the level of unexpectedness of system failure on a specific instance. We present a framework that evaluates the robustness of different ML models using item response theory-based estimates of instance difficulty for supervised tasks. This framework evaluates performance deviations by applying perturbation methods that simulate noise and variability in deployment conditions. Our findings result in the development of a comprehensive taxonomy of ML techniques, based on both the robustness of the models and the difficulty of the instances, providing a deeper understanding of the strengths and limitations of specific families of ML models. This study is a significant step towards exposing vulnerabilities of particular families of ML models.",
            "score": 0.32221277403471776,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31201171875
        },
        {
            "corpus_id": "272690054",
            "title": "Active Learning to Guide Labeling Efforts for Question Difficulty Estimation",
            "text": "Question Difficulty Estimation (QDE), also known as question calibration, is a regression task that estimates a question's difficulty directly from the question and answers' text. It is a crucial task in personalized support tools like computerized adaptive testing (Van der Linden and Glas, 2000), which tailors questions to a student's skill level. If the questions are too easy or too difficult, the student might lose motivation, negatively affecting their learning outcome (Wang et al., 2014). \n\nTraditionally, QDE has been performed with manual calibration (Attali et al., 2014) and pretesting (Lane et al., 2016), which are time-consuming and expensive. Recent studies aim to address these limitations by leveraging natural language processing (NLP) techniques. The NLP approaches train machine learning models to estimate question difficulty from its text. Once trained, the models can quickly calibrate unseen questions, reducing the need for pretesting and manual calibration. \n\nSupervised techniques dominate QDE with state-of-the-art results (Zhou and Tao, 2020;Benedetto et al., 2021) by fine-tuning the publicly available pre-trained models BERT (Devlin et al., 2018) and DistilBERT (Sanh et al., 2019). However, fine-tuning often requires a large labeled dataset containing tens of thousands of calibrated questions, almost impossible to collect for individual course instructors developing QDE tools on their exam data. An isolated study (Loginova et al., 2021) has delved into an unsupervised approach, relying solely on additional pre-training and evaluating pairwise difficulty. Although this approach is helpful, its performance cannot be directly compared to supervised methods and is more computationally expensive in practical implementations. \n\nIn this work, we explore active learning (AL) (Settles, 2009) for QDE, a data-efficient supervised approach aiming to minimize the labeling work for human annotators while matching the performance of state-of-the-art models. AL operates by iteratively training a model on an increasingly growing labeled subset by acquiring labels from an expert only for the most informative unlabeled data points.",
            "score": 0.32204780495122853,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 498
                },
                {
                    "start": 501,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1766
                },
                {
                    "start": 1769,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2167
                }
            ],
            "ref_mentions": [
                {
                    "start": 478,
                    "end": 497,
                    "matchedPaperCorpusId": "31281"
                },
                {
                    "start": 1054,
                    "end": 1074,
                    "matchedPaperCorpusId": "227122854"
                },
                {
                    "start": 1074,
                    "end": 1097,
                    "matchedPaperCorpusId": "233365134"
                },
                {
                    "start": 1454,
                    "end": 1477,
                    "matchedPaperCorpusId": "244007367"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.253662109375
        },
        {
            "corpus_id": "266825945",
            "title": "Generative AI in Management \u2013 Today and Tomorrow",
            "text": "Optimizing the output of the system with specific information without modifying the underlying language model. \n\nProviding more up-to-date information and fine-tuning to a particular organization and industry. \n\nSource: Authors' own study. \n\nLanguage modeling has been widely studied in the past two decades, evolving from statistical to neural language models. One of the most important techniques used in deep learning is transfer learning. Transfer learning is a machine learning (ML) technique that aims to transfer knowledge learned in one or more source tasks and use it to improve learning in a related target task (Torrey & Shavik, 2010). \n\nIt is made up of two components pre-training and fine-tuning. The pre-trained components were introduced while training Transformer models on extensive corpora, Pobrane z czasopisma Annales H -Oeconomia http://oeconomia.annales.umcs.pl Data: 06/04/2024 03:38:41 demonstrating remarkable proficiency in various natural language processing tasks. Recognizing that scaling up models enhances their capacity, researchers have explored further by increasing parameter sizes. Interestingly, when the parameter scale surpasses a certain threshold, these enlarged language models exhibit substantial performance gains and manifest unique abilities (e.g. in-context learning) absent in smaller models (e.g. BERT). Fine-tuning a large language model refers to adapting a pre-trained language model to a specific task or domain. When a large language model is pre-trained, it learns general language patterns and representations from many unlabeled text data. However, to make the model more effective and accurate for a particular task, it needs to be fine-tuned on a smaller labeled dataset specific to that task. \n\nWhile scaling up language models can generally improve their overall quality, addressing specific challenges, such as safety and factual grounding, is essential. The projects mentioned above aim to tackle these challenges by fine-tuning with annotated data and enabling the model to consult external knowledge sources. \n\nThe first challenge, safety, involves ensuring that the model's responses align with a set of predefined human values, preventing harmful suggestions or unfair biases. The researchers use a metric based on a representative set of human values to quantify safety.",
            "score": 0.32181544321474287,
            "section_title": "Augmented Generation",
            "char_start_offset": 18453,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 113,
                    "end": 209
                },
                {
                    "start": 212,
                    "end": 239
                },
                {
                    "start": 242,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 646
                },
                {
                    "start": 649,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1753
                },
                {
                    "start": 1756,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2074
                },
                {
                    "start": 2077,
                    "end": 2244
                },
                {
                    "start": 2245,
                    "end": 2339
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2032470703125
        },
        {
            "corpus_id": "267897557",
            "title": "KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models",
            "text": "Data contamination refers to the inclusion of information in the training set of models that provides insights into the test set of a benchmark dataset, and then evaluated in the same benchmark. Recently, the AI community has become increasingly concerned (Schaeffer, 2023;Zhou et al., 2023;Oren et al., 2023) about data contamination in LLMs. Detecting data contamination, a form of Membership Inference Attack (MIA), poses challenges for large language models (LLMs) due to their training on vast corpora and the difficulty of conducting ablation studies (Shi et al., 2023). To detect such contamination of LLMs, Wei et al. (2023) suggested comparing average loss values between training and test datasets, while Shi et al. (2023) introduced Min-K% Prob based on loss values to identify texts used in training. Our experiments show these methods are effective for pre-training but not for detecting contamination during fine-tuning. Zhu et al. (2023) leveraged DAG to dynamically generate evaluation data in reasoning tasks to avoid contamination. In comparison, KIEval only requires access to output texts of evaluated models and detects data contamination through evaluating its ability to generalize and utilize knowledge as well as generative ability, which requires a deeper understanding of knowledge instead of mere memorization of the answers. Moreover, our experiments suggest that KIEval is resilient to data contamination, offering a reliable means to discern whether models have been trained on test sets. This makes it a valuable tool for complementing traditional benchmarks, providing a more nuanced understanding of a model's exposure to and handling of contaminated data.",
            "score": 0.321483410364145,
            "section_title": "Addressing Data Contamination of LLMs",
            "char_start_offset": 7866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1690
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.217529296875
        },
        {
            "corpus_id": "266166181",
            "title": "Byte-ranked Curriculum Learning for BabyLM Strict-small Shared Task 2023",
            "text": "Since curriculum learning relies upon increasing the difficulty of training samples as training continues, determining what makes a training sample more difficult than another is centrally important. For language input, some proposed measures of difficulty include presence of rare words (Bengio et al., 2009), block size (Nagatsuka et al., 2021), and length (Nagatsuka et al., 2023) Amiri et al. (2017). Their work applies findings from psychology that human learners learn effectively when the same information is reviewed with increasing lengths of time between reviews. These findings suggest that human learners ability to learn information is impacted not only by repetition of material, but also by the interval of time between those repetitions. The work by Amiri et al. (2017) uses this as a basis for a curriculum learning schedule. That work created a scheduler which spends more time on difficulty training instances and less time on easy instances. This work, by contrast, by gradually increasing the size of the training set, also gradually increases the time between repetitions of the easiest training samples while saving the more difficulty samples for later in training. \n\nAs this work was part of a shared task BabyLM challenge, there will be other related works published at the same time as this work. While those works cannot be discussed here, they will also provide good comparisons of other possible approaches.",
            "score": 0.3214140071530508,
            "section_title": "Related Work",
            "char_start_offset": 5792,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1189
                },
                {
                    "start": 1192,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1437
                }
            ],
            "ref_mentions": [
                {
                    "start": 288,
                    "end": 309,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 322,
                    "end": 346,
                    "matchedPaperCorpusId": "244048238"
                },
                {
                    "start": 359,
                    "end": 383,
                    "matchedPaperCorpusId": "255221201"
                },
                {
                    "start": 384,
                    "end": 403,
                    "matchedPaperCorpusId": "1916665"
                },
                {
                    "start": 766,
                    "end": 785,
                    "matchedPaperCorpusId": "1916665"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.354736328125
        },
        {
            "corpus_id": "269156961",
            "title": "Question Difficulty Ranking for Multiple-Choice Reading Comprehension",
            "text": "Both of these approaches rely on supervised data for training.Additionally, we consider zero-shot approaches which can be applied out-of-the-box for difficulty estimation and hence difficulty ranking.With the universal capabilities of instructiontuned large language models (Minaee et al., 2024) (LLMs), we consider two zero-shot applications: 1. direct prompt for each MC question to return an absolute score of its difficulty; 2. pairwise comparison prompt to request the model to return which question is more difficult between a pair and then convert to an overall ranking across the corpus of questions.Both strategies only rely on black-box API access to instruction-tuned language models.",
            "score": 0.3213007535264759,
            "section_title": "Introduction",
            "char_start_offset": 2347,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 62,
                    "end": 200
                },
                {
                    "start": 200,
                    "end": 608
                },
                {
                    "start": 608,
                    "end": 695
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.272705078125
        },
        {
            "corpus_id": "269362156",
            "title": "Generalization Measures for Zero-Shot Cross-Lingual Transfer",
            "text": "Generalization enables models to use prior knowledge to reasonably respond to previously unseen stimuli. Although traditional machine learning evaluation is performed based on a preselected set of prediction or generation tasks, accuracy on many public benchmarks may often not be sufficient to extensively assess the ability to perform well in new settings. Therefore, a majority of researchers have found it worthwhile to investigate measures that could evaluate the generalization capability of models with properties, such as VC dimension (Vapnik and Chervonenkis, 1971), crossentropy (Shannon, 1948), complexity (Mohri et al., 2012) or variation in parameters during training 1 Code: https://anonymous.4open.science/r/strikegen-7288 (Nagarajan and Kolter, 2019). Among these, recent findings support the smoothness in the loss curvature to correlate best with generalization capability (Chaudhari et al., 2019;Petzka et al., 2021;Kaddour et al., 2022), motivating the development of learning methods that induce smoothness in the learning trajectory such that the model becomes more robust; either through data perturbation (Jiang et al., 2020a;Aghajanyan et al., 2021;Liang et al., 2021;Hua et al., 2021;Park et al., 2022;Zheng et al., 2021;Wang et al., 2021;Huang et al., 2021) or by integrating the measure directly to the optimization objective (Izmailov et al., 2018;Jastrzebski et al., 2021;Cha et al., 2021;Foret et al., 2021;Hu et al., 2022;Zaken et al., 2022;Stickland and Murray, 2021). However it might often not be straightforward to compute such measures in high-dimensional feature space in a stable fashion (Nachum et al., 2024). \n\nAs models get larger and cover more languages, the possibility of improving the applicability of NLP systems in many under-resourced languages gets more promising.",
            "score": 0.3213007535264759,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1649
                },
                {
                    "start": 1652,
                    "end": 1815
                }
            ],
            "ref_mentions": [
                {
                    "start": 891,
                    "end": 915,
                    "matchedPaperCorpusId": "13807351"
                },
                {
                    "start": 935,
                    "end": 956,
                    "matchedPaperCorpusId": "252873712"
                },
                {
                    "start": 1129,
                    "end": 1150,
                    "matchedPaperCorpusId": "207847598"
                },
                {
                    "start": 1150,
                    "end": 1174,
                    "matchedPaperCorpusId": "221083147"
                },
                {
                    "start": 1193,
                    "end": 1210,
                    "matchedPaperCorpusId": "235097474"
                },
                {
                    "start": 1210,
                    "end": 1228,
                    "matchedPaperCorpusId": "233241142"
                },
                {
                    "start": 1228,
                    "end": 1247,
                    "matchedPaperCorpusId": "235436224"
                },
                {
                    "start": 1247,
                    "end": 1265,
                    "matchedPaperCorpusId": "232233194"
                },
                {
                    "start": 1265,
                    "end": 1284,
                    "matchedPaperCorpusId": "233296638"
                },
                {
                    "start": 1354,
                    "end": 1377,
                    "matchedPaperCorpusId": "3833416"
                },
                {
                    "start": 1377,
                    "end": 1402,
                    "matchedPaperCorpusId": "229680169"
                },
                {
                    "start": 1402,
                    "end": 1419,
                    "matchedPaperCorpusId": "235367622"
                },
                {
                    "start": 1419,
                    "end": 1438,
                    "matchedPaperCorpusId": "222134093"
                },
                {
                    "start": 1438,
                    "end": 1454,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1454,
                    "end": 1473,
                    "matchedPaperCorpusId": "231672601"
                },
                {
                    "start": 1473,
                    "end": 1500,
                    "matchedPaperCorpusId": "241583594"
                },
                {
                    "start": 1627,
                    "end": 1648,
                    "matchedPaperCorpusId": "262459129"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2313232421875
        },
        {
            "corpus_id": "263828663",
            "title": "SpikeBERT: A Language Spikformer Learned from BERT with Knowledge Distillation",
            "text": "In stage 2, we take a BERT fine-tuned on a task-specific dataset as the teacher model, and the model completed stage 1 as the student. To accomplish a certain language task, there should be a task-specific head over the basic language model as shown in Figure 2. For example, it is necessary to add an MLP layer over BERT for text classification. Besides, data augmentation is a commonly used and highly effective technique in knowledge distillation (Jiao et al., 2019;Tang et al., 2019;Liu et al., 2022). In the following, we will discuss our approach to data augmentation, as well as the logits loss and cross-entropy loss. \n\nData Augmentation In the distillation approach, a small dataset may be insufficient for the teacher model to fully express its knowledge (Ba & Caruana, 2013). To tackle this issue, we augment the training set in order to facilitate effective knowledge distillation. We follow Tang et al. ( 2019) to augment the training set: Firstly, we randomly replace a word with [MASK] token with probability p mask . Secondly, we replace a word with another of the same POS tag with probability p pos . Thirdly, we randomly sample an n-gram from a training example with probability p ng , where n is randomly selected from {1, 2, ..., 5}. \n\nLogits Loss Following Hinton et al. (2015), we take logits, also known as soft labels, into consideration, which lets the student learn the prediction distribution of the teacher. To measure the distance between two distributions, we choose KL-divergence: L logits = c i p i log pi qi , where c is the number of categories, p i and q i denote the prediction distribution of the teacher model and student model. \n\nCross-entropy Loss Cross-entropy loss can help the student model learn from the samples in task-specific datasets: L ce = \u2212 c i qi log (q i ), where qi represents the one-hot label vector.",
            "score": 0.3208741951784978,
            "section_title": "THE SECOND STAGE: TASK-SPECIFIC DISTILLATION",
            "char_start_offset": 17571,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 625
                },
                {
                    "start": 628,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1254
                },
                {
                    "start": 1257,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1667
                },
                {
                    "start": 1670,
                    "end": 1858
                }
            ],
            "ref_mentions": [
                {
                    "start": 450,
                    "end": 469,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 487,
                    "end": 504,
                    "matchedPaperCorpusId": "248780060"
                },
                {
                    "start": 765,
                    "end": 785,
                    "matchedPaperCorpusId": "11536917"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.309814453125
        },
        {
            "corpus_id": "263830425",
            "title": "TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models",
            "text": "There are three principles for the creation of TRACE. First, the datasets should be novel enough that most LLMs have not been trained on them. Second, they should be challenging for large language models. Third, a variety of tasks should be covered in our benchmark. \n\nAccording to these three principles, in this section, we will provide a detailed introduction to the data collection process for each dataset. Domain-Specific. These tasks require specific knowledge, so models may perform poorly if they have not appeared frequently enough in the training data. We select three datasets, ScienceQA (Lu et al., 2022), FOMC (Shah et al., 2023) and MeetingBank (Hu et al., 2023). ScienceQA is a multi-hop QA dataset collected from elementary and high school science curricula, with a rich domain diversity from natural science, social science, and language science, requiring the model of reasoning ability and science knowledge. As we only test the performance of language models, only the examples without multi-modal contexts are included in TRACE. FOMC is a hawkish-dovish classification task, which is novel in the financial domain. The dataset is divided into three subsets: data on meeting minutes, press conference data, and speech data. We use a combination of them. MeetingBank is a new benchmark dataset for city council meeting summarization, an unstudied domain. It demands a global understanding of the whole long context. \n\nMulti-lingual. The cross-lingual ability of large language models is limited due to vocabulary and pre-training corpus. For instance, LLaMA's vocabulary contains few Chinese tokens, affecting its efficiency with Chinese text. (Cui et al., 2023) expand LLaMA's Chinese vocabulary and fine-tune with additional Chinese corpus. Yet, capabilities for other languages can be forgotten after training in a specific language, making it vital to evaluate cross-lingual ability in our benchmark. We select C-STANCE (Zhao et al., 2023) and 20Minuten (Rios et al., 2021)  Code completion.",
            "score": 0.3203864845348209,
            "section_title": "DATA CREATION",
            "char_start_offset": 9812,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 54,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 266
                },
                {
                    "start": 269,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1435
                },
                {
                    "start": 1438,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2015
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.148193359375
        },
        {
            "corpus_id": "269430781",
            "title": "Review of Data-centric Time Series Analysis from Sample, Feature, and Period",
            "text": "Training models in a meaningful order can improve model performance over the standard approach based on random order.Curriculum Learning strategies (CL) train models from easier data to harder data, imitating the human curricula.We can summarize the easy-to-hard CL methods into predefined CL, where the difficulty measurer and the training scheduler are provided before training, and automatic CL, where the evaluation and scheduling dynamically change with training by self-paced learning, transfer teacher, or reinforcement learning strategies [Portelas et al., 2020].\n\nAlthough such an easy-to-use plug-in has demonstrated its power in improving the generalization capacity and convergence rate of various models in CV field [Wang et al., 2022], few of them are specifically designed for TS data.[Koh, 2024] uses CL to smooth the noise during training for the control tasks over highly stochastic TS data.[Sun et al., 2023c] proposes a curricular and cyclical loss to train TS models based on the TS distribution changing dynamics.",
            "score": 0.32011396341905474,
            "section_title": "Learning Order Arrangement",
            "char_start_offset": 9664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 117,
                    "end": 229
                },
                {
                    "start": 229,
                    "end": 571
                },
                {
                    "start": 573,
                    "end": 800
                },
                {
                    "start": 800,
                    "end": 909
                },
                {
                    "start": 909,
                    "end": 1035
                }
            ],
            "ref_mentions": [
                {
                    "start": 547,
                    "end": 570,
                    "matchedPaperCorpusId": "250549027"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.19677734375
        },
        {
            "corpus_id": "254636533",
            "title": "Cross-Modal Similarity-Based Curriculum Learning for Image Captioning",
            "text": "Image captioning has been widely investigated in computer vision and language research. However, most current methods treat image-caption pairs for training indistinctively, thus neglecting the difference in terms of learning difficulty. As illustrated in Figure 1, an image is annotated with multiple references with diverse styles and complexity levels. Such diversity can introduce different levels of learning difficulty, and undertrained captioning models can be misled by wrong gradients when training on the difficult data (Dong et al., 2021).\n\nCurriculum learning (CL) has demonstrated improvements in model performance and training speed by presenting data sorted according to the learning difficulty (Bengio et al., 2009). Existing image captioning approaches using CL have drawbacks in their difficulty measurements: 1) Requir-",
            "score": 0.32001132596387594,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 530,
                    "end": 549,
                    "matchedPaperCorpusId": "236924384"
                },
                {
                    "start": 710,
                    "end": 731,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3798828125
        },
        {
            "corpus_id": "258967990",
            "title": "Empirical Sufficiency Lower Bounds for Language Modeling with Locally-Bootstrapped Semantic Structures",
            "text": "We measure language modeling performance in terms of surprisal or perplexity (PPL), which is computed as the exponential of the model's tokenaveraged negative log-likelihood (NLL). 6 Whenever we report aggregate performance over all data, we use PPL (tables 2 and 3), but in the detailed analysis of smaller subsets of data we switch to NLL for better readability ( fig. 6). For both metrics, lower is better. To evaluate B-SGS correctness, we consider binary micro-accuracy over individual vector dimensions, macro-accuracy over tokens, as well as edge precision, recall, and F1-score. 5 Representation Distillation: What makes semantic structure valuable to language modeling?\n\nCurrently well-known as a popular and effective deep learning technique (e.g., Polino et al., 2018;Sanh et al., 2019), distillation (of neural models) aims to reduce redundancy and unwieldiness ( \u00a75.1) while retaining core information. Here we apply a similar concept to a family of symbolic linguistic representations, SGS. Rather than relying on a blackbox training process to transfer knowledge from a large pretrained model to a smaller model, we manually design a less detailed variant of SGS, which we call B-SGS ( \u00a75.2). We use ground-truth B-SGS as additional input to the incremental LM as before and find that it does constitute a lower bound of sufficient richness ( \u00a75.3).",
            "score": 0.3188055742750154,
            "section_title": "Evaluation",
            "char_start_offset": 15246,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 759,
                    "end": 779,
                    "matchedPaperCorpusId": "3323727"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1339111328125
        },
        {
            "corpus_id": "269741199",
            "title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation",
            "text": "Wang et al., 2022 used the frequency of words as the complexity metric for the curriculum-based pre-training of LMs. They hypothesized that linguistically, the frequent words are learned first and the rare words later. They gradually introduced words with decreasing frequency levels during the pre-training and showed that the frequency-based CL results in better performance over the vanilla BERT on various downstream benchmarks without any extra computational overhead. Nagatsuka et al., 2021 proposed a new CL method that gradually increases the block size of the input text via the maximum available batch size for pre-training BERT LMs. Experiments in the low-resource settings showed that their approach resulted in a better convergence speed and performance on the downstream tasks compared to the baseline LMs. \n\nPucci et al., 2023 investigated whether the CL and the corresponding complexity measures are language-dependent or can be easily exported to other languages. They used a normalized measure based on the text length, rarity, and comprehensibility (or Flesch-Kincaid readability metric) as the measure for the English corpus. Then, deployed the CL method for two other languages: Italian & French and showed that the CL developed for English corpus can be easily exported to these two languages without any adaptation. Lee et al., 2022 proposed a novel Concept-based Curriculum Masking (CCM) method to efficiently pre-train a language model. The CCM approach was inspired by how human comprehension is formed, where first the simple concepts (e.g., vacuum cleaner) are learned, then the complex concepts (e.g., robot vacuum cleaner). To mimic human learning, CCM constructs a multi-stage curriculum that gradually adds harder concepts related to the easier concepts using a knowledge graph. CMM categorizes the concepts that are related to many other concepts as the easy ones and builds a set of initial (easy) concepts by selecting the ones with the highest degree of connections in the knowledge graph. Their experimental results demonstrated that CCM learning improves convergence speeds and the generalization performance of LMs. \n\nAnother They approximated that the learning task's difficulty is inversely proportional to the gap between the superior LM versus the inferior LM prediction of the task.",
            "score": 0.3187410243323816,
            "section_title": "I. Related Work",
            "char_start_offset": 82914,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2154
                },
                {
                    "start": 2157,
                    "end": 2326
                }
            ],
            "ref_mentions": [
                {
                    "start": 474,
                    "end": 496,
                    "matchedPaperCorpusId": "244048238"
                },
                {
                    "start": 1339,
                    "end": 1355,
                    "matchedPaperCorpusId": "254685579"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.487548828125
        },
        {
            "corpus_id": "247694098",
            "title": "Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU",
            "text": "Typically, CL requires a measure of difficulty for each example in the training set. Existing methods using CL in NLU tasks rely on heuristics such as sentence length, word rarity, depth of the dependency tree (Platanios et al., 2019;Tay et al., 2019), metrics based on item-response theory (Lalor and Yu, 2020) or task-agnostic model metrics such as perplexity (Zhou et al., 2020). Such metrics have been employed to either improve in-distribution performance on NLU or Machine Translation. However, their effect is still underexplored on other settings. \n\nIn this study instead, we propose to adopt training dynamics (Swayamdipta et al., 2020, TD) as difficulty measures for CL and fine-tune models with curricula on downstream tasks. TD were re-cently proposed as a set of statistics collected during the course of a model's training to automatically evaluate dataset quality, by identifying annotation artifacts. These statistics, offer a 3-dimensional view of a model's uncertainty towards each training example classifying them into distinct areas-easy, ambiguous and hard examples for a model to learn. \n\nWe test a series of easy-to-hard curricula using TD, namely TD-CL, with existing schedulers as well as novel modifications of those and experiment with other task-specific and task-agnostic metrics. We show performances and training times on three settings: in-distribution (ID), out-of-distribution (OOD) and zero-shot (ZS) transfer to languages different than English. To the best of our knowledge, no prior work on NLU considers the impact of CL on all these settings. To consolidate our findings, we evaluate models on different classification tasks, including Natural Language Inference, Paraphrase Identification, Commonsense Causal Reasoning and Document Classification. \n\nOur findings suggest that TD-CL provides better zero-shot cross-lingual transfer up to 1.2% over prior work and can gain an average speedup of 20%, up to 51% in certain cases. In ID settings CL has minimal to no impact, while in OOD settings models trained with TD-CL can boost performance up to 8.5% on a different domain.",
            "score": 0.31872029292056714,
            "section_title": "Introduction",
            "char_start_offset": 1638,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 555
                },
                {
                    "start": 558,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1109
                },
                {
                    "start": 1112,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1789
                },
                {
                    "start": 1792,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2115
                }
            ],
            "ref_mentions": [
                {
                    "start": 210,
                    "end": 234,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 234,
                    "end": 251,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 291,
                    "end": 311,
                    "matchedPaperCorpusId": "226226711"
                },
                {
                    "start": 362,
                    "end": 381,
                    "matchedPaperCorpusId": "220047761"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72509765625
        },
        {
            "corpus_id": "270063170",
            "title": "ConStat: Performance-Based Contamination Detection in Large Language Models",
            "text": "As large language models (LLMs) become increasingly effective at a wide range of tasks, many companies and research institutions compete to develop better models [2,5,28,35].To facilitate this development, a variety of benchmarks have been proposed that allow a standardized in-depth comparison of model performance across diverse tasks [15,16,26,32].\n\nData Contamination Modern LLMs are trained on vast amounts of internet-sourced data, raising the risk of unintentionally including benchmark samples in the training set.Such data contamination can lead to artificially inflated benchmark performance that does not accurately reflect a model's true ability to generalize to unseen tasks.However, model providers argue that the impact of this contamination on model performance is negligible [2,14,35] and the enormous size of current training sets almost guarantees contamination to some extent.This casts doubt on the relevance of this traditional definition of contamination in the context of LLMs.This Work: A New Perspective on Data Contamination We propose a new perspective on contamination, defining it based on its effect on model performance rather than its cause.Specifically, we define contamination as artificially inflated, non-generalizing performance, i.e., we say a model is contaminated if and only if its performance relative to other models is significantly higher on the original benchmark than on a similar reference benchmark.This definition captures the essence of the  contamination problem, i.e., performance measurements becoming unreliable for model comparisons.Furthermore, it enables principled detection methods that are robust against evasion attacks by malicious providers as this would require generalizing performance improvements.\n\nTraditional Contamination Detection Existing contamination detection methods [18,23,24,30,33,36,39,40,48] aim to detect the inclusion of benchmark samples in the training data as a measure of contamination.However, these approaches show limited success, cannot quantify the contamination's effect on model performance, and have to make strict assumptions about the contamination process, making them easy to evade [17].\n\nThis Work: A Statistical Test for Contamination In contrast, we leverage our novel performancebased definition of data contamination to propose a statistical contamination test called CONSTAT, illustrated in Fig. 1.",
            "score": 0.31864175495673863,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 174,
                    "end": 351
                },
                {
                    "start": 353,
                    "end": 522
                },
                {
                    "start": 522,
                    "end": 688
                },
                {
                    "start": 688,
                    "end": 896
                },
                {
                    "start": 896,
                    "end": 1001
                },
                {
                    "start": 1001,
                    "end": 1174
                },
                {
                    "start": 1174,
                    "end": 1449
                },
                {
                    "start": 1449,
                    "end": 1590
                },
                {
                    "start": 1590,
                    "end": 1766
                },
                {
                    "start": 1768,
                    "end": 1974
                },
                {
                    "start": 1974,
                    "end": 2187
                },
                {
                    "start": 2189,
                    "end": 2404
                }
            ],
            "ref_mentions": [
                {
                    "start": 341,
                    "end": 344,
                    "matchedPaperCorpusId": "239998651"
                },
                {
                    "start": 344,
                    "end": 347,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 347,
                    "end": 350,
                    "matchedPaperCorpusId": "237532606"
                },
                {
                    "start": 1858,
                    "end": 1861,
                    "matchedPaperCorpusId": "258967264"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2225341796875
        },
        {
            "corpus_id": "254366532",
            "title": "DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing",
            "text": "Recently, large-scale deep learning models are empowering us to achieve more in many ways, such as code generation (GitHub 2021) and text-to-image generation (Ramesh et al. 2022;Rombach et al. 2022). To keep improving the service quality, deep learning model architecture evolves rapidly, and the model size is also growing at a tremendous speed. The increasing model size leads to unprecedented training cost (especially for foundation model pretraining), which recently grows to 2 months on thousands of GPUs/T-PUs (Smith et al. 2022;Chowdhery et al. 2022). On the other hand, a less-emphasized perspective is that data scale is actually increasing at a similar speed as model scale, and Figure 1: Model scale (number of parameters) and data scale (number of consumed training tokens ) of representative language models in the last 5 years (Devlin et al. 2019;Shoeybi et al. 2019;Brown et al. 2020;Scao et al. 2022;Chowdhery et al. 2022). \n\nthe training cost is proportional to both of them. As plotted in Fig. 1, for several representative language models in the last 5 years both the model and data scales increase at a similar speed. Recent works including Chinchilla (Hoffmann et al. 2022) and PaLM 2 (Google 2023) emphasize the need of increasing data scale at an even faster speed. This demonstrates the importance of improving data efficiency: achieve same model quality with less data and reduced training cost, or achieve better model quality with the same amount of data and similar training cost. \n\nThere are two popular research directions among existing data efficiency techniques: Data sampling techniques aim to improve the convergence speed by sampling the most suitable next data batch from the whole data pool; Data routing techniques aim to reduce the computation by routing each data to only a subset of the model components. These techniques improve data and training efficiency, but existing solutions have several limitations: \u2022 Techniques like curriculum learning (CL) improve data efficiency by indexing and sampling training data based on certain difficulty metric (Bengio et al. 2009), and it has recently proved effective on large-scale pretraining tasks (Li, Zhang, and He 2022).",
            "score": 0.31853769111400465,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 940
                },
                {
                    "start": 943,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1509
                },
                {
                    "start": 1512,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 2210
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 178,
                    "matchedPaperCorpusId": "248097655"
                },
                {
                    "start": 178,
                    "end": 198,
                    "matchedPaperCorpusId": "248097655"
                },
                {
                    "start": 842,
                    "end": 862,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 882,
                    "end": 900,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 900,
                    "end": 917,
                    "matchedPaperCorpusId": "16503693"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33984375
        },
        {
            "corpus_id": "273162483",
            "title": "How Hard is this Test Set? NLI Characterization by Exploiting Training Dynamics",
            "text": "Natural Language Inference (NLI) evaluation is crucial for assessing language understanding models; however, popular datasets suffer from systematic spurious correlations that artificially inflate actual model performance. To address this, we propose a method for the automated creation of a challenging test set without relying on the manual construction of artificial and unrealistic examples. We categorize the test set of popular NLI datasets into three difficulty levels by leveraging methods that exploit training dynamics. This categorization significantly reduces spurious correlation measures, with examples labeled as having the highest difficulty showing markedly decreased performance and encompassing more realistic and diverse linguistic phenomena. When our characterization method is applied to the training set, models trained with only a fraction of the data achieve comparable performance to those trained on the full dataset, surpassing other dataset characterization techniques. Our research addresses limitations in NLI dataset construction, providing a more authentic evaluation of model performance with implications for diverse NLU applications.",
            "score": 0.3184631343911114,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51416015625
        },
        {
            "corpus_id": "276394867",
            "title": "Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples",
            "text": "While various metrics such as length (Spitkovsky et al., 2010;Tay et al., 2019;Nagatsuka et al., 2023) and perplexity (Wu et al., 2024) have been proposed to measure difficulty of text samples, their ability to reliably capture example difficulty remains controversial (Campos, 2021). We address this concern by demonstrating",
            "score": 0.31815208617474316,
            "section_title": "The Underlying Example Difficulty",
            "char_start_offset": 4590,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 325
                }
            ],
            "ref_mentions": [
                {
                    "start": 79,
                    "end": 102,
                    "matchedPaperCorpusId": "255221201"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.375
        },
        {
            "corpus_id": "265506572",
            "title": "Can training neural language models on a curriculum with developmentally plausible data improve alignment with human reading behavior?",
            "text": "The datasets with transcribed speech had the lowest average sentence difficulty scores. Even within transcribed speech, datasets with informal speech (such as child directed speech and subtitles) had lower average difficulty scores than datasets with 4 SAP Benchmark uses Bayesian mixed effects models. We use linear mixed effects models because they are less resource intensive to fit and yield nearly identical model estimates. more formal speech (such as BNC). Additionally, as expected the proportion of transcribed speech steadily decreased over time, as the proportion of written text increased. By the last \"epoch\", the distribution of datasets was very similar to the true distribution (see Figure 2), suggesting that the crossreview method we used as our difficulty-measurer was effective, as was the root-10 training scheduler. \n\nAgreement between LSTM teachers For any given sentence, there was a lot of variance in the surprisal estimates across the teachers: the average standard deviation was 113 bits of surprisal; the mean Spearman rank correlation between any two pairs of teachers was only 0.0009. This highlights the importance of averaging the surprisal estimates across different teachers to avoid over-fitting to idiosyncrasies of any particular teacher model. \n\nOther difficulty measures Figure 7 plots the correlation between our difficulty measure computed using the cross-review method and two other simpler difficulty measures: average unigram frequency of the words in a sentence and sentence length. Our difficulty measure is moderately correlated with unigram frequency (R = 0.27, p < 0.0001) and highly correlated with sentence length (R = 0.89, p < 0.0001). We also predicted our difficulty measure as a function of unigram frequency and sentence length in a linear regression model and found that unigram frequency explains variance in our difficulty measure over and above sentence length, and together they explain most of the variance in the difficulty measure (adjusted R-squared = 0.93). This suggests that for the specific BabyLM datasets, using cross-review, while effective, might not be necessary: using faster-tocompute measures such as sentence length would have likely resulted in a comparable curriculum.",
            "score": 0.31815208617474316,
            "section_title": "What curriculum was learned?",
            "char_start_offset": 16142,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 837
                },
                {
                    "start": 840,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2250
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15576171875
        },
        {
            "corpus_id": "248810776",
            "title": "Exploring the Learning Difficulty of Data: Theory and Measure",
            "text": "Under the discussion in Section 4.2, an approximate approach is proposed based on Err * \n\nx , and is utilized as the practical measure of learning difficulty. Most existing measurement methods utilize the training loss (or loss variance) which can be considered as an approximation of the bias (or the variance) term of Err * \n\nx as a measurement of learning difficulty. Nevertheless, few studies consider the bias and the variance terms simultaneously. \n\nConsidering that it is also infeasible to calculate Err * x traversing all values of \u03bb h , we only calculate the generalization errors Err(x, \u03bb h ) for each sample with the same 7 reasonable \u03bb h to approximate the learning difficulty. Specifically, the proposed approach adopts the cross-validation strategy to calculate the average learning errors for each sample. First, the whole training set is divided into M folds. M \u2212 1 folds are alternatively used for training, and the trained model is used to predict the label of all training samples. This cross-validation process is repeated for K times. Each sample receives K * M predictions, with which can we calculate the average prediction of each sample. Second, average losses and variance of losses for each training sample are calculated using corresponding average predictions. \n\nLet p k i,m be the prediction of x i in the m th cross-validation of the k th repeat. Then, according to [70], we calculate: \n\nSubsequently, the bias and the variance terms are calculated as follows \n\nwhere l CE is the standard cross-entropy loss. The actual value of learning difficulty of \n\nwhere \u00b5 is a tuning factor for the variance. The value of Err(x i , \u03bb h ) is used as the learning difficulty for x i . This approach is called generalization errorbased learning difficulty (GELD) measurement. The detailed steps of GELD are shown in Algorithm 1. The primary difference between our approach and the existing loss-based/cross-validation-based methods lies in that our approach does not discard the variance term but combines the importance of both term of generalization error. If \u00b5 = 0, then GELD is similar to the conventional cross-validation-based methods. Several existing methods including O2UNet [33] also point out that hard samples have high loss variances. \n\nM ; \n\n3:",
            "score": 0.31815208617474316,
            "section_title": "The Proposed Measure",
            "char_start_offset": 25389,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 90,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 325
                },
                {
                    "start": 328,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 453
                },
                {
                    "start": 456,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1290
                },
                {
                    "start": 1293,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1491
                },
                {
                    "start": 1494,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1583
                },
                {
                    "start": 1586,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2160
                },
                {
                    "start": 2161,
                    "end": 2266
                },
                {
                    "start": 2269,
                    "end": 2272
                },
                {
                    "start": 2275,
                    "end": 2277
                }
            ],
            "ref_mentions": [
                {
                    "start": 1398,
                    "end": 1402,
                    "matchedPaperCorpusId": "211505957"
                },
                {
                    "start": 2203,
                    "end": 2207,
                    "matchedPaperCorpusId": "207995300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55615234375
        },
        {
            "corpus_id": "257220027",
            "title": "DCLP: Neural Architecture Predictor with Curriculum Contrastive Learning",
            "text": "where \n\nwhere t denotes steps (t \u2208 {1, 2, ...T }), g is tanh, \u03c3 = 0.9, and n > 1, k > 1 controls the frequency and range of difficulty decrease respectively, the decreasing will be discussed in the next paragraph. \u03c4 t determines the preference for easy/hard data. For \u03c4 t < 0, larger difficulty corresponds to a smaller selection probability. Conversely, when \u03c4 t > 0, the curriculum favors hard data. Our strategy gradually adjusts \u03c4 t to achieve robust learning. Initially, we set a negative \u03c4 1 to emphasize learning from easy data at the start without being affected by noise and to quickly reach a relatively good ability. Then we gradually increase \u03c4 t to encourage deeper learning from hard data. Finally, we use a positive \u03c4 T , smoothly transitioning from easy to hard data. However, unlike the conventional easy-to-hard curriculum, we introduce a fluctuating term that causes the curriculum to fluctuate in difficulty controlled by n and k. This nonmonotonic increase in difficulty includes intervals of decreasing difficulty. Therefore, the model can revisit simpler data periodically during training. It is necessary since the harder positive data includes more noise, i.e., negative samples that are mistakenly labeled as positive. Training with increasing difficulty often causes the model to focus excessively on the noise, resulting in loss of learned knowledge in the correct data. Intermittent training with simple data can counteract the learned misinformation from the noise, improving training stability, and being more robust. \n\nThe curriculum facilitates the encoder to acquire a stronger ability by increasing data difficulty. Additionally, incorporating easy data prevents overfitting that may occur as difficulty increases, leading to efficient convergence. \n\nAs shown in Algorithm 1, we use the difficulty function to calculate the difficulty of each positive data and sort them according to the scheduler (Line 10). Next, we feed the positive samples into the contrastive learner for training.",
            "score": 0.31815208617474316,
            "section_title": "Curriculum Learning",
            "char_start_offset": 14473,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 5
                },
                {
                    "start": 8,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1548
                },
                {
                    "start": 1551,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1783
                },
                {
                    "start": 1786,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2021
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.391357421875
        },
        {
            "corpus_id": "248810776",
            "title": "Exploring the Learning Difficulty of Data: Theory and Measure",
            "text": "There are two strategies to measure the learning difficulty with the illumination of our definitions. The first is to seek an existing measure that best suits our definitions; the second is to derive a new measure on the basis of the definitions. This study chooses the second path. Results in Example 2 indicate a positive correlation between optimal model complexity and minimum generalization error. Moreover, the definition of optimal model complexity is closely related to the generalization error. As a consequence, we propose a new measure for learning difficulty with generalization error.",
            "score": 0.31815208617474316,
            "section_title": "Definitions of Easy and Hard Samples",
            "char_start_offset": 20012,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 597
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.355224609375
        },
        {
            "corpus_id": "254591617",
            "title": "Simplicity Bias Leads to Amplified Performance Disparities",
            "text": "In this work we choose to measure difficulty using cross-validated test accuracy, averaged over all samples in a group or class. While it may be possible to rewrite the specific results above in terms of accuracy disparity, we instead refer to difficulty disparity because our core claims involve relative, model-perceived group difficulties, and we expect difficulty amplification to also occur in settings where accuracy is not an appropriate performance metric. \n\nRecent works have investigated alternative methods for quantifying model-specific example difficulty, including loss [3,32] and prediction disagreement between models [72], mini-batches [13], and throughout training [74,76]. [37] identifies samples that are often forgotten after compression. Applying these sample-level measures to evaluating group-level difficulty disparity remains an interesting future direction.",
            "score": 0.31815208617474316,
            "section_title": "Measuring difficulty",
            "char_start_offset": 34731,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 464
                },
                {
                    "start": 467,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 884
                }
            ],
            "ref_mentions": [
                {
                    "start": 584,
                    "end": 587,
                    "matchedPaperCorpusId": "131777002"
                },
                {
                    "start": 587,
                    "end": 590,
                    "matchedPaperCorpusId": "52065462"
                },
                {
                    "start": 653,
                    "end": 657,
                    "matchedPaperCorpusId": "20785832"
                },
                {
                    "start": 687,
                    "end": 690,
                    "matchedPaperCorpusId": "55481903"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5185546875
        },
        {
            "corpus_id": "274777442",
            "title": "Are You Doubtful? Oh, It Might Be Difficult Then! Exploring the Use of Model Uncertainty for Question Difficulty Estimation",
            "text": "In an educational setting, an estimate of the difficulty of multiple-choice questions (MCQs), a commonly used strategy to assess learning progress, constitutes very useful information for both teachers and students. Since human assessment is costly from multiple points of view, automatic approaches to MCQ item difficulty estimation are investigated, yielding however mixed success until now. Our approach to this problem takes a different angle from previous work: asking various Large Language Models to tackle the questions included in three different MCQ datasets, we leverage model uncertainty to estimate item difficulty. By using both model uncertainty features as well as textual features in a Random Forest regressor, we show that uncertainty features contribute substantially to difficulty prediction, where difficulty is inversely proportional to the number of students who can correctly answer a question. In addition to showing the value of our approach, we also observe that our model achieves state-of-the-art results on the USMLE and CMCQRD publicly available datasets.",
            "score": 0.31815208617474316,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5537109375
        },
        {
            "corpus_id": "270923838",
            "title": "LLMs Plagiarize: Ensuring Responsible Sourcing of Large Language Model Training Data Through Knowledge Graph Comparison",
            "text": "There exists a large range of literature that addresses identifying large language model training data in a \"black-box\" environment where the training corpus is unknown. For instance, LLM training data sourcing has been assessed through min-k% prob, which is a detection method based on the assumption that a member of the training data is less likely to include words that have high negative log-likelihood (and are thus outlier words) compared to a non-member of the training data (Shi et al., 2024), therefore considering \"anomalous\" vocabulary within a text. \n\nSuch an approach, based on the principles of Membership Interference Attacks (MIAs), an adversarial technique that seeks to determine whether a knowledge source is part of a model's training data, is the most common method to identify LLM training data in \"black-box\" environments. Substantial literature also exists about utilizing MIA principles to identify corpora used to fine-tune LLMs, addressing word embeddings (Mahloujifar et al., 2021), addressing NLP classification models for members of training corpora (Shejwalkar et al., 2021), and addressing source text memorization (Song and Shmatikov, 2019). \n\nHowever, such approaches based on MIA principles take a statistical and probabilistic approach to identifying LLM training data, ignoring other \"signs\" of sourcing that extend beyond simple copying or paraphrasing. Statistical measures such as only considering the likelihood of \"anomalous\" words ignore the broad relationships between ideas that exist in sentences of a source corpora that may manifest themselves in an LLM's generated answer. \n\nAdditionally, traditional plagiarism detection systems (systems that compare the similarity of multiple corpora) often rely on simple matching techniques. For instance, these systems may search direct token (word, sentence, unique phrase, paragraph, etc.) matches between a document and others, using a threshold for matches as an indicator of plagiarism/similarity. Other systems narrow down at the individual word/phrase level, analyzing semantic relationships through simple synonym/antonym detection or more complex Semantic Role Labeling techniques between words in target and source sentences (Osman et al., 2012).",
            "score": 0.3173402981625538,
            "section_title": "Literature Review",
            "char_start_offset": 3486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 562
                },
                {
                    "start": 565,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1175
                },
                {
                    "start": 1178,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1622
                },
                {
                    "start": 1625,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2245
                }
            ],
            "ref_mentions": [
                {
                    "start": 1081,
                    "end": 1106,
                    "matchedPaperCorpusId": "245222525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06707763671875
        },
        {
            "corpus_id": "277398866",
            "title": "Distilling Wisdom: A Review on Optimizing Learning From Massive Language Models",
            "text": "For instance, if the student model performs well on easier samples but struggles with harder ones, the distillation process will emphasize these harder samples, providing more detailed and nuanced knowledge transfer from the teacher model. This targeted training helps in improving the student model's performance more efficiently compared to uniform distillation methods. Adaptive KD can be implemented using various techniques, such as adjusting the weight of each sample's loss based on its difficulty or using an adaptive temperature in the softmax function to control the smoothness of the teacher model's predictions. These methods ensure that the distillation process is more responsive to the learning needs of the student model, leading to more efficient and effective training outcomes. \n\nModel compression techniques like KD, pruning, quantization, and adaptive KD each have unique pros and cons. KD transfers knowledge from a larger teacher model to a smaller student model, significantly reducing size while maintaining accuracy [6], especially in natural language processing [16]. However, it relies on large labeled datasets and struggles with very small models [6]. Pruning removes redundant weights or neurons for better compression and lower computational demands, but often sacrifices accuracy and requires fine-tuning [48], [49]. Quantization reduces model precision, saving memory and speeding up inference, though it can degrade accuracy in complex tasks without specialized hardware [50]. Adaptive KD customizes the distillation process based on input difficulty or model alignment, improving performance but increasing training complexity [47], [51]. While KD and its adaptive forms balance performance and size, pruning and quantization excel in extreme compression scenarios, highlighting the complementary nature of these methods.",
            "score": 0.3173388872112666,
            "section_title": "1) ADDITIONAL TECHNIQUES AND APPLICATIONS",
            "char_start_offset": 26928,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 796
                },
                {
                    "start": 799,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1857
                }
            ],
            "ref_mentions": [
                {
                    "start": 1042,
                    "end": 1045,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1177,
                    "end": 1180,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1344,
                    "end": 1348,
                    "matchedPaperCorpusId": "212628335"
                },
                {
                    "start": 1506,
                    "end": 1510,
                    "matchedPaperCorpusId": "67855262"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69091796875
        },
        {
            "corpus_id": "259088773",
            "title": "NLU on Data Diets: Dynamic Data Subset Selection for NLP Classification Tasks",
            "text": "We bring up two main ethical considerations. First, this empirical study uses a large language model requiring a considerable amount of computations (at most 1,500 GPU hours), which is not without environmental consequences. However, since this study aims at making training more efficient, it will help reduce energy consumption in the future. Moreover, this study focuses on accuracy as a measure of performance, which can hide pervasive effects on under-represented marginalized groups. However, since our method is about evaluating importance of training examples over train steps, it can lead to improving techniques to decrease bias in the training process, particularly when marginalized groups are under-represented in the data and therefore challenging to identify accurately. We begin by defining the GraNd score, \u03c7 t (x, y) \u2208 R \u22650 , at time t > 0 and for any sample data (x, y) as a measure of sample importance (Paul et al., 2021):\n\nwhere g t is the gradient of the sample (x, y) obtained with model f at time t. For simplicity, we define \u03c7 \u2032 t (x, y) as the same expression as in Equation (8) but without the expectation term. We compensate for the improvement of this expectation with dynamic pruning by leveraging the exponential moving average:\n\nBecause processing per-sample gradients can be computationally burdensome, we look for an upper bound. First, we use the result from Katharopoulos and Fleuret (2018), which implies that the full gradient is upper bounded by a constant \u03b2 \u2208 R >0 multiplied by the gradient of the last classification layer. Let G t (x, y) \u2208 R K\u00d7d denote the gradient of the last classification layer, where K, d \u2208 N (number of classes and hidden dimensions, respectively). We can write this result as\n\nSince the last layer is a classification with the cross-entropy loss on a softmax activation, it naturally takes the form of a K \u00d7 d matrix defined by the outer product of the error vector \u20d7 p(x) \u2212 \u20d7 y and the transposed contextual word embedding \u20d7 h CLS (x) \u2208 R d . For simplification purposes, we do not carry the notation marking the dependence on x for \u20d7 p and \u20d7 h CLS :\n\nBy replacing Equation (11) in Equation (10), we",
            "score": 0.31673745646175033,
            "section_title": "Ethics Statement",
            "char_start_offset": 24700,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.302001953125
        },
        {
            "corpus_id": "256461264",
            "title": "Tutoring Helps Students Learn Better: Improving Knowledge Distillation for BERT with Tutor Network",
            "text": "Our key idea is to introduce a tutor network that controls the difficulty of training examples during KD. Specifically, the tutor generates training examples, which are easy for the teacher but difficult for the student, by replacing the masked tokens with corrupted tokens via masked language modeling (MLM). To accurately identify the difficulty of the tutor-generated training samples, we propose a novel method to train the tutor based on a policy gradient with carefully designed rewards. The tutor network is therefore optimized to generate tokens with relatively lower teacher losses (i.e., more accurate predictions) and consequently prevent overly difficult samples from being generated. Simultaneously, the student trained with generated examples can be benefited to learn more effectively due to the increased difficulty. \n\nWe conduct extensive experiments on downstream NLP tasks using various sizes of student models. Our experimental results show that the proposed approach significantly improves the language model distillation performance. Specifically, the 6-layer model with 768 hidden dimensions distilled from BERT-base outperforms the teacher model on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019). Moreover, our framework shows notable effectiveness for extremely small-sized student models that are designed 7.5\u00d7 smaller than BERT-base. Finally, we demonstrate that our designed rewards can generate effective samples for both the teacher and the student. As a summary of our main contributions: \n\n\u2022 We propose Tutor-KD, a novel KD framework for PLMs that improves the distillation effectiveness by considering the difficulty of training examples. \n\n\u2022 We present a tutor network with the corresponding training scheme to generate training samples that are easy for the teacher but difficult for the student based on a policy gradient. \n\n\u2022 We verify that Tutor-KD improves the effectiveness of KD on students with various sizes through extensive experiments. \n\n2 Related Work",
            "score": 0.31671026945622693,
            "section_title": "Introduction",
            "char_start_offset": 2032,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1555
                },
                {
                    "start": 1558,
                    "end": 1707
                },
                {
                    "start": 1710,
                    "end": 1894
                },
                {
                    "start": 1897,
                    "end": 2017
                },
                {
                    "start": 2020,
                    "end": 2034
                }
            ],
            "ref_mentions": [
                {
                    "start": 1236,
                    "end": 1255,
                    "matchedPaperCorpusId": "5034059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4208984375
        },
        {
            "corpus_id": "259212486",
            "title": "Sample Attackability in Natural Language Adversarial Attacks",
            "text": "Experiments in this section aim to understand how well a deep-learning based detector, described in Section 5, performs in identifying attackable samples for an unseen dataset and an unseen target model, F t , where the detector only has access to a separate set of seen models, M during training. There are equivalent experiments looking to detect the most robust samples too. The performance of the deep learning based detector is compared to a baseline of uncertainty-based detectors (model confidence), inspired by Zeng et al. (2020), in which the samples with the most uncertain model predictions are identified as attackable and in converse the most certain samples are deemed to be robust. Specifically, two forms of uncertainty-based detectors are considered: 1) conf-u, where there is no access to the confidence from the unseen target model and so a sample's uncertainty is measured by an average of the confidence of the seen models, M; and as a realistic reference we also consider 2) conf-s, where there is access to the target model output such that the target model's confidence is used directly as a measure of sample uncertainty. \n\nTwo popular natural language classification datasets are used in these experiments. First, the Stanford Sentiment Treebank2 dataset (sst) (Socher et al., 2013) is a movie review dataset with each review labelled as positive or negative. There are 6920 training samples, 872 validation samples and 1820 test samples. We also consider the Twitter Emotions dataset (Saravia et al., 2018), which categorizes tweets into one of six emotions: sadness, love, anger, surprise, joy and fear. This dataset contains 16,000 training samples, 2000 validation samples and 2000 test samples. For training of the attackability detectors, access was provided to only the validation data and hence the test data was used as an unseen set of samples to assess the performance of attackable sample detection. \n\nThese experiments work with four state of the art NLP transformer-based models: BERT (bert) (Devlin et al., 2018), XLNet (xlnet) (Yang et al., 2019), RoBERTa (roberta) (Liu et al., 2019) and Electra (electra) (Clark et al., 2020).",
            "score": 0.3164464756505725,
            "section_title": "Setup",
            "char_start_offset": 16083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 1146
                },
                {
                    "start": 1149,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1937
                },
                {
                    "start": 1940,
                    "end": 2170
                }
            ],
            "ref_mentions": [
                {
                    "start": 1287,
                    "end": 1308,
                    "matchedPaperCorpusId": "990233"
                },
                {
                    "start": 1511,
                    "end": 1533,
                    "matchedPaperCorpusId": "53080764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.169189453125
        },
        {
            "corpus_id": "269188036",
            "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
            "text": "The evaluation of text produced by large language models involves analyzing performance across a range of downstream tasks using standard metrics that assess linguistic quality, coherence, accuracy, and alignment with ground-truth data. Metrics like BLEU [107] and ROUGE-L [87] are often used to measure fluency, similarity to human-produced text, and the overlap with reference summaries, respectively, providing insights into how well the generated content captures key ideas and phrases. \n\nIn addition to these metrics, which focus on the quality of linguistic output, Accuracy and overlap with ground-truth data are evaluated using EM and F1 scores, which respectively measure the percentage of completely correct answers and offer a balanced view of precision and recall. This ensures that relevant answers are retrieved while inaccuracies are minimized. \n\nBeyond these standard evaluation techniques, more specialized criteria have been introduced to assess RAG systems in specific contexts. For dialogue generation, for instance, metrics like perplexity and entropy are employed to evaluate response diversity and naturalness. In scenarios where misinformation is a concern, metrics like Misleading Rate and Mistake Reappearance Rate [91] have been developed to measure a model's ability to avoid generating incorrect or misleading content. Other advanced metrics include Answer Relevance [33], which assesses the precision of responses to queries, Kendall's tau [117], used for evaluating the accuracy of system rankings, and Micro-F1 [117], which fine-tunes accuracy evaluation in tasks involving multiple correct answers. Prediction Accuracy further complements these by directly measuring how closely the generated responses align with the expected answers, offering a clear measure of a system's effectiveness in producing accurate content.",
            "score": 0.31567565872550035,
            "section_title": "Generation-based Aspect",
            "char_start_offset": 61590,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 490
                },
                {
                    "start": 493,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1852
                }
            ],
            "ref_mentions": [
                {
                    "start": 255,
                    "end": 260,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 273,
                    "end": 277,
                    "matchedPaperCorpusId": "964287"
                },
                {
                    "start": 1470,
                    "end": 1475,
                    "matchedPaperCorpusId": "265221210"
                },
                {
                    "start": 1543,
                    "end": 1548,
                    "matchedPaperCorpusId": "265221210"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.338623046875
        },
        {
            "corpus_id": "268379614",
            "title": "Language models scale reliably with over-training and on downstream tasks",
            "text": "Training large language models is expensive.Furthermore, training high-quality models requires a complex recipe of algorithmic techniques and training data.To reduce the cost of finding successful training recipes, researchers first evaluate ideas with small experiments and then extrapolate their efficacy to larger model and data regimes via scaling laws.With reliable extrapolation, it is possible to quickly iterate at small scale and still pick the method that will perform best for the final large training run.Indeed, this workflow has become commonplace for training state-of-the-art language models like Chinchilla 70B [45], PaLM 540B [19], GPT-4 [76], and many others.We fit a scaling law for model validation loss, parameterized by (i) a token multiplier M = N/D, which is the ratio of training tokens D to parameters N and (ii) the compute C in FLOPs used to train a model, approximated by C = 6N D. Larger values of M specify more over-training.We are able to extrapolate, in both N and M , the validation performance of models requiring more than 300\u00d7 the training compute used to construct the scaling law.(right) We also fit a scaling law to predict average downstream top-1 error as a function of validation loss.We find that fitting scaling laws for downstream error benefits from using more expensive models when compared to fitting for loss prediction.We predict the average error over 17 downstream tasks for models trained with over 20\u00d7 the compute.For this figure, we train all models on RedPajama [112].\n\nDespite their importance for model development, published scaling laws differ from the goals of training state-of-the-art models in important ways.For instance, scaling studies usually focus on the compute-optimal training regime (\"Chinchilla optimality\" [45]), where model and dataset size are set to yield minimum loss for a given compute budget.However, this setting ignores inference costs.As larger models are more expensive at inference, it is now common practice to over-train smaller models [113].Another potential mismatch is that most scaling laws quantify model performance by perplexity in next-token prediction instead of accuracy on widely used benchmark datasets.",
            "score": 0.3152830736581809,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 44
                },
                {
                    "start": 44,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 357
                },
                {
                    "start": 357,
                    "end": 517
                },
                {
                    "start": 517,
                    "end": 678
                },
                {
                    "start": 678,
                    "end": 958
                },
                {
                    "start": 958,
                    "end": 1121
                },
                {
                    "start": 1121,
                    "end": 1230
                },
                {
                    "start": 1230,
                    "end": 1372
                },
                {
                    "start": 1372,
                    "end": 1471
                },
                {
                    "start": 1471,
                    "end": 1527
                },
                {
                    "start": 1529,
                    "end": 1676
                },
                {
                    "start": 1676,
                    "end": 1877
                },
                {
                    "start": 1877,
                    "end": 1923
                },
                {
                    "start": 1923,
                    "end": 2034
                },
                {
                    "start": 2034,
                    "end": 2207
                }
            ],
            "ref_mentions": [
                {
                    "start": 628,
                    "end": 632,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 644,
                    "end": 648,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 1784,
                    "end": 1788,
                    "matchedPaperCorpusId": "247778764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30810546875
        },
        {
            "corpus_id": "258840917",
            "title": "Separating form and meaning: Using self-consistency to quantify task understanding across multiple senses",
            "text": "The staggering pace at which the capabilities of large language models (LLMs) have increased in the recent past comes with many questions related to what kind of progress we are making on the road towards true machine intelligence and human-level understanding. To assess such progress, practitioners often rely on benchmarks that measure natural language understanding (e.g. Williams et al., 2018;Nie et al., 2020), commonsense reasoning (e.g. Sap et al., 2019;Bisk et al., 2020), or probe for factual knowledge (e.g. Hendrycks et al., 2021), among other things. The extent to which such benchmarks can be used to assess whether LLMs \"understand\" language is widely debated (e.g. Mitchell and Krakauer, 2023;Raji et al., 2021). Often mentioned concerns in this context are that LLMs may learn specific lexical patterns rather than general principles (e.g. Ray Choudhury et al., 2022) and, relatedly, that benchmark scores may confuse competence in form with competence in meaning (e.g. Heineman, 2023). In support of these concerns, LLMs have been found to bypass certain tasks by relying on memorised information from the training data (McKenna et al., 2023). More recently, the enormous amount of data that models are trained on and the fact that this data is often not publically accessible have further increased the difficulty of assessing whether benchmarks really quantify what they are meant to quantify. A benchmark always makes assumptions about what a model has seen in its training phase, and, given the rapid changes on that front, it is difficult to design challenging benchmarks that remain informative past training rounds of new models. In addition, novel evaluation data may leak into the training data of newly trained models1 -which even the most future-proofed benchmarks may not withstand. \n\nIn this paper, we propose a novel approach to evaluate models' task or world understanding that aims to create some separation between form and meaning in benchmarking and simultaneously mitigates the challenging evaluation-contamination loop.",
            "score": 0.31522944278065046,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1812
                },
                {
                    "start": 1815,
                    "end": 2058
                }
            ],
            "ref_mentions": [
                {
                    "start": 398,
                    "end": 415,
                    "matchedPaperCorpusId": "207756753"
                },
                {
                    "start": 445,
                    "end": 462,
                    "matchedPaperCorpusId": "128296356"
                },
                {
                    "start": 462,
                    "end": 480,
                    "matchedPaperCorpusId": "208290939"
                },
                {
                    "start": 519,
                    "end": 542,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 681,
                    "end": 709,
                    "matchedPaperCorpusId": "253107905"
                },
                {
                    "start": 709,
                    "end": 727,
                    "matchedPaperCorpusId": "244729397"
                },
                {
                    "start": 861,
                    "end": 884,
                    "matchedPaperCorpusId": "252283929"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2120361328125
        },
        {
            "corpus_id": "275212729",
            "title": "Adaptive Hardness-driven Augmentation and Alignment Strategies for Multi-Source Domain Adaptations",
            "text": "Recently, hardness or uncertainty has gradually emerged as an extra focus for hard sample mining [21], [30] and curriculum learning [31]. For instance, some works [20], [21] leverage the entropy values of target predictions as an uncertainty measurement and minimize them to facilitate domain transfer. Similarly, some other studies [32], [33] endeavor to employ entropy-based hardness analysis on all unlabeled target samples, ranking and categorizing them into hard and easy groups based on a predefined threshold. Furthermore, some recent research has addressed uncertainty regarding label-wise and pair-wise correspondence in other fields, exploring the implications of noisy labels [62]- [64]. For instance, Yang et al. [62] proposed a novel method addressing coupled noisy labels in object Re-ID, effectively rectifying annotation errors. Lin Y. et al. [63] proposed a contrastive matching with momentum distillation for addressing bi-level noisy correspondence in graph matching. \n\nHowever, most hardness-driven methods in MDA classification tasks primarily rely on direct and instantaneous entropy measurement, lacking the involvement of class attributes (label information) and specialized designs (mechanisms) for diverse scenarios. Such reliance severely restricts the applicability and generalizability of evaluated hardness values. Consequently, despite a few works [34], [35] using model-adaptive hardness strategies in other tasks, most MDA methods only engage sample hardness for qualitative selecting purposes, leaving quantitative hardness applications largely unexplored. As shown in Fig. 1    ).  represents the real class (index)  or the pseudo-class (index)  \u0302. The Kronecker delta \u03b4 , is used to ascertain whether index j matches the pseudo or real class index () of    . In simple terms, the role of Basic AHM is to set the element representing the real class () in  ,   and the pseudo-class (  \u0302, also the element with the maximum value) in  ,  to 0, and then calculates the L2-norm of the remaining elements. Smooth AHM. Basic AHM may cause instability due to random fluctuations since it only provides assessments of sample hardness in the current epoch.",
            "score": 0.31503165799208427,
            "section_title": "B. Hardness and Uncertainty-based Measurements",
            "char_start_offset": 8788,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2046
                },
                {
                    "start": 2047,
                    "end": 2181
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 101,
                    "matchedPaperCorpusId": "233722138"
                },
                {
                    "start": 103,
                    "end": 107,
                    "matchedPaperCorpusId": "7849657"
                },
                {
                    "start": 132,
                    "end": 136,
                    "matchedPaperCorpusId": "51914060"
                },
                {
                    "start": 163,
                    "end": 167,
                    "matchedPaperCorpusId": "54216961"
                },
                {
                    "start": 169,
                    "end": 173,
                    "matchedPaperCorpusId": "233722138"
                },
                {
                    "start": 333,
                    "end": 337,
                    "matchedPaperCorpusId": "215786096"
                },
                {
                    "start": 339,
                    "end": 343,
                    "matchedPaperCorpusId": "214667381"
                },
                {
                    "start": 687,
                    "end": 691,
                    "matchedPaperCorpusId": "267512607"
                },
                {
                    "start": 693,
                    "end": 697,
                    "matchedPaperCorpusId": "247190295"
                },
                {
                    "start": 725,
                    "end": 729,
                    "matchedPaperCorpusId": "267512607"
                },
                {
                    "start": 859,
                    "end": 863,
                    "matchedPaperCorpusId": "254408875"
                },
                {
                    "start": 1379,
                    "end": 1383,
                    "matchedPaperCorpusId": "253734647"
                },
                {
                    "start": 1385,
                    "end": 1389,
                    "matchedPaperCorpusId": "244709482"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06292724609375
        },
        {
            "corpus_id": "269449667",
            "title": "TextGram: Towards a better domain-adaptive pretraining",
            "text": "In this method, the main idea is that the sentences in the general-domain corpus are scored by their perplexity score using an in-domain language model, and only the lowest ones are kept.\n\nPerplexity is an intrinsic evaluation metric for language models.It entails determining some metric to evaluate the language model as a whole, without regard for the particular tasks it will be used for.\n\nThere are several approaches to perplexity-based data selection, the one used in this work is described below.\n\nPerplexity as the normalized inverse probability of the test set Unigram-Normalized Perplexity as a Language Model Performance Measure with Different Vocabulary Sizes [13] explains the perplexity as the inverse probability of the test set, normalized by the number of words in the sequence.\n\nModels which assign probabilities to sequences of words (w 1 , w 2 , w 3 ...w n ) are called language models.\n\nFor a test set,\n\nBy Chain Rule of Probability, P (W ) = P (w 1 )P (w 2 |w 1 )P (w 3 |w 1:2 )...P (w n |w 1:n \u22121 ) ( 6)\n\nAn n-gram language model looks at the previous (n-1) words to estimate the next word in the sequence.For example, a bigram model will look at the previous 1 word, so:\n\nAs per the definition of perplexity in this approach, the paper thus defines the formula for perplexity (PPL) for a Bigram model as,\n\nTherefore, as we're using the inverse probability, a lower perplexity indicates a better model.This method is dependent on the size of the sequence, and it also implies that adding more sentences to the dataset will introduce more uncertainty, thus reducing the probability and increasing the perplexity.To make the metric independent of the dataset size, normalization of the probability is done by dividing it by the total number of tokens to obtain a per-word measure.This is why we take the n-th root (where n is the length of the sequence) of the inverse probability of the sequence.\n\nCross Entropy Cross-entropy is a popular loss function in machine learning.It is used to compute the overall entropy between distributions.",
            "score": 0.31503165799208427,
            "section_title": "Perplexity Based Data Selection",
            "char_start_offset": 11686,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 189,
                    "end": 254
                },
                {
                    "start": 254,
                    "end": 392
                },
                {
                    "start": 394,
                    "end": 504
                },
                {
                    "start": 506,
                    "end": 796
                },
                {
                    "start": 798,
                    "end": 907
                },
                {
                    "start": 909,
                    "end": 924
                },
                {
                    "start": 926,
                    "end": 1027
                },
                {
                    "start": 1029,
                    "end": 1130
                },
                {
                    "start": 1130,
                    "end": 1195
                },
                {
                    "start": 1197,
                    "end": 1329
                },
                {
                    "start": 1331,
                    "end": 1426
                },
                {
                    "start": 1426,
                    "end": 1635
                },
                {
                    "start": 1635,
                    "end": 1802
                },
                {
                    "start": 1802,
                    "end": 1919
                },
                {
                    "start": 1921,
                    "end": 1996
                },
                {
                    "start": 1996,
                    "end": 2060
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5673828125
        },
        {
            "corpus_id": "255749213",
            "title": "Understanding Difficulty-based Sample Weighting with a Universal Difficulty Measure",
            "text": "The commonly used difficulty measures, such as loss [3] and gradient norm [9], are mainly related to the bias term. Shin et al. [27] emphasized that only using loss as the measurement cannot distinguish clean and noisy samples, especially for uniform label noise. There are also a few existing studies that use variance [28,29]. For instance, Agarwal et al. [30] applied the variance of gradient norms as the difficulty measure. Indeed, both the variance and bias terms should not be underestimated when measuring the samples' learning difficulty. Our theoretical analyses support that generalization error including both the two terms can capture four main factors influencing the samples' learning difficulty. Thus, the error can be leveraged as a universal measure that is more reasonable than existing measures. Existing studies generally apply the K-fold cross-validation method [26] to calculate the generalization error. More efficient error calculation algorithms are supposed to be proposed which will be our future work.",
            "score": 0.31503165799208427,
            "section_title": "Discussion about Generalization Error",
            "char_start_offset": 19652,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1030
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 55,
                    "matchedPaperCorpusId": "1977996"
                },
                {
                    "start": 74,
                    "end": 77,
                    "matchedPaperCorpusId": "225499092"
                },
                {
                    "start": 320,
                    "end": 324,
                    "matchedPaperCorpusId": "20785832"
                },
                {
                    "start": 884,
                    "end": 888,
                    "matchedPaperCorpusId": "211505957"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62255859375
        },
        {
            "corpus_id": "267897619",
            "title": "Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach",
            "text": "For CuPUL, one assumption adopted is that difficulty scores can reflect the quality of distant supervision, where \"easier\" tokens have \"cleaner\" labels. \n\nTo validate this assumption and evaluate the quality of the difficulty score estimation, we examine the correlation between the difficulty scores and the quality of distant labels. We use Wikigold as the testbed, and the results are illustrated in Figure 2. \n\nFor each training curriculum, we compute the token-level positive error rate (positive errors include false positives and positive type errors), and plot the rate for each curriculum using the left y- We can see that as the number of curricula increases, the average token difficulty scores and positive error rate have a clear increase. This illustrates a strong negative correlation between the difficulty scores and the quality of distant labels. Specifically, as the difficulty score increases, the quality of the distant labels significantly decreases. This result validates our assumption that \"easy\" data have cleaner labels and \"hard\" data have noisier labels. The clean data can initialize the model from a better starting point and improve the model's robustness to noise in the latter curricula. \n\nAnother important assumption we adopt for the design of curricula is that the difficulty scores follow a long-tail distribution. We illustrate the distribution of difficulty scores estimated on the Wikigold dataset in Figure 3. It clearly demonstrates the long-tail phenomenon, with most tokens having low difficulty scores. This phenomenon can be observed in other datasets. Due to the space limit, we omit the plots for other datasets.",
            "score": 0.31503165799208427,
            "section_title": "Difficulty Score Estimation",
            "char_start_offset": 22133,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 155,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 412
                },
                {
                    "start": 415,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1221
                },
                {
                    "start": 1224,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1661
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35107421875
        },
        {
            "corpus_id": "276647269",
            "title": "LLM-driven Effective Knowledge Tracing by Integrating Dual-channel Difficulty",
            "text": "Difficulty, as a crucial indicator in the learning process, deserves thorough investigation in its semantic data mining and application in knowledge tracing. Previous difficulty models typically employed single difficulty indicators, either using LLM-assessed difficulty or statistical difficulty, without combining subjective question difficulty with students' objective difficulty. To deeply explore the impact of difficulty features on knowledge tracing and mitigate the cold start problem common in knowledge tracing, our proposed DDKT model incorporates LLM-assessed difficulty as part of the calibrated difficulty fusion, combines students' subjective and objective difficulty perceptions, and achieves more precise student modeling. The combination of LLMassessed and statistical difficulty provides educators with enhanced interpretability by allowing them to analyze learning patterns from both algorithmic and statistical perspectives, enabling more informed decision-making in educational interventions and content adaptation. Our DDKT model demonstrates superior performance among all baselines, and ablation studies confirm the collaborative effectiveness of its three modules. In our future work, we will focus on exploring deeper connections between student performance and difficulty from the student perspective, such as the relationship between problem-solving time and difficulty, the connection between repeated attempts and difficulty, etc., to construct a more refined knowledge tracing model that improves both accuracy and interpretability.",
            "score": 0.31503165799208427,
            "section_title": "Conclusion",
            "char_start_offset": 49738,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1564
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.216796875
        },
        {
            "corpus_id": "269156961",
            "title": "Question Difficulty Ranking for Multiple-Choice Reading Comprehension",
            "text": "Since the recently popularized ChatGPT, instruction finetuned large language models (Touvron et al., 2023;Jiang et al., 2023Jiang et al., , 2024;;Tunstall et al., 2023;Team et al., 2024) have demonstrated impressive zero-shot performance across various tasks.They tackle novel instructions without being explicitly trained on such tasks.These models have repeatedly demonstrated to be effective for evaluation (Chiang and Lee, 2023).Hence, we consider 2 zero-shot approaches, inspired by Liusie et al. (2023a), for MC difficulty estimation.\n\nAbsolute: For a given MC question, the LLM is prompted to return an absolute score for the difficulty.The model is requested to return a score in the range 1 to 10 where 10 is most difficult.See Appendix A for the exact prompt.For each question, K samples are drawn of the difficulty estimates.The final difficulty score is taken as the mean of the K samples.\n\nComparative: For a target MC question, K other questions are randomly selected from the corpus.The LLM is pairwise prompted to state which question is more difficult between the target and each of the K other questions.A win is defined if the target is deemed more difficult than the other question in the comparison.The difficulty score for the target question is simply the count of its wins.See Appendix A for the prompt.are combined together, reflecting that cohorts of different abilities are taking the tests for each grade (see the definition of difficulty in Section 3).",
            "score": 0.31503165799208427,
            "section_title": "Zero-shot",
            "char_start_offset": 6835,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 259,
                    "end": 337
                },
                {
                    "start": 337,
                    "end": 433
                },
                {
                    "start": 433,
                    "end": 540
                },
                {
                    "start": 542,
                    "end": 644
                },
                {
                    "start": 644,
                    "end": 733
                },
                {
                    "start": 733,
                    "end": 769
                },
                {
                    "start": 769,
                    "end": 836
                },
                {
                    "start": 836,
                    "end": 901
                },
                {
                    "start": 903,
                    "end": 998
                },
                {
                    "start": 998,
                    "end": 1122
                },
                {
                    "start": 1122,
                    "end": 1220
                },
                {
                    "start": 1220,
                    "end": 1297
                },
                {
                    "start": 1297,
                    "end": 1327
                },
                {
                    "start": 1327,
                    "end": 1481
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2117919921875
        },
        {
            "corpus_id": "278207682",
            "title": "Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges",
            "text": "Contemporary large language models (LLMs) [Brown et al., 2020][ Ouyang et al., 2022] [Radford et al., 2018][ Radford et al., 2019][Zhao et al., 2023] exhibit probabilistic output characteristics, yet current evaluation systems often employ deterministic methodologies for effectiveness determination. Such assessment mechanisms generate scalar scores as quantitative measures, theoretically grounded in the assumption of a unidimensional latent variable (analogous to the concept of intelligence quotient in humans) that supposedly underlies model capabilities. This quantitative metric then serves as the basis for cross-model comparisons and performance rankings. This methodological approach occasionally presents a paradoxical evaluation challenge. Consider two LLM A and B, administering an exam comprising two questions X and Y. Model A correctly answers only X, whereas model B correctly answers only Y, both achieving 50% accuracy. An uninformed evaluator might equate their performance, perceiving equivalence in competence. However, in most practical settings, questions are administered to multiple LLMs, yielding empirical data on their discriminative capability. For instance, question X is answered correctly by 90% of models, while question Y is solved by only 20%. With this context, the evaluation may shift. This raises critical questions about comparative assessment: How should we quantify the likelihood that one model surpasses the other, given their differential performance on a limited number of evaluation tasks? \n\nThere are several potential strategies to address this challenge. First, assigning differential score weights to questions X and Y based on their perceived difficulty level, though such scoring remains inherently subjective and fails to account for the statistical properties of LLMs. Second, Pass@N frameworks [Chen et al., 2021][ Chowdhery et al., 2022][Touvron et al., 2023] assess LLMs' upper-bound performance but face implementation challenges due to unspecified decoding strategies (e.g., temperature, top-k, top-p sampling), often requiring tradeoffs between optimizing rare high-quality outputs and ensuring consistent performance.",
            "score": 0.31503165799208427,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1538
                },
                {
                    "start": 1541,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 2181
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1646728515625
        },
        {
            "corpus_id": "248392007",
            "title": "Evaluating Automatic Difficulty Estimation of Logic Formalization Exercises",
            "text": "Perikos et al. ( 2016)'s difficulty estimation system does identify some features that, taken collectively, contribute to difficulty, as indicated by their generally quite good correlation with the empirical performance metrics. Interestingly, however, most of the features in their algorithm appear not to contribute much: one feature, namely the number of quantifiers, appears to be by far the most important one, as evidenced by the fact that the correlation of the empirical metrics with just that one feature grouped into five levels is not significantly different from the correlation of those metrics with Perikos et al. (2016)'s final score using all features. \n\nOur error analysis revealed some other interesting factors contributing to complexity. First, we saw that, like in language comprehension more generally (Grice (1975)), pragmatic factors play a role: if the sentence to be formalized is pragmatically improbable, the learner might make an error trying to make sense of that sentence. This idea would be very challenging to operationalize and to measure automatically; however, it could be borne in mind when developing or selecting exercises. Second, we saw that familiarity is a factor that influences difficulty: notably, students had no trouble with an exercise that was assigned the highest difficulty class by Perikos et al. (2016); closer investigation revealed that a practically identical exercise was explained in the chapter of the book used by the students in the course. A familiarity metric could be incorporated into the algorithm for determining difficulty by, for instance, comparing the exercise to those in the textbook and computing a similarity score. Finally, we saw that the average number of arguments of the predicates in the formula was a significant predictor of difficulty which improved on the system's predictions, presumably because a larger number of predicates signals a more complex relation, which would also translate into a more complicated sentence. \n\nExisting work has started to explore the relationship between the computational complexity of quantified logical formulas and the cognitive difficulty humans experience when processing those formulas. For example, McMillan et al. (2005) showed that different brain regions are involved in processing sentences containing quantifiers which are recognized by finite state automata (FA), versus push-down automata (PDA), higher-order quantifiers.",
            "score": 0.31503165799208427,
            "section_title": "Discussion of the findings",
            "char_start_offset": 29881,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 668
                },
                {
                    "start": 671,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 2006
                },
                {
                    "start": 2009,
                    "end": 2209
                },
                {
                    "start": 2210,
                    "end": 2452
                }
            ],
            "ref_mentions": [
                {
                    "start": 613,
                    "end": 634,
                    "matchedPaperCorpusId": "37207629"
                },
                {
                    "start": 1335,
                    "end": 1356,
                    "matchedPaperCorpusId": "37207629"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0855712890625
        },
        {
            "corpus_id": "270199394",
            "title": "Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models",
            "text": "A large focus of the machine learning community has been improving the performance of large language models (LLMs) while reducing their training costs.In this work, we consider how to improve the quality of an LLM by improving the quality of its pretraining data.Although there are many techniques to improve data quality, such as augmenting training samples with additional information (Li et al., 2024;Korbak et al., 2023), in this work we focus on the predominant method of data pruning: intelligently selecting a high-quality subset of a larger dataset to train on.\n\nData pruning is commonly used for quality filtering of noisy text data.Simple approaches include using symbolic rules (Bane et al., 2022;Raffel et al., 2020) or using simple classifiers to determine high-quality samples (Wenzek et al., 2020).However, in addition to basic quality filtering, more complex data pruning techniques are also applied to datasets to further improve their quality.Xie et al. (2023b) perform importance resampling where importance scores are calculated based on feature similarity to a target text.Tirumala et al. (2023) prune datasets by deduplicating and diversifying data based on a pretrained language model's embeddings of the text samples.Xie et al. (2023a) re-weight domain proportions based on learnability as determined by a smaller proxy model.Marion et al. (2023) investigate data pruning based on multiple neural heuristics of sample difficulty, ultimately concluding that the perplexity of a sample under a reference language model is the best pruning metric.\n\nIn this work, we thoroughly investigate the impact that data pruning based on sample perplexity (Marion et al., 2023) has on LLM pretraining.In particular, we focus on the interplay between pretraining dataset composition and pruning methodology.We further evaluate perplexity pruning in the overtrained and data-constrained regimes.We also investigate whether evaluating the quality of data interventions based on upstream test set perplexity is a sound methodology for gauging downstream performance.",
            "score": 0.31458772771458765,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 151,
                    "end": 263
                },
                {
                    "start": 263,
                    "end": 569
                },
                {
                    "start": 571,
                    "end": 642
                },
                {
                    "start": 642,
                    "end": 813
                },
                {
                    "start": 813,
                    "end": 961
                },
                {
                    "start": 961,
                    "end": 1094
                },
                {
                    "start": 1094,
                    "end": 1241
                },
                {
                    "start": 1241,
                    "end": 1350
                },
                {
                    "start": 1350,
                    "end": 1568
                },
                {
                    "start": 1570,
                    "end": 1711
                },
                {
                    "start": 1711,
                    "end": 1816
                },
                {
                    "start": 1816,
                    "end": 1903
                },
                {
                    "start": 1903,
                    "end": 2072
                }
            ],
            "ref_mentions": [
                {
                    "start": 387,
                    "end": 404,
                    "matchedPaperCorpusId": "260866107"
                },
                {
                    "start": 404,
                    "end": 424,
                    "matchedPaperCorpusId": "257020046"
                },
                {
                    "start": 689,
                    "end": 708,
                    "matchedPaperCorpusId": "252186406"
                },
                {
                    "start": 708,
                    "end": 728,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 791,
                    "end": 812,
                    "matchedPaperCorpusId": "207870323"
                },
                {
                    "start": 961,
                    "end": 979,
                    "matchedPaperCorpusId": "256627727"
                },
                {
                    "start": 1094,
                    "end": 1116,
                    "matchedPaperCorpusId": "261076313"
                },
                {
                    "start": 1241,
                    "end": 1259,
                    "matchedPaperCorpusId": "258741043"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.775390625
        },
        {
            "corpus_id": "270095084",
            "title": "Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models",
            "text": "Uncertainty Quantification in Conventional NLP Tasks.Uncertainty Quantification of machine translation (MT) has been studied for years to evaluate the performance of MT better.(Ott et al., 2018) access uncertainty by comparing multiple model outputs to multiple references with inter-sentence BLEU.(Glushkova et al., 2021) measure uncertainty through techniques of Monte Carlo dropout (Gal and Ghahramani, 2016) and deep ensembles (Lakshminarayanan et al., 2017).(Fomicheva et al., 2020) use uncertainty quantification methods to improve probability estimates in neural networks.(Lahlou et al., 2021) proposed Direct Epistemic Uncertainty Prediction, a modelagnostic framework, for estimating epistemic uncertainty in machine learning models.For regression tasks, (Wang et al., 2022) use uncertainty quantification to address both data uncertainty and model uncertainty, and (Malinin et al., 2020) proposes a method for uncertainty quantification using Prior Networks to obtain interpretable measures of uncertainty at a low computational cost.For Natural Language Understanding tasks, (Talman et al., 2023) use uncertainty quantification by applying Bayesian uncertainty modeling using Stochastic Weight Averaging-Gaussian.\n\nUncertainty Quantification in LLMs.Although uncertainty quantification has been thoroughly examined in models with distinct labels, such as classification models (Ulmer et al., 2022;Vazhentsev et al., 2022), it is still under-explored for popular free-form LLMs, e.g., GPT (Radford et al., 2019), OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023a).These models present a unique challenge in uncertainty quantification as their solution domains are flexible and effectively infinite, i.e., any generation can be deemed correct as long as the semantics align consistently with the real answer.(Xiao et al., 2022) conducts large-scale empirical evaluations on how the configuration (e.g., model size, architecture, training loss) of LLMs affect uncertainty.",
            "score": 0.3135069037592372,
            "section_title": "Related Works",
            "char_start_offset": 5408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 53,
                    "end": 176
                },
                {
                    "start": 176,
                    "end": 298
                },
                {
                    "start": 298,
                    "end": 463
                },
                {
                    "start": 463,
                    "end": 579
                },
                {
                    "start": 579,
                    "end": 742
                },
                {
                    "start": 742,
                    "end": 1044
                },
                {
                    "start": 1044,
                    "end": 1224
                },
                {
                    "start": 1226,
                    "end": 1261
                },
                {
                    "start": 1261,
                    "end": 1579
                },
                {
                    "start": 1579,
                    "end": 1822
                },
                {
                    "start": 1822,
                    "end": 1985
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 194,
                    "matchedPaperCorpusId": "4375156"
                },
                {
                    "start": 298,
                    "end": 322,
                    "matchedPaperCorpusId": "237503447"
                },
                {
                    "start": 385,
                    "end": 411,
                    "matchedPaperCorpusId": "160705"
                },
                {
                    "start": 764,
                    "end": 783,
                    "matchedPaperCorpusId": "249679786"
                },
                {
                    "start": 1086,
                    "end": 1107,
                    "matchedPaperCorpusId": "258049176"
                },
                {
                    "start": 1408,
                    "end": 1432,
                    "matchedPaperCorpusId": "248780161"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25390625
        },
        {
            "corpus_id": "274208705",
            "title": "UIE-Based Relational Extraction Task for Mine Hoist Fault Data",
            "text": "The UIE model employs a SEL to uniformly encode different IE structures and adaptively generate target extraction structures through pattern-based prompts. Large-scale pre-trained models utilize these prompts to capture standard IE capabilities in natural language and adapt to new IE tasks. \n\nFine-tuning uses SSI prompts to fine-tune the model's focus, helping the model learn what information is relevant and how to find it in the text. For example, an SSI instruction might direct the model to associate \" (abnormal noise)\" with the location of a \" (traveling motor).\" \n\nGiving the UIE a positive model, it will simply remember the triples from the pre-training data. To learn the general mapping capabilities, the model automatically constructs negative samples for each data pair, as example, \" (fault mode)\" and \" (location)\" are the positive modes in the record\"(( : ( : ))),\" \" (fault causes)\" and \" (repair measure)\" are used as the negative modes to construct the meta-triad. This is a process that uses cross-entropy loss to make the model's predictions as close as possible to the manually labeled data, and finally, the goal of text-to-structure pretraining is shown in Equation ( 5): \n\n\u03b8 e and \u03b8 d are the encoder and decoder parameters, respectively. In the fine-tuning phase, to improve the prediction accuracy of the model on task-specific data, the cross-entropy loss function of the Teacher Forcing (TF) technique was used [ 36 ]. By \"Teacher Forcing, \" it means that the real label sequence (instead of the model's prediction) is used as the input for the next timestep during the model training process. This approach speeds up convergence and improves model stability. Given a labeled corpus D task = { (s, x , y )}, the UIE model is finetuned using teacher-forced cross-entropy loss, and Equation ( 6) describes the goal of the entire fine-tuning process, which adapts the model so it can make effective predictions on the provided labeled corpus:",
            "score": 0.31342691326479544,
            "section_title": "Pre-training and Fine-tuning of UIE",
            "char_start_offset": 19932,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 291
                },
                {
                    "start": 294,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 572
                },
                {
                    "start": 575,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1198
                },
                {
                    "start": 1201,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1971
                }
            ],
            "ref_mentions": [
                {
                    "start": 1443,
                    "end": 1449,
                    "matchedPaperCorpusId": "14711886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1375732421875
        },
        {
            "corpus_id": "271874686",
            "title": "Evaluating Fine-Tuning Efficiency of Human-Inspired Learning Strategies in Medical Question Answering",
            "text": "Fine-tuning data size may affect performance The relatively small size of the LEK dataset, due to the limited human labels on medical questions, may restrict the visibility of certain effects of learning strategies, especially those that only emerge with larger data. For example, the benefits of Interleaved Learning might become only apparent over longer revision intervals, which our dataset might not fully capture. Similarly, the relative narrow range of question difficulties in the LEK dataset may limit the effectiveness of curriculum-based learning. \n\nAlternative notions for defining LLM-based difficulty The LLM-defined question difficulty was labelled based on the average accuracy on questions from ensemble LLMs, which may not fully capture all aspects of difficulty. Alternative measures, such as topic-specific perplexity to assess topic familiarity or the average language modeling loss on sequences as an indicator of LLMs' pre-training medical knowledge, could be explored further. \n\nLimited exploration of fine-tuning methods Our study focused exclusively on supervised finetuning using the QLoRA approach. Future work could explore the effects of other fine-tuning methods, such as domain-adaptive pretraining (DAPT) [12], continual learning [1], or adapter-based fine-tuning [14].",
            "score": 0.3134009710159298,
            "section_title": "Limitations",
            "char_start_offset": 15599,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 558
                },
                {
                    "start": 561,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 1000
                },
                {
                    "start": 1003,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1302
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1959228515625
        },
        {
            "corpus_id": "273162483",
            "title": "How Hard is this Test Set? NLI Characterization by Exploiting Training Dynamics",
            "text": "To show that our method is model-agnostic, we further provide a comparison between the dataset characterizations obtained by RoBERTa and De-BERTa. Table 4 showcases the accuracies of the two models on each others' data characterizations. The difficulty splits are maintained cross-model. Across datasets and difficulty levels, the performance sharply drops for the \"hard\" split for both models. DeBERTa achieved higher accuracy for \"hard\" set, most likely due to better overall performance compared to RoBERTa. In Figure 5, we show that overall heuristic values for \"Contains Negation\" are maintained across both models. Extended results for are presented in Appendix A. \n\nOur proposed methodology is general and independent of the underlying encoder model since we process training dynamics computed from raw logit scores. This characterization procedure may be adapted to using Large Language Models (LLMs) (Lee et al., 2023) in a zero-shot classification setting by manipulating the log-likelihood for the tokens of the correct classes. However, using LLMs requires a different approach than the one presented here since the networks are usually used without further training, in an in-context-learning manner (Dong et al., 2022). Furthermore, even if the LLMs are fine-tuned (Hu et al., 2021), it is not straightforward how the logits of each of the three classes are tracked across training. We leave this approach for future work. Our method highlights significant shortcomings in widely used NLI evaluation datasets (SNLI and MultiNLI) due to spurious correlations in the annotation process. To address these issues, we proposed an automatic method for constructing more challenging test sets, effectively filtering out problematic instances and providing a more realistic measure of model performance. Our approach, which categorizes examples in increasing difficulty levels using a wide range of training dynamics features, enhances evaluation reliability and offers insights into underlying challenges in NLI. Importantly, our methodology is general and model-agnostic, and can be applied across different datasets and models, promising improved evaluation practices in NLP.",
            "score": 0.31336664555735627,
            "section_title": "Impact of the Underlying Encoder",
            "char_start_offset": 20609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 670
                },
                {
                    "start": 673,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 909,
                    "end": 927,
                    "matchedPaperCorpusId": "258841424"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38916015625
        },
        {
            "corpus_id": "273350935",
            "title": "Fine-tuning can Help Detect Pretraining Data from Large Language Models",
            "text": "In the era of large language models (LLMs), detecting pretraining data has been increasingly important due to concerns about fair evaluation and ethical risks. Current methods differentiate members and non-members by designing scoring functions, like Perplexity and Min-k%. However, the diversity and complexity of training data magnifies the difficulty of distinguishing, leading to suboptimal performance in detecting pretraining data. In this paper, we first explore the benefits of unseen data, which can be easily collected after the release of the LLM. We find that the perplexities of LLMs shift differently for members and non-members, after fine-tuning with a small amount of previously unseen data. In light of this, we introduce a novel and effective method termed Fine-tuned Score Deviation(FSD), which improves the performance of current scoring functions for pretraining data detection. In particular, we propose to measure the deviation distance of current scores after fine-tuning on a small amount of unseen data within the same domain. In effect, using a few unseen data can largely decrease the scores of all non-members, leading to a larger deviation distance than members. Extensive experiments demonstrate the effectiveness of our method, significantly improving the AUC score on common benchmark datasets across various models.",
            "score": 0.313311888254348,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.339111328125
        },
        {
            "corpus_id": "258236695",
            "title": "Learning Sample Difficulty from Pre-trained Models for Reliable Prediction",
            "text": "It aims to ensure that the confidence-based ranking between the two samples should be consistent with their frequency-based ranking. Table 7 shows our method scales better with larger datasets. Our RMD score is derived from the whole data distribution, thus considering all samples at once rather than a random pair at each iteration. The ranking between the two samples also changes over training, while RMD is constant, thus ensuring a stable training objective. \n\nNext, we compare different distance measures in the feature space of CLIP-ViT-B, i.e., K-means, MD and RMD. Table 8 confirms the superiority of RMD in measuring the sample difficulty and ranking the training samples. This work introduced a new application of pre-trained models, exploiting them to improve the calibration and quality of uncertainty quantification of the downstream model. Profiting from rich large-scale (or even multi-modal) datasets and self-supervised learning for pre-training, pre-trained models do not overfit the downstream training data. Hence, we derive a relative Mahalanobis distance (RMD) via the Gaussian modeling in the feature space of pre-trained models to measure the sample difficulty and leverage such information to penalize overconfident predictions adaptively. We perform extensive experiments to verify our method's effectiveness, showing that the proposed method can improve prediction performance and uncertainty quantification simultaneously. \n\nIn the future, the proposed method may promote more approaches to explore the potential of largescale pre-trained models and exploit them to enhance the reliability and robustness of the downstream model. For example, for the medical domain, MedCLIP [53] can be an interesting alternative to CLIP/DINOv2 for practicing our method. Besides, conjoining accuracy and calibration is vital for the practical deployment of the model, so we hope our method could bridge this gap. Furthermore, many new large-scale models (including CLIP already) have text-language alignment. In our method, we do not explore the language part yet, however, it would be interesting to use the text encoder to \"explain\" the hard/easy samples in human language.",
            "score": 0.3126616793405506,
            "section_title": "Comparison of different sample difficulty measures",
            "char_start_offset": 27249,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 464
                },
                {
                    "start": 467,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1452
                },
                {
                    "start": 1455,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2190
                }
            ],
            "ref_mentions": [
                {
                    "start": 1705,
                    "end": 1709,
                    "matchedPaperCorpusId": "252992913"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50244140625
        },
        {
            "corpus_id": "267682083",
            "title": "How to Train Data-Efficient LLMs",
            "text": "DENSITY, Perplexity, and Loss. When a language model is trained to minimize perplexity, the LLM itself is a data distribution model. Therefore, the perplexity and loss fil-tering approaches of Marion et al. (2023), Muennighoff et al. (2023), and other authors can be viewed as modelbased density sampling. However, our sampler measures the density of the training dataset in a latent geometric space, while perplexity measures the likelihood under the scoring model. The samplers also differ in terms of decision complexity. Thanks to the capacity of the LLM, a perplexity filter can make highly-nuanced decisions between two texts on the same topic. On the other hand, our DENSITY sampler is constructed from a simple nonparametric density model (Rosenblatt, 1956) that does not have the capacity to distinguish examples at such a granular level. \n\nASK-LLM and Perplexity. Perplexity filters exhibit a strong in-distribution bias, making decisions based on the data used to train the scoring model (not the dataset we wish to sample). By using the LLM for quality evaluation rather than likelihood estimation, our sampler can escape this bias because the additional context and alternative task change the sampling distribution. This occurs even when the ASK-LLM and perplexity models are the same size. \n\nDENSITY and Clustering. The kernel sum procedure at the core of our DENSITY sampler operates on embeddingsimilarity relationships in a similar way to D4, SemDeDup, and SSL prototypes. Indeed, near-duplicate detection can be viewed as a discretized version of similarity-based density estimation (Kirsch & Mitzenmacher, 2006). Outlier rejection, which motivates the \"nearest-to-centroid\" heuristic of SSL prototypes, also has intimate connections with density estimation (Schubert et al., 2014). \n\nIntuition. Perplexity should be viewed as a \"difficulty\" or \"quality\" score rather than as a coverage-maximizing score. Our ASK-LLM sampler should be viewed as a contextualized quality score that incorporates reasoning.2",
            "score": 0.31265392209029347,
            "section_title": "Relationships Between Methods",
            "char_start_offset": 13087,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 847
                },
                {
                    "start": 850,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1304
                },
                {
                    "start": 1307,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1801
                },
                {
                    "start": 1804,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2024
                }
            ],
            "ref_mentions": [
                {
                    "start": 1602,
                    "end": 1631,
                    "matchedPaperCorpusId": "12622481"
                },
                {
                    "start": 1777,
                    "end": 1800,
                    "matchedPaperCorpusId": "17944400"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21435546875
        },
        {
            "corpus_id": "270442371",
            "title": "Advancing machine learning with OCR2SEQ: an innovative approach to multi-modal data augmentation",
            "text": "It fails to directly measure how well the model understands or reproduces language as humans do, potentially overlooking errors that affect comprehension and communication quality.To overcome this challenge, more advanced approaches involve sequence de-noising using text-to-text transformers as well as more sophisticated methods like tokenless, byte-level learning tasks.All these techniques fundamentally hinge on the nuances of various tokenization schemes.\n\nIn addition to explicit training methodologies for enhancing post-OCR data correction, significant research efforts are being channeled into more subtle techniques aimed at improving OCR performance.These include data augmentation and conditional error reproduction [10].Such tasks, while on the higher end of computational complexity, offer reliable means for enhancing performance, particularly in datasets with a higher incidence of noise.This often involves the manual annotation of error types common to specific document groups or estimating a probability distribution for manually entered data errors.Data augmentation strategies, typically governed by a set of rules, provide a systematic and repeatable framework for simulating erroneous data, which is crucial for training deep learning models.These methodologies not only enhance the robustness of the models but also contribute to more accurate and efficient data processing in scenarios where error prevalence is high [10][11][12].\n\nThe healthcare sector, particularly in the realm of precision health, presents a prime opportunity for enhancing data quality in large language models, where OCR plays a pivotal role [13].The emergence of patient-centered, data-driven ecosystems, such as learning health systems and value-based care models, underscores the value of leveraging historical data for process and quality improvement.Despite ongoing digitization efforts in healthcare services, these data-driven approaches often face challenges, including the absence of general support mechanisms like open-source datasets [14], specialized language models, and extensive historical data.In the healthcare industry, most data sources are proprietary and heavily regulated, necessitating significant manual effort for data annotation and cleaning, which is essential for their use in domain-specific clinical language systems [15].\n\nContrary to common belief, not all datasets and their generating systems are created equal.This discrepancy has spurred research into fine-tuning and other domain-specific data adaptation strategies to enhance performance.",
            "score": 0.31265178173379476,
            "section_title": "Introduction",
            "char_start_offset": 2275,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 180,
                    "end": 373
                },
                {
                    "start": 373,
                    "end": 461
                },
                {
                    "start": 463,
                    "end": 662
                },
                {
                    "start": 662,
                    "end": 734
                },
                {
                    "start": 734,
                    "end": 905
                },
                {
                    "start": 905,
                    "end": 1071
                },
                {
                    "start": 1071,
                    "end": 1267
                },
                {
                    "start": 1267,
                    "end": 1457
                },
                {
                    "start": 1459,
                    "end": 1647
                },
                {
                    "start": 1647,
                    "end": 1855
                },
                {
                    "start": 1855,
                    "end": 2111
                },
                {
                    "start": 2111,
                    "end": 2353
                },
                {
                    "start": 2355,
                    "end": 2446
                },
                {
                    "start": 2446,
                    "end": 2577
                }
            ],
            "ref_mentions": [
                {
                    "start": 729,
                    "end": 733,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1444,
                    "end": 1448,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1448,
                    "end": 1452,
                    "matchedPaperCorpusId": "195811894"
                },
                {
                    "start": 1452,
                    "end": 1456,
                    "matchedPaperCorpusId": "236096559"
                },
                {
                    "start": 1642,
                    "end": 1646,
                    "matchedPaperCorpusId": "186203708"
                },
                {
                    "start": 2348,
                    "end": 2352,
                    "matchedPaperCorpusId": "33285731"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.080810546875
        },
        {
            "corpus_id": "272690054",
            "title": "Active Learning to Guide Labeling Efforts for Question Difficulty Estimation",
            "text": "In recent years, there has been a surge in research on Question Difficulty Estimation (QDE) using natural language processing techniques. Transformer-based neural networks achieve state-of-the-art performance, primarily through supervised methods but with an isolated study in unsupervised learning. While supervised methods focus on predictive performance, they require abundant labeled data. On the other hand, unsupervised methods do not require labeled data but rely on a different evaluation metric that is also computationally expensive in practice. This work bridges the research gap by exploring active learning for QDE, a supervised human-in-the-loop approach striving to minimize the labeling efforts while matching the performance of state-of-the-art models. The active learning process iteratively trains on a labeled subset, acquiring labels from human experts only for the most informative unlabeled data points. Furthermore, we propose a novel acquisition function PowerVariance to add the most informative samples to the labeled set, a regression extension to the PowerBALD function popular in classification. We employ DistilBERT for QDE and identify informative samples by applying Monte Carlo dropout to capture epistemic uncertainty in unlabeled samples. The experiments demonstrate that active learning with PowerVariance acquisition achieves a performance close to fully supervised models after labeling only 10% of the training data. The proposed methodology promotes the responsible use of educational resources, makes QDE tools more accessible to course instructors, and is promising for other applications such as personalized support systems and question-answering tools.",
            "score": 0.31255729559603973,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.281494140625
        },
        {
            "corpus_id": "277066612",
            "title": "RAG-RL: Advancing Retrieval-Augmented Generation via RL and Curriculum Learning",
            "text": "The results in Tables 1 and 2 demonstrate that including samples of easier difficulty in the training process can help the model achieve higher performance during evaluation. Given a fixed number of training steps, the metrics in Table 3 shows that the baseline curriculum has consistently has lower F1 scores across all questions regardless of the number of hops required to answer the question correctly. Previous work in the area of self-improvement has shown that LLMs exhibit limited generalizability and that weak-to-strong curricula are effective for helping models generalize beyond their initial training distributions [10]. However, our results suggest that this is not always necessary as the linear curriculum is outperformed by the min-max curriculum in both shuffled and non-shuffled settings. We believe that this is a byproduct of our base model having relatively strong performance on the task before any post-training is applied. We show our base model's Pass@32, which is calculated by taking the maximum F1 score across 32 generations per question, in 4. We find that these scores are comparable to our baseline GRPO, demonstrating relatively strong baseline performance on the task. Curriculum learning strategies where the training sets are ordered from easy to hard have been successfully employed in many areas of machine learning [20]. Table 5 and Figure 3 include an ablation in which we sort the training set from easiest to hardest by sorting both by the number of hops each question requires and by the number of distractor passages, resulting in batches at the end of training containing the maximum number of passages with the greatest number of hops. We compare this against the linear and linear shuffled curricula which are constructed from the same training set but feature different dataset orderings. Although we see that the evaluation scores for both the final answer and citations start lower for the linear shuffled setting, all three curves converge to around the same point towards the end of training. Across all settings, we observe that the F1 scores are highest when the dataset is shuffled randomly, leading to our conclusion that the results do not conclusively support that using a specific ordering during training leads to significant performance gains or declines.",
            "score": 0.31245390499998865,
            "section_title": "Importance of a Including Additional Easier Samples",
            "char_start_offset": 15285,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2317
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1917724609375
        },
        {
            "corpus_id": "267897619",
            "title": "Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach",
            "text": "To avoid overfitting of negative samples, we set the model learning process as Positive-Unlabeled (PU) learning, where data labeled with 0 is considered unlabeled rather than non-entity. PU learning assumes that the unlabeled data follows the distribution of the entire dataset (Zhou et al., 2022). \n\nTo fulfill the assumption, we directly incorporate all unlabeled data into the curriculum to prevent too less unlabeled tokens in curricula. Different curriculum partitions are executed solely on the labeled positive data. In this way, only the learning order of positive samples can affect CuPUL performance. \n\nOur curriculum is built on token difficulty scores H, which are mostly \"easy\" as they follow a longtail distribution (Figure 3). Previous work (Platanios et al., 2019;Gnana Sheela and Deepa, 2013) suggests that a curriculum with uniformly ranged difficulty scores might lead to most tokens belonging to the first curriculum, making curriculum learning ineffective. Hence, we propose using a powerlaw selector to construct a more effective curriculum scheduler. \n\nTo build the curricula, we first arrange T u unlabeled tokens followed by T p positive-labeled tokens sorted by their difficulty scores in ascending order. The first curriculum consists of all unlabeled tokens and the first \u03c4 T p labeled positive tokens, where \u03c4 (0 < \u03c4 < 1) is a selective factor. The second curriculum consists of the first \u03c4 2 T tokens from the remaining (1 \u2212 \u03c4 )T p tokens. This selection process continues until the penultimate curriculum. The remaining tokens are placed in the final curriculum. These curricula are denoted as C 1 , C 2 , ..., C \u03b7 : \n\nFor example, suppose T p = 20, T u = 80, \u03c4 = 0.5, and \u03b7 = 3. Then, the first curriculum consists of tokens indexed from 1 to 90 (80 unlabeled tokens and the 10 easiest positive tokens), the second curriculum consists of tokens indexed from 91 to 95, and the last curriculum consists of tokens indexed from 96 to 100.",
            "score": 0.3123794902589778,
            "section_title": "Curriculum Design",
            "char_start_offset": 11678,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 298
                },
                {
                    "start": 301,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 610
                },
                {
                    "start": 613,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1073
                },
                {
                    "start": 1076,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1647
                },
                {
                    "start": 1650,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1966
                }
            ],
            "ref_mentions": [
                {
                    "start": 278,
                    "end": 297,
                    "matchedPaperCorpusId": "248266826"
                },
                {
                    "start": 780,
                    "end": 809,
                    "matchedPaperCorpusId": "45965215"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.239013671875
        },
        {
            "corpus_id": "278535360",
            "title": "Learning Like Humans: Advancing LLM Reasoning Capabilities via Adaptive Difficulty Curriculum Learning and Expert-Guided Self-Reformulation",
            "text": "Drawing inspiration from human learning strategies, this work addresses key challenges in enhancing the capabilities of large language models to solve complex tasks. We first observed the Difficulty Shift phenomenon, where a model's perception of problem difficulty changes dynamically during training, hindering static curriculum learning. To counteract this, we propose Adaptive Difficulty Curriculum Learning (ADCL), which periodically re-estimates difficulty to maintain an aligned learning path. Secondly, recognizing that existing methods often fail to help models assimilate knowledge beyond their initial capabilities, we introduce Expert-Guided Self-Reformulation (EGSR). EGSR guides models to actively reformulate expert solutions within their own conceptual framework, rather than relying on direct imitation, fostering deeper knowledge assimilation. Experiments confirm ADCL's significant improvements over predefined curricula and EGSR's effectiveness in advancing beyond the standard RL baseline by assimilating expert knowledge, thereby expanding model capabilities. Crucially, their combination yields superior performance, significantly improving standard RL baselines. These findings highlight the value of incorporating adaptive humanlike learning mechanisms into LLM training. \n\nA Pseudocode for ADCL Algorithm 1 Adaptive Difficulty Curriculum Learning Input: Dataset D = {x i } M i=1 ; initial policy model \u03c0 \u03b8 0 ; number of batches K;. \n\n\u25b7 Update \u03c0 \u03b8 using RL algorithm (e.g., GRPO) 6: \n\nif k < K then 7: \n\nB Pseudocode for EGSR Algorithm 2 Expert-Guided Self-Reformulation Input initial policy model \u03c0 \u03b8 init ; reward models r \u03d5 ; Dataset D = {(q i , s i , a i )} N i=1 , where q i is the question, s i is the solution, and a i is the answer; 1: policy model \u03c0 \u03b8 \u2190 \u03c0 \u03b8 init 2: for iteration = 1, . . . , I do Compute rewards {R(\u03c4 i )} G i=1 for each sampled trajectory \u03c4 i using r \u03d5 8: \n\nif G i=1 R(\u03c4 i ) = 0 then \u25b7 All rewards are zero 9: \n\nGenerate guided trajectories {\u03c4 \n\nRecompute rewards for \u03c4 mixed 12: \n\nCompute advantages A i,t for each token t in each trajectory \u03c4 i from T mixed 13: \n\nfor GRPO iteration = 1, . . .",
            "score": 0.31205578167309656,
            "section_title": "Conclusion",
            "char_start_offset": 28841,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1296
                },
                {
                    "start": 1299,
                    "end": 1457
                },
                {
                    "start": 1460,
                    "end": 1507
                },
                {
                    "start": 1510,
                    "end": 1526
                },
                {
                    "start": 1529,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1908
                },
                {
                    "start": 1911,
                    "end": 1962
                },
                {
                    "start": 1965,
                    "end": 1996
                },
                {
                    "start": 1999,
                    "end": 2032
                },
                {
                    "start": 2035,
                    "end": 2116
                },
                {
                    "start": 2119,
                    "end": 2148
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53125
        },
        {
            "corpus_id": "259095925",
            "title": "Uncertainty in Natural Language Processing: Sources, Quantification, and Applications",
            "text": "The field of NLP is challenged by the extremely highdimensional language space, as highlighted by the need to estimate predictive entropy, which requires taking an expectation in output space. This output space has a dimensionality of O(|T | N ), which presents significant computational challenges. Additionally, the lack of a normalized probability density function over sentences necessitates approximating the expectation using Monte Carlo integration, which involves averaging the likelihoods of a finite set of sampled sentences. However, Monte Carlo integration becomes difficult for entropy as it is often dominated by lowprobability sentences that have large and negative logs. MT models, for example, can contain hundreds of millions of parameters, which exponentially increases the search space. Additionally, with only a single reference for each source sentence, it becomes difficult to measure the fitness of MT models to data distributions. As a result, researchers face significant scientific challenges in adapting tools from both the machine learning and statistics fields to effectively tackle this problem. \n\nIn addition, the complexity of uncertainty in NLP systems is also constantly increasing due to the evolving mod-els and the demand for human-AI interaction. Although the current uncertainty estimation techniques are rich, further exploration is still needed to address issues such as time cost and inference accuracy, and how to consider the impact of natural language features on uncertainty estimation. Some uncertainty estimation techniques can also be limited in their scalability. Firstly, TS is a convenient calibration technique, but the performance of TS is very sensitive to the choice of the scaling factor. Choosing the optimal scaling factor may require extensive hyperparameter tuning, which can be time-consuming and computationally expensive. Secondly, ensembling multiple models or MCD can be effective for uncertainty estimation but can also be resource-intensive and difficult to scale to large models. Finally, distributionbased uncertainty estimation can provide valuable insights into the uncertainty of DNNs, but it requires distributional assumptions, such as Gaussian or a mixture of Gaussian distributions, which may not apply in all situations and limit its applicability. In the era of large PLMs such as ChatGPT and GPT-4, researchers are not accessible to the model. Even if a PLM is open-sourced, retraining is often necessary, which can be computationally expensive and time-consuming.",
            "score": 0.31203891047312404,
            "section_title": "Extremely High-Dimensional Language-space",
            "char_start_offset": 76884,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1126
                },
                {
                    "start": 1129,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2327
                },
                {
                    "start": 2328,
                    "end": 2424
                },
                {
                    "start": 2425,
                    "end": 2545
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11859130859375
        },
        {
            "corpus_id": "259075310",
            "title": "Adaptive and Personalized Exercise Generation for Online Language Learning",
            "text": "removing lookahead would bias the distribution toward 2 nd and 3 rd quarter. This confirms our assumption that naively applying F d would greedily select difficult words in the early steps, which is not the distribution of reference exercises. Our decoding algorithm avoids this issue by estimating the future and therefore achieves better results. Upper Bound Analysis. When we train our model, we use ground-truth difficulty d and target words C obtained from references; however, the student states s are estimated from the KT model. We conduct an upper bound analysis to understand the influence of the accuracy of s on the generation performance. Since a student's actual mastery of every vocabulary word is not available, we choose to replace the ground-truth difficulty levels d with those estimated from s. As shown in the last section of Table 2, all metrics are considerably boosted when the inconsistency between states s and difficulty d is eliminated. This again proves the effect Figure 4: Generating 50 additional exercises of specified difficulty levels for different student groups using APEG s+C+d (adaptive) and non-adaptive EG C+d models. The Y-axis is the ratio of output difficulty d out to input difficulty d in ; the closer to 1 (dotted line) the better. Solid lines are averaged results of group students at each step, and shadows represent standard deviations.  Table 4: Examples of exercises based on different controls. d in is the input difficulty while d out is the output difficulty estimated by our knowledge tracing model. The degree of highlight represents a student's mastery of vocabulary words (the darker the harder).\n\nof incorporating student states and explains how such information comes to play: the knowledge states explicitly convey the dynamics between control signals d, C, and target exercises e, which is non-trivial to learn by the model itself. Case Study. We provide a few cases in Table 4.\n\nWe can see our model can dynamically adjust the exercise content according to specified words, target difficulty, as well as students' different mastery states of the vocabulary. The exercises generated for advanced students (avg. state = 0.65) are generally more difficult than for poor students (avg. state = 0.32) under the same input difficulty.",
            "score": 0.311939090775342,
            "section_title": "Exercise Generation Evaluation",
            "char_start_offset": 23644,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10614013671875
        },
        {
            "corpus_id": "267770178",
            "title": "An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling",
            "text": "The former determines the relative \"easiness\" of each data example, which can be pre-defined or determined automatically. The latter determines the sequence of data subsets during the training process,guided by the judgment from the difficulty measurer (Wang et al., 2021), using either a linear or non-linear scheduler. When both components are designed based on human prior knowledge, it is referred to as pre-defined CL. On the other hand, if one of the components is driven by a data-driven algorithm, it is known as automatic CL, exemplified by self-paced learning and transfer teacher. Predefined difficulty measurer is typically designed by considering data characteristics such as sentence length and word rarity. In general, designing difficulty metrics specifically tailored to the task at hand is more efficient and typically involves data-driven algorithms. For instance, Mohiuddin et al. (2022) developed three deterministic scoring methods for machine translation that leverage the similarity of a parallel sentence pairs and cross-entropy. Zhu et al. (2021) evaluated sample difficulty in dialogue generation based on model loss. Lobov et al. (2022) utilized metrics such as sentence length, average confidence, and perplexity value to assess difficulty in named entity recognition (NER). However, these metrics primarily concentrate on the sentence-level and do not adequately address token-and word-level difficulty.",
            "score": 0.311939090775342,
            "section_title": "Related Work",
            "char_start_offset": 5939,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1433
                }
            ],
            "ref_mentions": [
                {
                    "start": 1055,
                    "end": 1072,
                    "matchedPaperCorpusId": "244119160"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8076171875
        },
        {
            "corpus_id": "273654515",
            "title": "Sparse Linear Bandits with Blocking Constraints",
            "text": "Due to the ambiguity in hardness definition, often heuristics are used to infer the difficulty of samples (Soviany et al., 2022) and can turn out unreliable and not generalizable. \n\nFor niche tasks where an expert annotator is available, the difficulty ratings from the annotator are more informative since the goal is to train a model to mimic the expert. In computer vision, there has been recent work regarding estimating the difficulty of a dataset for the model using implicit difficulty ratings of annotation (Ionescu et al., 2016;Mayo et al., 2023). For NLP tasks, Ethayarajh et al. (2022) constructs information theoretic metrics to estimate the difficulty of data points. \n\nNotation: We denote vectors by bold small letters (say x), scalars by plain letters (say x or X), sets by curly capital letters (say X ) and matrices by bold capital letters (say X). We use [m] to denote the set {1, 2, . . . , m}, \u2225x\u2225 p to denote the p-norm of vector x. For a set T of indices v T is used to denote the sub-vector of v restricted to the indices in T . \u03bb min (A) denotes the minimum eigenvalue of the matrix A and diag(x) denotes a diagonal matrix with entries as x. We use B d and S d\u22121 to denote the unit ball and unit sphere in d dimensions, respectively. We will write EX to denote the expectation of a random variable X. O(\u2022) notation hides logarithmic factors in T, M, d.",
            "score": 0.311939090775342,
            "section_title": "INTRODUCTION",
            "char_start_offset": 12053,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 182,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 680
                },
                {
                    "start": 683,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1376
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 128,
                    "matchedPaperCorpusId": "231709290"
                },
                {
                    "start": 515,
                    "end": 537,
                    "matchedPaperCorpusId": "879067"
                },
                {
                    "start": 537,
                    "end": 555,
                    "matchedPaperCorpusId": "266565244"
                },
                {
                    "start": 572,
                    "end": 596,
                    "matchedPaperCorpusId": "250340652"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50244140625
        },
        {
            "corpus_id": "272969316",
            "title": "IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation",
            "text": "However, if the evaluation scores are 0 and 3, we can calculate that its difficulty score is 1.5, and the discrimination score is 1, indicating that the question can effectively distinguish the level of LLMs. \n\nWe propose a difficulty estimation model by fine-tuning BaiChuan2-13B pretrained model. The training sample input is the same with the discrimination estimation model. The output is 1-3, representing the difficulty level, and the training instruction is changed to estimate the difficulty of the problem. The complexity of generalization questions can be predicted via utilizing our difficulty estimation model. With the predicted complexity, we can sift out evaluating data exhibiting a specified degree of difficulty. \n\nIn order to obtain a more accurate measure of the difficulty of the dataset, we calculate the difficulty scores through manual annotation. After obtaining the annotators' scores for the responses of various models to the questions, we can calculate the difficulty score for each sample using Formula 4. By calculating the average value of the difficulty scores for all samples in the dataset, we obtain the difficulty score for these samples.",
            "score": 0.311939090775342,
            "section_title": "Difficulty Estimation Model",
            "char_start_offset": 18977,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 211,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 730
                },
                {
                    "start": 733,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1175
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1588134765625
        },
        {
            "corpus_id": "276647269",
            "title": "LLM-driven Effective Knowledge Tracing by Integrating Dual-channel Difficulty",
            "text": "After these two phases, the large language model generates an LLM-based difficulty assessment, which serves as a crucial component in the objective difficulty dimension. By employing the Chain of Thought technique, the model can analyze various aspects of the question through explicit reasoning processes, including concept complexity, number of solution steps, and knowledge point correlations, resulting in more accurate and interpretable difficulty assessments. This fine-grained analysisbased evaluation method not only improves the accuracy of difficulty judgment but also provides educational practitioners with specific difficulty attribution evidence. Compared to traditional statistics-based assessment methods, this approach better captures the latent characteristics of question difficulty, showing particular advantages when dealing with new questions or scenarios lacking extensive historical data.",
            "score": 0.311939090775342,
            "section_title": "Utilizing LLMs to determine the question's difficulty",
            "char_start_offset": 20439,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 912
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.255615234375
        },
        {
            "corpus_id": "266933185",
            "title": "InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks",
            "text": "For the difficulty assessment of DAEval, we randomly sample 80 groups from the dataset, each comprising 5 questions. \n\nGPT-4 ranks the questions within each group according to their difficulty, based on criteria such as data processing complexity, domain-specific knowledge, and coding sophistication. This ranking process is repeated 100 times for each group to ensure the robustness of the results. Each question has a score based on its ranking, with the most challenging question in a group earning 5 points and the least challenging 0 points. An average score was then calculated for each question, leading to a numerical difficulty ranking. These rankings allowed for the classification of questions into three levels: easy (0 to 1.667), medium (1.667 to 3.335), and hard (3.335 to 5).",
            "score": 0.311939090775342,
            "section_title": "K. Method for Assessing Difficulty",
            "char_start_offset": 33752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 119,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 791
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1280517578125
        },
        {
            "corpus_id": "271244843",
            "title": "GeoHard: Towards Measuring Class-wise Hardness through Modelling Class Semantics",
            "text": "Data acts as a crucial intermediary proxy for AI systems to understand and tackle real-world tasks (Torralba and Efros, 2011;Vodrahalli et al., 2018).Therefore, evaluating the hardness of individual instances, or instance-level hardness (Kong et al., 2020;Hahn et al., 2021;Ethayarajh et al., 2022;Zhao et al., 2022), relative to the dataset is key for learning and analyzing NLP tasks.This evaluation is increasingly important with the rise of large language models (LLMs; Touvron et al. 2023;Chung et al. 2024).Measuring hardness aids in selecting 1 https://github.com/TRUMANCFY/geohardENT (93%) NEU (84%)",
            "score": 0.311939090775342,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 150,
                    "end": 386
                },
                {
                    "start": 386,
                    "end": 513
                },
                {
                    "start": 513,
                    "end": 588
                },
                {
                    "start": 588,
                    "end": 607
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 125,
                    "matchedPaperCorpusId": "2777306"
                },
                {
                    "start": 256,
                    "end": 274,
                    "matchedPaperCorpusId": "233324123"
                },
                {
                    "start": 274,
                    "end": 298,
                    "matchedPaperCorpusId": "250340652"
                },
                {
                    "start": 298,
                    "end": 316,
                    "matchedPaperCorpusId": "253475828"
                },
                {
                    "start": 494,
                    "end": 512,
                    "matchedPaperCorpusId": "253018554"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.122314453125
        },
        {
            "corpus_id": "264819795",
            "title": "Ling-CL: Understanding NLP Models through Linguistic Curricula",
            "text": "Linguistic disparity can be quantified by the extent of asymmetry in the probability distribution of the linguistic complexity of samples in a dataset, e.g., see Figure 1 in \u00a71. A natural solution to evaluate models is to group samples based on their linguistic complexity. Such grouping is crucial because if easy samples are overrepresented in a dataset, then models can result in unrealistically high performance on that dataset. Therefore, we propose to partition datasets based on a difficulty metric (linguistic index or loss) and compute balanced accuracy of different models on the resulting groups. This evaluation approach reveals great weaknesses in models, and benchmark datasets or tasks that seemed almost \"solved\" such as as the complex tasks of NLI.",
            "score": 0.311939090775342,
            "section_title": "Evaluation Metrics",
            "char_start_offset": 16411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 765
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38330078125
        },
        {
            "corpus_id": "264299364",
            "title": "NetDistiller: Empowering Tiny Deep Learning via In Situ Distillation",
            "text": "Specifically, we measure the uncertainty via the entropy of the student outputs. When the entropy of the student output is high (i.e., uncertain), the student is distilled by the weight-sharing teacher via the KL divergence loss, otherwise, the student is trained by the ground truth label via the cross-entropy loss. We formulate this process as follows: \n\nHere, T denotes the uncertainty threshold; L stu denotes the student model loss; KL() and CE() denote the KL divergence loss and the cross-entropy loss, respectively; x and y denote the input data and the ground truth labels; and uncertainty denotes the entropy of the student model outputs W stu (x).",
            "score": 0.311939090775342,
            "section_title": "NetDistiller's Enabler 2: Gradient Surgery for Resolving Gradient Conflicts",
            "char_start_offset": 12163,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 355
                },
                {
                    "start": 358,
                    "end": 659
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.263671875
        },
        {
            "corpus_id": "255941798",
            "title": "Improving Reliability of Fine-tuning with Block-wise Optimisation",
            "text": "Compared to CV, NLP models was typically more shallow and thus require different fine-tuning methods [41]. In NLP, Mikolov et al. [42] proposed a simple transfer technique by fine-tuning pre-trained word embeddings, a model's first layer, but has had a large impact in practice and is now used in most state-of-the-art models. To mitigate LMs' overfitting to small datasets, Jeremy et al. [41] proposed discriminative language model fine-tuning to retain previous knowledge and avoid catastrophic forgetting. In the last couple of years, large language models, such as GPT [43] and BLOOM [44], were developed by using mask learning on large amounts of text data. Given the size of these large language models, finetuning all the model parameters can be compute and memory intensive [45]. Some recent studies [46], [47] have proposed new parameter efficient fine-tuning methods that update only a subset of the model's parameters. As adversarial samples of new task are usually out-of-distribution, adversarial fine-tuning fails to memorize all the robust and generic linguistic features already learned during pre-training. To mitigate the impacts caused by this, Dong [48] et al. proposed to use mutual information to measure how well an objective model memorizes the useful features captured before. Furthermore, Mireshghallah et al. [49] empirically studied memorization of fine-tuning methods using membership inference and extraction attacks as large models have a high capacity for memorizing training samples during pre-training.",
            "score": 0.3116710539746512,
            "section_title": "B. Fine-tuning in Natural Language Processing",
            "char_start_offset": 8503,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1536
                }
            ],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 134,
                    "matchedPaperCorpusId": "16447573"
                },
                {
                    "start": 573,
                    "end": 577,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 782,
                    "end": 786,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 808,
                    "end": 812,
                    "matchedPaperCorpusId": "230433941"
                },
                {
                    "start": 814,
                    "end": 818,
                    "matchedPaperCorpusId": "238583580"
                },
                {
                    "start": 1169,
                    "end": 1173,
                    "matchedPaperCorpusId": "245116938"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26904296875
        },
        {
            "corpus_id": "263828737",
            "title": "Balancing Specialized and General Skills in LLMs: The Impact of Modern Tuning and Data Strategy",
            "text": "Current research on evaluating large language model generation quality employs a diverse set of techniques, each with distinct tradeoffs [5]. Human evaluation through ratings and reviews provides nuanced assessments accounting for subjective aspects of quality, but is time-consuming, inconsistent, and doesn't scale [21]. Automated metrics like BLEU [16] are fast and consistent, but focus narrowly on n-gram overlap with reference texts. Adversarial evaluation [4] can reveal flaws invisible to standard tests, yet constructing effective adversarial examples remains challenging. Designing specific benchmark tasks can test particular skills relevant to generation quality, but requires developing comprehensive suites covering diverse skills [8]. Human-in-the-loop training iteratively improves models using human feedback, but is slow and introduces confounding factors. Overall, human evaluation remains the gold standard despite difficulties with scalability and subjectivity. Automated metrics are the most widely adopted for development due to speed and consistency, complemented by adversarial techniques and specifically designed tests to evaluate particular aspects of generation quality. But effectively incorporating human assessment during training remains an open challenge [14].",
            "score": 0.31147781413361253,
            "section_title": "LLM evaluation",
            "char_start_offset": 7181,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1294
                }
            ],
            "ref_mentions": [
                {
                    "start": 351,
                    "end": 355,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 463,
                    "end": 466,
                    "matchedPaperCorpusId": "30536426"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.327880859375
        },
        {
            "corpus_id": "258833284",
            "title": "To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis",
            "text": "Recent research has highlighted the importance of dataset size in scaling language models. However, large language models (LLMs) are notoriously token-hungry during pre-training, and high-quality text data on the web is approaching its scaling limit for LLMs. To further enhance LLMs, a straightforward approach is to repeat the pre-training data for additional epochs. In this study, we empirically investigate three key aspects under this approach. First, we explore the consequences of repeating pre-training data, revealing that the model is susceptible to overfitting, leading to multi-epoch degradation. Second, we examine the key factors contributing to multi-epoch degradation, finding that significant factors include dataset size, model parameters, and training objectives, while less influential factors consist of dataset quality and model FLOPs. Finally, we explore whether widely used regularization can alleviate multi-epoch degradation. Most regularization techniques do not yield significant improvements, except for dropout, which demonstrates remarkable effectiveness but requires careful tuning when scaling up the model size. Additionally, we discover that leveraging mixture-of-experts (MoE) enables cost-effective and efficient hyper-parameter tuning for computationally intensive dense LLMs with comparable trainable parameters, potentially impacting efficient LLM development on a broader scale.",
            "score": 0.3113716151045369,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27783203125
        },
        {
            "corpus_id": "273404268",
            "title": "LLM Confidence Evaluation Measures in Zero-Shot CSS Classification",
            "text": "Our results are shown in Table 1. Overall, the confidence ensemble uncertainty quantification measure is the most robust evaluated UQ strategy, proving to be effective across all model types. In the RLHF model evaluated, GPT-4o, quantitative selfreporting seemed also to be an effective strategy. Interestingly, for GPT-4o the log inverse performance did not closely resemble the confidence score or ensemble metrics. In the evaluated data, GPT appeared to return less deterministic responses, meaning that it was not as likely to achieve a high log inverse score when searching for a selected token, even when the model found it to be an easy task when evaluated using our other UQ techniques. On the contrary, the difficulty or ease of the tasks is Figure 1: Graph depicts the percent of incorrect data annotations identified given the amount of data sampled for stance detection via Flan UL2. This shows we can find approximately half of all incorrect data annotations by checking only the bottom 20% of data evaluated by our confidence ensemble method. This graph also is meant to show a natural understanding of why AUC is a valuable measure for uncertainty quantification when measuring by percent of false labels detected. Table 1: Depicts the Area Under Curve (AUC) metric across selected sampling strategy, dataset, and language model. The top performing sampling strategy for each task is in bold. We also report the average performance for each sampling strategy. Across all LLM types, the confidence ensemble method shows the most robustness.",
            "score": 0.3112181260641446,
            "section_title": "Results",
            "char_start_offset": 10551,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1554
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2454833984375
        },
        {
            "corpus_id": "252545066",
            "title": "PROD: Progressive Distillation for Dense Retrieval",
            "text": "Previous work found that different data instances would have various difficulties for a particular family of models to learn [8,44]. \n\nSimilarly, we believe that the probability distributions of different teacher models when selecting the relevant passages from a candidate pool are also different. Therefore, when facing the various difficulties of data instances in training, we use three different teacher models with different ability levels to civilize the student model gradually. More specifically, we employ three progressive teacher models, i.e., a 12-layer DE, a 12-layer CE and a 24-layer CE, to boost a 6-layer DE student model1 in order. Before learning from a teacher model, we retrieve the top- negatives [46] with the current student checkpoint, randomly sample a subset and train a teacher model using the mined hard negatives. \n\nDistilling with a DE Teacher. We first use the original data to warm up the 12-layer teacher DE and the 6-layer student DE, and then use the isomorphic distillation, i.e., 12-layer DE \u2192 6-layer DE. The loss function can be divided into two parts. \n\nThe first part is calculated by the output distribution of the student model and hard labels, which is called hard loss: \n\nwhere  + and P \u2212 is the relevant passage and negative passage pool of , respectively;    is the similarity scores of the student model. The other part measures the differences of the probability distributions between the teacher model and the student model given the same batch of data, which is called soft loss: \n\nwhere    is the similarity scores of the teacher model;  is the temperature of distillation. At last, the final distillation loss with a DE teacher is a weighted sum of the before-mentioned two parts: \n\nwhere  1 and  1 control the importance of hard loss and soft loss. \n\nThere are two important reasons why we use isomorphic distillation. (1) First, according to previous research on DE [17], adding more hard negatives when training DE can not lead to further improvements. Therefore, simply training a DE may be not suitable for difficult data instances. Meanwhile, the special in-batch negative techniques [17] for DE allows it to assign more appropriate similarity scores for easy negatives, which is more suitable for the first stage training of the student model.",
            "score": 0.3110299798068906,
            "section_title": "Teacher Progressive Distillation",
            "char_start_offset": 8828,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 135,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1093
                },
                {
                    "start": 1096,
                    "end": 1216
                },
                {
                    "start": 1219,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1532
                },
                {
                    "start": 1535,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1735
                },
                {
                    "start": 1738,
                    "end": 1804
                },
                {
                    "start": 1807,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2305
                }
            ],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 128,
                    "matchedPaperCorpusId": "250340652"
                },
                {
                    "start": 128,
                    "end": 131,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 720,
                    "end": 724,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1923,
                    "end": 1927,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.131591796875
        },
        {
            "corpus_id": "265213396",
            "title": "CLIMB: Curriculum Learning for Infant-inspired Model Building",
            "text": "Different representations of vocabulary difficulty work better for different tasks. When representing difficulty in the vocabulary curriculum experiments, token ID -our proxy for frequency -appears to work better than word classes (POS tags) or a combination of token ID and POS tags on the BLiMP evaluation tasks, but worse than POS tags on SuperGLUE and MSGS (Table 5). \n\nIn multi-corpora datasets, ordering by difficulty is a good first step. Training data requirements have grown so much in modern NLP that usually training a language model from scratch will involve multiple datasets, or multiple domains. The results of our data curriculum experiments indicate that a good first step is to put these sub-corpora into some order of intuitive difficulty, as we did (Table 6). In the case of BLiMP this approach outperforms our perplexity-based data curricula, and with log pacing our vanilla model. The same is true of MSGS (with log pacing), as well as BLiMP-supplement and SuperGLUE (though the last two do not beat our vanilla model). Amongst the perplexity-driven models, the picture is less positive: out of 24 tests, only one model outperforms our vanilla model (log pacing, random initialisation + model perplexity in Table 6). \n\nMultitask learning holds sway over sequentially swapping objective functions for now. In our experiments with curricula for the objective function, we compare training on simultaneous tasks -known as multitask learning (Caruana, 1997) with predefined sequences of objective functions which swap from one to another at set thresholds in the training process. We set up two sequential curricula: one with 2 tasks (predicting the 10 universal POS tags found in our dataset, and MLM) and the other with 3 (like the 2 task curriculum, additionally with noun/verb/other prediction). We compare these against multitasking alternatives. In general the sequential curricula are outperformed by the multitasking ones, though the 3-task sequential curriculum outperforms our BabyBERTa-style vanilla model on SuperGLUE and is second only marginally to our best-performing multitask model (Table 7).",
            "score": 0.31089989414941466,
            "section_title": "Results",
            "char_start_offset": 26069,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 371
                },
                {
                    "start": 374,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1238
                },
                {
                    "start": 1241,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2127
                }
            ],
            "ref_mentions": [
                {
                    "start": 1460,
                    "end": 1475,
                    "matchedPaperCorpusId": "45998148"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.343017578125
        },
        {
            "corpus_id": "259837207",
            "title": "Procedurally generating rules to adapt difficulty for narrative puzzle games",
            "text": "In the next iteration of the project, we plan to train a language model specifically on children's literature, provided by the Gutenberg project (gutenberg.org). This should produce more children-themed stories and hopefully use a language that children are more familiar with. \n\nLarge language models have been used to guide reinforcement learning with the use of prompting techniques and evaluation of text output [4]. We will try to use large language models to analyze the communicated rules and use prompting techniques to get suggestions for possible solutions. These suggestions will then be used to guide a solver algorithm and at the same time determine comprehensive difficulty based on the accuracy of the predictions. \n\nThe number of unique solutions is a temporary guide for difficulty, but may not be the most fitting evaluator. Alternatively, entropy has been used as a measure for game depth in strategic games [7] and may potentially be used to determine resource requirements. It is currently possible to measure the entropy for a given board state by counting the number of tiles that can fit each board position. This has been used as an evaluator together with the genetic generator to produce rules with a target entropy level between high and low. However, this needs to be studied further in future iterations. \n\nThe current approach also lacks actual player feedback on difficulty, which will be an important factor to balance and guide future experiments. Players will be invited to play test different prototypes, while data is collected to model their behavior in response to different levels of difficulty. This will be used to evaluate different guiding evaluators for the procedural content generator as well as different language models' ability to communicate game rules.",
            "score": 0.31075583854401956,
            "section_title": "VI. FUTURE PLANS",
            "char_start_offset": 12145,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 277
                },
                {
                    "start": 280,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1334
                },
                {
                    "start": 1337,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1804
                }
            ],
            "ref_mentions": [
                {
                    "start": 927,
                    "end": 930,
                    "matchedPaperCorpusId": "3809514"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1544189453125
        },
        {
            "corpus_id": "258999803",
            "title": "Explanation Graph Generation via Generative Pre-training over Synthetic Graphs",
            "text": "Effect of the difficulty of the query In EG 3 P we construct a query in three different difficulties and mix the corpus in the main experiment as multitask training. Table 2 shows the results on different queries. It is significant that the utilization of a mixed corpus leads to a more substantial improvement than training on a single sub-task alone. Due to the same graph generation form, the structural accuracy(StCA) of all sub-task is improved significantly; the benefits brought by the mixed corpus are mainly reflected in the semantic accuracy(SeCA). \n\nA comparison of different sub-tasks reveals that the results for queries of normal difficulty are the most favorable. The queries in normal difficulty retain the form of a natural language compared to easy and retain more intermediate reasoning information compared to hard. This suggests that, in the training process based on a large-scale synthetic corpus, the closer the training task is to the SA\u2191 StCA\u2191 SeCA\u2191 G-BS\u2191 GED\u2193 EA\u2191 BART-Base (Saha et al.,  downstream task and the simpler it is, the better the model learns. \n\nThe model pre-trained on simple corpus demonstrates superior performance in comparison to the one based on the easy corpus. Compared to easy difficulty, the pair of simple query and graph has a form that is more congruent to the explanation graph generation task. This finding aligns with previous work (Devlin et al., 2019), which suggests that pre-training on a task that is more closely aligned to the downstream task leads to improved performance. Besides, the model pre-trained on simple corpus also outperforms the one based on the hard corpus, despite the fact that both present the same form. This highlights the importance of selecting an appropriate difficulty level for pretraining tasks in order to achieve optimal efficiency. Effect of the scale of corpus Figure 5 shows the results of the model pre-trained on a different scale of the corpus. We compare the effect of six different scales of corpus on the experiment. Within a certain range, the experimental results are improved by the scale of the corpus. However, when the corpus size exceeds a certain threshold, the marginal benefit of a larger corpus becomes increasingly diminishing, likely due to the limitations of computational resources and insufficient training on a large-scale corpus.",
            "score": 0.3107173804393709,
            "section_title": "Other Analysis",
            "char_start_offset": 20834,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 558
                },
                {
                    "start": 561,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1083
                },
                {
                    "start": 1086,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2107
                },
                {
                    "start": 2108,
                    "end": 2348
                }
            ],
            "ref_mentions": [
                {
                    "start": 1389,
                    "end": 1410,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1326904296875
        },
        {
            "corpus_id": "247694098",
            "title": "Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU",
            "text": "We presented a set of experiments using training dynamics (Swayamdipta et al., 2020) as difficulty metrics for CL on several NLU tasks. Differently from existing works, we focus our evaluation on indistribution, out-of-distribution and zero-shot crosslingual transfer data by testing existing discrete and continuous schedulers as well as modifications of those in a transfer-teacher curriculum setting. \n\nOur findings offer evidence that simply reordering the training examples in a meaningful way has mostly an impact on zero-shot cross-lingual transfer and OOD data, with no improvement on ID. Our proposed Continuous scheduler with confidence and variability sampling provided a boost up to 8.5% on a challenging OOD dataset over prior work. Comparing our proposed application of training dynamics to other transfer-teacher curriculum methods that are using more than 1 teacher model, we observed greater speedups, improved performance and more stable training. In particular, we found that task-agnostic metrics do not perform better than task-specific ones on ID and ZS data but can offer good performance on OOD settings. \n\nOverall, our experiments suggest there is no curriculum outperforming others by a large margin which is consistent with findings in Zhang et al. (2018) and that task-agnostic metrics should not be rejected when transferring to challenging new domains. However we show that training dynamics are potentially better difficulty metrics for CL in both monolingual and multilingual models even with a limited budget. \n\nAlthough in this study we focused on using CL on a single language only (English), a reasonable extension is considering training data from other languages as well and investigate instance difficulties based on language or following efforts towards continual learning (Parisi et al., 2019). Finally, using TD in a dynamic rather than a static curriculum is another interesting direction that can potentially offer further training speedups as well as ways to improve model pre-training (Nagatsuka et al., 2021;Li et al., 2021).",
            "score": 0.3104056117783496,
            "section_title": "Conclusion",
            "char_start_offset": 23268,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 403
                },
                {
                    "start": 406,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1128
                },
                {
                    "start": 1131,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1542
                },
                {
                    "start": 1545,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 2072
                }
            ],
            "ref_mentions": [
                {
                    "start": 58,
                    "end": 84,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 1813,
                    "end": 1834,
                    "matchedPaperCorpusId": "73497737"
                },
                {
                    "start": 2031,
                    "end": 2055,
                    "matchedPaperCorpusId": "244048238"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3662109375
        },
        {
            "corpus_id": "272753601",
            "title": "Exploring and Enhancing the Transfer of Distribution in Knowledge Distillation for Autoregressive Language Models",
            "text": "a) Supervised Finetuning: Many works [25]- [29] employ distilled datasets and curate various data for fine-tuning language models, achieving enhanced performance. These methods utilize a hyperscale model, such as GPT-4's [2] distillation data, to perform high-quality distributional mimicry. Early works like FLAN 2021 [30] and Super-Natural Instructions [31] are to convert traditional NLP tasks into instruction format through manually defined instruction templates. FLAN-CoT [32] and FLAN 2022 [33] employee Chain-of-Thought training prompts to strengthen the reasoning of the model. [34] state that instruction finetuning can be thought of as a \"continual finetuning stage\". [35] demonstrate how various text encodings represent different tasks, enabling a single model to accomplish multiple downstream tasks and thereby achieving a more generalized model. These works on supervised finetuning predominantly concentrate on fine-tuning large language models, with minimal focus on using instructional fine-tuning data to develop smaller, more efficient models through student imitation of teacher learning. \n\nb) Knowledge Distillation: Knowledge distillation is a popular technique for training compacting pre-trained models from a big teacher network to a small student network. It can be applied to many domains like natural language processing [36]- [38] , image-text retrieval [39] and image classification [22], [40], [41]. Some works of online learning [11], [42], [43] evaluated on image classification focus on updating all parameters in both networks using labels (hard labels) and feedback information (soft labels) in the training process. This approach necessitates updating the parameters of all teachers or relies heavily on data augmentation during training, rendering it less suitable for more complex generative scenarios [44]- [47]. Existing studies have primarily focused on simpler distributions (classification tasks), and the recent development of large language models (LMs) underscores an urgent need to explore distillation within generative task scenarios. Recent discussions [17], [48] on Kullback-Leibler Divergence in large LMs distillation have intensified.",
            "score": 0.3097120474246917,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 3802,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1110
                },
                {
                    "start": 1113,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2086
                },
                {
                    "start": 2087,
                    "end": 2191
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 47,
                    "matchedPaperCorpusId": "259108263"
                },
                {
                    "start": 221,
                    "end": 224,
                    "matchedPaperCorpusId": "257532815"
                },
                {
                    "start": 319,
                    "end": 323,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 355,
                    "end": 359,
                    "matchedPaperCorpusId": "253098274"
                },
                {
                    "start": 478,
                    "end": 482,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 497,
                    "end": 501,
                    "matchedPaperCorpusId": "253018554"
                },
                {
                    "start": 587,
                    "end": 591,
                    "matchedPaperCorpusId": "258865923"
                },
                {
                    "start": 679,
                    "end": 683,
                    "matchedPaperCorpusId": "254853816"
                },
                {
                    "start": 1351,
                    "end": 1355,
                    "matchedPaperCorpusId": "237250417"
                },
                {
                    "start": 1357,
                    "end": 1361,
                    "matchedPaperCorpusId": "243399696"
                },
                {
                    "start": 1385,
                    "end": 1389,
                    "matchedPaperCorpusId": "250264430"
                },
                {
                    "start": 1415,
                    "end": 1419,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1421,
                    "end": 1425,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1427,
                    "end": 1431,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1463,
                    "end": 1467,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 1469,
                    "end": 1473,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1475,
                    "end": 1479,
                    "matchedPaperCorpusId": "249209818"
                },
                {
                    "start": 1843,
                    "end": 1847,
                    "matchedPaperCorpusId": "56895382"
                },
                {
                    "start": 1849,
                    "end": 1853,
                    "matchedPaperCorpusId": "272423937"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18896484375
        },
        {
            "corpus_id": "267412334",
            "title": "PanGu-$\\pi$ Pro:Rethinking Optimization and Architecture for Tiny Language Models",
            "text": "Large language models (LLMs), trained on extensive corpora, have demonstrated impressive performance across diverse natural language tasks. The release of ChatGPT, with its robust generalization capabilities, has captured global attention and holds the potential to revolutionize the interaction between humans and computers. \n\nIn addition to the GPT-series models (Radford et al., 2018;Brown et al., 2020;Achiam et al., 2023), various large language models have emerged. PaLM (Chowdhery et al., 2023) trains a model with an impressive 540B parameters across 6144 TPU v4 chips. LLaMA (Touvron et al., 2023) releases a series of foundational language models, ranging from 7B to 70B parameters. Both the model architecture and trained weights are open-source, fostering collaboration within the AI community. Most of the following large models leverage similar architectures and training methodologies. For instance, Baichuan teams (Yang et al., 2023) train 7B and 13B parameter models on a 2.6T token dataset encompassing both Chinese and English corpora. Qwen (Bai et al., 2023), Yi (Yi, 2023), and Skywork (Wei et al., 2023) pursue similar paths, training models with 2.4T, 3T, and 3.2T tokens, respectively. Primarily attributed to the increas-ing accumulation of cleaned data, the performance of LLMs improves rapidly. \n\nWhile numerous studies have successfully trained various high-performance language models (Ren et al., 2023;Zeng et al., 2022), the methodologies employed in training such models remain insufficiently analyzed. On one hand, a substantial body of work concentrates on collecting and cleaning data, with less emphasis on researching effective training strategies. On the other hand, the training of large models demands an exceedingly high computational resource investment, making it impractical to explore a wide range of optimization strategies. As a result, recent works often adopt similar training recipes when constructing LLMs (Touvron et al., 2023;Yi, 2023;Bai et al., 2023;Wei et al., 2023).",
            "score": 0.3096683782514674,
            "section_title": "Introduction",
            "char_start_offset": 381,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 325
                },
                {
                    "start": 328,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1321
                },
                {
                    "start": 1324,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 2023
                }
            ],
            "ref_mentions": [
                {
                    "start": 387,
                    "end": 406,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 477,
                    "end": 501,
                    "matchedPaperCorpusId": "247951931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.206298828125
        },
        {
            "corpus_id": "251371542",
            "title": "Towards No.1 in CLUE Semantic Matching Challenge: Pre-trained Language Model Erlangshen with Propensity-Corrected Loss",
            "text": "In Chines Natural Language Understanding (NLU) tasks, Pre-trained Language Models (PLMs) have proven to be effective, such as BERT (Devlin et al., 2019), MacBERT (Cui et al., 2020) and AL-BERT (Lan et al., 2020). MacBERT and RoBERTa improve the pre-training tasks of BERT and AL-BERT by considering Chinese grammatical structure (Cui et al., 2020). However, those models only randomly masks tokens, which leads to a simple pre-training task. Therefore, we introduce Knowledge-based Dynamic Masking (KDM) method to mask semantically rich tokens. Moreover, we pre-train Erlangshen in a large-scale Chinese corpus and optimize the pre-training settings such as employing pre-layer normalization (Xiong et al., 2020) method. In addition, several designed prompts are assigned into surprised datasets for processing surprised pre-training tasks. \n\nAs a specific Chinese NLU task, the CLUE Semantic Matching Challenge has higher difficulty than binary classification semantic matching. By our observation, the QBQTC dataset has serious data imbalance problem, which includes a imbalance label distribution with severe bias. The large number of negative samples drives the model to ignore learning difficult positive samples under the effect of popular cross-entropy. Therefore, the poor prediction on positive samples directly leads to a low F1 score. This problem comes from the nature of cross-entropy, which treats each sample 2 https://browser.qq.com/ arXiv:2208.02959v1 [cs.CL] 5 Aug 2022 equally and tries to push them to positive or negative. An workable loss to address unbalanced samples is Dice loss (Li et al., 2020). In classification scenario, we only need to concern a probability < 0.5 instead of an extract probability 0 when it is a negative sample. Although Dice loss achieves the success of data imbalance, it only consider target categories while ignoring non-target categories. Therefore, this method is susceptible to the influence of a single sample.",
            "score": 0.3092844115083955,
            "section_title": "Introduction",
            "char_start_offset": 931,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 840
                },
                {
                    "start": 843,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 1967
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 152,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 692,
                    "end": 712,
                    "matchedPaperCorpusId": "211082816"
                },
                {
                    "start": 1604,
                    "end": 1621,
                    "matchedPaperCorpusId": "215745536"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1695556640625
        },
        {
            "corpus_id": "269921863",
            "title": "EnviroExam: Benchmarking Environmental Science Knowledge of Large Language Models",
            "text": "In the field of environmental science, it is crucial to have robust evaluation metrics for large language models to ensure their efficacy and accuracy. We propose EnviroExam, a comprehensive evaluation method designed to assess the knowledge of large language models in the field of environmental science. EnviroExam is based on the curricula of top international universities, covering undergraduate, master's, and doctoral courses, and includes 936 questions across 42 core courses. By conducting 0-shot and 5-shot tests on 31 open-source large language models, EnviroExam reveals the performance differences among these models in the domain of environmental science and provides detailed evaluation standards. The results show that 61.3% of the models passed the 5-shot tests, while 48.39% passed the 0-shot tests. By introducing the coefficient of variation as an indicator, we evaluate the performance of mainstream open-source large language models in environmental science from multiple perspectives, providing effective criteria for selecting and fine-tuning language models in this field. Future research will involve constructing more domain-specific test sets using specialized environmental science textbooks to further enhance the accuracy and specificity of the evaluation.",
            "score": 0.3091985061101324,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1524658203125
        },
        {
            "corpus_id": "258546757",
            "title": "Data Complexity: A New Perspective for Analyzing the Difficulty of Defect Prediction Tasks",
            "text": "In the field of machine learning, there has been growing interest in characterizing the complexity of both individual instances and entire datasets, known as data complexity analysis. The concept of instance hardness was initially proposed by M. Smith et al. [105], which estimates the probability that a given instance in a particular dataset will be misclassified. Limited training samples are distributed in a high-dimensional and sparse feature space, which naturally results in certain samples being difficult to classify. For example, overlapping instances and minority instances in small disjuncts tend to have a relatively high instance hardness value, as a ML algorithm has to overfit the limited training instances to learn an accurate decision boundary on the entire dataset. Recently, some researchers have explored incorporating the information of instance hardness into the learning progress to improve the performance of ML models or alleviate data complexity issues [19,42,59,60,107,132]. Moreover, Zhou et al. expanded on the notion of instance hardness by proposing dynamic instance hardness and integrated it into curriculum learning, effectively improving the efficiency and performance of curriculum learning approaches [106,133]. \n\nInstance hardness values reveal instances that are prone to misclassification, while hardness measures explain why they are difficult to classify. M. Smith et al. [107] proposed a set of interpretable hardness measures by examining the learning mechanisms of several simple ML algorithms. These measures provide insights into why particular instances are challenging to classify and how to detect them. Specifically, these measures reveal the difficulty of a classification problem at the instance level, rather than at the aggregate level of the entire dataset. However, these measures can also be averaged to estimate the hardness of the dataset as a whole. After that, Arruda et al. proposed new instance hardness measures as a supplement to the previous work [4]. An open-source Python library named PyHard2 has been developed to implement all of the measures proposed in [107] and [4]. Recent work on dynamic classifier selection has demonstrated the utility of instance hardness measures in identifying classifiers that perform well in confusing or overlapping areas of the dataset, providing indications of local competency.",
            "score": 0.30887401304497075,
            "section_title": "Prior Research on Data Complexity Analysis",
            "char_start_offset": 14516,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1251
                },
                {
                    "start": 1254,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2385
                }
            ],
            "ref_mentions": [
                {
                    "start": 982,
                    "end": 986,
                    "matchedPaperCorpusId": "231580628"
                },
                {
                    "start": 986,
                    "end": 989,
                    "matchedPaperCorpusId": "57365456"
                },
                {
                    "start": 989,
                    "end": 992,
                    "matchedPaperCorpusId": "202542410"
                },
                {
                    "start": 992,
                    "end": 995,
                    "matchedPaperCorpusId": "224703399"
                },
                {
                    "start": 995,
                    "end": 999,
                    "matchedPaperCorpusId": "11674608"
                },
                {
                    "start": 999,
                    "end": 1003,
                    "matchedPaperCorpusId": "249861618"
                },
                {
                    "start": 1241,
                    "end": 1246,
                    "matchedPaperCorpusId": "265098679"
                },
                {
                    "start": 1246,
                    "end": 1250,
                    "matchedPaperCorpusId": "227275560"
                },
                {
                    "start": 1417,
                    "end": 1422,
                    "matchedPaperCorpusId": "11674608"
                },
                {
                    "start": 2017,
                    "end": 2020,
                    "matchedPaperCorpusId": "224817007"
                },
                {
                    "start": 2130,
                    "end": 2135,
                    "matchedPaperCorpusId": "11674608"
                },
                {
                    "start": 2140,
                    "end": 2143,
                    "matchedPaperCorpusId": "224817007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7568359375
        },
        {
            "corpus_id": "259164635",
            "title": "CMMLU: Measuring massive multitask language understanding in Chinese",
            "text": "We analyze the difficulty distribution of CMMLU from two perspectives. Firstly, the CMMLU benchmark encompasses a diverse range of difficulty levels: 5 subjects at primary school level, 10 at middle/high school level, 23 at college level, and 29 at professional level, ensuring a comprehensive difficulty spectrum. \n\nSecondly, to estimate the difficulty distribution within each subject, we evaluated the top 20 models from our main results table. Each question was treated as a data point, and we recorded the number of models correctly answering each question. This approach allowed us to map out the difficulty distribution across subjects. where the x-axis represents the number of models that correctly answer a question, and the y-axis indicates the quantity of such questions. A peak on the left side of the plot (e.g., college actuarial science at position [3, 3]) suggests that the subject is generally challenging, as most questions are correctly answered by only a few models. Conversely, a peak on the right (e.g., arts at position [1, 4]) indicates a relatively simpler subject, where most questions are correctly answered by many models. Subjects exhibiting multi-peak distributions reveal a varied difficulty range within that subset. For instance, a hypothetical scenario with a dataset comprising basic arithmetic problems and complex calculus questions would result in a distribution with two distinct peaks separated by a notable gap, resembling a horizontal funnel. This indicates a wide spectrum of difficulty levels, from very easy to highly challenging. Figure 8 reveals that the majority of subjects exhibit a single peak in their difficulty distribution. This single-peak pattern indicates a uniform level of difficulty within these subjects, suggesting a consistent challenge for models across the range of questions. However, certain subjects, such as machine learning (located at position [9, 1]) and professional law (at position [10, 3]), display dual peaks. This dual-peak pattern signifies a notable presence of both relatively easy and challenging questions, with fewer intermediate-level questions. Despite the presence of two peaks, the transition between these peaks is gradual rather than abrupt, indicating a smooth progression in difficulty levels within these subjects. We assessed the concept of emergent ability using the LLaMA-2 model family.",
            "score": 0.30887401304497075,
            "section_title": "D CMMLU DIFFICULTY DISTRIBUTION",
            "char_start_offset": 25410,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 314
                },
                {
                    "start": 317,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2309
                },
                {
                    "start": 2310,
                    "end": 2385
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11517333984375
        },
        {
            "corpus_id": "251719464",
            "title": "DiscrimLoss: A Universal Loss for Hard Samples and Incorrect Samples Discrimination",
            "text": "The basic idea of CL is to estimate a priori the difficulty of a given sample by measuring the importance of each sample directly during the training in the form of a weight, such that easy samples with small loss can receive larger weights during the early stages of training. However, no matter hard and incorrect samples would be learned in previous CL methods, as they cannot explicitly discriminate between these two kinds of samples. Based on the observation of large-scale training samples in Fig. 2, we argue that: (1) the loss of correct samples should be smaller than incorrect ones; (2) partial correct samples, which overlap with incorrect samples, can be regarded as hard samples, whose losses are larger than easy ones and smaller than incorrect ones. The challenge is how to accurately separate easy, hard and incorrect samples. \n\nTo address this issue, we propose a stage-wise training strategy. As shown in Fig. 3, we first separate easy samples and difficult ones using a threshold k dyn , where k dyn \u2208 R is an adaptive threshold and initialized with a small value at the early training stages (Stage 1). The learning capability of the model is weak at this stage. Most samples are regarded as difficult ones, which would generate large losses. With the improvement of the model's learning ability, the value of k dyn gradually becomes larger, and more hard samples can be learned by the model. We use an extra learnable parameter \u03b4 i \u2265 0 to monitor whether the model can learn knowledge from correct samples, where \u03b4 i denotes the importance (or weight) of the i-th sample. When the model cannot gain any knowledge from correct samples, k dyn reaches its maximum value, so that we can separate hard and incorrect samples using the maximum value of k dyn at this stage (Stage 2). Formally, let {(x i , y i )} N i=1 denote the data, where x i is the i-th sample, and y i is its corresponding label. l i = g(M \u03b8 (x i ), y i ) is the loss of sample x i predicted by the model M with trainable parameters \u03b8, where g(\u2022) denotes the task loss function.",
            "score": 0.30887401304497075,
            "section_title": "A. Overview",
            "char_start_offset": 4069,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2065
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37255859375
        },
        {
            "corpus_id": "274610173",
            "title": "Predictable Emergent Abilities of LLMs: Proxy Tasks Are All You Need",
            "text": "However, predicting the onset of these capabilities remains challenging, as evidenced by the extensive exploration of this topic in the literature (Ganguli et al., 2022;Suzgun et al., 2022;Wei et al., 2022a). Recent work suggests that this unpredictability may be due to the metrics used (Lu et al., 2023;Srivastava et al., 2022) or the limited number of data points considered (Hu et al., 2023;Anwar et al., 2024). Further research has delved into the correlation between pretraining loss and downstream performance in language models (Huang et al., 2024;Xia et al., 2022), contributing to our understanding of scaling and emergent abilities (Gadre et al., 2024;Du et al., 2024). A theoretical framework proposed by (Arora and Goyal, 2023) posits that the performance of LLMs on complex tasks can be viewed as a combination of basic competencies. \n\nIn the early evaluation stages of large models, diverse methodologies have been employed to ensure comprehensive and precise assessments. Metrics such as perplexity, derived from the concept of entropy in information theory (Shannon, 1948), have been crucial. Lower perplexity values indicate stronger predictive capabilities. The DoReMi study (Xie et al., 2024) used perplexity to fine-tune data mixtures, achieving optimal performance with significantly less data prior to training. Similarly, the Falcon series (Almazrouei et al., 2023) introduced a suite of downstream task evaluation metrics as an alternative to perplexity for early-stage assessments. This approach, which evaluates models using a set of downstream tasks and averages their scores, provides a practical and interpretable measure of model capabilities, making it particularly valuable during the preliminary stages of model development, despite its inherent subjectivity.",
            "score": 0.30887401304497075,
            "section_title": "Related Work",
            "char_start_offset": 5996,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 847
                },
                {
                    "start": 850,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1793
                }
            ],
            "ref_mentions": [
                {
                    "start": 147,
                    "end": 169,
                    "matchedPaperCorpusId": "246867298"
                },
                {
                    "start": 378,
                    "end": 395,
                    "matchedPaperCorpusId": "263672005"
                },
                {
                    "start": 1194,
                    "end": 1212,
                    "matchedPaperCorpusId": "258741043"
                },
                {
                    "start": 1364,
                    "end": 1389,
                    "matchedPaperCorpusId": "268096300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31787109375
        },
        {
            "corpus_id": "276558440",
            "title": "Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning",
            "text": "Curriculum learning is a machine learning strategy (Bengio et al., 2009). It suggests that the training process should mimic human cognitive learning by starting with simple examples and gradually increasing in difficulty. The core of this method lies in how to measure the difficulty of examples, which often depends on the characteristics of the specific task. For example, in the field of computer vision, the number of objects in an image (Wei et al., 2016) or noise (Chen and Gupta, 2015) contained can be used to measure difficulty. In the field of natural language processing, sentence length (Platanios et al., 2019) can be used as a measure of difficulty. In addition to these, the difficulty can also be measured by human educational level (Lee et al., 2023) or evaluation models (Soviany et al., 2020).",
            "score": 0.30887401304497075,
            "section_title": "Curriculum Learning",
            "char_start_offset": 6125,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 813
                }
            ],
            "ref_mentions": [
                {
                    "start": 51,
                    "end": 72,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 443,
                    "end": 461,
                    "matchedPaperCorpusId": "1821165"
                },
                {
                    "start": 471,
                    "end": 493,
                    "matchedPaperCorpusId": "5658192"
                },
                {
                    "start": 600,
                    "end": 624,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 790,
                    "end": 812,
                    "matchedPaperCorpusId": "204801162"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.400146484375
        },
        {
            "corpus_id": "266977266",
            "title": "The Unreasonable Effectiveness of Easy Training Data for Hard Tasks",
            "text": "How can we train models to perform well on hard test data when hard training data is by definition difficult to label correctly? This question has been termed the scalable oversight problem and has drawn increasing attention as language models have continually improved. In this paper, we present the surprising conclusion that current pretrained language models often generalize relatively well from easy to hard data, even performing as well as oracle models finetuned on hard data. We demonstrate this kind of easy-to-hard generalization using simple finetuning methods like in-context learning, linear classifier heads, and QLoRA for seven different measures of datapoint hardness, including six empirically diverse human hardness measures (like grade level) and one model-based measure (loss-based). Furthermore, we show that even if one cares most about model performance on hard data, it can be better to collect easy data rather than hard data for finetuning, since hard data is generally noisier and costlier to collect. Our experiments use open models up to 70b in size and four publicly available question-answering datasets with questions ranging in difficulty from 3rd grade science questions to college level STEM questions and general-knowledge trivia. We conclude that easy-to-hard generalization in LMs is surprisingly strong for the tasks studied. Our code is available at: https://github.com/allenai/easy-to-hard-generalization",
            "score": 0.30887401304497075,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4482421875
        },
        {
            "corpus_id": "259000158",
            "title": "Spoken Language Identification System for English-Mandarin Code-Switching Child-Directed Speech",
            "text": "The Development and Evaluation set was highly imbalanced, so we considered testing two configurations of cross-entropy loss, one with equal weight factor for both classes and one with unequal weights. For the unequal weight case, the weight for each category was decided using the equation: wx = N Nx ; where wx is weight for language X, N is the total number of samples, and Nx is the total number of samples of language X. We also tried Additive Angular Margin (AAM) [22] loss which is supposed to help learn better intermediate features. For AAM, the loss scale was 30, and the margin was 0.01.",
            "score": 0.30887401304497075,
            "section_title": "Loss function",
            "char_start_offset": 10675,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 597
                }
            ],
            "ref_mentions": [
                {
                    "start": 469,
                    "end": 473,
                    "matchedPaperCorpusId": "8923541"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1453857421875
        },
        {
            "corpus_id": "258546757",
            "title": "Data Complexity: A New Perspective for Analyzing the Difficulty of Defect Prediction Tasks",
            "text": "By utilizing instance hardness measures to refine the ensemble generation process and leveraging dataset complexity measures for preprocessing method selection, data complexity information serves to boost an algorithm's learning capacity. Nevertheless, they may fail for datasets with inadequate training data or an extremely high degree of class imbalance.",
            "score": 0.30887401304497075,
            "section_title": "Summary of answers to RQ4 and their implications:",
            "char_start_offset": 95363,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 357
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5302734375
        },
        {
            "corpus_id": "273507997",
            "title": "A Simple Model of Inference Scaling Laws",
            "text": "The result in Eq. ( 7) not only allows one to predict a threshold for inference in order to get an increase in model performance on difficult problems, but could also offer some interpretation for the difficulty of tasks with respect to the trained model. We can gain some insights regarding the real-world models by fitting the pass@k metric according to Eq. ( 6) to the ones given in Brown et al. [2024], and attempt to interpret the properties of the test data from the parameters \u03b1 and \u03b2. In particular, note that the ratio \u03b1 \u03b1+\u03b2 gives the mean of the Beta distribution, which represents the average failure probability across the samples. If \u03b1 > \u03b2, the mean failure probability is high (i.e., most samples are harder). On the other hand, if \u03b1 < \u03b2, the mean failure probability is low, implying that most samples are easy. Furthermore, the denominator \u03b1 + \u03b2 governs the concentration of the distribution, such that when \u03b1 + \u03b2 is large, the failure probabilities p i are more tightly clustered around the mean (more homogeneity in difficulty), and conversely for small \u03b1 + \u03b2. \n\nFrom Fig. 1, we can see that the typical values for the \"harder\" problem parameter \u03b2 \u223c 0.35, while the tail parameter is 2 < \u03b1 < 20. This implies that most of the problems in the datasets used to measure the pass@k curves were indeed difficult in terms of the model's perception, while the existence of a left tail implies that the easy samples are covered quickly with less than 100 trials. This is reflected in the right panel of Fig. 1. \n\nAnother point of interest is that the average performed in Eq. ( 6) can be thought of as a Laplace transform from the variable \u03c3 = log (1/p) to the trials space k, in the sense that \n\nThis interpretation implies that it is possible to derive the probability distribution function of the samples in terms of their perceived difficulty by performing the inverse transform. In particular, given an empirical pass@k metric obtained for a given model, the inverse transform on f (k) = (A \u2212 pass@k)/A will yield the perceived difficulty PDF.",
            "score": 0.308487939987376,
            "section_title": "Interpretability",
            "char_start_offset": 8224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1078
                },
                {
                    "start": 1081,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1704
                },
                {
                    "start": 1707,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2058
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12103271484375
        },
        {
            "corpus_id": "273162494",
            "title": "Scaling Parameter-Constrained Language Models with Quality Data",
            "text": "Recent advancements in language model (LM) development have been significantly influenced by the exploration of scaling laws, which articulate the relationship between training loss, dataset size, and the number of model parameters (Hestness et al., 2017;Kaplan et al., 2020;Aghajanyan et al., 2023). These scaling laws have been instrumental in predicting the computational resources necessary for training increasingly large models and have provided a framework for understanding how model performance scales with data and parameters (Hoffmann et al., 2022;Kaplan et al., 2020). However, these laws primarily focus on the quantity of data and model size, often underestimating the critical role of data quality in model generalization. \n\nIn this work, we challenge the prevailing focus 1 on merely increasing data volume and model size by emphasizing the importance of data quality, particularly in scenarios constrained by the number of model parameters. We argue that for sub-billion parameter models, the quality of data-or what we term as effective training tokens -plays a more decisive role in model performance than previously recognized. This perspective shifts the paradigm from a quantity-centric view to a quality-centric approach in the development of language models. Further, we provide qualitative measures of standard data refinement techniques including data sampling (Penedo et al., 2023;Wang et al., 2024;Albalak et al., 2024) and text synthesis (Liu et al., 2024), applied to a pretraining corpus such as Re-finedWeb (Penedo et al., 2023). This helps to formulate the relationship between the diversity and syntheticity of pretraining data in order to compute the number of effective training tokens, which evaluate the impact of data quality in terms of model size and the token number. Further, we conduct extensive experiments across eight different benchmarks to evaluate the impact of data refinement techniques which allow us to significantly outperform models trained on randomly selected data samples, across a spectrum of model sizes ranging from 25 million to 1.5 billion parameters. \n\nBy integrating the notion of effective token size into the scaling law formulation, we extend the existing scaling law formulation to better capture the nuances of data quality.",
            "score": 0.3084659991404852,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 2115
                },
                {
                    "start": 2118,
                    "end": 2295
                }
            ],
            "ref_mentions": [
                {
                    "start": 275,
                    "end": 299,
                    "matchedPaperCorpusId": "255570036"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.256103515625
        },
        {
            "corpus_id": "249191943",
            "title": "Precise learning curves and higher-order scaling limits for dot-product kernel regression",
            "text": "Modern machine learning has entered an era in which scaling is arguably the most critical ingredient to improve performance. Recent breakthroughs such as GPT-3 [24] and PaLM [11] have demonstrated that performance of various learning algorithms improves in a predictable manner as the amount of data and computational resources used in training increases. The functional relationships between performance and resources are loosely referred to as learning curves. While extrapolation of empirical learning curves is widely used to make predictions about how a model might perform when extra resources become available, a rigorous theoretical understanding is lacking. A fundamental obstacle in developing a detailed theoretical model of such learning curves is that they depend on many moving parts, e.g. the data distribution, the network architecture, the training algorithm, among others. In addition, even in the simplest possible settings, the learning curves can exhibit non-trivial structure that naive scaling laws fail to model, e.g. the well-known double-descent phenomenon [3,7]. \n\nIn the past couple years, a large amount of effort from the community has improved our theoretical understanding of such phenomena and in some cases precise characterizations of learning curves have been obtained (see e.g., [1,21,28,31,34]). These results have helped clarify several puzzling empirical observations, such as the origin of the double-descent peak [2,13,27] and linear trends between in-and out-of-distribution generalization performance [30,38,39], among many others. However, the precise predictions from many of these analyses have been possible only in the linear high-dimensional scaling regime in which the number of training samples m scales linearly with the dimension d, i.e. m \u221d d. In these asymptotics, the model's effective capacity is limited to linear functions of the features. In contrast, many state-of-the-art models operate in a regime where the amount of data is much larger than the data dimensionality; for example, large text corpora can contain trillions of tokens, whereas the effective input dimensionality of language models is at most millions.",
            "score": 0.30844900927841407,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1089
                },
                {
                    "start": 1092,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2179
                }
            ],
            "ref_mentions": [
                {
                    "start": 1083,
                    "end": 1086,
                    "matchedPaperCorpusId": "38293418"
                },
                {
                    "start": 1316,
                    "end": 1319,
                    "matchedPaperCorpusId": "221082525"
                },
                {
                    "start": 1319,
                    "end": 1322,
                    "matchedPaperCorpusId": "84186305"
                },
                {
                    "start": 1322,
                    "end": 1325,
                    "matchedPaperCorpusId": "235262792"
                },
                {
                    "start": 1325,
                    "end": 1328,
                    "matchedPaperCorpusId": "199668852"
                },
                {
                    "start": 1455,
                    "end": 1458,
                    "matchedPaperCorpusId": "226278106"
                },
                {
                    "start": 1458,
                    "end": 1461,
                    "matchedPaperCorpusId": "211678117"
                },
                {
                    "start": 1461,
                    "end": 1464,
                    "matchedPaperCorpusId": "222291394"
                },
                {
                    "start": 1552,
                    "end": 1555,
                    "matchedPaperCorpusId": "245018380"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1470947265625
        },
        {
            "corpus_id": "256697118",
            "title": "Bag of Tricks for Training Data Extraction from Language Models",
            "text": "Recent advances in language models (LMs) have led to impressive performance in a variety of downstream language tasks (Kenton & Toutanova, 2019;Brown et al., 2020). It has been demonstrated, however, that training data can be extracted from LMs due to the memorization effects (Kenton & Toutanova, 2019;Carlini et al., 2019;Feldman, 2020;Brown et al., 2020). These training data may contain sensitive information such as names, email addresses, phone numbers, and physical addresses, resulting in privacy leak-  age that hinders the widespread adoption of LMs (Carlini et al., 2021;2022;Lehman et al., 2021). \n\nAs privacy security has been an important issue of public concern, a crucial topic is to develop efficient methods for evaluating privacy leakage. Thus, the focus of our research is on the adversarial task of training data extraction from LMs, a relatively new area of study (Carlini et al., 2021;Lehman et al., 2021). Existing extraction methods have yielded successful records, but there are instances in which these methods are even less effective than simply selecting the most popular entity based on prior score. In addition, successful data extraction requires a high generation ratio, i.e., the need to generate and rank a large number of candidates in order to identify a single successful instance. These suboptimal results suggest that, despite the viability of training data extraction and developed pioneering methods as demonstrated in previous research, this task is still relatively new with an abundance of problems to solve. \n\nIn this study, we aim to develop techniques for efficient training data extraction. We adhere to the criteria of the recent Training Data Extraction Challenge, 1 which employs a 1.3B parameter GPT-Neo model (Black et al., 2021) for targeted extraction of 1-eidetic memorized data. Targeted extraction refers to the scenario where a prefix of the data is provided, such as 'Yu's phone number is', and the adversary attempts to recover the suffix '12345'. Accroding to Carlini et al. (2021), \u03ba-eidetic memorization is defined as the capacity of a language model to memorize a string that appears \u03ba times in the training material.",
            "score": 0.3082518266221506,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1553
                },
                {
                    "start": 1556,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2183
                }
            ],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 144,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 144,
                    "end": 163,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 277,
                    "end": 303,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 303,
                    "end": 324,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 324,
                    "end": 338,
                    "matchedPaperCorpusId": "186206616"
                },
                {
                    "start": 338,
                    "end": 357,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 560,
                    "end": 582,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 886,
                    "end": 908,
                    "matchedPaperCorpusId": "220831381"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12261962890625
        },
        {
            "corpus_id": "265351868",
            "title": "Machine Translation to Control Formality Features in the Target Language",
            "text": "We tokenized and preprocessed the data, applying text cleaning techniques to ensure consistency and reduce noise. The dataset is then split into training and testing sets for evaluating model performance. \n\nWe initialized the IndicBERTv2-MLM model and attached a language modeling head for masked language modeling finetuning. We employed the Adam optimizer and set the learning rate to 1e-4 to optimize the model during training. Instead of employing a constant learning rate during the training process, we opted for a dynamic approach using a learning rate scheduler. The purpose of this scheduler is to adjust the learning rate dynamically throughout training, which aids the model in converging faster while avoiding issues like overshooting or being trapped in local minima. To achieve this, we utilized the linear scheduler with warm-up function from the transformers library to construct a learning rate scheduler with warm-up steps. This scheduler gradually increases the learning rate from a very low value to the desired learning rate, which is particularly beneficial in the initial stages of training, leading to more stable and effective. \n\nWe iteratively train the model over 30 epochs, optimizing the parameters to minimize the cross-entropy loss. The model's progress and loss values are monitored through each epoch to assess training efficacy. After every 30 epochs, a new set of 1000 records are trained again with the same epochs to introduce fresh data for the model to learn from. This process is known as \"restarting\" or \"re-batching\" the training. By using a new set of records, we ensure that the model encounters a diverse range of examples in each batch, leading to better generalization and reducing the risk of overfitting. Additionally, this approach helps to mitigate any potential bias that may have been introduced due to the order of the data during initial training. \n\nDuring the fine-tuning process, we leveraged the power of random token masking to simulate formal-informal variations in the training data. By randomly masking tokens within sen-tences, we enhance the model's ability to generalize and make contextually appropriate predictions. Additionally, we created masked versions of the ground truth sentences and evaluate the model's loss on these masked tokens to assess its formality control capabilities.",
            "score": 0.3079614449993312,
            "section_title": "E. Training & Fine Tuning",
            "char_start_offset": 24844,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 204
                },
                {
                    "start": 207,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1152
                },
                {
                    "start": 1155,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1902
                },
                {
                    "start": 1905,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2182
                },
                {
                    "start": 2183,
                    "end": 2352
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.266357421875
        },
        {
            "corpus_id": "273350576",
            "title": "Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws",
            "text": "The existence of good curricula. Curriculum learning (Bengio et al., 2009) is a conceptually appealing idea that has been regularly studied over the years, but these methods have not been widely adopted in deep learning outside of a few relatively niche areas (Graves et al., 2017;Florensa et al., 2017;Jesson et al., 2018;Tang et al., 2018). To the best of our knowledge, there is no consensus on why curriculum learning has not had wide success in deep learning. Some hypothesize that deep learning is different from human learning or that overparameterization makes curriculum less effective (Wu et al., 2021;Mannelli et al., 2024). On the other hand, some experimental evidence suggests that deep neural networks implicitly learn functions of increasing complexity from easy to hard (Kalimeris et al., 2019;Baldock et al., 2021), loosely resembling some aspects of human learning (Lawrence, 1952;Baker & Osgood, 1954;Skinner, 1958). For overparameterization, it is not clear if modern foundation model pretraining is in the overparameterized regime since dataset size generally scales with the number of model parameters (Hoffmann et al., 2022). \n\nTo motivate our later work, we propose an alternative hypothesis: good curricula exist but they are computationally difficult to find. Let's start with an instructive experiment of finding good curricula with more computation. Suppose we have a fixed dataset of size N and want to train SGD at a minibatch size of 1 for 1 epoch (i.e., each data point is used once, which is common for language model pretraining). We are interested in finding a data ordering that is better than a random shuffle when training a model initialized at random. Brute force search is intractable even for a small dataset. Unlike most existing curricula that rely on some prior notions of difficulty, we will find a good curriculum with only training data by optimizing over the space of permutations with meta learning. Due to space constraints, we defer the details of this meta learning procedure to Appendix B.",
            "score": 0.3079257007281282,
            "section_title": "MOTIVATIONS",
            "char_start_offset": 5498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2044
                }
            ],
            "ref_mentions": [
                {
                    "start": 53,
                    "end": 73,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 260,
                    "end": 281,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 595,
                    "end": 612,
                    "matchedPaperCorpusId": "227343966"
                },
                {
                    "start": 787,
                    "end": 811,
                    "matchedPaperCorpusId": "167217440"
                },
                {
                    "start": 811,
                    "end": 832,
                    "matchedPaperCorpusId": "235458180"
                },
                {
                    "start": 884,
                    "end": 900,
                    "matchedPaperCorpusId": "45615744"
                },
                {
                    "start": 900,
                    "end": 921,
                    "matchedPaperCorpusId": "26460618"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2337646484375
        },
        {
            "corpus_id": "267770178",
            "title": "An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling",
            "text": "Sequence labeling is a fundamental task in natural language processing (NLP) that involves assigning labels to individual elements in a sequence. Recent advancements in neural network approaches have demonstrated remarkable performance in sequence labeling tasks (Zhang et al., 2014;Chen et al., 2017;Zhang et al., 2018;Tian et al., 2020a;Nguyen et al., 2021;Hou et al., 2021;Liu et al., 2021). Some of these works have explored the integration of external knowledge, such as n-grams, lexicons, and syntax, to enhance sequence labeling models. However, this integration increases the heterogeneity and complexity of the input data. Furthermore, incorporating knowledge into sequence labeling models often requires additional encode modules, such as attention mechanisms (Liu et al., 2021;Tian et al., 2020a) or graph neural network (GNN) (Gui et al., 2019;Nie et al., 2022). These additional modules increase the model's parameters, making it more time-consuming and resourceintensive to develop a high-performing system. \n\nCurriculum Learning (CL) is an effective approach to tackle these challenges (Bengio et al., 2009), as it mimics the human learning process by gradually introducing training samples from easy to hard, thereby enabling efficient learning in heterogeneous data while improving both the speed and quality of the model (Wang et al., 2021). CL has demonstrated success in various NLP tasks, including machine translation (Wan et al., 2020), dialogue generation (Zhu et al., 2021), and text classification (Zhang et al., 2022). \n\nIn CL, data-selection curriculum strategies play a crucial role. Recently, Mohiuddin et al. (2022) proposed metrics that rely on the similarity of parallel sentence pairs to guide data selection in machine translation tasks. Similarly, Yuan et al. (2022) introduced difficulty metrics for entity typing based on training loss. However, these difficulty metrics primarily focus on the sentence level, and there is a lack of token-level and word-level metrics to measure the difficulty of sequence labeling tasks.",
            "score": 0.307904036235079,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1021
                },
                {
                    "start": 1024,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 263,
                    "end": 283,
                    "matchedPaperCorpusId": "10585502"
                },
                {
                    "start": 283,
                    "end": 301,
                    "matchedPaperCorpusId": "30285737"
                },
                {
                    "start": 301,
                    "end": 320,
                    "matchedPaperCorpusId": "44135302"
                },
                {
                    "start": 320,
                    "end": 339,
                    "matchedPaperCorpusId": "220046453"
                },
                {
                    "start": 359,
                    "end": 376,
                    "matchedPaperCorpusId": "241583255"
                },
                {
                    "start": 788,
                    "end": 807,
                    "matchedPaperCorpusId": "220046453"
                },
                {
                    "start": 856,
                    "end": 873,
                    "matchedPaperCorpusId": "245844443"
                },
                {
                    "start": 1101,
                    "end": 1122,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1440,
                    "end": 1458,
                    "matchedPaperCorpusId": "222272176"
                },
                {
                    "start": 1480,
                    "end": 1498,
                    "matchedPaperCorpusId": "244119160"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.351318359375
        },
        {
            "corpus_id": "259370548",
            "title": "Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach",
            "text": "A curriculum is a planned sequence of learning materials and an effective one can make learning efficient and effective for both humans and machines. Recent studies developed effective data-driven curriculum learning approaches for training graph neural networks in language applications. However, existing curriculum learning approaches often employ a single criterion of difficulty in their training paradigms. In this paper, we propose a new perspective on curriculum learning by introducing a novel approach that builds on graph complexity formalisms (as difficulty criteria) and model competence during training. The model consists of a scheduling scheme which derives effective curricula by accounting for different views of sample difficulty and model competence during training. The proposed solution advances existing research in curriculum learning for graph neural networks with the ability to incorporate a fine-grained spectrum of graph difficulty criteria in their training paradigms. Experimental results on real-world link prediction and node classification tasks illustrate the effectiveness of the proposed approach.",
            "score": 0.3078801513508629,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72314453125
        },
        {
            "corpus_id": "278535360",
            "title": "Learning Like Humans: Advancing LLM Reasoning Capabilities via Adaptive Difficulty Curriculum Learning and Expert-Guided Self-Reformulation",
            "text": "CL is a training strategy that mimics human learning progression by systematically increasing the complexity of training data, typically following an easier-to-harder trajectory. This principle has demonstrated significant efficacy across various machine learning domains, enhancing both generalization capabilities and convergence rates in applications ranging from computer vision (CV) (Guo et al., 2018;Jiang et al., 2014) and natural language processing (NLP) (Platanios et al., 2019;Tay et al., 2019) to reinforcement learning (RL), and notably in both the pre-training (Zhang et al., 2025) and post-training (Wang et al., 2025;Shi et al., 2025) phases of LLMs. Specifically for reinforcement fine-tuning (RFT) of LLMs, CL strategies are increasingly being adopted to optimize the training process and improve model performance (Lee et al., 2024;Na\u00efr et al., 2024;Team et al., 2025;Deng et al., 2025). Many current CL applications in this context rely on static curricula, where task difficulty is predetermined offline, and data is curated accordingly. For example, tasks might be assigned at fixed difficulty levels (Team et al., 2025), or data subsets selected based on predefined impact measures (Lee et al., 2024). Although such predefined curricula have demonstrated effectiveness in specific scenarios such as instruction-tuning and coding (Lee et al., 2024;Na\u00efr et al., 2024), they may not dynamically adapt to the model's evolving learning state. To address this limitation of static curricula, we propose the ADCL strategy, which dynamically adjusts training curricula by periodically reestimating difficulty within upcoming data batches to align with evolving model capabilities.",
            "score": 0.30778630850169236,
            "section_title": "Curriculum Learning for LLMs",
            "char_start_offset": 5331,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1695
                }
            ],
            "ref_mentions": [
                {
                    "start": 388,
                    "end": 406,
                    "matchedPaperCorpusId": "51920640"
                },
                {
                    "start": 406,
                    "end": 425,
                    "matchedPaperCorpusId": "207216972"
                },
                {
                    "start": 464,
                    "end": 488,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 488,
                    "end": 505,
                    "matchedPaperCorpusId": "166228313"
                },
                {
                    "start": 833,
                    "end": 851,
                    "matchedPaperCorpusId": "264146463"
                },
                {
                    "start": 1205,
                    "end": 1223,
                    "matchedPaperCorpusId": "264146463"
                },
                {
                    "start": 1352,
                    "end": 1370,
                    "matchedPaperCorpusId": "264146463"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.376953125
        },
        {
            "corpus_id": "266180578",
            "title": "Mmi01 at The BabyLM Challenge: Linguistically Motivated Curriculum Learning for Pretraining in Low-Resource Settings",
            "text": "Curriculum learning (CL) was first proposed by Bengio et al. (2009). The idea behind curriculum learning comes from the pedagogical observation that animals and humans learn better when knowledge is presented in a meaningfully organised way. For instance, starting with simple examples and gradually advancing to more complex ones (Skinner, 1958;Sweller, 1994;Krueger and Dayan, 2009). In the language modelling experiment carried out by Bengio et al. (2009), a corpus replacement method was used to make the data increasingly difficult. This way of pertaining was found to be more effective, producing improved results. \n\nThere have been then numerous works have explored using CL as the pretraining approach for language models. Whilst some works reported CL as beneficial to pretraining, others have reported the opposite results. Nagatsuka et al. (2021) investigated a CL-based pretraining scheme that utilises the length of the input text as the measure of \"difficulty\" in curriculum design. It was found that using length-based curriculum training alongside using the maximum available batch size, models achieved drastically faster convergence speed, and higher scores on downstream tasks ( Nagatsuka et al., 2021( Nagatsuka et al., , 2022) ) . \n\nCurriculum design greatly varies in each work. Linguistic features that have been used in curriculum formation include Parts-of-Speech (POS) information, n-gram frequency (Platanios et al., 2019), average number of dependents per word in the sentence parse tree (Jafarpour et al., 2021), edit distance (Kadotani et al., 2021;Chang et al., 2021). However, arguably, the most common curriculum formations are based on measures of frequency (Liu et al., 2018) and text length (Tay et al., 2019;Cirik et al., 2016). \n\nComparing curriculum learning studies becomes challenging due to the inherent variability in curriculum choices across different tasks. However, it is undeniable that the arrangement of data holds significance. As a result, in distinction from prior research, our work is oriented towards investigating diverse linguistic features in curriculum formation. Notably, we investigate 5 different measures of linguistic complexity. They are: \n\n\u2022 Average dependency distance (ADD) \n\n\u2022 Dependents per word (DPW)",
            "score": 0.3077169962517124,
            "section_title": "Related Works",
            "char_start_offset": 2045,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 620
                },
                {
                    "start": 623,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1251
                },
                {
                    "start": 1254,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1765
                },
                {
                    "start": 1768,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2194
                },
                {
                    "start": 2195,
                    "end": 2204
                },
                {
                    "start": 2207,
                    "end": 2242
                },
                {
                    "start": 2245,
                    "end": 2272
                }
            ],
            "ref_mentions": [
                {
                    "start": 47,
                    "end": 67,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 360,
                    "end": 384,
                    "matchedPaperCorpusId": "206863670"
                },
                {
                    "start": 438,
                    "end": 458,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 834,
                    "end": 857,
                    "matchedPaperCorpusId": "244048238"
                },
                {
                    "start": 1196,
                    "end": 1220,
                    "matchedPaperCorpusId": "244048238"
                },
                {
                    "start": 1220,
                    "end": 1249,
                    "matchedPaperCorpusId": "255221201"
                },
                {
                    "start": 1425,
                    "end": 1449,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 1516,
                    "end": 1540,
                    "matchedPaperCorpusId": "236486249"
                },
                {
                    "start": 1556,
                    "end": 1579,
                    "matchedPaperCorpusId": "237329957"
                },
                {
                    "start": 1579,
                    "end": 1598,
                    "matchedPaperCorpusId": "231846815"
                },
                {
                    "start": 1692,
                    "end": 1709,
                    "matchedPaperCorpusId": "51606954"
                },
                {
                    "start": 1727,
                    "end": 1745,
                    "matchedPaperCorpusId": "166228313"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2763671875
        },
        {
            "corpus_id": "263310392",
            "title": "A Large Language Model Approach to Educational Survey Feedback Analysis",
            "text": "The data used in this study was from a specific domain (online biomedical science courses) and was in English. Other domains and languages were not tested. \n\nLLM results were highly prompt-dependent, and others may achieve even more accurate results than those we have shown. Even within the most capable models, we observed that prompting techniques and prompt tuning made a significant difference. There is considerable literature on effective methods of prompting. There is an interplay of prompting techniques with the behavior of instruction-tuned models in a way that may or may not fully elicit the capabilities of each model, with prompts being seen as a form of hyperparameter to the model and with responses changing depending on updates to model training (Chen et al., 2023). \n\nOther than the comparison to SetFit and to the RoBERTa sentiment analysis model, we limited our exploration to recent OpenAI models; future work may expand this to include other models such as the most capable proprietary models (e.g., Claude and Gemini), and the most capable open-source models.",
            "score": 0.3071631995080376,
            "section_title": "Limitations",
            "char_start_offset": 60995,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 155
                },
                {
                    "start": 158,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 1085
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11474609375
        },
        {
            "corpus_id": "258937784",
            "title": "A Mathematical Interpretation of Autoregressive Generative Pre-Trained Transformer and Self-Supervised Learning",
            "text": "). While increasing model complexity, as defined in Definition 25, may improve the ability of a deep learning model to approximate a target function, as stated in Corollary 1, it may also lead to overfitting, resulting in reduced generalization capabilities. This trade-off between complexity and generalization is an important consideration when designing and training GPT-based models. \n\nExample 3 (Large Language Models and Universal Approximation). Large language models, such as GPT-based models, have gained popularity due to their ability to satisfy the conditions of the Universal Approximation Theorem. The large number of parameters in these models enables them to capture intricate patterns and relationships within the input data, allowing them to perform tasks such as natural language understanding and generation with high accuracy. Consider a large language model such as GPT-3 being utilized for translation tasks. Given a complex sentence in English, the model, due to its extensive parameter space, is able to understand the intricate relationships between words and their context, translating it accurately to another language such as French. This capability demonstrates how the large number of parameters enables GPT-3 to approximate the intricacies of language translation. \n\nThe advantages of large language models, such as GPT-based models, in terms of their ability to satisfy the conditions of the Universal Approximation Theorem, as described in Example 3, should not overshadow the potential pitfalls associated with their scale. As these models grow in size, they may become increasingly susceptible to overfitting, noise, or biases present in the training data. This can manifest as a heightened sensitivity to specific patterns, phrases, or concepts within the input data, potentially leading to the generation of responses that are less adaptable, less diverse, or less contextually appropriate. Consequently, it is important to carefully consider the impact of model size and complexity on both the benefits and potential drawbacks associated with GPT-based models. \n\nExample 4 (Large Language Models and Generalization Challenges). While large language models, such as GPT-based models, excel at capturing intricate patterns and relationships within input data, they may also face challenges related to generalization. As discussed in Corollary 3, the trade-off between complexity and generalization can result in overfitting, limiting the model's ability to generalize to new, unseen data or contexts.",
            "score": 0.3071418021376729,
            "section_title": "Corollary 3 (Model Complexity and Generalization",
            "char_start_offset": 49767,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 2
                },
                {
                    "start": 3,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 387
                },
                {
                    "start": 390,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1296
                },
                {
                    "start": 1299,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2099
                },
                {
                    "start": 2102,
                    "end": 2166
                },
                {
                    "start": 2167,
                    "end": 2353
                },
                {
                    "start": 2354,
                    "end": 2537
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.278564453125
        },
        {
            "corpus_id": "259370648",
            "title": "HuCurl: Human-induced Curriculum Discovery",
            "text": "are assigned weights around 0.5, unlike the three other cases of SNLI. We attribute this result to the reduced presence of hard samples (skewed entropy in Figure 3b).  Figure 5 shows the accuracy obtained when the topperforming discovered curriculum for one dataset (from Figure 4) is applied to other datasets. Each cell is the average result of 5 seeds. We observe common characteristics among datasets that cause the curriculum to be transferable between them. First, the top generalizable configuration is obtained from ChaosNLI, the dataset with the richest inter-annotator entropy signal. Therefore, the quality of the difficulty score is important to the discovery of an effective curriculum. Second, the inc configuration is among the most generalizable configurations, with no added cost in its creation. Third, the curricula obtained using the small, down-sampled difficulty-balanced datasets generalize well and achieve high performance on the large datasets. This is useful as curriculum discovery is much faster on smaller datasets, and the framework can be applied to large datasets by searching for a curriculum on a small subset of the data, mitigating  Table 2: Transferability of the specialized curricula discovered for small models to large models on ChaosNLI. \"Best baseline\" shows the best performance obtained by baselines (DP, SL, Mentornet). \"Ent (sp) n\" indicates the curriculum discovered on the model with n parameters. Column headers indicate the model trained using the discovered curricula of the corresponding rows.",
            "score": 0.3070908960255585,
            "section_title": "Settings",
            "char_start_offset": 21219,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.269287109375
        },
        {
            "corpus_id": "272366536",
            "title": "Is Difficulty Calibration All We Need? Towards More Practical Membership Inference Attacks",
            "text": "In practice, difficulty calibration primarily measures the difficulty of target samples by feeding them into models trained on similar data (reference models). A category of attacks known as reference-based attacks employs this technique, aiming to achieve finer-grained calibration at the cost of extensive computational resources and numerous queries [5,40,57,61]. \n\nWhile existing reference-based attacks have achieved significant breakthroughs on recently recommended True-Positive Rate at low False-Positive Rate (TPR at low FPR), we argue that difficulty calibration is \"not all we need\" to achieve more powerful and practical MIAs. We have observed that some non-members, which could have been correctly classified, are inadvertently misclassified after difficulty calibration [56], leading to suboptimal performance. Typically, difficulty calibration assumes that outputs from reference models can effectively represent the difficulty of target samples. However, such an assumption is optimistic and thus unrealistic in many cases. In this paper, we examine and highlight that calibration errors primarily stem from two contributing factors: 1) the reference dataset is a subset sampled from the potential distribution; 2) membership scores are highly dependent on the model parameters. A more comprehensive analysis of this will be provided in Section 3. \n\nTo effectively and efficiently address this issue, we propose RAPID that directly Re-leverAges the original membershiP scores to mItigate the errors in Difficulty calibration, rather than treating it merely as a component of obtained calibrated scores. Specifically, while the original scores are strongly influenced by the inherent difficulty of the samples, they can provide reliable non-membership evidence because the target model directly fits member points during training. In other words, samples exhibiting extremely low original scores (e.g., high losses) are almost non-members. RAPID re-leverages the original outputs to correct misclassifications of non-members after difficulty calibration, thereby outperforming existing reference-based attacks. \n\nTo mount RAPID, we adopt a typical supervised learning approach. Concretely, the adversary first trains a surrogate model (shadow model) for the target model and several reference models.",
            "score": 0.3070430387435822,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2114,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 366
                },
                {
                    "start": 369,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1363
                },
                {
                    "start": 1366,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2125
                },
                {
                    "start": 2128,
                    "end": 2192
                },
                {
                    "start": 2193,
                    "end": 2315
                }
            ],
            "ref_mentions": [
                {
                    "start": 353,
                    "end": 356,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 356,
                    "end": 359,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 359,
                    "end": 362,
                    "matchedPaperCorpusId": "252992499"
                },
                {
                    "start": 362,
                    "end": 365,
                    "matchedPaperCorpusId": "244345608"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.307373046875
        },
        {
            "corpus_id": "273811618",
            "title": "Does the Definition of Difficulty Matter? Scoring Functions and their Role for Curriculum Learning",
            "text": "Curriculum learning (CL) describes a machine learning training strategy in which samples are gradually introduced into the training process based on their difficulty. Despite a partially contradictory body of evidence in the literature, CL finds popularity in deep learning research due to its promise of leveraging human-inspired curricula to achieve higher model performance. Yet, the subjectivity and biases that follow any necessary definition of difficulty, especially for those found in orderings derived from models or training statistics, have rarely been investigated. To shed more light on the underlying unanswered questions, we conduct an extensive study on the robustness and similarity of the most common scoring functions for sample difficulty estimation, as well as their potential benefits in CL, using the popular benchmark dataset CIFAR-10 and the acoustic scene classification task from the DCASE2020 challenge as representatives of computer vision and computer audition, respectively. We report a strong dependence of scoring functions on the training setting, including randomness, which can partly be mitigated through ensemble scoring. While we do not find a general advantage of CL over uniform sampling, we observe that the ordering in which data is presented for CL-based training plays an important role in model performance. Furthermore, we find that the robustness of scoring functions across random seeds positively correlates with CL performance. Finally, we uncover that models trained with different CL strategies complement each other by boosting predictive power through late fusion, likely due to differences in the learnt concepts. Alongside our findings, we release the aucurriculum toolkit (https://github.com/autrainer/aucurriculum), implementing sample difficulty and CL-based training in a modular fashion.",
            "score": 0.30692423046106876,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25927734375
        },
        {
            "corpus_id": "266166227",
            "title": "Non-compositional Expression Generation Based on Curriculum Learning and Continual Learning",
            "text": "In this section, we briefly introduce our proposed curriculum learning method for non-compositional expression generation. Curriculum learning for efficiently leveraging available data resources consists of two main parts: a measure of difficulty of training instances, and an arrangement of the training examples using this measure. Accordingly, for non-compositional expression generation, we propose a data arrangement method for dynamically arranging the training examples according to a newly studied difficulty metric. In addition, due to the current large pre-trained language models' insufficiency in processing non-compositional expressions (Dankers et al., 2022), non-compositional expressions that are difficulty for LMs to understand would have a high perplexity score and the representations between non-compositional expressions and their constituent words would be large. Therefore, we use a combination of the representation distance and perplexity score as a measure of examples' difficulty. \n\nMoreover, in our experiments, we observe that following the curriculum learning principle of arranging the training examples based on their difficulty levels, the problem of forgetting arises due to the gradual shift of distribution in domain difficulty. Therefore, to alleviate this forgetting problem, we propose a simple yet effective continual learning method. Figure 1 demonstrates the workflow of our proposed curriculum learning framework and its details as follows.",
            "score": 0.30674922134930377,
            "section_title": "Framework",
            "char_start_offset": 9802,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1008
                },
                {
                    "start": 1011,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1484
                }
            ],
            "ref_mentions": [
                {
                    "start": 650,
                    "end": 672,
                    "matchedPaperCorpusId": "248780588"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69091796875
        },
        {
            "corpus_id": "259593617",
            "title": "On the Effectiveness of Curriculum Learning in Educational Text Scoring",
            "text": "Curriculum learning (CL) refers to the strategy used to train a prediction model by imitating the meaningful learning order in human curricula, i.e., presenting the training samples in an easy-to-hard manner so as to enable the model to first optimize an easier version of the target problem and gradually consider harder versions, until solving the full target task of interest (Bengio et al. 2009). As indicated before, most of the existing CL strategies consist of two key components, i.e., difficulty measurer and training scheduler (Wang, Chen, and Zhu 2021) (or scoring function and pacing function in other relevant literature). Depending on whether any (or both) of the two components are designed with the aid of human expertise (or data-driven approaches), a strategy can be classified as either pre-defined or automatic CL. Take difficulty measurer as an example, Platanios et al. (2019) developed a pre-defined strategy in which the difficulty of input text was determined by using its length as a proxy (i.e., the longer the input text, the higher difficulty it has), while Gan, Xu, and Zan (2021) proposed an automatic strategy in which the difficulty was measured by calculating its training loss in a specific epoch. \n\nSince its inception Bengio et al. (2009), CL has been demonstrated effective in boosting performance of various models in the research of computer vision and NLP (Soviany et al. 2020;Spitkovsky, Alshawi, and Jurafsky 2010;Tudor Ionescu et al. 2016;Gan, Xu, and Zan 2021;Platanios et al. 2019;Wei et al. 2016;Liu et al. 2018). For instance, when training on imbalance-distributed image data, the Dynamic Curriculum Learning framework proposed by Wang et al. (2019) employed a two-level curriculum schedulers, which consist of a dynamic sampling scheduler that adjusts the data distribution at each time step and balances the importance between the classification loss and the metric learning loss.",
            "score": 0.30668969488489767,
            "section_title": "Curriculum Learning",
            "char_start_offset": 10061,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1232
                },
                {
                    "start": 1235,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1931
                }
            ],
            "ref_mentions": [
                {
                    "start": 379,
                    "end": 398,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 875,
                    "end": 898,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 1087,
                    "end": 1110,
                    "matchedPaperCorpusId": "243865335"
                },
                {
                    "start": 1255,
                    "end": 1275,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1397,
                    "end": 1418,
                    "matchedPaperCorpusId": "204801162"
                },
                {
                    "start": 1418,
                    "end": 1457,
                    "matchedPaperCorpusId": "1363892"
                },
                {
                    "start": 1457,
                    "end": 1483,
                    "matchedPaperCorpusId": "879067"
                },
                {
                    "start": 1483,
                    "end": 1505,
                    "matchedPaperCorpusId": "243865335"
                },
                {
                    "start": 1505,
                    "end": 1527,
                    "matchedPaperCorpusId": "85498775"
                },
                {
                    "start": 1527,
                    "end": 1543,
                    "matchedPaperCorpusId": "1821165"
                },
                {
                    "start": 1543,
                    "end": 1559,
                    "matchedPaperCorpusId": "51606954"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4697265625
        },
        {
            "corpus_id": "268680561",
            "title": "Can Language Models Pretend Solvers? Logic Code Simulation with LLMs",
            "text": "Large Language Models (LLMs) are generative models based on the pre-trained Transformer architecture.Most LLMs utilize a generative model architecture, where given a sentence of n tokens, the model is trained to maximize the likelihood of the ground-truth token t i at the current time step t based on its preceding sequence t i\u22121 , ..., t 1 .The training of LLMs typically follows three main processes: unsupervised training on large amounts of unlabeled text data without explicit human annotations, supervised fine-tuning on labeled data relevant to specific tasks or domains, and reinforcement learning on feedback from human annotators or evaluators.Leveraging extensive multimodal data and employing pre-training and fine-tuning techniques, LLMs have demonstrated state-of-theart performance across various downstream tasks, such as machine translation, numerical reasoning, and code clone detection, with minimal examples (few-shot) or task-specific prompt instructions (zero-shot).",
            "score": 0.30666486659603487,
            "section_title": "A. Large Language Model",
            "char_start_offset": 5929,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 101,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 655
                },
                {
                    "start": 655,
                    "end": 989
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.284423828125
        },
        {
            "corpus_id": "267866729",
            "title": "The Convergence of AI/ML and DevSecOps: Revolutionizing Software Development",
            "text": "LLMs, or Large Language Models, represent a class of artificial intelligence systems specifically designed to comprehend and process natural language. These models are a subset of machine learning (ML) models that utilize deep artificial neural networks to analyze vast amounts of language data. Trained on extensive datasets comprising millions of sentences and words, LLMs undergo a training process that involves predicting the next word in a sentence based on the preceding word. This methodology enables the model to learn the intricate grammar and syntax of a given language, ultimately facilitating the production of natural-sounding sentences. Equipped with robust computational capabilities and a multitude of parameters, LLMs excel in discerning complex linguistic relationships and generating text that closely mimics human language. \n\nCurrent LLMs have achieved significant advancements in various natural language processing (NLP) tasks, including but not limited to machine translation, headline generation, question answering, and automatic text generation. They possess the ability to generate high-quality text that seamlessly aligns with the provided content and context. \n\nFor instance, when presented with an incomplete sentence like \"The book is on the,\" these models utilize training data to generate a probability distribution, thereby determining the most probable next word, such as \"table\" or \"bookshelf.\" Early efforts to construct large-scale language models relied on N-gram methods and basic smoothing techniques. However, more advanced methodologies leveraged various neural network architectures, such as feedforward networks and recurrent networks, for the language modeling task. This progression also spurred the development of word embeddings and related techniques aimed at mapping words to semantically meaningful representations. \n\nThe introduction of the Transformer architecture, initially devised for machine translation, sparked renewed interest in language models. This led to the emergence of contextualized word embeddings and Generative Pre-trained Transformers (GPTs). Notably, a common strategy to enhance model performance involves augmenting parameter size and training data, resulting in significant advancements in the machine's ability to process natural language. \n\nParameter-Efficient Fine-Tuning: \n\nIn October 2018, the BERT Large model was introduced, boasting 350 million parameters, making it the largest publicly disclosed Transformer model at that time.",
            "score": 0.30642198038493906,
            "section_title": "Fundamentals of Large Language Models (LLMs):",
            "char_start_offset": 9539,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1189
                },
                {
                    "start": 1192,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1868
                },
                {
                    "start": 1871,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2116
                },
                {
                    "start": 2117,
                    "end": 2318
                },
                {
                    "start": 2321,
                    "end": 2353
                },
                {
                    "start": 2356,
                    "end": 2515
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26025390625
        },
        {
            "corpus_id": "247972493",
            "title": "Can Ready-to-Use RNNs Generate \u201cGood\u201d Text Training Data?",
            "text": "Training data is crucial for machine learning tasks. In cases where there is little availability of training data due to limited time and resources, it becomes difficult for machines to be able to learn. \n\nIn the field of computer vision, there have been successes and benefits from using different methods to generate new training data; this is called data augmentation. This is done by creating new training data based on the original labeled training data. An example of data augmentation in image data is manipulating labeled training data of images through transformations such as rotation and cropping. This generates new training data images for the machine to learn from. However, in the field of natural language processing (NLP), data augmentation has been a challenging problem as there have been difficulties in establishing universal rules for transformations in textual data while maintaining the quality of the label [1]. For example, the sentence \"this is good\" is a statement with positive intent, but with a small transformation such as swapping the words to make it \"is this good,\" it becomes a question with no positive intent; the meaning completely changes. There are also various studies on identifying effective ways to transform labeled textual training [1]. \n\nAnother technique used to generate more training data is generative augmentation, where more new data is generated compared to switching or cropping images. There has been success in using GAN networks to generate more training data such as images of human faces [2]. \n\nGenerative models have also been explored in the NLP field with existing language models [1]. There is ongoing research in generative models for both computer vision and NLP. However, for an individual who is building a machine learning system with classification tasks, these state-of-the-art techniques for generating text data can be quite difficult to implement. \n\nThus, this paper's intended purpose is to help individuals who want a \"ready-to-use\" method to generate more textual training data, by assessing the effectiveness of some of these available methods. The types of individuals that this is focused on are, for example, an entry-level data scientist who is hired in a small team for a machine learning and NLP project, a freelancer who is working on a personal project or a computer science student who is building a chatbot as part of their degree.",
            "score": 0.30614863201676784,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 53,
                    "end": 203
                },
                {
                    "start": 206,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1283
                },
                {
                    "start": 1286,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1553
                },
                {
                    "start": 1556,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1922
                },
                {
                    "start": 1925,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2420
                }
            ],
            "ref_mentions": [
                {
                    "start": 1549,
                    "end": 1552,
                    "matchedPaperCorpusId": "195811894"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12249755859375
        },
        {
            "corpus_id": "278237268",
            "title": "100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models",
            "text": "Quantity and Diversity. Quantity and diversity are emphasized as key aspects in training reasoning language models suitable for multiple tasks. Skywork-OR1 (He et al., 2025a), Seed-Thinking-v1.5 (ByteDance-Seed, 2025), MiMo (Xiaomi LLM-Core Team, 2025) and Qwen3 series (Qwen-Team, 2025a) have all proposed that they leverage massive RL data from various domains during training. Open-Reasoner-Zero (Hu et al., 2025) leverages data synthesis and self-distillation to expand the training dataset. \n\nDifficulty. Several works introduce their data preparation pipeline to construct challenging datasets for RL training, providing inspirations on difficulty rating. Light-R1 (Wen et al., 2025a) and Skywork-OR1 (He et al., 2025a) conduct offline data selection that leverages a trained checkpoint to sample and verify responses for the query of each training sample, keeping only the samples with a moderate pass rate, and filtering out the samples with overly high or low pass rates, indicating that the corresponding queries are either too easy or too hard. DeepScaleR (Luo et al., 2025b) revealed that samples with an overly high pass rate are too easy for model training, while samples with a zero pass rate are often unverifiable or contain errors, therefore, both should be filtered out. \n\nOpen-Reasoner-Zero (Hu et al., 2025) further adopts this filtering strategy to construct training data from synthetic and distilled data that is more noisy. KodCode (Xu et al., 2025b) performs difficulty rating with an off-the-shelf LLM rather than a trained checkpoint of their own model. LIMR (Li et al., 2025b) proposes the learning impact measurement to filter samples whose learning patterns complement the model's overall performance trajectory, proving that these samples tend to be more valuable for training. \n\nData Cleaning. As a fundamental step during data preparation, data cleaning is crucial to construct less noisy datasets for effective training. Especially, for RL training on reasoning tasks, several works emphasize the necessity to filter out unsolvable questions and unverifiable answers.",
            "score": 0.306146060209543,
            "section_title": "RECIPES OF TRAINING DATA",
            "char_start_offset": 48956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 495
                },
                {
                    "start": 498,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1289
                },
                {
                    "start": 1292,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1809
                },
                {
                    "start": 1812,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2102
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.307373046875
        },
        {
            "corpus_id": "257038100",
            "title": "Boosting classification reliability of NLP transformer models in the long run",
            "text": "Using a large domain-specific corpus and pre-training the model with it is often difficult and expensive. Another approach is to pre-train the model using the training classification dataset, which is often a much smaller corpus (Araci, 2019). \n\nIn addition to the difficulties of applying a language model to an OOD dataset (a domain distinct from the materials used to train the model), the performance of trained language models may also degrade over time. Measuring the robustness of such a model over time has recently attracted considerable research attention. Measuring the performance of transfer models is challenging because transformer models tend to overestimate their performance when the training and evaluation periods overlap (Lazaridou et al., 2021). The literature suggests temporal misalignment degrades performance when the testing period is temporally distant from the training period (Luu et al., 2022;R\u00f6ttger and Pierrehumbert, 2021). \n\nPrevious work also examined the changes in the language itself over time. The pace of language change has become increasingly rapid in the context of social media (Goel et al., 2016). \n\nLinguistic shifts can also affect the performance of classification tasks based on machine learning. Both changes in word usage and altering the context of the same words can lead to performance degradation (Huang and Paul, 2019). Temporal shifts have been studied in the context of several NLP tasks, including domain classification (Huang and Paul, 2018;Agarwal & Nenkova, 2022), named entity recognition (NER) (Rijhwani and Preotiuc-Pietro, 2020;Chen et al., 2021), and sentiment analysis (Lukes and S\u00f8gaard, 2018). Florio et al. (2020) evaluated the temporal robustness of an Italian version of the model BERT and found that the model's efficiency is highly time-dependent. Although the Italian BERT model was trained on social media data and it was fine-tuned on a labeled dataset on hate speech, the content of the texts used for testing changed rapidly. When the temporal distance between the training set and the test set increased, the model's precision and F1 values decreased significantly.",
            "score": 0.3060708898175699,
            "section_title": "Introduction",
            "char_start_offset": 4190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 243
                },
                {
                    "start": 246,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1143
                },
                {
                    "start": 1146,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2147
                }
            ],
            "ref_mentions": [
                {
                    "start": 742,
                    "end": 766,
                    "matchedPaperCorpusId": "239886013"
                },
                {
                    "start": 1123,
                    "end": 1142,
                    "matchedPaperCorpusId": "16847913"
                },
                {
                    "start": 1353,
                    "end": 1375,
                    "matchedPaperCorpusId": "196194576"
                },
                {
                    "start": 1480,
                    "end": 1502,
                    "matchedPaperCorpusId": "21695934"
                },
                {
                    "start": 1502,
                    "end": 1526,
                    "matchedPaperCorpusId": "244709020"
                },
                {
                    "start": 1638,
                    "end": 1663,
                    "matchedPaperCorpusId": "53246679"
                },
                {
                    "start": 1665,
                    "end": 1685,
                    "matchedPaperCorpusId": "225684248"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09417724609375
        },
        {
            "corpus_id": "269756933",
            "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
            "text": "A high loss signifies a substantial discrepancy between the predicted and actual outcomes, indicating that the model perceives the data as challenging.\n\nWe establish a direct correlation between high loss values and increased difficulty.Therefore, prompts that generate higher loss are considered more challenging for the model to learn.This method computes the cross-entropy loss associated with the answer segment of the prompt.",
            "score": 0.3058360598962842,
            "section_title": "Loss",
            "char_start_offset": 8187,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 153,
                    "end": 237
                },
                {
                    "start": 237,
                    "end": 337
                },
                {
                    "start": 337,
                    "end": 430
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83935546875
        },
        {
            "corpus_id": "270285465",
            "title": "ELFS: Label-Free Coreset Selection with Proxy Training Dynamics",
            "text": "The data difficulty score of an example measures how difficult a model learns this example. \n\nFor instance, the forgetting score (Toneva et al., 2018) quantifies how often a sample, once correctly classified, is subsequently misclassified during training. EL2N (Paul et al., 2021) quantifies data difficulty by calculating L2 training loss of an example in the first several epochs. Besides, the area under the margin (AUM) (Pleiss et al., 2020)",
            "score": 0.3058360598962842,
            "section_title": "TRAINING DYNAMIC-BASED DATA DIFFICULTY SCORES",
            "char_start_offset": 9856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 94,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 445
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 150,
                    "matchedPaperCorpusId": "55481903"
                },
                {
                    "start": 261,
                    "end": 280,
                    "matchedPaperCorpusId": "235898952"
                },
                {
                    "start": 424,
                    "end": 445,
                    "matchedPaperCorpusId": "210932316"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5380859375
        },
        {
            "corpus_id": "263622295",
            "title": "A method to assess trustworthiness of machine coding at scale",
            "text": "Education researchers are interested in using the tools of machine learning and natural language processing to make quantitative claims from natural language and text data, such as open-ended responses to survey questions. The aspiration is that this form of machine coding may be more efficient and consistent than human coding, allowing much larger and broader data sets to be analyzed than is practical with human coders. Existing work that uses these tools, however, does not address methods to make trustworthy quantitative claims without cross-checking with human coding, which erases the improved efficiency of using these automated tools. Here we propose a four-part method for making such claims: evaluating a trained model, calculating statistical uncertainty, calculating systematic uncertainty from the trained algorithm, and calculating systematic uncertainty from novel data sources. We provide evidence for this method using data from two distinct short response survey questions with two distinct coding schemes. We also provide a real-world example of using these practices to machine code a data set unseen by human coders.",
            "score": 0.3058360598962842,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1370849609375
        },
        {
            "corpus_id": "267740312",
            "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
            "text": "In this paper, we introduce a learning percentagebased metric for assessing the difficulty of samples. We demonstrate that language models ranging from 1B to 13B sizes can self-select high-quality training data by employing this metric. Additionally, we empirically validate the transferability of data hardness across different model sizes, showcasing the efficient curation of high-quality training data by smaller models. Furthermore, we propose an optimized version of the metric that offers increased speed while maintaining equal efficacy. In future, we aim to explore methods for automatically transforming easy training samples into more challenging ones. \n\nOur examination reveals prevalence of noisy samples within the LP(1) and LP app (1) subsets of data. The detection and mitigation of noisy samples are imperative to mitigate their influence on the dataset. We leave this for future work.",
            "score": 0.3058360598962842,
            "section_title": "Conclusion",
            "char_start_offset": 22528,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 902
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.861328125
        },
        {
            "corpus_id": "271962861",
            "title": "CL4KGE: A Curriculum Learning Method for Knowledge Graph Embedding",
            "text": "After the difficulty measure module which measures the difficulty of each sample in training set T train , we demonstrate a curriculum learning framework to train a better KGE method.",
            "score": 0.3058360598962842,
            "section_title": "Training scheduler",
            "char_start_offset": 17213,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2496337890625
        },
        {
            "corpus_id": "263334839",
            "title": "Investigating the Performance of Data Complexity & Instance Hardness Measures as A Meta-Feature in Overlapping Classes Problem",
            "text": "According to their study outcomes, this meta-knowledge can be used as a descriptor for characterising a data set's hardness profile and provided an insight into the leading causes of the difficulties they represent in the dataset. Furthermore, a comprehensive survey done by [7] aims to review the applications of both Complexity Measures and Instance Hardness Measures in different areas, including the meta-learning field. Moreover, to overcome the limitation of the global Complexity Measures in characterising the overall complexity of the entire dataset, [12] have decomposed some of the global measures into instance/ local-level measures, so the analysis is conducted based on the individual contribution of each instance instead of global complexity of the entire dataset from the class prospective. Then, they compared the performance of the proposed decomposed instance/local-level measures against the global equivalents and concluded that the former provided better performance than the latter. \n\nDespite the advances shown in the recent work, an empirical comprehensive review of the ability of these measures (from both global and local perspectives) to give an estimation of the difficulty of a given data problem independent of the learning algorithm has not yet been undertaken. Most of the works undertaken in the literature are limited to examining the correlation between the values of the measures with the learning algorithms' prediction accuracy performance. From our study point of view, examining this correlation is not a good independent indicator to validate the complexity measure performance in estimating the actual data difficulty nor for showing the causes of the poor prediction of the learning algorithm's performance. Thus, in this work, the analysis of learning algorithm performance will be omitted from this study.",
            "score": 0.3058360598962842,
            "section_title": "RELATED WORK",
            "char_start_offset": 11609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1853
                }
            ],
            "ref_mentions": [
                {
                    "start": 560,
                    "end": 564,
                    "matchedPaperCorpusId": "224817007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5869140625
        },
        {
            "corpus_id": "246996522",
            "title": "Curriculum Optimization for Low-Resource Speech Recognition",
            "text": "Curriculum learning yields decreased WER in 4 low-resource languages. Our approach has shown that the derived curriculum based on the student's learning progress improves performance and that the appropriate choice of complexity metric serves as a good prior for the model. Although text-based difficulty measures improve over the baseline model, CR outper-forms them in several instances. We believe that the performance of curriculum learning is dependent on the complexity metric distribution and can be further optimized by selecting other values of K. We leave this study for the future work.",
            "score": 0.3058360598962842,
            "section_title": "CONCLUSION",
            "char_start_offset": 13675,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 70,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 597
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.365234375
        },
        {
            "corpus_id": "263334839",
            "title": "Investigating the Performance of Data Complexity & Instance Hardness Measures as A Meta-Feature in Overlapping Classes Problem",
            "text": "The aim of this study is to investigate the performance of the measures under different degrees of noise which correspond to increasing levels of overlapping classes; the results indicate that most measures can correctly describe the underlying relationship of the relevant features in Level One. However, in Levels Two and Three, when the degree of noise increases, some measures have overestimated the complexity while a few have underestimated it, as shown in Table 5 which gives a summary of the performance of the metrics, identifying their level of performance across the three Levels of problem difficulty. In contrast, all Separability of Classes Measures, and Instance-Hardness Measures (apart from DS) show good performance in estimating the actual data challenges aligned with the difficulty levels.",
            "score": 0.3058360598962842,
            "section_title": "Instance Hardness Measures. As mentioned earlier, Instance",
            "char_start_offset": 29855,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 810
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.310302734375
        },
        {
            "corpus_id": "258352812",
            "title": "DataComp: In search of the next generation of multimodal datasets",
            "text": ", developers write programmatic labeling functions to automatically label a large amount of unlabeled data. The labeling functions could produce noisy and conflicting labels, so researchers have developed methods to aggregate noisy votes to produce the final training labels [114,47,129].\n\nPrevious literature also studies methods for training data attribution, which seek to link a model's behavior (e.g., its accuracy on a particular task or subset of data) to particular subsets of its training data. Such methods include influence functions, a classic technique from robust statistics [56,35] that uses a second-order Taylor expansion to approximate the effect of removing a training point on the learned model parameters [79,80,57,52], as well as methods that fit attribution functions directly to the dynamics of repeated training runs [49,106,69,55]. Training data attribution methods assume that we have already trained a model, though they can be subsequently used to refine the training data (e.g., by identifying potentially mislabeled training points [79]). Our focus in this paper is instead on data curation methods-that is, methods for selecting a subset of the training data to train a model in the first place.\n\nIn the context of natural language processing, Swayamdipta et al. [133] proposes a tool for characterizing samples in a dataset based on training dynamics, labelling instances as ambiguous, easy to learn or hard to learn. Previous literature such as work by Le Bras et al. [86], Li & Vasconcelos [89], Gururangan et al. [54] advocate for removing easy instances from the training data. Ethayarajh et al. [41] propose a measure of how difficult a dataset is to learn, V-usable information. Such techniques could be promising directions of further exploration in the context of our benchmark.\n\nFinally, another related line of work is studying scaling trends. In addition to Sorscher et al. [130], researchers have investigated how model performance changes as a function of compute budget, model size, and number of training samples [77,65,21,28]. However, this line of work does not consider how dataset design may affects scaling trends. Beyond dataset size, we measure",
            "score": 0.3058360598962842,
            "section_title": "C Additional related work",
            "char_start_offset": 28673,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 275,
                    "end": 280,
                    "matchedPaperCorpusId": "52083994"
                },
                {
                    "start": 280,
                    "end": 283,
                    "matchedPaperCorpusId": "211532351"
                },
                {
                    "start": 283,
                    "end": 287,
                    "matchedPaperCorpusId": "244920924"
                },
                {
                    "start": 589,
                    "end": 593,
                    "matchedPaperCorpusId": "119361583"
                },
                {
                    "start": 593,
                    "end": 596,
                    "matchedPaperCorpusId": "2556544"
                },
                {
                    "start": 726,
                    "end": 730,
                    "matchedPaperCorpusId": "13193974"
                },
                {
                    "start": 730,
                    "end": 733,
                    "matchedPaperCorpusId": "173188850"
                },
                {
                    "start": 842,
                    "end": 846,
                    "matchedPaperCorpusId": "102350503"
                },
                {
                    "start": 1063,
                    "end": 1067,
                    "matchedPaperCorpusId": "13193974"
                },
                {
                    "start": 1295,
                    "end": 1300,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 1502,
                    "end": 1506,
                    "matchedPaperCorpusId": "211076210"
                },
                {
                    "start": 1525,
                    "end": 1529,
                    "matchedPaperCorpusId": "119314167"
                },
                {
                    "start": 1549,
                    "end": 1553,
                    "matchedPaperCorpusId": "4537113"
                },
                {
                    "start": 1633,
                    "end": 1637,
                    "matchedPaperCorpusId": "250340652"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.355224609375
        },
        {
            "corpus_id": "270764411",
            "title": "DataGen: Unified Synthetic Dataset Generation via Large Language Models",
            "text": "Each dataset uniquely contributes to language model assessment, covering dimensions from mathematical problemsolving and factual accuracy verification to extensive language understanding and commonsense reasoning. We show the details of these four datasets in Appendix B. For dataset generation, we utilize GPT-4 [1], Claude3-Opus [2], and Llama3-70b [3], as these LLMs are among the most robust available, exhibiting exceptional ability to follow instructions. For benchmarking, our study utilizes eight cutting-edge and popular models from notable entities in the AI domain (the details are shown in Appendix B.), reflecting a mix of open-source and proprietary technologies. The number of generated data items and more details are shown in Appendix D. Note that difficulty enhancement is not applied to the generated data for benchmarking. We will discuss the effectiveness of difficult enhancement in subsection 4.3.",
            "score": 0.3058360598962842,
            "section_title": "Post-Processing",
            "char_start_offset": 17238,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 920
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.134521484375
        },
        {
            "corpus_id": "266741845",
            "title": "Dataset Difficulty and the Role of Inductive Bias",
            "text": "We provide a brief overview of difficulty scores, focusing on their conceptual relations. Since the variance of a score depends strongly on sample size, we categorize scores by their sampling method: \n\n\u2022 Scores computed at a single point in training. This category includes typical performance metrics computed at the end of training, such as accuracy. Since large models effectively have zero training loss, more sophisticated scores are needed to differentiate examples, including prediction depth (Baldock et al., 2021), adversarial robustness (Carlini et al., 2019), and normalized margin from decision boundary (Jiang et al., 2019b). Scores can also be computed during training for applications such as data pruning, including the L 2 norm of the gradient with respect to model parameters, or the norm of the output error (Paul et al., 2021). Although scores in this category can also be averaged over training, they are typically evaluated at specific times for a specific purpose such as model evaluation. (Toneva et al., 2019). Simple pointwise evaluations which can be averaged over all samples include loss (Jiang et al., 2020), accuracy (Jiang et al., 2020), entropy (Jiang et al., 2020), model confidence (Jiang et al., 2020;Carlini et al., 2019;Swayamdipta et al., 2020), and the classification probability margin (Toneva et al., 2019;Jiang et al., 2020;Pleiss et al., 2020). Derived quantities include the variance of the above quantities over training (Swayamdipta et al., 2020), the number of forgetting events (Toneva et al., 2019), the time when an example is learned (Toneva et al., 2019) or remains consistently learned (Baldock et al., 2021;Siddiqui et al., 2022), and the use of entire loss trajectories as features for a downstream classifier (Siddiqui et al., 2022). Gradient-based methods include the variance of gradients (VoG) at the input (Hooker et al., 2020). \n\n\u2022 Scores computed from an ensemble of models.",
            "score": 0.3058360598962842,
            "section_title": "Difficulty Scores and Related Work",
            "char_start_offset": 11999,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 199
                },
                {
                    "start": 202,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1889
                },
                {
                    "start": 1892,
                    "end": 1937
                }
            ],
            "ref_mentions": [
                {
                    "start": 500,
                    "end": 522,
                    "matchedPaperCorpusId": "235458180"
                },
                {
                    "start": 616,
                    "end": 637,
                    "matchedPaperCorpusId": "52902256"
                },
                {
                    "start": 827,
                    "end": 846,
                    "matchedPaperCorpusId": "235898952"
                },
                {
                    "start": 1013,
                    "end": 1034,
                    "matchedPaperCorpusId": "55481903"
                },
                {
                    "start": 1258,
                    "end": 1283,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 1327,
                    "end": 1348,
                    "matchedPaperCorpusId": "55481903"
                },
                {
                    "start": 1367,
                    "end": 1387,
                    "matchedPaperCorpusId": "210932316"
                },
                {
                    "start": 1467,
                    "end": 1493,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 1527,
                    "end": 1548,
                    "matchedPaperCorpusId": "55481903"
                },
                {
                    "start": 1586,
                    "end": 1607,
                    "matchedPaperCorpusId": "55481903"
                },
                {
                    "start": 1640,
                    "end": 1662,
                    "matchedPaperCorpusId": "235458180"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.544921875
        },
        {
            "corpus_id": "277667729",
            "title": "2D-Curri-DPO: Two-Dimensional Curriculum Learning for Direct Preference Optimization",
            "text": "This initial phase prepares the preference dataset D = {(x i , y w,i , y l,i )} N i=1 by quantifying difficulty along two dimensions and structuring the data into an ordered curriculum based on these dimensions. A cornerstone of any curriculum learning system is a meaningful measure of difficulty [20]. In the context of preference-based LLM alignment, we posit that difficulty arises from both input processing and output evaluation. We therefore propose metrics to capture these distinct challenges. \n\n1) Prompt Complexity (PC): Quantifying Input Challenge: Accurately gauging the intrinsic difficulty a prompt x presents to an LLM is non-trivial yet crucial for effective curriculum design. We estimate Prompt Complexity (PC) using the Single-Model Perplexity Fluctuation approach. This method, inspired by the observation that models exhibit greater output variability on challenging tasks [32], measures the consistency of responses generated by a single reference policy (\u03c0 ref , typically the SFT model). \n\nSpecifically, for a given prompt x, we generate N diverse candidate responses {y (i) } N i=1 using the reference policy y (i) \u223c \u03c0 ref (\u2022|x). Let L i be the length (number of tokens) of response y (i) = (y i,1 , . . . , y i,Li ). We then assess the perplexity (PPL) of each response under a fixed, external language model p LM , which provides a stable measure of linguistic quality: \n\nlog p LM (y i,t |x, y i,<t ) \n\n(1) Here, PPL p LM (y (i) |x) denotes the perplexity of the i-th response y (i) given prompt x, calculated using the external model p LM . y i,t is the t-th token of the i-th response, and y i,<t represents the preceding tokens in that response. \n\nThe Prompt Complexity, denoted as PC(x) (we drop the PPL subscript for brevity as it's the only PC method detailed here), is defined as the standard deviation of these perplexity scores across the N samples. This captures the variability in the quality of generated responses:",
            "score": 0.3058360598962842,
            "section_title": "A. Curriculum Construction",
            "char_start_offset": 10595,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 1012
                },
                {
                    "start": 1015,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1397
                },
                {
                    "start": 1400,
                    "end": 1428
                },
                {
                    "start": 1431,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1676
                },
                {
                    "start": 1679,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 1955
                }
            ],
            "ref_mentions": [
                {
                    "start": 298,
                    "end": 302,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 895,
                    "end": 899,
                    "matchedPaperCorpusId": "248810776"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33544921875
        },
        {
            "corpus_id": "276938222",
            "title": "Token Weighting for Long-Range Language Modeling",
            "text": "Different weighted cross-entropy criteria have been proposed in the literature. For example, focal loss weighs each class by its confidence, i.e. the predicted class probability (Lin et al., 2017). Related to this, MiLe (Su et al., 2024b) weighs tokens in language modeling according to one minus the entropy of the token-level model distribution. As entropy is a measurement of uncertainty, this weighs uncertain tokens higher, but in doing so runs the risk of excessively favoring tokens that are simply hard to learn or ambiguous, for example, due to plausible variation (Baan et al., 2024, inter alia). \n\nEmpirically, it is also observed that such tokens remain challenging throughout training and Lin et al. (2024)   training the target model \u03b8 on noisy D, the checkpoint \u03b8 0 is then used to filter out tokens whose weights w i are set to 0. Formally, the weights are determined by \n\nwhere we will use the shorthand p \u03b8 (i) := p \u03b8 (y i |y <i ) for notational simplicity from now on. We call this stage token scoring. It can be shown theoretically that this form of token scoring approximately selects optimal examples for reducing test loss (Mindermann et al., 2022). Note that this requires training a second model which can be prohibitively expensive. A final stage is then postprocessing which is applied after token scoring. In \u03c1, weights are set to 0 or 1 /\u03ba according to whether they score low or high such that a quantile 1 \u2212 \u03ba \u2208 (0, 1] is set to 0, where \u03ba is the sparsity ratio. This quantile-based postprocessing is naturally robust to outliers produced by the scoring function. Yet, sparsification can also have drawbacks, since part of the signal is ignored. Furthermore, it is not clear whether the choice of \u03ba will be robust across datasets and even sequences in the same dataset, which might exhibit different signal-to-noise ratios. In the following, we will explore such tokenweighting methods thoroughly with the focus of long-context understanding and propose new ones that could overcome the mentioned issues.",
            "score": 0.3058360598962842,
            "section_title": "Token Weighting",
            "char_start_offset": 9557,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 886
                },
                {
                    "start": 889,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 2034
                }
            ],
            "ref_mentions": [
                {
                    "start": 178,
                    "end": 196,
                    "matchedPaperCorpusId": "47252984"
                },
                {
                    "start": 702,
                    "end": 719,
                    "matchedPaperCorpusId": "276184891"
                },
                {
                    "start": 1146,
                    "end": 1171,
                    "matchedPaperCorpusId": "235743024"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.306640625
        },
        {
            "corpus_id": "270122967",
            "title": "Large Language Models Can Self-Improve At Web Agent Tasks",
            "text": "Large language models (LLMs) have demonstrated impressive capabilities in a variety of natural language processing (NLP) tasks such as summarization and question answering (Radford et al., 2019;Raffel et al., 2020;Brown et al., 2020) through zero-shot and few-shot prompting techniques (Ouyang et al., 2022;Wei et al., 2021). However, prompting techniques alone are insufficient to enable LLMs to act as agents and navigate environments in order to solve complex, multi-step, longhorizon tasks (Yao et al., 2023). Fine-tuning LLMs to perform such tasks is also infeasible due to the scarcity of training data suitable for these tasks. Acquiring data for sequential decision-making and complex interactions is not only time-consuming, but also costly. Additionally, automatic evaluation of trajectories (or sequences of actions) taken by an agent is also difficult (Dinu et al., 2024). The absence of metrics that accurately capture the efficacy of each step in a sequence complicates the assessment of incremental improvements or degradations in an agent's performance. \n\nA number of proposed self-improvement techniques have demonstrated that LLMs can use zeroshot and few-shot prompting to achieve performance above the baseline without any additional supervised training data (Huang et al., 2022;Chen et al., 2024). In place of supervised data as a learning signal, many of these techniques use a self-critique technique (Weng et al., 2022;Yuan et al., 2024), or obtain a critique through interactions with tools or environments (Gou et al., 2024). While self-improvement techniques have shown promise on standard NLP benchmark tasks like machine translation or question answering (Han et al., 2021;Huang et al., 2022;Chen et al., 2024), their efficacy has not yet been thoroughly investigated for long-horizon tasks that require multi-step interactions with a complex and realistic environment. \n\nWebArena (Zhou et al., 2023) is a recently proposed benchmark wherein an LLM agent is required to solve tasks using a web browser. One example WebArena task is to use the OpenStreetMap",
            "score": 0.3057536447279058,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1069
                },
                {
                    "start": 1072,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1898
                },
                {
                    "start": 1901,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2085
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 214,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 214,
                    "end": 233,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 286,
                    "end": 307,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1443,
                    "end": 1461,
                    "matchedPaperCorpusId": "267035293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1885986328125
        },
        {
            "corpus_id": "246652398",
            "title": "Transferable Student Performance Modeling for Intelligent Tutoring Systems",
            "text": "(ii) In the inductive transfer setting where small-scale target course data is available, we tune pre-trained course-agnostic performance models to the target course by learning target course-specific question and knowledge component (KC) difficulty parameters. This inductive transfer setting mimics the case where the course designer can run a pilot with a small number of students before large-scale deployment. \n\nWe evaluate the proposed techniques using student log data collected from five different mathematics courses describing learning trajectories from over 47,000 students in a real world large-scale ITS. In both settings, the proposed transfer learning techniques mitigate the cold-start problem for all courses successfully. We hope that transfer learning techniques will become a standard tool for ITS course designers and improve the learning experience of early students. To summarize, the key contributions of this paper include: \n\n\u2022 Course-agnostic student performance modeling. We present the first course-agnostic modeling techniques for predicting student performance on future questions in newly introduced courses where no previous students have yet taken this target course. Even though our agnostic models have no access to training data logs of students taking the new target course, they exhibit predictive performance comparable to conventional BKT and PFA student performance models -found in many real world ITSs -which were trained on data from thousands of students taking the new target course. Our course-agnostic models can enable effective personalized learning experiences even when introducing a new course for which no prior student log data is available. \u2022 Inductive transfer learning for effective tuning. We use transfer learning techniques to efficiently adapt our pre-trained course-agnostic performance models to individual target courses by learning question-and KC-specific parameters. \n\nOur experiments show how our approach leads to more accurate performance predictions than conventional modeling techniques in settings in which only limited student log data from the target course is available (< 100 students). \u2022 Guidance for practice. By analyzing data from five different courses offered by a large-scale ITS this work provides various insights which can inform the design of future ITSs. \n\nAmong others, our experiments show how manually assigned difficulty ratings and information about different learning contexts provided by domain experts during content creation can be used to boost the prediction accuracy of course-agnostic models.",
            "score": 0.30570859065357525,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2437,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 414
                },
                {
                    "start": 417,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 948
                },
                {
                    "start": 951,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1934
                },
                {
                    "start": 1937,
                    "end": 2164
                },
                {
                    "start": 2165,
                    "end": 2189
                },
                {
                    "start": 2190,
                    "end": 2344
                },
                {
                    "start": 2347,
                    "end": 2595
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1842041015625
        },
        {
            "corpus_id": "258332051",
            "title": "ESimCSE Unsupervised Contrastive Learning Jointly with UDA Semi-Supervised Learning for Large Label System Text Classification Mode",
            "text": "The challenges faced by text classification with large tag systems in natural language processing tasks include multiple tag systems, uneven data distribution, and high noise. To address these problems, the ESimCSE unsupervised comparative learning and UDA semi-supervised comparative learning models are combined through the use of joint training techniques in the models.The ESimCSE model efficiently learns text vector representations using unlabeled data to achieve better classification results, while UDA is trained using unlabeled data through semi-supervised learning methods to improve the prediction performance of the models and stability, and further improve the generalization ability of the model. In addition, adversarial training techniques FGM and PGD are used in the model training process to improve the robustness and reliability of the model. The experimental results show that there is an 8% and 10% accuracy improvement relative to Baseline on the public dataset Ruesters as well as on the operational dataset, respectively, and a 15% improvement in manual validation accuracy can be achieved on the operational dataset, indicating that the method is effective.",
            "score": 0.3056520463056231,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.19482421875
        },
        {
            "corpus_id": "250492260",
            "title": "A Transfer Learning Based Model for Text Readability Assessment in German",
            "text": "In a recent effort for classifying German text into different categories, based on text difficulty for language learners, Sz\u00fcgyi et al. developed a classifier based on linguistic features extracted from the texts [20]. They extracted similar feature as [24], and also error measures and N-grams from input text. To train and evaluate the proposed classification model, they combined five different German readability assessment sources. Their results show that morphological features plays the most important role in accurately classifying the input text into the corresponding difficulty level. Moreover, some of the syntactic and lexical features were also given a high weight by the used ML algorithm. \n\nWeiss et al. proposed another learning-based model for complexity modeling for text readability across language [22]. They trained different ML models on extracted features from German and English texts. They extracted 312 features includes surface length, syntactic complexity, lexical complexity, morphological complexity and so forth. Feature extraction and selection leads to decreasing the number of features to 301 for model training. Among the ML models that have been trained on the data, SVM could outperform the others and achieve the best results. They achieved the F1 scores of 94.0, 89.6, and 92.6 for elementary, intermediate, and advanced texts, respectively. \n\nTo the best of our knowledge, the proposed model in this paper is the first German model based on transferring knowledge from pre-trained language models to the specific domain of readability evaluation. In the both proposed models we skipped the time-consuming and resource-intensive process of feature engineering.",
            "score": 0.30557546848827455,
            "section_title": "Related Work",
            "char_start_offset": 5120,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 704
                },
                {
                    "start": 707,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1381
                },
                {
                    "start": 1384,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1700
                }
            ],
            "ref_mentions": [
                {
                    "start": 213,
                    "end": 217,
                    "matchedPaperCorpusId": "208334765"
                },
                {
                    "start": 253,
                    "end": 257,
                    "matchedPaperCorpusId": "5617603"
                },
                {
                    "start": 819,
                    "end": 823,
                    "matchedPaperCorpusId": "235258287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.138427734375
        },
        {
            "corpus_id": "257937538",
            "title": "A Review of the Evaluation System for Curriculum Learning",
            "text": "In addition to outputting sample difficulty scores through models and algorithms, there are also methods to maximize reward [99], maximize learning progress [131], mine difficult samples online [132], and perform direct computation [23,103] for specific evaluation methods. This type of evaluator has the same goal as the aforementioned difficulty evaluator: to design a sample learning order that helps model learning. However, the former is based on human intuitive prior knowledge or an easy-to-hard order directly related to the model, while the latter is a certain order designed for a specific task and does not follow an easy-to-hard order. This type of evaluator, represented by the field of reinforcement learning, uses data selection as the action and model feedback as the state and reward, and dynamically selects sub-tasks for training based on model feedback, with the goal of finding a series of optimal strategies that use the knowledge quickly gained in simple tasks to reduce exploration of more complex tasks [100,101], allowing model performance to be maximized [98]. For example, the process of learning a sequence of edge types is formalized as a Markov decision process in node representation learning for heterogeneous star networks, where the appropriate types of edges are selected for node representation learning by cumulative rewards maximization [99]. The training sequence learns meaningful different types of edges to improve representation learning.",
            "score": 0.30557417728180114,
            "section_title": "Others",
            "char_start_offset": 55300,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1482
                }
            ],
            "ref_mentions": [
                {
                    "start": 124,
                    "end": 128,
                    "matchedPaperCorpusId": "11009476"
                },
                {
                    "start": 157,
                    "end": 162,
                    "matchedPaperCorpusId": "11137059"
                },
                {
                    "start": 194,
                    "end": 199,
                    "matchedPaperCorpusId": "209050760"
                },
                {
                    "start": 232,
                    "end": 236,
                    "matchedPaperCorpusId": "58981386"
                },
                {
                    "start": 236,
                    "end": 240,
                    "matchedPaperCorpusId": "44130298"
                },
                {
                    "start": 1028,
                    "end": 1033,
                    "matchedPaperCorpusId": "212657666"
                },
                {
                    "start": 1033,
                    "end": 1037,
                    "matchedPaperCorpusId": "240260448"
                },
                {
                    "start": 1082,
                    "end": 1086,
                    "matchedPaperCorpusId": "211102338"
                },
                {
                    "start": 1376,
                    "end": 1380,
                    "matchedPaperCorpusId": "11009476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2010498046875
        },
        {
            "corpus_id": "253244504",
            "title": "A Close Look into the Calibration of Pre-trained Language Models",
            "text": "Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs\u2019 calibration performance in training. We consider six factors as control variables, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. We observe a consistent change in calibration performance across six factors. We find that PLMs don\u2019t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not. We highlight that our finding somewhat contradicts two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining improves model calibration. Next, we study the effectiveness of existing calibration methods in mitigating the overconfidence issue. Besides unlearnable calibration methods (e.g., label smoothing), we adapt and extend two recently proposed learnable methods that directly collect data to train models to have reasonable confidence estimations. Experimental results show that learnable methods significantly reduce PLMs\u2019 confidence in wrong predictions.",
            "score": 0.30549800293826396,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57666015625
        },
        {
            "corpus_id": "272690054",
            "title": "Active Learning to Guide Labeling Efforts for Question Difficulty Estimation",
            "text": "This work explores AL for QDE, a supervised approach aiming to minimize the labeling effort for human annotators while matching the performance of state-of-the-art models. By using a human-in-the-loop method, it bridges the gap between the performance-driven supervised domain and the data-centric unsupervised domain. Additionally, we introduce a novel acquisition function PowerVariance, which leverages epistemic uncertainty from unlabeled samples obtained through MC dropout to identify the most informative data points. Unlike conventional Variance acquisition, PowerVariance is designed to limit redundant information in a batch of samples. \n\nExperimental results indicate that the proposed PowerVariance acquisition outperforms both Uniform and Variance acquisition. It effectively selects observations from the minority difficulty level 2 for labeling and does not neglect level 0 questions, an issue observed with Variance acquisition. We see no reason for practitioners to consider the flawed top-K Variance acquisition. Even with only 10% of the training data labeled, AL with PowerVariance achieves good performance, only 5% higher discrete RMSE than the model trained on 100% of the training data. This methodology promotes the responsible use of educational resources by significantly reducing the labeling work for educational professionals while maintaining predictive performance. Consequently, it makes QDE tools more accessible to course instructors who might otherwise be demotivated by the large number of calibrated questions required. \n\nThe study is potentially limited by the small number of coarse difficulty levels. Course instructors are often reluctant to share exam questions, making it challenging to find datasets with more realistic difficulty levels. Future research can explore more fine-grained settings with more closely spaced difficulty levels. The inability to use public datasets highlights the relevance of active learning strategies for course instructors when labeling exam questions. Furthermore, adding difficulty levels may introduce class imbalance, a scenario where PowerVariance performs strongly. \n\nThe proposed AL approach holds promise for diverse applications such as personalized support tools, essay correction tools, and question-answering systems. It can easily be adapted to alternative pre-trained language models and datasets, as MC dropout works on any architecture that uses dropout. For models not employing dropout, ensembles of NNs can provide epistemic uncertainty, enabling similar AL strategies.",
            "score": 0.3050278130736491,
            "section_title": "Conclusion",
            "char_start_offset": 21866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 646
                },
                {
                    "start": 649,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1557
                },
                {
                    "start": 1560,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2146
                },
                {
                    "start": 2149,
                    "end": 2304
                },
                {
                    "start": 2305,
                    "end": 2445
                },
                {
                    "start": 2446,
                    "end": 2563
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.341796875
        },
        {
            "corpus_id": "247292373",
            "title": "ILDAE: Instance-Level Difficulty Analysis of Evaluation Data",
            "text": "Performance Metric: We measure the efficacy of an instance selection technique by computing accuracies of candidate models on the selected instances and calculating their Kendall's correlation (Kendall, 1938) with accuracies obtained on the full evaluation dataset. High correlation implies that the performance scores obtained using the selected instances display the same behavior as the performance scores obtained using the complete dataset. Hence, high correlations values are preferred. Figure 3: Demonstrating difficulty score generalization. Difficulty scores computed using RoBERTa-large show negative correlation with accuracy averaged over 27 other models, hence satisfying the desiderata mentioned in Section 2.1. Note that we depict this trend for a few datasets only to avoid cluttering the image. Similar trend is observed for other dataset also 2 .\n\nDatasets: We experiment with a total of 23 datasets across Natural Language Inference, Duplicate Detection, Sentiment Analysis, Question Answering, Commonsense Reasoning, and several other tasks. Refer to Appendix section B for an exhaustive list of datasets for each task.  (Iandola et al., 2020), ELECTRA (Clark et al., 2020) in our experiments. We also use different variants of ConvBert (small, mediumsmall, base) and ELECTRA (small, base) models. For comprehensive experiments, we train each of the above models with training data of three different sizes (2k, 5k, and 10k examples) resulting in 27 candidate models for each dataset. We intentionally exclude RoBERTa from this list as we use it for computing the difficulty scores.\n\nInstance Selection Baselines: We compare the proposed instance selection approach with the following baselines:\n\nRandom Selection: Select a random subset of instances from the evaluation dataset.\n\nHeuristic Selection: Select instances based on the length heuristic (number of characters in the instance text) instead of the difficulty scores.",
            "score": 0.30502405262028015,
            "section_title": "Experimental Details",
            "char_start_offset": 10918,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 208,
                    "matchedPaperCorpusId": "120478295"
                },
                {
                    "start": 1141,
                    "end": 1163,
                    "matchedPaperCorpusId": "219966938"
                },
                {
                    "start": 1173,
                    "end": 1193,
                    "matchedPaperCorpusId": "208229926"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.373779296875
        }
    ],
    "quotes": {
        "cost": 0.21048,
        "quotes": [
            {
                "idx": 0,
                "key": "[247694098 | Christopoulou et al. | 2022 | Citations: 4]",
                "snippets": "In this work, instead, we employ CL for NLU by taking advantage of training dynamics as difficulty metrics, i.e., statistics that measure the behavior of the model at hand on specific task-data instances during training and propose modifications of existing CL schedulers based on these statistics.\n\nWe list three difficulty metrics that use statistics during the course of a model's training, as follows: CONFIDENCE (CONF) of an example x i is the average probability assigned to the gold label y i by a model with parameters \u03b8 across a number of epochs E. This is a continuous metric with higher values corresponding to easier examples. \n\nVARIABILITY (VAR) of an example x i is the standard deviation of the probabilities assigned to the gold label y i across E epochs. It is a continuous metric with higher values indicating greater uncertainty for a training example. \n\nConfidence and correctness are the primary metrics that we use in our curricula since low and high values correspond to hard and easy examples respectively. On the other hand, variability is used as an auxiliary metric since only high scores clearly represent uncertain examples while low scores offer no important information on their own.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this work, instead, we employ CL for NLU by taking advantage of training dynamics as difficulty metrics, i.e., statistics that measure the behavior of the model at hand on specific task-data instances during training and propose modifications of existing CL schedulers based on these statistics.\n\nWe list three difficulty metrics that use statistics during the course of a model's training, as follows: CONFIDENCE (CONF) of an example x i is the average probability assigned to the gold label y i by a model with parameters \u03b8 across a number of epochs E. This is a continuous metric with higher values corresponding to easier examples. \n\nVARIABILITY (VAR) of an example x i is the standard deviation of the probabilities assigned to the gold label y i across E epochs. It is a continuous metric with higher values indicating greater uncertainty for a training example. \n\nConfidence and correctness are the primary metrics that we use in our curricula since low and high values correspond to hard and easy examples respectively. On the other hand, variability is used as an auxiliary metric since only high scores clearly represent uncertain examples while low scores offer no important information on their own.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[248810776 | Zhu et al. | 2022 | Citations: 6]",
                "snippets": "The partition of training data into different subsets according to their learning difficulties and adoption of separate learning schemes (e.g., weighting) are proven to be useful in many learning tasks [21,43,46,68]. The learning difficulty investigated in this study refers to the degrees of easy or hard to learn of training samples in a given learning task.\n\nNumerous methods are proposed to measure the learning difficulty of a training sample. The most common practice is to leverage the training output (e.g., loss and the predicted value on the true category) of a sample to construct the measurements. In Self-paced Learning (SPL) [26,68], the training loss is used to determine whether a sample is easy or not, and easy samples are first learned. We assume that p i,yi is the prediction on the ground-truth category for a training sample x i . In object detection, the value of (1 \u2212 p i,yi ) is used to indicate the learning difficulty for x i [43] . Given that the training output in an epoch may be unreliable, some methods utilize the average training output of a sample during the training to measure the difficulty. Huang et al. [33] designed a cyclic training procedure, and the model is trained from underfitting to over-fitting in one cycle. The average training loss in the whole cyclic procedure is used as the noisy indicator for a training sample. Feng et al. [20] utilized the magnitude of the loss gradient to measure the learning difficulty of a training sample. A large gradient magnitude indicates a high degree of difficulty.\n\nLearning difficulty is considered as an intrinsic property of data in machine learning [46,75]. Existing measurements are usually based on heuristic cues or inspirations, and they can be divided into the following main categories: \n\n\u2022 Loss-based measurement. This category directly uses the loss as the measure. Most measures fall into this category because it is simple yet effective in various learning tasks. Some methods [68] directly utilize the loss in one epoch as the degree of difficulty. Accordingly, the degrees for the same samples vary in different epochs. Some others utilize the average loss [45] during the partial or whole training procedure for measurement. \u2022 Cross-validation-based measurement. This category adopts a crossvalidation strategy [65]. For example, five-fold cross-validation is performed, and the whole cross-validation is repeated ten times. Consequently, each ELDD training sample receives ten predictions. The value of error predictions is used as the indicator of difficulty. \u2022 Uncertainty-based measurement. This category uses the (model) uncertainty of a sample to measure the difficulty. Aguilar et al. [3] identified hard samples based on the epistemic uncertainty (also known as the model uncertainty). They leveraged the Bayesian Neural Network [67] to infer the epistemic uncertainty. \u2022 Margin-based measurement. This category uses the margin (distance) of a sample to the underlying decision surface as the measurement. The rationale is that a small margin denotes a large difficulty [42,68]. \u2022 Gradient-based measurement. This category uses the loss gradient of a sample to measure the difficulty. Agarwal and Hooker [2] proposed the variance of gradients (VOG) across different epochs to rank data from difficult to easy. They considered that samples with high VOG values are far more difficult for the model to learn. Santiago et al. [54] applied the norm of the gradients to measure the difficulty, and high norms indicate large difficulty for learning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The partition of training data into different subsets according to their learning difficulties and adoption of separate learning schemes (e.g., weighting) are proven to be useful in many learning tasks [21,43,46,68]. The learning difficulty investigated in this study refers to the degrees of easy or hard to learn of training samples in a given learning task.\n\nNumerous methods are proposed to measure the learning difficulty of a training sample. The most common practice is to leverage the training output (e.g., loss and the predicted value on the true category) of a sample to construct the measurements. In Self-paced Learning (SPL) [26,68], the training loss is used to determine whether a sample is easy or not, and easy samples are first learned. We assume that p i,yi is the prediction on the ground-truth category for a training sample x i . In object detection, the value of (1 \u2212 p i,yi ) is used to indicate the learning difficulty for x i [43] . Given that the training output in an epoch may be unreliable, some methods utilize the average training output of a sample during the training to measure the difficulty. Huang et al. [33] designed a cyclic training procedure, and the model is trained from underfitting to over-fitting in one cycle. The average training loss in the whole cyclic procedure is used as the noisy indicator for a training sample. Feng et al. [20] utilized the magnitude of the loss gradient to measure the learning difficulty of a training sample. A large gradient magnitude indicates a high degree of difficulty.\n\nLearning difficulty is considered as an intrinsic property of data in machine learning [46,75]. Existing measurements are usually based on heuristic cues or inspirations, and they can be divided into the following main categories: \n\n\u2022 Loss-based measurement. This category directly uses the loss as the measure. Most measures fall into this category because it is simple yet effective in various learning tasks. Some methods [68] directly utilize the loss in one epoch as the degree of difficulty. Accordingly, the degrees for the same samples vary in different epochs. Some others utilize the average loss [45] during the partial or whole training procedure for measurement. \u2022 Cross-validation-based measurement. This category adopts a crossvalidation strategy [65]. For example, five-fold cross-validation is performed, and the whole cross-validation is repeated ten times. Consequently, each ELDD training sample receives ten predictions. The value of error predictions is used as the indicator of difficulty. \u2022 Uncertainty-based measurement. This category uses the (model) uncertainty of a sample to measure the difficulty. Aguilar et al. [3] identified hard samples based on the epistemic uncertainty (also known as the model uncertainty). They leveraged the Bayesian Neural Network [67] to infer the epistemic uncertainty. \u2022 Margin-based measurement. This category uses the margin (distance) of a sample to the underlying decision surface as the measurement. The rationale is that a small margin denotes a large difficulty [42,68]. \u2022 Gradient-based measurement. This category uses the loss gradient of a sample to measure the difficulty. Agarwal and Hooker [2] proposed the variance of gradients (VOG) across different epochs to rank data from difficult to easy. They considered that samples with high VOG values are far more difficult for the model to learn. Santiago et al. [54] applied the norm of the gradients to measure the difficulty, and high norms indicate large difficulty for learning.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[248887633 | Croitoru et al. | 2022 | Citations: 9]",
                "snippets": "The main challenge for the methods that build the curriculum at the data level is measuring the difficulty of the data samples, which is required to order the samples from easy to hard. Most studies have addressed the problem with human input (Pentina et al., 2014)(Jim\u00e9nez-S\u00e1nchez et al., 2019)(Wei et al., 2020) or metrics based on domain-specific heuristics. For instance, the text length (Kocmi et al., 2017)[44](Tay et al., 2019)(Zhang et al., 2021) and the word frequency [1,(Liu et al., 2018) have been employed in natural language processing. In computer vision, the samples containing fewer and larger objects have been considered to be easier in some works (Shi et al., 2016)[33]. Other solutions employed difficulty estimators (Ionescu et al., 2016) or even the confidence level of the predictions made by the neural network (Gong et al., 2016)(Hacohen et al., 2019) to approximate the complexity of the data samples. Other studies (Khan et al., 2024)(Khan et al., 2022)(Khan et al., 2023) used the error of a previously trained model to estimate the difficulty of each sample.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[102350936 | Hacohen et al. | 2019 | Citations: 449]": "Training neural networks is traditionally done by providing a sequence of random mini-batches sampled uniformly from the entire training data. In this work, we analyze the effect of curriculum learning, which involves the non-uniform sampling of mini-batches, on the training of deep networks, and specifically CNNs trained for image recognition. To employ curriculum learning, the training algorithm must resolve 2 problems: (i) sort the training examples by difficulty; (ii) compute a series of mini-batches that exhibit an increasing level of difficulty. We address challenge (i) using two methods: transfer learning from some competitive ``teacher\" network, and bootstrapping. In our empirical evaluation, both methods show similar benefits in terms of increased learning speed and improved final performance on test data. We address challenge (ii) by investigating different pacing functions to guide the sampling. The empirical investigation includes a variety of network architectures, using images from CIFAR-10, CIFAR-100 and subsets of ImageNet. We conclude with a novel theoretical analysis of curriculum learning, where we show how it effectively modifies the optimization landscape. We then define the concept of an ideal curriculum, and show that under mild conditions it does not change the corresponding global minimum of the optimization function.",
                    "[166228313 | Tay et al. | 2019 | Citations: 110]": "This paper tackles the problem of reading comprehension over long narratives where documents easily span over thousands of tokens. We propose a curriculum learning (CL) based Pointer-Generator framework for reading/sampling over large documents, enabling diverse training of the neural model based on the notion of alternating contextual difficulty. This can be interpreted as a form of domain randomization and/or generative pretraining during training. To this end, the usage of the Pointer-Generator softens the requirement of having the answer within the context, enabling us to construct diverse training samples for learning. Additionally, we propose a new Introspective Alignment Layer (IAL), which reasons over decomposed alignments using block-based self-attention. We evaluate our proposed method on the NarrativeQA reading comprehension benchmark, achieving state-of-the-art performance, improving existing baselines by 51% relative improvement on BLEU-4 and 17% relative improvement on Rouge-L. Extensive ablations confirm the effectiveness of our proposed IAL and CL components.",
                    "[204539326 | Jimenez-Sanchez et al. | 2019 | Citations: 48]": "Current deep-learning based methods do not easily integrate to clinical protocols, neither take full advantage of medical knowledge. In this work, we propose and compare several strategies relying on curriculum learning, to support the classification of proximal femur fracture from X-ray images, a challenging problem as reflected by existing intra- and inter-expert disagreement. Our strategies are derived from knowledge such as medical decision trees and inconsistencies in the annotations of multiple experts, which allows us to assign a degree of difficulty to each training sample. We demonstrate that if we start learning \"easy\" examples and move towards \"hard\", the model can reach a better performance, even with fewer data. The evaluation is performed on the classification of a clinical dataset of about 1000 X-ray images. Our results show that, compared to class-uniform and random strategies, the proposed medical knowledge-based curriculum, performs up to 15% better in terms of accuracy, achieving the performance of experienced trauma surgeons.",
                    "[221995570 | Wei et al. | 2020 | Citations: 49]": "Applying curriculum learning requires both a range of difficulty in data and a method for determining the difficulty of examples. In many tasks, however, satisfying these requirements can be a formidable challenge.In this paper, we contend that histopathology image classification is a compelling use case for curriculum learning. Based on the nature of histopathology images, a range of difficulty inherently exists among examples, and, since medical datasets are often labeled by multiple annotators, annotator agreement can be used as a natural proxy for the difficulty of a given example. Hence, we propose a simple curriculum learning method that trains on progressively-harder images as determined by annotator agreement.We evaluate our hypothesis on the challenging and clinically-important task of colorectal polyp classification. Whereas vanilla training achieves an AUC of 83.7% for this task, a model trained with our proposed curriculum learning approach achieves an AUC of 88.2%, an improvement of 4.5%. Our work aims to inspire researchers to think more creatively and rigorously when choosing contexts for applying curriculum learning.",
                    "[233433844 | Zhang et al. | 2021 | Citations: 19]": "BERT [1] is very computationally expensive, which is a hurdle for its training and deployment. This work focuses on removing the unnecessary computation due to input padding in BERT. The input of BERT consists of two concatenated sentences. If the length of the two concatenated sentences is shorter than the maximum sequence length, padding must be added to the end of the sentences to fill the empty slots in the input. Because the lengths of sentences vary greatly, there can be a large amount of padding in input. For the English Wikipedia & BooksCorpus dataset, the percentage of padding among all the input tokens is 17% and 48%, respectively, when the max sequence length is set to 128 and 512. For the Chinese Wikipedia dataset, this percentage is 35% and 79%, respectively, when the max sequence length is 128 and 512. For SQuAD-v1.1 [2], padding accounts for 54% of the total input tokens when the max sequence length is 384. Thus, there is a lot of wasted computation on padding, which significantly increases the training and inference time.",
                    "[254246401 | Khan et al. | 2022 | Citations: 5]": "Deep learning models require an enormous amount of data for training. However, recently there is a shift in machine learning from model-centric to data-centric approaches. In datacentric approaches, the focus is to refine and improve the quality of the data to improve the learning performance of the models rather than redesigning model architectures. In this paper, we propose CLIP i.e., Curriculum Learning with Iterative data Pruning. CLIP combines two data-centric approaches i.e., curriculum learning and dataset pruning to improve the model learning accuracy and convergence speed. The proposed scheme applies loss-aware dataset pruning to iteratively remove the least significant samples and progressively reduces the size of the effective dataset in the curriculum learning training. Extensive experiments performed on crowd density estimation models validate the notion behind combining the two approaches by reducing the convergence time and improving generalization. To our knowledge, the idea of data pruning as an embedded process in curriculum learning is novel.",
                    "[256808576 | Khan et al. | 2023 | Citations: 28]": "Automatic crowd counting using density estimation has gained significant attention in computer vision research. As a result, a large number of crowd counting and density estimation models using convolution neural networks (CNN) have been published in the last few years. These models have achieved good accuracy over benchmark datasets. However, attempts to improve the accuracy often lead to higher complexity in these models. In real-time video surveillance applications using drones with limited computing resources, deep models incur intolerable higher inference delay. In this paper, we propose (i) a Lightweight Crowd Density estimation model (LCDnet) for real-time video surveillance, and (ii) an improved training method using curriculum learning (CL). LCDnet is trained using CL and evaluated over two benchmark datasets i.e., DroneRGBT and CARPK. Results are compared with existing crowd models. Our evaluation shows that the LCDnet achieves a reasonably good accuracy while significantly reducing the inference time and memory requirement and thus can be deployed over edge devices with very limited computing resources.",
                    "[26468344 | Kocmi et al. | 2017 | Citations: 141]": "We examine the effects of particular orderings of sentence pairs on the on-line training of neural machine translation (NMT). We focus on two types of such orderings: (1) ensuring that each minibatch contains sentences similar in some aspect and (2) gradual inclusion of some sentence types as the training progresses (so called \u201ccurriculum learning\u201d). In our English-to-Czech experiments, the internal homogeneity of minibatches has no effect on the training but some of our \u201ccurricula\u201d achieve a small improvement over the baseline.",
                    "[266998701 | Khan et al. | 2024 | Citations: 1]": "Recent advances in deep learning techniques have achieved remarkable performance in several computer vision problems. A notably intuitive technique called Curriculum Learning (CL) has been introduced recently for training deep learning models. Surprisingly, curriculum learning achieves significantly improved results in some tasks but marginal or no improvement in others. Hence, there is still a debate about its adoption as a standard method to train supervised learning models. In this work, we investigate the impact of curriculum learning in crowd counting using the density estimation method. We performed detailed investigations by conducting 112 experiments using six different CL settings using eight different crowd models. Our experiments show that curriculum learning improves the model learning performance and shortens the convergence time.",
                    "[51606954 | Liu et al. | 2018 | Citations: 85]": "By reason of being able to obtain natural language responses, natural answers are more favored in real-world Question Answering (QA) systems. Generative models learn to automatically generate natural answers from large-scale question answer pairs (QA-pairs). However, they are suffering from the uncontrollable and uneven quality of QA-pairs crawled from the Internet. To address this problem, we propose a curriculum learning based framework for natural answer generation (CL-NAG), which is able to take full advantage of the valuable learning data from a noisy and uneven-quality corpus. Specifically, we employ two practical measures to automatically measure the quality (complexity) of QA-pairs. Based on the measurements, CL-NAG firstly utilizes simple and low-quality QA-pairs to learn a basic model, and then gradually learns to produce better answers with richer contents and more complete syntaxes based on more complex and higher-quality QA-pairs. In this way, all valuable information in the noisy and uneven-quality corpus could be fully exploited. Experiments demonstrate that CL-NAG outperforms the state-of-the-arts, which increases 6.8% and 8.7% in the accuracy for simple and complex questions, respectively.",
                    "[6954583 | Shi et al. | 2016 | Citations: 81]": "We present a technique for weakly supervised object localization (WSOL), building on the observation that WSOL algorithms usually work better on images with bigger objects. Instead of training the object detector on the entire training set at the same time, we propose a curriculum learning strategy to feed training images into the WSOL learning loop in an order from images containing bigger objects down to smaller ones. To automatically determine the order, we train a regressor to estimate the size of the object given the whole image as input. Furthermore, we use these size estimates to further improve the re-localization step of WSOL by assigning weights to object proposals according to how close their size matches the estimated object size. We demonstrate the effectiveness of using size order and size weighting on the challenging PASCAL VOC 2007 dataset, where we achieve a significant improvement over existing state-of-the-art WSOL techniques.",
                    "[8502955 | Pentina et al. | 2014 | Citations: 245]": "Sharing information between multiple tasks enables algorithms to achieve good generalization performance even from small amounts of training data. However, in a realistic scenario of multi-task learning not all tasks are equally related to each other, hence it could be advantageous to transfer information only between the most related tasks. In this work we propose an approach that processes multiple tasks in a sequence with sharing between subsequent tasks instead of solving all tasks jointly. Subsequently, we address the question of curriculum learning of tasks, i.e. finding the best order of tasks to be learned. Our approach is based on a generalization bound criterion for choosing the task order that optimizes the average expected classification performance over all tasks. Our experimental results show that learning multiple related tasks sequentially can be more effective than learning them jointly, the order in which tasks are being solved affects the overall performance, and that our model is able to automatically discover a favourable order of tasks.",
                    "[879067 | Ionescu et al. | 2016 | Citations: 142]": "We address the problem of estimating image difficulty defined as the human response time for solving a visual search task. We collect human annotations of image difficulty for the PASCAL VOC 2012 data set through a crowd-sourcing platform. We then analyze what human interpretable image properties can have an impact on visual search difficulty, and how accurate are those properties for predicting difficulty. Next, we build a regression model based on deep features learned with state of the art convolutional neural networks and show better results for predicting the ground-truth visual search difficulty scores produced by human annotators. Our model is able to correctly rank about 75% image pairs according to their difficulty score. We also show that our difficulty predictor generalizes well to new classes not seen during training. Finally, we demonstrate that our predicted difficulty scores are useful for weakly supervised object localization (8% improvement) and semi-supervised object classification (1% improvement)."
                },
                "metadata": [
                    {
                        "section_title": "Curriculum Learning",
                        "pdf_hash": "",
                        "start": 972,
                        "end": 1829,
                        "sentence_offsets": [
                            {
                                "start": 972,
                                "end": 1157
                            },
                            {
                                "start": 1158,
                                "end": 1275
                            },
                            {
                                "start": 1276,
                                "end": 1403
                            },
                            {
                                "start": 1404,
                                "end": 1528
                            },
                            {
                                "start": 1529,
                                "end": 1714
                            },
                            {
                                "start": 1715,
                                "end": 1829
                            }
                        ],
                        "ref_mentions": [
                            "8502955",
                            "204539326",
                            "221995570",
                            "26468344",
                            "166228313",
                            "233433844",
                            "51606954",
                            "6954583",
                            "879067",
                            "10364203",
                            "102350936",
                            "266998701",
                            "254246401",
                            "256808576"
                        ],
                        "quote": "The main challenge for the methods that build the curriculum at the data level is measuring the difficulty of the data samples, which is required to order the samples from easy to hard. Most studies have addressed the problem with human input (Pentina et al., 2014)(Jim\u00e9nez-S\u00e1nchez et al., 2019)(Wei et al., 2020) or metrics based on domain-specific heuristics. For instance, the text length (Kocmi et al., 2017)[44](Tay et al., 2019)(Zhang et al., 2021) and the word frequency [1,(Liu et al., 2018) have been employed in natural language processing. In computer vision, the samples containing fewer and larger objects have been considered to be easier in some works (Shi et al., 2016)[33]. Other solutions employed difficulty estimators (Ionescu et al., 2016) or even the confidence level of the predictions made by the neural network (Gong et al., 2016)(Hacohen et al., 2019) to approximate the complexity of the data samples. Other studies (Khan et al., 2024)(Khan et al., 2022)(Khan et al., 2023) used the error of a previously trained model to estimate the difficulty of each sample."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[250408174 | Raymond-Saez et al. | 2022 | Citations: 0]",
                "snippets": "Curriculum Learning (CL) strategies are based on providing a model with an increasingly difficult selection of examples, similar to how children learn in school. CL strategies are popular in Reinforcement Learning -where a curriculum of tasks is progressively learned -and Natural Language Processing applications -where commonly sentence length has been a proxy for difficulty.\n\nThe problem, however, lies in how to determine the difficulty of samples. Typically, an ad hoc difficulty measure needs to be crafted by hand for a specific problem, which limits their application in practice. Even worse, for some applications like image classification, sometimes there's no natural difficulty ordering that can be created. This has led to the development of more general measures of difficulty: Self Paced Learning (Kumar et al., 2010) utilizes the current loss of an example as a proxy for difficulty; however, this requires extra computation and is prone to overfitting (Jiang et al., 2015). As an alternative, a metric called C-Score was proposed (Jiang et al., 2021). This metric measures how consistently a particular example is learned across various models, that is, it measures the proportion of iterations where a sample is successfully classified.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Curriculum Learning (CL) strategies are based on providing a model with an increasingly difficult selection of examples, similar to how children learn in school. CL strategies are popular in Reinforcement Learning -where a curriculum of tasks is progressively learned -and Natural Language Processing applications -where commonly sentence length has been a proxy for difficulty.\n\nThe problem, however, lies in how to determine the difficulty of samples. Typically, an ad hoc difficulty measure needs to be crafted by hand for a specific problem, which limits their application in practice. Even worse, for some applications like image classification, sometimes there's no natural difficulty ordering that can be created. This has led to the development of more general measures of difficulty: Self Paced Learning (Kumar et al., 2010) utilizes the current loss of an example as a proxy for difficulty; however, this requires extra computation and is prone to overfitting (Jiang et al., 2015). As an alternative, a metric called C-Score was proposed (Jiang et al., 2021). This metric measures how consistently a particular example is learned across various models, that is, it measures the proportion of iterations where a sample is successfully classified.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[250626773 | Peng et al. | 2022 | Citations: 9]",
                "snippets": "Very recently, deep ensemble methods (Baldock et al., 2021)[18] measure example difficulty with agreement either from last layers' predictions or from intermediate layers' predictions. Reducing estimation uncertainty with ensembling requires selected treatments and controls such as architectures, number of submodels, and number of data splits. In this work, we show faithful image difficulty can be efficiently estimated by deep metric learning. Hyperspherical learning [26], a weakly supervised learning framework, groups instances of the same concept together and pushes instances of different concepts apart by enforcing angular discrimination during training.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[235458180 | Baldock et al. | 2021 | Citations: 161]": "Existing work on understanding deep learning often employs measures that compress all data-dependent information into a few numbers. In this work, we adopt a perspective based on the role of individual examples. We introduce a measure of the computational difficulty of making a prediction for a given input: the (effective) prediction depth. Our extensive investigation reveals surprising yet simple relationships between the prediction depth of a given input and the model's uncertainty, confidence, accuracy and speed of learning for that data point. We further categorize difficult examples into three interpretable groups, demonstrate how these groups are processed differently inside deep models and showcase how this understanding allows us to improve prediction accuracy. Insights from our study lead to a coherent view of a number of separately reported phenomena in the literature: early layers generalize while later layers memorize; early layers converge faster and networks learn easy data and simple functions first."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 1747,
                        "end": 2392,
                        "sentence_offsets": [
                            {
                                "start": 1747,
                                "end": 1911
                            },
                            {
                                "start": 1912,
                                "end": 2072
                            },
                            {
                                "start": 2073,
                                "end": 2174
                            },
                            {
                                "start": 2175,
                                "end": 2392
                            }
                        ],
                        "ref_mentions": [
                            "235458180"
                        ],
                        "quote": "Very recently, deep ensemble methods (Baldock et al., 2021)[18] measure example difficulty with agreement either from last layers' predictions or from intermediate layers' predictions. Reducing estimation uncertainty with ensembling requires selected treatments and controls such as architectures, number of submodels, and number of data splits. In this work, we show faithful image difficulty can be efficiently estimated by deep metric learning. Hyperspherical learning [26], a weakly supervised learning framework, groups instances of the same concept together and pushes instances of different concepts apart by enforcing angular discrimination during training."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[254366532 | Li et al. | 2022 | Citations: 25]",
                "snippets": "These techniques improve data and training efficiency, but existing solutions have several limitations: \u2022 Techniques like curriculum learning (CL) improve data efficiency by indexing and sampling training data based on certain difficulty metric (Bengio et al., 2009), and it has recently proved effective on large-scale pretraining tasks (Li, Zhang, and He 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 362,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 362
                            }
                        ],
                        "ref_mentions": [
                            "873046"
                        ],
                        "quote": "These techniques improve data and training efficiency, but existing solutions have several limitations: \u2022 Techniques like curriculum learning (CL) improve data efficiency by indexing and sampling training data based on certain difficulty metric (Bengio et al., 2009), and it has recently proved effective on large-scale pretraining tasks (Li, Zhang, and He 2022)."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[255749213 | Zhou et al. | 2023 | Citations: 5]",
                "snippets": "The commonly used difficulty measures, such as loss (Kumar et al., 2010) and gradient norm (Santiago et al., 2021), are mainly related to the bias term. Shin et al. [27] emphasized that only using loss as the measurement cannot distinguish clean and noisy samples, especially for uniform label noise. There are also a few existing studies that use variance (Chang et al., 2017)[29]. For instance, Agarwal et al. [30] applied the variance of gradient norms as the difficulty measure. Indeed, both the variance and bias terms should not be underestimated when measuring the samples' learning difficulty. Our theoretical analyses support that generalization error including both the two terms can capture four main factors influencing the samples' learning difficulty. Thus, the error can be leveraged as a universal measure that is more reasonable than existing measures. Existing studies generally apply the K-fold cross-validation method (Yang et al., 2020) to calculate the generalization error.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[20785832 | Chang et al. | 2017 | Citations: 354]": "Self-paced learning and hard example mining re-weight training instances to improve learning accuracy. This paper presents two improved alternatives based on lightweight estimates of sample uncertainty in stochastic gradient descent (SGD): the variance in predicted probability of the correct class across iterations of mini-batch SGD, and the proximity of the correct class probability to the decision threshold. Extensive experimental results on six datasets show that our methods reliably improve accuracy in various network architectures, including additional gains on top of other popular training techniques, such as residual learning, momentum, ADAM, batch normalization, dropout, and distillation.",
                    "[211505957 | Yang et al. | 2020 | Citations: 185]": "The classical bias-variance trade-off predicts that bias decreases and variance increase with model complexity, leading to a U-shaped risk curve. Recent work calls this into question for neural networks and other over-parameterized models, for which it is often observed that larger models generalize better. We provide a simple explanation for this by measuring the bias and variance of neural networks: while the bias is monotonically decreasing as in the classical theory, the variance is unimodal or bell-shaped: it increases then decreases with the width of the network. We vary the network architecture, loss function, and choice of dataset and confirm that variance unimodality occurs robustly for all models we considered. The risk curve is the sum of the bias and variance curves and displays different qualitative shapes depending on the relative scale of bias and variance, with the double descent curve observed in recent literature as a special case. We corroborate these empirical results with a theoretical analysis of two-layer linear networks with random first layer. Finally, evaluation on out-of-distribution data shows that most of the drop in accuracy comes from increased bias while variance increases by a relatively small amount. Moreover, we find that deeper models decrease bias and increase variance for both in-distribution and out-of-distribution data."
                },
                "metadata": [
                    {
                        "section_title": "Discussion about Generalization Error",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 927,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 115
                            },
                            {
                                "start": 116,
                                "end": 263
                            },
                            {
                                "start": 264,
                                "end": 328
                            },
                            {
                                "start": 329,
                                "end": 428
                            },
                            {
                                "start": 429,
                                "end": 547
                            },
                            {
                                "start": 548,
                                "end": 711
                            },
                            {
                                "start": 712,
                                "end": 815
                            },
                            {
                                "start": 816,
                                "end": 927
                            }
                        ],
                        "ref_mentions": [
                            "1977996",
                            "225499092",
                            "20785832",
                            "211505957"
                        ],
                        "quote": "The commonly used difficulty measures, such as loss (Kumar et al., 2010) and gradient norm (Santiago et al., 2021), are mainly related to the bias term. Shin et al. [27] emphasized that only using loss as the measurement cannot distinguish clean and noisy samples, especially for uniform label noise. There are also a few existing studies that use variance (Chang et al., 2017)[29]. For instance, Agarwal et al. [30] applied the variance of gradient norms as the difficulty measure. Indeed, both the variance and bias terms should not be underestimated when measuring the samples' learning difficulty. Our theoretical analyses support that generalization error including both the two terms can capture four main factors influencing the samples' learning difficulty. Thus, the error can be leveraged as a universal measure that is more reasonable than existing measures. Existing studies generally apply the K-fold cross-validation method (Yang et al., 2020) to calculate the generalization error."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[256460262 | Xu et al. | 2023 | Citations: 22]",
                "snippets": "To alleviate these problems, various automatic difficulty measurers have been developed and explored in the literature, including self-paced learning (Kumar et al., 2010), transfer teacher [69], reinforcement learning teacher (Graves et al., 2017), and other automatic difficulty measurers (Jiang et al., 2017)(Sanger, 1994)[54]. Inspired by these methods, we modify difficulty scores automatically with prediction errors, which are critical indicators of CPS uncertainty. Substantial work has been conducted in the literature, demonstrating the importance of handling uncertainties in CPS security and safety [21,(Han et al., 2022)(Ma et al., 2019)(Xu et al., 2022)(Zhang et al., 2019)[68]...we define the following two types of automatic difficulty measurers: Hamming Distance-based Measurer (HDM, Definition 5.1) and Cross Entropy-based Measurer (CEM, Definition 5.2). Hamming distance is commonly used to calculate the difference of two strings of equal length, while cross-entropy loss estimates uncertainty by comparing real distribution and prediction.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[11137059 | Graves et al. | 2017 | Citations: 530]": "We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multi-armed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.",
                    "[35187967 | Sanger | 1994 | Citations: 123]": "Trajectory extension learning is an incremental method for training an artificial neural network to approximate the inverse dynamics of a robot manipulator. Training data near a desired trajectory is obtained by slowly varying a parameter of the trajectory from a region of easy solvability of the inverse dynamics toward the desired behavior. The parameter can be average speed, path shape, feedback gain, or any other controllable variable. As learning proceeds, an approximate solution to the local inverse dynamics for each value of the parameter is used to guide learning for the next value of the parameter. Convergence conditions are given for two variations on the algorithm. Examples are shown of application to a real 2-joint direct drive robot arm and a simulated 3-joint redundant arm, both using simulated equilibrium point control. >",
                    "[51876228 | Jiang et al. | 2017 | Citations: 1456]": "Recent deep networks are capable of memorizing the entire data even when the labels are completely random. To overcome the overfitting on corrupted labels, we propose a novel technique of learning another neural network, called MentorNet, to supervise the training of the base deep networks, namely, StudentNet. During training, MentorNet provides a curriculum (sample weighting scheme) for StudentNet to focus on the sample the label of which is probably correct. Unlike the existing curriculum that is usually predefined by human experts, MentorNet learns a data-driven curriculum dynamically with StudentNet. Experimental results demonstrate that our approach can significantly improve the generalization performance of deep networks trained on corrupted training data. Notably, to the best of our knowledge, we achieve the best-published result on WebVision, a large benchmark containing 2.2 million images of real-world noisy labels. The code are at this https URL"
                },
                "metadata": [
                    {
                        "section_title": "Automatic Difficulty Measurer.",
                        "pdf_hash": "",
                        "start": 727,
                        "end": 1295,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "1977996",
                            "11137059",
                            "51876228",
                            "35187967",
                            "253421545",
                            "84185986",
                            "253421605",
                            "36524392"
                        ],
                        "quote": "To alleviate these problems, various automatic difficulty measurers have been developed and explored in the literature, including self-paced learning (Kumar et al., 2010), transfer teacher [69], reinforcement learning teacher (Graves et al., 2017), and other automatic difficulty measurers (Jiang et al., 2017)(Sanger, 1994)[54]. Inspired by these methods, we modify difficulty scores automatically with prediction errors, which are critical indicators of CPS uncertainty. Substantial work has been conducted in the literature, demonstrating the importance of handling uncertainties in CPS security and safety [21,(Han et al., 2022)(Ma et al., 2019)(Xu et al., 2022)(Zhang et al., 2019)[68]"
                    },
                    {
                        "section_title": "Automatic Difficulty Measurer.",
                        "pdf_hash": "",
                        "start": 1782,
                        "end": 2149,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "we define the following two types of automatic difficulty measurers: Hamming Distance-based Measurer (HDM, Definition 5.1) and Cross Entropy-based Measurer (CEM, Definition 5.2). Hamming distance is commonly used to calculate the difference of two strings of equal length, while cross-entropy loss estimates uncertainty by comparing real distribution and prediction."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[257937538 | Liu et al. | 2023 | Citations: 6]",
                "snippets": "In the process of model training, samples with larger losses are harder to learn for the current stage of the model. Conversely, samples with smaller losses prove that the model has been able to correctly predict or classify that sample, which should reduce the sampling probability of that sample. Using sample loss (Gan et al., 2021)[29] as a difficulty evaluation metric, such as Negative Log Likelihood loss [79], square loss (Tang et al., 2019), and cross-entropy loss (Shu et al., 2019)110,(Guo et al., 2018) as difficulty scores, is widely used in self-paced learning (Kumar et al., 2010) and its variant methods (Li et al., 2016)(Fan et al., 2016)...Cross entropy is used as a measure of transferability (Shu et al., 2019), domain relevance (Zhan et al., 2021), uncertainty (Zhou et al., 2020), and representativeness (Tang et al., 2019)[114], as in Shu et al. (Shu et al., 2019), where cross entropy loss is used as a measure of sample transferability for solving sample noises of the source domain and distribution shift across domains...Rather than focusing on the instantaneous loss values of a sample, some studies have focused on its loss value during training, calculating the change in model loss over two consecutive training iterations (Zhou et al., 2020) as a difficulty metric, proving that a sample is very difficult when its loss fluctuates between maximum and minimum values during the sequence. Zhou et al. (Zhou et al., 2021) proposed the exponential moving average (EMA) method for the detection of clean and pseudo-labeled samples.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220047761 | Zhou et al. | 2020 | Citations: 96]": "Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule.",
                    "[232104951 | Zhan et al. | 2021 | Citations: 46]": "Meta-learning has been sufficiently validated to be beneficial for low-resource neural machine translation (NMT). However, we find that meta-trained NMT fails to improve the translation performance of the domain unseen at the meta-training stage. In this paper, we aim to alleviate this issue by proposing a novel meta-curriculum learning for domain adaptation in NMT. During meta-training, the NMT first learns the similar curricula from each domain to avoid falling into a bad local optimum early, and finally learns the curricula of individualities to improve the model robustness for learning domain-specific knowledge. Experimental results on 10 different low-resource domains show that meta-curriculum learning can improve the translation performance of both familiar and unfamiliar domains. All the codes and data are freely available at https://github.com/NLP2CT/Meta-Curriculum.",
                    "[243865335 | Gan et al. | 2021 | Citations: 13]": "Spelling Error Correction (SEC) that requires high-level language understanding is a challenging but useful task. Current SEC approaches normally leverage a pre-training then fine-tuning procedure that treats data equally. By contrast, Curriculum Learning (CL) utilizes training data differently during training and has shown its effectiveness in improving both performance and training efficiency in many other NLP tasks. In NMT, a model\u2019s performance has been shown sensitive to the difficulty of training examples, and CL has been shown effective to address this. In SEC, the data from different language learners are naturally distributed at different difficulty levels (some errors made by beginners are obvious to correct while some made by fluent speakers are hard), and we expect that designing a curriculum correspondingly for model learning may also help its training and bring about better performance. In this paper, we study how to further improve the performance of the state-of-the-art SEC method with CL, and propose a Self-Supervised Curriculum Learning (SSCL) approach. Specifically, we directly use the cross-entropy loss as criteria for: 1) scoring the difficulty of training data, and 2) evaluating the competence of the model. In our approach, CL improves the model training, which in return improves the CL measurement. In our experiments on the SIGHAN 2015 Chinese spelling check task, we show that SSCL is superior to previous norm-based and uncertainty-aware approaches, and establish a new state of the art (74.38% F1).",
                    "[37032764 | Li et al. | 2016 | Citations: 37]": "\n \n Current self-paced learning (SPL) regimes adopt the greedy strategy to obtain the solution with a gradually increasing pace parameter while where to optimally terminate this increasing process is difficult to determine.Besides, most SPL implementations are very sensitive to initialization and short of a theoretical result to clarify where SPL converges to with pace parameter increasing.In this paper, we propose a novel multi-objective self-paced learning (MOSPL) method to address these issues.Specifically, we decompose the objective functions as two terms, including the loss and the self-paced regularizer, respectively, and treat the problem as the compromise between these two objectives.This naturally reformulates the SPL problem as a standard multi-objective issue.A multi-objective evolutionary algorithm is used to optimize the two objectives simultaneously to facilitate the rational selection of a proper pace parameter.The proposed technique is capable of ameliorating a set of solutions with respect to a range of pace parameters through finely compromising these solutions inbetween, and making them perform robustly even under bad initialization.A good solution can then be naturally achieved from these solutions by making use of some off-the-shelf tools in multi-objective optimization.Experimental results on matrix factorization and action recognition demonstrate the superiority of the proposed method against the existing issues in current SPL research.\n \n",
                    "[57784927 | Tang et al. | 2019 | Citations: 80]": "Active learning queries labels from the oracle for the most valuable instances to reduce the labeling cost. In many active learning studies, informative and representative instances are preferred because they are expected to have higher potential value for improving the model. Recently, the results in self-paced learning show that training the model with easy examples first and then gradually with harder examples can improve the performance. While informative and representative instances could be easy or hard, querying valuable but hard examples at early stage may lead to waste of labeling cost. In this paper, we propose a self-paced active learning approach to simultaneously consider the potential value and easiness of an instance, and try to train the model with least cost by querying the right thing at the right time. Experimental results show that the proposed approach is superior to state-of-the-art batch mode active learning methods.",
                    "[69357784 | Shu et al. | 2019 | Citations: 103]": "Domain adaptation improves a target task by knowledge transfer from a source domain with rich annotations. It is not uncommon that \u201csource-domain engineering\u201d becomes a cumbersome process in domain adaptation: the high-quality source domains highly related to the target domain are hardly available. Thus, weakly-supervised domain adaptation has been introduced to address this difficulty, where we can tolerate the source domain with noises in labels, features, or both. As such, for a particular target task, we simply collect the source domain with coarse labeling or corrupted data. In this paper, we try to address two entangled challenges of weaklysupervised domain adaptation: sample noises of the source domain and distribution shift across domains. To disentangle these challenges, a Transferable Curriculum Learning (TCL) approach is proposed to train the deep networks, guided by a transferable curriculum informing which of the source examples are noiseless and transferable. The approach enhances positive transfer from clean source examples to the target and mitigates negative transfer of noisy source examples. A thorough evaluation shows that our approach significantly outperforms the state-of-the-art on weakly-supervised domain adaptation tasks.",
                    "[9431293 | Fan et al. | 2016 | Citations: 54]": "\n \n Self-paced learning (SPL) mimics the cognitive mechanism of humans and animals that gradually learns from easy to hard samples. One key issue in SPL is to obtain better weighting strategy that is determined by the minimizer function. Existing methods usually pursue this by artificially designing the explicit form of SPL regularizer. In this paper, we study a group of new regularizer (named self-paced implicit regularizer) that is deduced from robust loss function. Based on the convex conjugacy theory, the minimizer function for self-paced implicit regularizer can be directly learned from the latent loss function, while the analytic form of the regularizer can be even unknown. A general framework (named SPL-IR) for SPL is developed accordingly. We demonstrate that the learning procedure of SPL-IR is associated with latent robust loss functions, thus can provide some theoretical insights for its working mechanism. We further analyze the relation between SPL-IR and half-quadratic optimization and provide a group of self-paced implicit regularizer. Finally, we implement SPL-IR to both supervised and unsupervised tasks, and experimental results corroborate our ideas and demonstrate the correctness and effectiveness of implicit regularizers.\n \n"
                },
                "metadata": [
                    {
                        "section_title": "Self-Scoring",
                        "pdf_hash": "",
                        "start": 46,
                        "end": 600,
                        "sentence_offsets": [
                            {
                                "start": 46,
                                "end": 162
                            },
                            {
                                "start": 163,
                                "end": 344
                            },
                            {
                                "start": 345,
                                "end": 601
                            }
                        ],
                        "ref_mentions": [
                            "243865335",
                            "57784927",
                            "69357784",
                            "52952193",
                            "1977996",
                            "37032764",
                            "9431293"
                        ],
                        "quote": "In the process of model training, samples with larger losses are harder to learn for the current stage of the model. Conversely, samples with smaller losses prove that the model has been able to correctly predict or classify that sample, which should reduce the sampling probability of that sample. Using sample loss (Gan et al., 2021)[29] as a difficulty evaluation metric, such as Negative Log Likelihood loss [79], square loss (Tang et al., 2019), and cross-entropy loss (Shu et al., 2019)110,(Guo et al., 2018) as difficulty scores, is widely used in self-paced learning (Kumar et al., 2010) and its variant methods (Li et al., 2016)(Fan et al., 2016)"
                    },
                    {
                        "section_title": "Self-Scoring",
                        "pdf_hash": "",
                        "start": 913,
                        "end": 1228,
                        "sentence_offsets": [
                            {
                                "start": 913,
                                "end": 1228
                            }
                        ],
                        "ref_mentions": [
                            "69357784",
                            "232104951",
                            "220047761",
                            "57784927",
                            "69357784"
                        ],
                        "quote": "Cross entropy is used as a measure of transferability (Shu et al., 2019), domain relevance (Zhan et al., 2021), uncertainty (Zhou et al., 2020), and representativeness (Tang et al., 2019)[114], as in Shu et al. (Shu et al., 2019), where cross entropy loss is used as a measure of sample transferability for solving sample noises of the source domain and distribution shift across domains"
                    },
                    {
                        "section_title": "Self-Scoring",
                        "pdf_hash": "",
                        "start": 1510,
                        "end": 1993,
                        "sentence_offsets": [
                            {
                                "start": 1510,
                                "end": 1866
                            },
                            {
                                "start": 1867,
                                "end": 1992
                            }
                        ],
                        "ref_mentions": [
                            "227275560",
                            "233452059"
                        ],
                        "quote": "Rather than focusing on the instantaneous loss values of a sample, some studies have focused on its loss value during training, calculating the change in model loss over two consecutive training iterations (Zhou et al., 2020) as a difficulty metric, proving that a sample is very difficult when its loss fluctuates between maximum and minimum values during the sequence. Zhou et al. (Zhou et al., 2021) proposed the exponential moving average (EMA) method for the detection of clean and pseudo-labeled samples."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[259370648 | Elgaar et al. | 2023 | Citations: 0]",
                "snippets": "Annotation entropy is a natural measure of difficulty (for humans) and may serve as a reliable difficulty metric for models. Entropy of each sample x i is calculated as \u2212 l p c log p c (Shannon, 1948), where c is a class category and p c is the fraction of annotators who chose label c for the sample. The use of entropy is supported in (Nie et al., 2020), reporting a consistent positive correlation between model accuracy and level of human agreement.\n\nFurthermore, moving average of a sample's instantaneous loss is a good metric for difficulty (Zhou et al., 2020). Using a baseline model trained with no curriculum and with default hyperparameters, we collect the loss values of all training instances at intervals of 0.5 epochs and use the average loss as prior knowledge about sample difficulty. We obtain twenty observations of the loss and compute the average for each sample.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[222177851 | Nie et al. | 2020 | Citations: 138]": "Despite the subjective nature of many NLP tasks, most NLU evaluations have focused on using the majority label with presumably high agreement as the ground truth. Less attention has been paid to the distribution of human opinions. We collect ChaosNLI, a dataset with a total of 464,500 annotations to study Collective HumAn OpinionS in oft-used NLI evaluation sets. This dataset is created by collecting 100 annotations per example for 3,113 examples in SNLI and MNLI and 1,532 examples in Abductive-NLI. Analysis reveals that: (1) high human disagreement exists in a noticeable amount of examples in these datasets; (2) the state-of-the-art models lack the ability to recover the distribution over human labels; (3) models achieve near-perfect accuracy on the subset of data with a high level of human agreement, whereas they can barely beat a random guess on the data with low levels of human agreement, which compose most of the common errors made by state-of-the-art models on the evaluation sets. This questions the validity of improving model performance on old metrics for the low-agreement part of evaluation datasets. Hence, we argue for a detailed examination of human agreement in future data collection efforts, and evaluating model outputs against the distribution over collective human opinions. The ChaosNLI dataset and experimental scripts are available at this https URL"
                },
                "metadata": [
                    {
                        "section_title": "Prior Knowledge of Difficulty",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 885,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "222177851",
                            "227275560"
                        ],
                        "quote": "Annotation entropy is a natural measure of difficulty (for humans) and may serve as a reliable difficulty metric for models. Entropy of each sample x i is calculated as \u2212 l p c log p c (Shannon, 1948), where c is a class category and p c is the fraction of annotators who chose label c for the sample. The use of entropy is supported in (Nie et al., 2020), reporting a consistent positive correlation between model accuracy and level of human agreement.\n\nFurthermore, moving average of a sample's instantaneous loss is a good metric for difficulty (Zhou et al., 2020). Using a baseline model trained with no curriculum and with default hyperparameters, we collect the loss values of all training instances at intervals of 0.5 epochs and use the average loss as prior knowledge about sample difficulty. We obtain twenty observations of the loss and compute the average for each sample."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[259593617 | Zeng et al. | 2023 | Citations: 6]",
                "snippets": "Automatic. Though pre-defined strategies have been demonstrated effective in various application scenarios, they are often plagued by their strong reliance on human expertise to define an appropriate difficulty measurer and an extensive search for effective combinations of difficulty measurer and training scheduler. Therefore, in addition to the four pre-defined CL strategies described above, we further designed an automatic difficulty measurer to dynamically select data samples based on instance-wise training loss and enable a more flexible training process. Specifically, the automatic difficulty measurer used in this study characterizes data samples as either easy and difficult, which represents the samples whose ground truth scores are correctly or incorrectly predicted by a model in a training epoch.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Curriculum Learning Design",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 815,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 10
                            },
                            {
                                "start": 11,
                                "end": 317
                            },
                            {
                                "start": 318,
                                "end": 565
                            },
                            {
                                "start": 566,
                                "end": 815
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Automatic. Though pre-defined strategies have been demonstrated effective in various application scenarios, they are often plagued by their strong reliance on human expertise to define an appropriate difficulty measurer and an extensive search for effective combinations of difficulty measurer and training scheduler. Therefore, in addition to the four pre-defined CL strategies described above, we further designed an automatic difficulty measurer to dynamically select data samples based on instance-wise training loss and enable a more flexible training process. Specifically, the automatic difficulty measurer used in this study characterizes data samples as either easy and difficult, which represents the samples whose ground truth scores are correctly or incorrectly predicted by a model in a training epoch."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[261557260 | Chen et al. | 2023 | Citations: 3]",
                "snippets": "We follow the general curriculum learning framework of difficulty measure and training scheduler (Wang et al., 2021), where difficulty measure determines the relative \"difficulty\" of each data sample and training scheduler decides the sequence of data subsets throughout the training process. Additionally, curriculum learning has been approved as an effective method for data cleaning, and many works have shown that the performance can be improved on denoised data (Wang et al., 2017)(Wang et al., 2019).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[1054586 | Wang et al. | 2017 | Citations: 132]": "Instance weighting has been widely applied to phrase-based machine translation domain adaptation. However, it is challenging to be applied to Neural Machine Translation (NMT) directly, because NMT is not a linear model. In this paper, two instance weighting technologies, i.e., sentence weighting and domain weighting with a dynamic weight learning strategy, are proposed for NMT domain adaptation. Empirical results on the IWSLT English-German/French tasks show that the proposed methods can substantially improve NMT performance by up to 2.7-6.7 BLEU points, outperforming the existing baselines by up to 1.6-3.6 BLEU points.",
                    "[174798125 | Wang et al. | 2019 | Citations: 59]": "Noise and domain are important aspects of data quality for neural machine translation. Existing research focus separately on domain-data selection, clean-data selection, or their static combination, leaving the dynamic interaction across them not explicitly examined. This paper introduces a \u201cco-curricular learning\u201d method to compose dynamic domain-data selection with dynamic clean-data selection, for transfer learning across both capabilities. We apply an EM-style optimization procedure to further refine the \u201cco-curriculum\u201d. Experiment results and analysis with two domains demonstrate the effectiveness of the method and the properties of data scheduled by the co-curriculum.",
                    "[232362223 | Wang et al. | 2021 | Citations: 611]": "Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of <italic>Difficulty Measurer <inline-formula><tex-math notation=\"LaTeX\">$+$</tex-math><alternatives><mml:math><mml:mo>+</mml:mo></mml:math><inline-graphic xlink:href=\"wang-ieq1-3069908.gif\"/></alternatives></inline-formula> Training Scheduler</italic> and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. We also analyze principles to select different CL designs that may benefit practical applications. Finally, we present our insights on the relationships connecting CL and other machine learning concepts including transfer learning, meta-learning, continual learning and active learning, etc., then point out challenges in CL as well as potential future research directions deserving further investigations."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 509,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 293
                            },
                            {
                                "start": 294,
                                "end": 509
                            }
                        ],
                        "ref_mentions": [
                            "232362223",
                            "1054586",
                            "174798125"
                        ],
                        "quote": "We follow the general curriculum learning framework of difficulty measure and training scheduler (Wang et al., 2021), where difficulty measure determines the relative \"difficulty\" of each data sample and training scheduler decides the sequence of data subsets throughout the training process. Additionally, curriculum learning has been approved as an effective method for data cleaning, and many works have shown that the performance can be improved on denoised data (Wang et al., 2017)(Wang et al., 2019)."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[264451551 | Wu et al. | 2023 | Citations: 1]",
                "snippets": "As shown in Fig. 7, there are five major manners to measure learning difficulty of samples, namely, loss-based, gradient-based, uncertainty-based, multi-training-based, and distance-based...Model uncertainty can be calculated based on information entropy of the DNN prediction or the variance of multiple predictions output by a DNN with the dropout trick (Kendall et al., 2017). \u2022 Diversity. This type aims to identify the diversity of a subset of training samples. The subset is usually a category. The measurement for subset diversity is useful in the design of data augmentation strategy for the subset (Kumar et al., 2019) and data selection (Su et al., 2022). Friedman and Dieng [80] leveraged the exponential of the Shannon entropy of the eigenvalues of a similarity matrix, namely, vendi score to measure diversity.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[174799713 | Kumar et al. | 2019 | Citations: 116]": "Inducing diversity in the task of paraphrasing is an important problem in NLP with applications in data augmentation and conversational agents. Previous paraphrasing approaches have mainly focused on the issue of generating semantically similar paraphrases while paying little attention towards diversity. In fact, most of the methods rely solely on top-k beam search sequences to obtain a set of paraphrases. The resulting set, however, contains many structurally similar sentences. In this work, we focus on the task of obtaining highly diverse paraphrases while not compromising on paraphrasing quality. We provide a novel formulation of the problem in terms of monotone submodular function maximization, specifically targeted towards the task of paraphrasing. Additionally, we demonstrate the effectiveness of our method for data augmentation on multiple tasks such as intent classification and paraphrase recognition. In order to drive further research, we have made the source code available.",
                    "[71134 | Kendall et al. | 2017 | Citations: 4720]": "There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model - uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks."
                },
                "metadata": [
                    {
                        "section_title": "A. Data perception",
                        "pdf_hash": "",
                        "start": 467,
                        "end": 654,
                        "sentence_offsets": [
                            {
                                "start": 433,
                                "end": 561
                            },
                            {
                                "start": 564,
                                "end": 750
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "As shown in Fig. 7, there are five major manners to measure learning difficulty of samples, namely, loss-based, gradient-based, uncertainty-based, multi-training-based, and distance-based"
                    },
                    {
                        "section_title": "A. Data perception",
                        "pdf_hash": "",
                        "start": 1798,
                        "end": 2385,
                        "sentence_offsets": [
                            {
                                "start": 1798,
                                "end": 1969
                            },
                            {
                                "start": 1970,
                                "end": 1982
                            },
                            {
                                "start": 1983,
                                "end": 2056
                            },
                            {
                                "start": 2057,
                                "end": 2090
                            },
                            {
                                "start": 2091,
                                "end": 2226
                            },
                            {
                                "start": 2227,
                                "end": 2384
                            }
                        ],
                        "ref_mentions": [
                            "71134",
                            "174799713",
                            "252494342"
                        ],
                        "quote": "Model uncertainty can be calculated based on information entropy of the DNN prediction or the variance of multiple predictions output by a DNN with the dropout trick (Kendall et al., 2017). \u2022 Diversity. This type aims to identify the diversity of a subset of training samples. The subset is usually a category. The measurement for subset diversity is useful in the design of data augmentation strategy for the subset (Kumar et al., 2019) and data selection (Su et al., 2022). Friedman and Dieng [80] leveraged the exponential of the Shannon entropy of the eigenvalues of a similarity matrix, namely, vendi score to measure diversity."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[264819795 | Elgaar et al. | 2023 | Citations: 2]",
                "snippets": "Second, existing approaches quantify the difficulty of data based on instantaneous training loss. However, training loss provides noisy estimates of sample difficulty due to data memorization (Zhang et al., 2016)(Arpit et al., 2017) in neural models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[11455421 | Arpit et al. | 2017 | Citations: 1827]": "We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.",
                    "[6212000 | Zhang et al. | 2016 | Citations: 4637]": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. \nThrough extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. \nWe interpret our experimental findings by comparison with traditional models."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 249,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 97
                            },
                            {
                                "start": 98,
                                "end": 249
                            }
                        ],
                        "ref_mentions": [
                            "6212000",
                            "11455421"
                        ],
                        "quote": "Second, existing approaches quantify the difficulty of data based on instantaneous training loss. However, training loss provides noisy estimates of sample difficulty due to data memorization (Zhang et al., 2016)(Arpit et al., 2017) in neural models."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[265351926 | Vakil et al. | 2023 | Citations: 3]",
                "snippets": "Current curriculum learning methodologies in NLP rely on data properties, e.g., sentence length, word rarity, or syntactic features (Platanios et al., 2019)(Liu et al., 2022), or annotation disagreement (Elgaar et al., 2023); as well as model properties such as training loss and its variations (Graves et al., 2017)(Amiri et al., 2017) to sequence data samples for training. (Elgaar et al., 2023) developed a curriculum discovery framework based on prior knowledge of sample difficulty, utilized annotation entropy and loss values.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[259370648 | Elgaar et al. | 2023 | Citations: 0]": "We introduce the problem of curriculum discovery and describe a curriculum learning framework capable of discovering effective curricula in a curriculum space based on prior knowledge about sample difficulty. Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as apposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy transition curricula are often at the risk of underperforming, and (iii): the curricula discovered for smaller datasets and models perform well on larger datasets and models respectively. The proposed framework encompasses some of the existing curriculum learning approaches and can discover curricula that outperform them across several NLP tasks.",
                    "[11137059 | Graves et al. | 2017 | Citations: 530]": "We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multi-armed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.",
                    "[1916665 | Amiri et al. | 2017 | Citations: 28]": "We present a novel approach for training artificial neural networks. Our approach is inspired by broad evidence in psychology that shows human learners can learn efficiently and effectively by increasing intervals of time between subsequent reviews of previously learned materials (spaced repetition). We investigate the analogy between training neural models and findings in psychology about human memory model and develop an efficient and effective algorithm to train neural models. The core part of our algorithm is a cognitively-motivated scheduler according to which training instances and their \u201creviews\u201d are spaced over time. Our algorithm uses only 34-50% of data per epoch, is 2.9-4.8 times faster than standard training, and outperforms competing state-of-the-art baselines. Our code is available at scholar.harvard.edu/hadi/RbF/.",
                    "[236460239 | Liu et al. | 2022 | Citations: 140]": "Medical report generation task, which targets to produce long and coherent descriptions of medical images, has attracted growing research interests recently. Different from the general image captioning tasks, medical report generation is more challenging for data-driven neural models. This is mainly due to 1) the serious data bias and 2) the limited medical data. To alleviate the data bias and make best use of available data, we propose a Competence-based Multimodal Curriculum Learning framework (CMCL). Specifically, CMCL simulates the learning process of radiologists and optimizes the model in a step by step manner. Firstly, CMCL estimates the difficulty of each training instance and evaluates the competence of current model; Secondly, CMCL selects the most suitable batch of training instances considering current model competence. By iterating above two steps, CMCL can gradually improve the model\u2019s performance. The experiments on the public IU-Xray and MIMIC-CXR datasets show that CMCL can be incorporated into existing models to improve their performance.",
                    "[85498775 | Platanios et al. | 2019 | Citations: 344]": "Current state-of-the-art NMT systems use large neural networks that are not only slow to train, but also often require many heuristics and optimization tricks, such as specialized learning rate schedules and large batch sizes. This is undesirable as it requires extensive hyperparameter tuning. In this paper, we propose a curriculum learning framework for NMT that reduces training time, reduces the need for specialized heuristics or large batch sizes, and results in overall better performance. Our framework consists of a principled way of deciding which training samples are shown to the model at different times during training, based on the estimated difficulty of a sample and the current competence of the model. Filtering training samples in this manner prevents the model from getting stuck in bad local optima, making it converge faster and reach a better solution than the common approach of uniformly sampling training examples. Furthermore, the proposed method can be easily applied to existing NMT models by simply modifying their input data pipelines. We show that our framework can help improve the training time and the performance of both recurrent neural network models and Transformers, achieving up to a 70% decrease in training time, while at the same time obtaining accuracy improvements of up to 2.2 BLEU."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 1034,
                        "end": 1569,
                        "sentence_offsets": [
                            {
                                "start": 1034,
                                "end": 1410
                            },
                            {
                                "start": 1411,
                                "end": 1569
                            }
                        ],
                        "ref_mentions": [
                            "85498775",
                            "236460239",
                            "259370648",
                            "11137059",
                            "1916665",
                            "259370648"
                        ],
                        "quote": "Current curriculum learning methodologies in NLP rely on data properties, e.g., sentence length, word rarity, or syntactic features (Platanios et al., 2019)(Liu et al., 2022), or annotation disagreement (Elgaar et al., 2023); as well as model properties such as training loss and its variations (Graves et al., 2017)(Amiri et al., 2017) to sequence data samples for training. (Elgaar et al., 2023) developed a curriculum discovery framework based on prior knowledge of sample difficulty, utilized annotation entropy and loss values."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[265552007 | Ding et al. | 2023 | Citations: 23]",
                "snippets": "Difficulty Metric. In the realm of curriculum learning for natural language processing, the most widely used difficulty metric is perhaps the sequence length (Geiping et al., 2022)157,(Li et al., 2021). The underlying assumption is that processing longer sentences poses greater challenges than shorter ones. Another prevalent metric is vocabulary rarity [285](Zhao et al., 2020), based on the intuition that sentences with less frequently used words in the training set are inherently more complex to comprehend. In addition, this metric could be measured by the uncertainty sampling principle in active learning, where the uncertainty indicated by other pre-trained models could serves as a gauge of difficulty as well.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[211102338 | Zhao et al. | 2020 | Citations: 42]": "The competitive performance of neural machine translation (NMT) critically relies on large amounts of training data. However, acquiring high-quality translation pairs requires expert knowledge and is costly. Therefore, how to best utilize a given dataset of samples with diverse quality and characteristics becomes an important yet understudied question in NMT. Curriculum learning methods have been introduced to NMT to optimize a model's performance by prescribing the data input order, based on heuristics such as the assessment of noise and difficulty levels. However, existing methods require training from scratch, while in practice most NMT models are pre-trained on big data already. Moreover, as heuristics, they do not generalize well. In this paper, we aim to learn a curriculum for improving a pre-trained NMT model by re-selecting influential data samples from the original training set and formulate this task as a reinforcement learning problem. Specifically, we propose a data selection framework based on Deterministic Actor-Critic, in which a critic network predicts the expected change of model performance due to a certain sample, while an actor network learns to select the best sample out of a random batch of samples presented to it. Experiments on several translation datasets show that our method can further improve the performance of NMT when original batch training reaches its ceiling, without using additional new training data, and significantly outperforms several strong baseline methods.",
                    "[252917575 | Li et al. | 2021 | Citations: 38]": "Recent works have demonstrated great success in pre-training large-scale autoregressive language models on massive GPUs. To reduce the wall-clock training time, a common practice is to increase the batch size and learning rate. However, such practice is often brittle and leads to a so-called stability-efficiency dilemma: increasing the batch sizes and learning rates leads to better training efficiency but can also result in training instability, leading to poor generalization accuracy or failed runs. To better understand this phenomenon, we conduct an in-depth analysis on large-scale pre-training experiments replicating the GPT-2 model. We find that there is a strong correlation between training instability and extreme values of gradient variance, and that samples with long sequence lengths contribute to these extreme gradient variance values, especially at the beginning of the training, indicating that long sequence length can be a main source of training instability. Based on the analysis, we present a Sequence Length Warmup method that aims to solve the training stability-efficiency dilemma. Experiments replicating GPT-2 models show that our approach enables stable training with 8x larger batch size and 4x larger learning rate, whereas the baseline approach struggles with training instability. To achieve the same or better zero-shot evaluation results, our method reduces the required number of training tokens and wall clock time by up to 2.2x and 3.7x, respectively. Experiments replicating GPT-3 model (125M) show that our approach enables stable training with 8x larger batch size and 40x larger learning rate, and retains 99% of the zero-shot accuracy on 11 tasks using 10x less data and 17x less time compared to the original GPT-3 training recipe, while the baseline diverges under the same settings and only retain 95% of accuracy under lower learning rate.",
                    "[255185900 | Geiping et al. | 2022 | Citations: 90]": "Recent trends in language modeling have focused on increasing performance through scaling, and have resulted in an environment where training language models is out of reach for most researchers and practitioners. While most in the community are asking how to push the limits of extreme computation, we ask the opposite question: How far can we get with a single GPU in just one day? We investigate the downstream performance achievable with a transformer-based language model trained completely from scratch with masked language modeling for a single day on a single consumer GPU. Aside from re-analyzing nearly all components of the pretraining pipeline for this scenario and providing a modified pipeline with performance close to BERT, we investigate why scaling down is hard, and which modifications actually improve performance in this scenario. We provide evidence that even in this constrained setting, performance closely follows scaling laws observed in large-compute settings. Through the lens of scaling laws, we categorize a range of recent improvements to training and architecture and discuss their merit and practical applicability (or lack thereof) for the limited compute setting."
                },
                "metadata": [
                    {
                        "section_title": "Curriculum Learning",
                        "pdf_hash": "",
                        "start": 919,
                        "end": 1594,
                        "sentence_offsets": [
                            {
                                "start": 919,
                                "end": 937
                            },
                            {
                                "start": 938,
                                "end": 1090
                            },
                            {
                                "start": 1091,
                                "end": 1196
                            },
                            {
                                "start": 1197,
                                "end": 1386
                            },
                            {
                                "start": 1387,
                                "end": 1594
                            }
                        ],
                        "ref_mentions": [
                            "255185900",
                            "252917575",
                            "211102338"
                        ],
                        "quote": "Difficulty Metric. In the realm of curriculum learning for natural language processing, the most widely used difficulty metric is perhaps the sequence length (Geiping et al., 2022)157,(Li et al., 2021). The underlying assumption is that processing longer sentences poses greater challenges than shorter ones. Another prevalent metric is vocabulary rarity [285](Zhao et al., 2020), based on the intuition that sentences with less frequently used words in the training set are inherently more complex to comprehend. In addition, this metric could be measured by the uncertainty sampling principle in active learning, where the uncertainty indicated by other pre-trained models could serves as a gauge of difficulty as well."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[266166227 | Zhou et al. | 2023 | Citations: 2]",
                "snippets": "Therefore, we use a combination of the representation distance and perplexity score as a measure of examples' difficulty.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Framework",
                        "pdf_hash": "",
                        "start": 887,
                        "end": 1008,
                        "sentence_offsets": [
                            {
                                "start": 887,
                                "end": 1008
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Therefore, we use a combination of the representation distance and perplexity score as a measure of examples' difficulty."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[266741778 | Li et al. | 2024 | Citations: 1]",
                "snippets": "In the case of self-paced CL (SPL, where difficulty is measured during training), the loss function is often used as the difficulty measure (Kumar et al., 2010)- (Gong et al., 2019). Initially, during training, samples with higher losses are excluded from optimization. As training advances, the threshold is gradually increased to incorporate more complex data into the optimization process. However, relying solely on loss might not accurately represent the inherent difficulty of data, as difficulty is an intrinsic attribute of samples and should be independent of ground truth labels. To overcome this limitation, we propose employing uncertainty as the difficulty measure for SPL.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[94785766 | Gong et al. | 2019 | Citations: 54]": "Self-paced learning (SPL) is a recently proposed paradigm to imitate the learning process of humans/animals. SPL involves easier samples into training at first and then gradually takes more complex ones into consideration. Current SPL regimes incorporate a self-paced (SP) regularizer into the learning objective with a gradually increasing pace parameter. Therefore, it is difficult to obtain the solution path of the SPL regime and determine where to optimally stop this increasing process. In this paper, a multiobjective SPL method is proposed to optimize the loss function and the SP regularizer simultaneously. A decomposition-based multiobjective particle swarm optimization algorithm is used to simultaneously optimize the two objectives for obtaining the solutions. In the proposed method, a polynomial soft weighting regularizer is proposed to penalize the loss. Theoretical studies are conducted to show that the previous regularizers are roughly particular cases of the proposed polynomial soft weighting regularizer family. Then an implicit decomposition method is proposed to search the solutions with respect to the sample number involved into training. A set of solutions can be obtained by the proposed method and naturally constitute the solution path of the SPL regime. Then a satisfactory solution can be naturally obtained from these solutions by utilizing some effective tools in evolutionary multiobjective optimization. Experiments on matrix factorization and classification problems demonstrate the effectiveness of the proposed technique."
                },
                "metadata": [
                    {
                        "section_title": "B. Curriculum Learning",
                        "pdf_hash": "",
                        "start": 232,
                        "end": 887,
                        "sentence_offsets": [
                            {
                                "start": 232,
                                "end": 383
                            },
                            {
                                "start": 384,
                                "end": 470
                            },
                            {
                                "start": 471,
                                "end": 593
                            },
                            {
                                "start": 594,
                                "end": 790
                            },
                            {
                                "start": 791,
                                "end": 887
                            }
                        ],
                        "ref_mentions": [
                            "1977996",
                            "94785766"
                        ],
                        "quote": "In the case of self-paced CL (SPL, where difficulty is measured during training), the loss function is often used as the difficulty measure (Kumar et al., 2010)- (Gong et al., 2019). Initially, during training, samples with higher losses are excluded from optimization. As training advances, the threshold is gradually increased to incorporate more complex data into the optimization process. However, relying solely on loss might not accurately represent the inherent difficulty of data, as difficulty is an intrinsic attribute of samples and should be independent of ground truth labels. To overcome this limitation, we propose employing uncertainty as the difficulty measure for SPL."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[267740312 | Mekala et al. | 2024 | Citations: 21]",
                "snippets": "Drawing inspiration from the learning order metric in (Mekala et al., 2022), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249060677 | Mekala et al. | 2022 | Citations: 20]": "Weakly supervised text classification methods typically train a deep neural classifier based on pseudo-labels. The quality of pseudo-labels is crucial to final performance but they are inevitably noisy due to their heuristic nature, so selecting the correct ones has a huge potential for performance boost. One straightforward solution is to select samples based on the softmax probability scores in the neural classifier corresponding to their pseudo-labels. However, we show through our experiments that such solutions are ineffective and unstable due to the erroneously high-confidence predictions from poorly calibrated models. Recent studies on the memorization effects of deep neural models suggest that these models first memorize training samples with clean labels and then those with noisy labels. Inspired by this observation, we propose a novel pseudo-label selection method LOPS that takes learning order of samples into consideration. We hypothesize that the learning order reflects the probability of wrong annotation in terms of ranking, and therefore, propose to select the samples that are learnt earlier. LOPS can be viewed as a strong performance-boost plug-in to most of existing weakly-supervised text classification methods, as confirmed in extensive experiments on four real-world datasets."
                },
                "metadata": [
                    {
                        "section_title": "arXiv:2402.10430v1 [cs.CL] 16 Feb 2024",
                        "pdf_hash": "",
                        "start": 96,
                        "end": 429,
                        "sentence_offsets": [
                            {
                                "start": 96,
                                "end": 330
                            },
                            {
                                "start": 331,
                                "end": 429
                            }
                        ],
                        "ref_mentions": [
                            "249060677"
                        ],
                        "quote": "Drawing inspiration from the learning order metric in (Mekala et al., 2022), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[267770178 | Tang et al. | 2024 | Citations: 0]",
                "snippets": "For instance, Mohiuddin et al. (2022) developed three deterministic scoring methods for machine translation that leverage the similarity of a parallel sentence pairs and cross-entropy. (Zhu et al., 2021) evaluated sample difficulty in dialogue generation based on model loss. Lobov et al. (2022) utilized metrics such as sentence length, average confidence, and perplexity value to assess difficulty in named entity recognition (NER).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[244119160 | Zhu et al. | 2021 | Citations: 27]": "Curriculum learning, a machine training strategy that feeds training instances to the model from easy to hard, has been proven to facili-tate the dialogue generation task. Meanwhile, knowledge distillation, a knowledge transformation methodology among teachers and students networks can yield signi\ufb01cant performance boost for student models. Hence, in this paper, we introduce a combination of curriculum learning and knowledge distillation for ef\ufb01cient dialogue generation models, where curriculum learning can help knowledge distillation from data and model aspects. To start with, from the data aspect, we cluster the training cases according to their complexity, which is calculated by various types of features such as sentence length and coherence between dialog pairs. Furthermore, we employ an adversarial training strategy to identify the complexity of cases from model level. The intuition is that, if a discriminator can tell the generated response is from the teacher or the student, then the case is dif\ufb01cult that the student model has not adapted to yet. Finally, we use self-paced learning, which is an extension to curriculum learning to assign weights for distillation. In conclusion, we arrange a hierarchical curriculum based on the above two aspects for the student model under the guidance from the teacher model. Experimental results demon-strate that our methods achieve improvements compared with competitive baselines."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 870,
                        "end": 1303,
                        "sentence_offsets": [
                            {
                                "start": 870,
                                "end": 1054
                            },
                            {
                                "start": 1055,
                                "end": 1144
                            },
                            {
                                "start": 1145,
                                "end": 1303
                            }
                        ],
                        "ref_mentions": [
                            "244119160"
                        ],
                        "quote": "For instance, Mohiuddin et al. (2022) developed three deterministic scoring methods for machine translation that leverage the similarity of a parallel sentence pairs and cross-entropy. (Zhu et al., 2021) evaluated sample difficulty in dialogue generation based on model loss. Lobov et al. (2022) utilized metrics such as sentence length, average confidence, and perplexity value to assess difficulty in named entity recognition (NER)."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[268696658 | Zhao et al. | 2024 | Citations: 5]",
                "snippets": "Current methods evaluate example difficulty through predefined metrics, such as sentence length, or dynamically update it based on training loss, such as cross-entropy loss. However, predefined difficulty measures do not integrate the model's feedback, and training loss, focusing only on the difference between predictions and ground truth, raises an inaccurate difficulty measurement issue when different examples yield identical training losses.\n\nTo address these limitations, we propose the Gradientbased Difficulty Measure (GDM), which evaluates example difficulty through dynamic measurement of the gradient magnitude with respect to the example itself. Unlike training loss, the gradient avoids inaccurate difficulty assessment by taking input features into account. As a result, even if examples yield the same loss, their gradients can differ. Additionally, loss landscapes can encompass plateaus or saddle points, where training loss remains relatively stable even with substantial shifts in model parameters. In these scenarios, utilizing training loss for evaluating example difficulty can be misleading. Conversely, the gradient provides finergrained insights into changes in model parameters, making the gradient magnitude a more informative approach for evaluating difficulty.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Gradient-based Difficulty Measure",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1291,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 173
                            },
                            {
                                "start": 174,
                                "end": 448
                            },
                            {
                                "start": 451,
                                "end": 660
                            },
                            {
                                "start": 661,
                                "end": 774
                            },
                            {
                                "start": 775,
                                "end": 853
                            },
                            {
                                "start": 854,
                                "end": 1020
                            },
                            {
                                "start": 1021,
                                "end": 1117
                            },
                            {
                                "start": 1118,
                                "end": 1292
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Current methods evaluate example difficulty through predefined metrics, such as sentence length, or dynamically update it based on training loss, such as cross-entropy loss. However, predefined difficulty measures do not integrate the model's feedback, and training loss, focusing only on the difference between predictions and ground truth, raises an inaccurate difficulty measurement issue when different examples yield identical training losses.\n\nTo address these limitations, we propose the Gradientbased Difficulty Measure (GDM), which evaluates example difficulty through dynamic measurement of the gradient magnitude with respect to the example itself. Unlike training loss, the gradient avoids inaccurate difficulty assessment by taking input features into account. As a result, even if examples yield the same loss, their gradients can differ. Additionally, loss landscapes can encompass plateaus or saddle points, where training loss remains relatively stable even with substantial shifts in model parameters. In these scenarios, utilizing training loss for evaluating example difficulty can be misleading. Conversely, the gradient provides finergrained insights into changes in model parameters, making the gradient magnitude a more informative approach for evaluating difficulty."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[269741199 | Lee et al. | 2024 | Citations: 2]",
                "snippets": "Curriculum Learning It is observed that deep learning model training can benefit from the implementation of Curriculum Learning (CL), i.e., using data samples sorted based on a curriculum versus training on the randomly shuffled data (Soviany et al., 2021). Recently, CL methods have been developed and deployed for the LMs as well, at pre-training and post-training stages using a variety of linguistically motivated curricula such as sentence length or term frequency complexity measure based ranking (Liu et al., 2018)(Zhang et al., 2021)Campos, 2021;Weber et al., 2023)...Li et al., 2021 implemented a CL at the pre-training of LMs using the sequence length as the difficulty metric with the curriculum of starting from the shorter sequence training data toward the longer sequence...(Ranaldi et al., 2023) proposed a new complexity measure based on the length, rarity, and comprehensibility of the samples and sorted the corpus according to the proposed complexity measure during the pre-training stage and showed that their CL approach led to better performance in downstream tasks...Wang et al., 2022 used the frequency of words as the complexity metric for the curriculum-based pre-training of LMs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[231709290 | Soviany et al. | 2021 | Citations: 359]": "Training machine learning models in a meaningful order, from the easy samples to the hard ones, using curriculum learning can provide performance improvements over the standard training approach based on random data shuffling, without any additional computational costs. Curriculum learning strategies have been successfully employed in all areas of machine learning, in a wide range of tasks. However, the necessity of finding a way to rank the samples from easy to hard, as well as the right pacing function for introducing more difficult data can limit the usage of the curriculum approaches. In this survey, we show how these limits have been tackled in the literature, and we present different curriculum learning instantiations for various tasks in machine learning. We construct a multi-perspective taxonomy of curriculum learning approaches by hand, considering various classification criteria. We further build a hierarchical tree of curriculum learning methods using an agglomerative clustering algorithm, linking the discovered clusters with our taxonomy. At the end, we provide some interesting directions for future work.",
                    "[243766208 | Zhang et al. | 2021 | Citations: 14]": "With the notable success of pretrained language models, the pretraining-fine-tuning paradigm has become a dominant solution for natural language understanding (NLU) tasks. Typically, the training instances of a target NLU task are introduced in a completely random order and treated equally at the fine-tuning stage. However, these instances can vary greatly in difficulty, and similar to human learning procedures, language models can benefit from an easy-to-difficult curriculum. Based on this concept, we propose a curriculum learning (CL) framework. Our framework consists of two stages, Review and Arrange, targeting the two main challenges in curriculum learning, i.e., how to define the difficulty of instances and how to arrange a curriculum based on the difficulty, respectively. In the first stage, we devise a cross-review (CR) method to train several teacher models first and then review the training set in a crossed manner to distinguish easy instances from difficult instances. In the second stage, two sampling algorithms, a coarse-grained arrangement (CGA) and a fine-grained arrangement (FGA), are proposed to arrange a curriculum for language models in which the learning materials start from the easiest instances, and more difficult instances are gradually added into the training procedure. Compared to previous heuristic CL methods, our framework can avoid the errors caused by a gap in difficulty between humans and machines and has strong generalization ability. We conduct comprehensive experiments, and the results show that our curriculum learning framework, without any manual model architecture design or use of external data, obtains significant and universal performance improvements on a wide range of NLU tasks in different languages.",
                    "[265068175 | Ranaldi et al. | 2023 | Citations: 13]": "Directly learning from complex examples is generally problematic for humans and machines. Indeed, a better strategy is exposing learners to examples in a reasonable, pedagogically-motivated order. Curriculum Learning (CL) has been proposed to import this strategy when training machine learning models. In this paper, building on Curriculum Learning, we propose a novel, linguistically motivated measure to determine example complexity for organizing examples during learning. Our complexity measure - LRC- is based on length, rarity, and comprehensibility. Our resulting learning model is CL-LRC, that is, CL with LRC. Experiments on downstream tasks show that CL-LRC outperforms existing CL and non-CL methods for training BERT and RoBERTa from scratch. Furthermore, we analyzed different measures, including perplexity, loss, and learning curve of different models pre-trained from scratch, showing that CL-LRC performs better than the state-of-the-art.",
                    "[51606954 | Liu et al. | 2018 | Citations: 85]": "By reason of being able to obtain natural language responses, natural answers are more favored in real-world Question Answering (QA) systems. Generative models learn to automatically generate natural answers from large-scale question answer pairs (QA-pairs). However, they are suffering from the uncontrollable and uneven quality of QA-pairs crawled from the Internet. To address this problem, we propose a curriculum learning based framework for natural answer generation (CL-NAG), which is able to take full advantage of the valuable learning data from a noisy and uneven-quality corpus. Specifically, we employ two practical measures to automatically measure the quality (complexity) of QA-pairs. Based on the measurements, CL-NAG firstly utilizes simple and low-quality QA-pairs to learn a basic model, and then gradually learns to produce better answers with richer contents and more complete syntaxes based on more complex and higher-quality QA-pairs. In this way, all valuable information in the noisy and uneven-quality corpus could be fully exploited. Experiments demonstrate that CL-NAG outperforms the state-of-the-arts, which increases 6.8% and 8.7% in the accuracy for simple and complex questions, respectively."
                },
                "metadata": [
                    {
                        "section_title": "I. Related Work",
                        "pdf_hash": "",
                        "start": 567,
                        "end": 1139,
                        "sentence_offsets": [
                            {
                                "start": 567,
                                "end": 824
                            },
                            {
                                "start": 825,
                                "end": 1140
                            }
                        ],
                        "ref_mentions": [
                            "231709290",
                            "51606954",
                            "243766208"
                        ],
                        "quote": "Curriculum Learning It is observed that deep learning model training can benefit from the implementation of Curriculum Learning (CL), i.e., using data samples sorted based on a curriculum versus training on the randomly shuffled data (Soviany et al., 2021). Recently, CL methods have been developed and deployed for the LMs as well, at pre-training and post-training stages using a variety of linguistically motivated curricula such as sentence length or term frequency complexity measure based ranking (Liu et al., 2018)(Zhang et al., 2021)Campos, 2021;Weber et al., 2023)"
                    },
                    {
                        "section_title": "I. Related Work",
                        "pdf_hash": "",
                        "start": 1418,
                        "end": 1628,
                        "sentence_offsets": [
                            {
                                "start": 1418,
                                "end": 1628
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Li et al., 2021 implemented a CL at the pre-training of LMs using the sequence length as the difficulty metric with the curriculum of starting from the shorter sequence training data toward the longer sequence"
                    },
                    {
                        "section_title": "I. Related Work",
                        "pdf_hash": "",
                        "start": 1945,
                        "end": 2243,
                        "sentence_offsets": [
                            {
                                "start": 1945,
                                "end": 2243
                            }
                        ],
                        "ref_mentions": [
                            "265068175"
                        ],
                        "quote": "(Ranaldi et al., 2023) proposed a new complexity measure based on the length, rarity, and comprehensibility of the samples and sorted the corpus according to the proposed complexity measure during the pre-training stage and showed that their CL approach led to better performance in downstream tasks"
                    },
                    {
                        "section_title": "I. Related Work",
                        "pdf_hash": "",
                        "start": 2246,
                        "end": 2363,
                        "sentence_offsets": [
                            {
                                "start": 2246,
                                "end": 2362
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Wang et al., 2022 used the frequency of words as the complexity metric for the curriculum-based pre-training of LMs."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[269756933 | Kim et al. | 2024 | Citations: 10]",
                "snippets": "In the development of training data for curriculum learning, accurately measuring data difficulty is crucial. Specifically, for LLMs (Large Language Models), determining the degree of data difficulty is challenging. Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021; Nagatsuka et al., 2023). However, these metrics may not fully reflect the complexity of a dataset. It is essential to assess data difficulty from the model's perspective, rather than relying solely on data-specific metrics. Our research proposes a new approach to calculate the degree of difficulty based on a model-centric perspective. By organizing the training dataset according to difficulty using our new metric, we aim to improve the model's performance compared to random shuffling.\n\n3 Methods for Quantitative Difficulty Measurement\n\nIn this section, we introduce a novel methodology for training models that begins with easier tasks and methodically progresses to more challenging ones. This approach requires organizing data by its level of complexity, for which we have established three principal criteria. By arranging the data in an order that goes from less to more difficult, this approach establishes a structured progression for learning.\n\n## Loss\nA high loss signifies a substantial discrepancy between the predicted and actual outcomes, indicating that the model perceives the data as challenging.\n\nWe establish a direct correlation between high loss values and increased difficulty. Therefore, prompts that generate higher loss are considered more challenging for the model to learn. This method computes the cross-entropy loss associated with the answer segment of the prompt.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In the development of training data for curriculum learning, accurately measuring data difficulty is crucial. Specifically, for LLMs (Large Language Models), determining the degree of data difficulty is challenging. Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021; Nagatsuka et al., 2023). However, these metrics may not fully reflect the complexity of a dataset. It is essential to assess data difficulty from the model's perspective, rather than relying solely on data-specific metrics. Our research proposes a new approach to calculate the degree of difficulty based on a model-centric perspective. By organizing the training dataset according to difficulty using our new metric, we aim to improve the model's performance compared to random shuffling.\n\n3 Methods for Quantitative Difficulty Measurement\n\nIn this section, we introduce a novel methodology for training models that begins with easier tasks and methodically progresses to more challenging ones. This approach requires organizing data by its level of complexity, for which we have established three principal criteria. By arranging the data in an order that goes from less to more difficult, this approach establishes a structured progression for learning.\n\n## Loss\nA high loss signifies a substantial discrepancy between the predicted and actual outcomes, indicating that the model perceives the data as challenging.\n\nWe establish a direct correlation between high loss values and increased difficulty. Therefore, prompts that generate higher loss are considered more challenging for the model to learn. This method computes the cross-entropy loss associated with the answer segment of the prompt.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[270199394 | Ankner et al. | 2024 | Citations: 34]",
                "snippets": "Marion et al. (2023) investigate data pruning based on multiple neural heuristics of sample difficulty, ultimately concluding that the perplexity of a sample under a reference language model is the best pruning metric.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1350,
                        "end": 1568,
                        "sentence_offsets": [
                            {
                                "start": 1350,
                                "end": 1568
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Marion et al. (2023) investigate data pruning based on multiple neural heuristics of sample difficulty, ultimately concluding that the perplexity of a sample under a reference language model is the best pruning metric."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[271051051 | Jarca et al. | 2024 | Citations: 0]",
                "snippets": "Researchers have made significant strides in finding improved metrics for various domains and tasks.For instance, images containing fewer and larger objects in computer vision are deemed easier than other images (Shi et al., 2016)(Soviany et al., 2019).In natural language processing, word frequency (Bengio et al., 2009)(Liu et al., 2018) and sequence length [8](Kocmi et al., 2017)(Tay et al., 2019)(Zhang et al., 2021) are utilized to assess the sample difficulty.In some cases, researchers have also integrated human feedback into their metric design (Jim\u00e9nez-S\u00e1nchez et al., 2019)(Pentina et al., 2014)(Wei et al., 2020).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[166228313 | Tay et al. | 2019 | Citations: 110]": "This paper tackles the problem of reading comprehension over long narratives where documents easily span over thousands of tokens. We propose a curriculum learning (CL) based Pointer-Generator framework for reading/sampling over large documents, enabling diverse training of the neural model based on the notion of alternating contextual difficulty. This can be interpreted as a form of domain randomization and/or generative pretraining during training. To this end, the usage of the Pointer-Generator softens the requirement of having the answer within the context, enabling us to construct diverse training samples for learning. Additionally, we propose a new Introspective Alignment Layer (IAL), which reasons over decomposed alignments using block-based self-attention. We evaluate our proposed method on the NarrativeQA reading comprehension benchmark, achieving state-of-the-art performance, improving existing baselines by 51% relative improvement on BLEU-4 and 17% relative improvement on Rouge-L. Extensive ablations confirm the effectiveness of our proposed IAL and CL components.",
                    "[204539326 | Jimenez-Sanchez et al. | 2019 | Citations: 48]": "Current deep-learning based methods do not easily integrate to clinical protocols, neither take full advantage of medical knowledge. In this work, we propose and compare several strategies relying on curriculum learning, to support the classification of proximal femur fracture from X-ray images, a challenging problem as reflected by existing intra- and inter-expert disagreement. Our strategies are derived from knowledge such as medical decision trees and inconsistencies in the annotations of multiple experts, which allows us to assign a degree of difficulty to each training sample. We demonstrate that if we start learning \"easy\" examples and move towards \"hard\", the model can reach a better performance, even with fewer data. The evaluation is performed on the classification of a clinical dataset of about 1000 X-ray images. Our results show that, compared to class-uniform and random strategies, the proposed medical knowledge-based curriculum, performs up to 15% better in terms of accuracy, achieving the performance of experienced trauma surgeons.",
                    "[221995570 | Wei et al. | 2020 | Citations: 49]": "Applying curriculum learning requires both a range of difficulty in data and a method for determining the difficulty of examples. In many tasks, however, satisfying these requirements can be a formidable challenge.In this paper, we contend that histopathology image classification is a compelling use case for curriculum learning. Based on the nature of histopathology images, a range of difficulty inherently exists among examples, and, since medical datasets are often labeled by multiple annotators, annotator agreement can be used as a natural proxy for the difficulty of a given example. Hence, we propose a simple curriculum learning method that trains on progressively-harder images as determined by annotator agreement.We evaluate our hypothesis on the challenging and clinically-important task of colorectal polyp classification. Whereas vanilla training achieves an AUC of 83.7% for this task, a model trained with our proposed curriculum learning approach achieves an AUC of 88.2%, an improvement of 4.5%. Our work aims to inspire researchers to think more creatively and rigorously when choosing contexts for applying curriculum learning.",
                    "[233433844 | Zhang et al. | 2021 | Citations: 19]": "BERT [1] is very computationally expensive, which is a hurdle for its training and deployment. This work focuses on removing the unnecessary computation due to input padding in BERT. The input of BERT consists of two concatenated sentences. If the length of the two concatenated sentences is shorter than the maximum sequence length, padding must be added to the end of the sentences to fill the empty slots in the input. Because the lengths of sentences vary greatly, there can be a large amount of padding in input. For the English Wikipedia & BooksCorpus dataset, the percentage of padding among all the input tokens is 17% and 48%, respectively, when the max sequence length is set to 128 and 512. For the Chinese Wikipedia dataset, this percentage is 35% and 79%, respectively, when the max sequence length is 128 and 512. For SQuAD-v1.1 [2], padding accounts for 54% of the total input tokens when the max sequence length is 384. Thus, there is a lot of wasted computation on padding, which significantly increases the training and inference time.",
                    "[26468344 | Kocmi et al. | 2017 | Citations: 141]": "We examine the effects of particular orderings of sentence pairs on the on-line training of neural machine translation (NMT). We focus on two types of such orderings: (1) ensuring that each minibatch contains sentences similar in some aspect and (2) gradual inclusion of some sentence types as the training progresses (so called \u201ccurriculum learning\u201d). In our English-to-Czech experiments, the internal homogeneity of minibatches has no effect on the training but some of our \u201ccurricula\u201d achieve a small improvement over the baseline.",
                    "[51606954 | Liu et al. | 2018 | Citations: 85]": "By reason of being able to obtain natural language responses, natural answers are more favored in real-world Question Answering (QA) systems. Generative models learn to automatically generate natural answers from large-scale question answer pairs (QA-pairs). However, they are suffering from the uncontrollable and uneven quality of QA-pairs crawled from the Internet. To address this problem, we propose a curriculum learning based framework for natural answer generation (CL-NAG), which is able to take full advantage of the valuable learning data from a noisy and uneven-quality corpus. Specifically, we employ two practical measures to automatically measure the quality (complexity) of QA-pairs. Based on the measurements, CL-NAG firstly utilizes simple and low-quality QA-pairs to learn a basic model, and then gradually learns to produce better answers with richer contents and more complete syntaxes based on more complex and higher-quality QA-pairs. In this way, all valuable information in the noisy and uneven-quality corpus could be fully exploited. Experiments demonstrate that CL-NAG outperforms the state-of-the-arts, which increases 6.8% and 8.7% in the accuracy for simple and complex questions, respectively.",
                    "[6954583 | Shi et al. | 2016 | Citations: 81]": "We present a technique for weakly supervised object localization (WSOL), building on the observation that WSOL algorithms usually work better on images with bigger objects. Instead of training the object detector on the entire training set at the same time, we propose a curriculum learning strategy to feed training images into the WSOL learning loop in an order from images containing bigger objects down to smaller ones. To automatically determine the order, we train a regressor to estimate the size of the object given the whole image as input. Furthermore, we use these size estimates to further improve the re-localization step of WSOL by assigning weights to object proposals according to how close their size matches the estimated object size. We demonstrate the effectiveness of using size order and size weighting on the challenging PASCAL VOC 2007 dataset, where we achieve a significant improvement over existing state-of-the-art WSOL techniques.",
                    "[8502955 | Pentina et al. | 2014 | Citations: 245]": "Sharing information between multiple tasks enables algorithms to achieve good generalization performance even from small amounts of training data. However, in a realistic scenario of multi-task learning not all tasks are equally related to each other, hence it could be advantageous to transfer information only between the most related tasks. In this work we propose an approach that processes multiple tasks in a sequence with sharing between subsequent tasks instead of solving all tasks jointly. Subsequently, we address the question of curriculum learning of tasks, i.e. finding the best order of tasks to be learned. Our approach is based on a generalization bound criterion for choosing the task order that optimizes the average expected classification performance over all tasks. Our experimental results show that learning multiple related tasks sequentially can be more effective than learning them jointly, the order in which tasks are being solved affects the overall performance, and that our model is able to automatically discover a favourable order of tasks."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 1066,
                        "end": 1517,
                        "sentence_offsets": [
                            {
                                "start": 1066,
                                "end": 1166
                            },
                            {
                                "start": 1166,
                                "end": 1286
                            },
                            {
                                "start": 1286,
                                "end": 1418
                            },
                            {
                                "start": 1418,
                                "end": 1517
                            }
                        ],
                        "ref_mentions": [
                            "6954583",
                            "208138033",
                            "873046",
                            "51606954",
                            "26468344",
                            "166228313",
                            "233433844",
                            "204539326",
                            "8502955",
                            "221995570"
                        ],
                        "quote": "Researchers have made significant strides in finding improved metrics for various domains and tasks.For instance, images containing fewer and larger objects in computer vision are deemed easier than other images (Shi et al., 2016)(Soviany et al., 2019).In natural language processing, word frequency (Bengio et al., 2009)(Liu et al., 2018) and sequence length [8](Kocmi et al., 2017)(Tay et al., 2019)(Zhang et al., 2021) are utilized to assess the sample difficulty.In some cases, researchers have also integrated human feedback into their metric design (Jim\u00e9nez-S\u00e1nchez et al., 2019)(Pentina et al., 2014)(Wei et al., 2020)."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[273098749 | Xie et al. | 2024 | Citations: 2]",
                "snippets": "Metrics that measure model confidence include the entropy of the prediction vector (Coleman et al., 2019) and the variance of the predicted probabilities across training epochs (Swayamdipta et al., 2020). Metrics that measure model error include EL2N (Paul et al., 2021), which calculates the L 2 norm of the error vector, and the Forgetting score (Toneva et al., 2018), which tracks the frequency of misclassifications after initial correct classifications. AUM (Pleiss et al., 2020) combines both perspectives by measuring the confidence for correct classifications and the error for misclassifications.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[195750622 | Coleman et al. | 2019 | Citations: 349]": "Data selection methods such as active learning and core-set selection are useful tools for machine learning on large datasets, but they can be prohibitively expensive to apply in deep learning. Unlike in other areas of machine learning, the feature representations that these techniques depend on are learned in deep learning rather than given, which takes a substantial amount of training time. In this work, we show that we can significantly improve the computational efficiency of data selection in deep learning by using a much smaller proxy model to perform data selection for tasks that will eventually require a large target model (e.g., selecting data points to label for active learning). In deep learning, we can scale down models by removing hidden layers or reducing their dimension to create proxies that are an order of magnitude faster. Although these small proxy models have significantly higher error, we find that they empirically provide useful rankings for data selection that have a high correlation with those of larger models. We evaluate this \"selection via proxy\" (SVP) approach on several data selection tasks. For active learning, applying SVP to Sener and Savarese [2018]'s recent method for active learning in deep learning gives a 4x improvement in execution time while yielding the same model accuracy. For core-set selection, we show that a proxy model that trains 10x faster than a target ResNet164 model on CIFAR10 can be used to remove 50% of the training data without compromising the accuracy of the target model, making end-to-end training time improvements via core-set selection possible.",
                    "[210932316 | Pleiss et al. | 2020 | Citations: 274]": "Not all data in a typical training set help with generalization; some samples can be overly ambiguous or outrightly mislabeled. This paper introduces a new method to identify such samples and mitigate their impact when training neural networks. At the heart of our algorithm is the Area Under the Margin (AUM) statistic, which exploits differences in the training dynamics of clean and mislabeled samples. A simple procedure - adding an extra class populated with purposefully mislabeled indicator samples - learns a threshold that isolates mislabeled data based on this metric. This approach consistently improves upon prior work on synthetic and real-world datasets. On the WebVision50 classification task our method removes 17% of training data, yielding a 2.6% (absolute) improvement in test error. On CIFAR100 removing 13% of the data leads to a 1.2% drop in error.",
                    "[221856637 | Swayamdipta et al. | 2020 | Citations: 452]": "Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps---a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example---the model's confidence in the true class, and the variability of this confidence across epochs---obtained in a single run of training. Experiments across four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of \"ambiguous\" regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are \"easy to learn\" for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds \"hard to learn\"; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization.",
                    "[235898952 | Paul et al. | 2021 | Citations: 461]": "Recent success in deep learning has partially been driven by training increasingly overparametrized networks on ever larger datasets. It is therefore natural to ask: how much of the data is superfluous, which examples are important for generalization, and how do we find them? In this work, we make the striking observation that, in standard vision datasets, simple scores averaged over several weight initializations can be used to identify important examples very early in training. We propose two such scores -- the Gradient Normed (GraNd) and the Error L2-Norm (EL2N) scores -- and demonstrate their efficacy on a range of architectures and datasets by pruning significant fractions of training data without sacrificing test accuracy. In fact, using EL2N scores calculated a few epochs into training, we can prune half of the CIFAR10 training set while slightly improving test accuracy. Furthermore, for a given dataset, EL2N scores from one architecture or hyperparameter configuration generalize to other configurations. Compared to recent work that prunes data by discarding examples that are rarely forgotten over the course of training, our scores use only local information early in training. We also use our scores to detect noisy examples and study training dynamics through the lens of important examples -- we investigate how the data distribution shapes the loss surface and identify subspaces of the model's data representation that are relatively stable over training.",
                    "[55481903 | Toneva et al. | 2018 | Citations: 741]": "Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift. We define a \u201cforgetting event\u201d to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning. Across several benchmark data sets, we find that: (i) certain examples are forgotten with high frequency, and some not at all; (ii) a data set\u2019s (un)forgettable examples generalize across neural architectures; and (iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 402,
                        "end": 1007,
                        "sentence_offsets": [
                            {
                                "start": 402,
                                "end": 606
                            },
                            {
                                "start": 607,
                                "end": 860
                            },
                            {
                                "start": 861,
                                "end": 1007
                            }
                        ],
                        "ref_mentions": [
                            "195750622",
                            "221856637",
                            "235898952",
                            "55481903",
                            "210932316"
                        ],
                        "quote": "Metrics that measure model confidence include the entropy of the prediction vector (Coleman et al., 2019) and the variance of the predicted probabilities across training epochs (Swayamdipta et al., 2020). Metrics that measure model error include EL2N (Paul et al., 2021), which calculates the L 2 norm of the error vector, and the Forgetting score (Toneva et al., 2018), which tracks the frequency of misclassifications after initial correct classifications. AUM (Pleiss et al., 2020) combines both perspectives by measuring the confidence for correct classifications and the error for misclassifications."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[273901573 | Liu et al. | 2024 | Citations: 1]",
                "snippets": "CCL integrates a difficulty measure and a proficiency estimator directly derived from the CL losses observed in both the training and validation phases. This innovation eliminates the dependency on task-specific metrics, streamlining the implementation process.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 261,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 152
                            },
                            {
                                "start": 153,
                                "end": 261
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "CCL integrates a difficulty measure and a proficiency estimator directly derived from the CL losses observed in both the training and validation phases. This innovation eliminates the dependency on task-specific metrics, streamlining the implementation process."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[277398866 | Zhang et al. | 2025 | Citations: 0]",
                "snippets": "For instance, if the student model performs well on easier samples but struggles with harder ones, the distillation process will emphasize these harder samples, providing more detailed and nuanced knowledge transfer from the teacher model. This targeted training helps in improving the student model's performance more efficiently compared to uniform distillation methods. Adaptive KD can be implemented using various techniques, such as adjusting the weight of each sample's loss based on its difficulty or using an adaptive temperature in the softmax function to control the smoothness of the teacher model's predictions.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "1) ADDITIONAL TECHNIQUES AND APPLICATIONS",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 623,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 239
                            },
                            {
                                "start": 240,
                                "end": 372
                            },
                            {
                                "start": 373,
                                "end": 623
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "For instance, if the student model performs well on easier samples but struggles with harder ones, the distillation process will emphasize these harder samples, providing more detailed and nuanced knowledge transfer from the teacher model. This targeted training helps in improving the student model's performance more efficiently compared to uniform distillation methods. Adaptive KD can be implemented using various techniques, such as adjusting the weight of each sample's loss based on its difficulty or using an adaptive temperature in the softmax function to control the smoothness of the teacher model's predictions."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[277667729 | Li et al. | 2025 | Citations: 1]",
                "snippets": "While directly quantifying prompt difficulty for LLM alignment is relatively underexplored, insights can be drawn from methods measuring sample difficulty in broader machine learning contexts. These methods often inform adaptive training strategies. For instance, Zhu et al. (Zhu et al., 2022) provided a formal definition of sample learning difficulty inspired by the bias-variance trade-off in statistical learning theory, proposing a variance-based measure reflecting learning stability. Their work suggests that samples leading to higher variance in model predictions or gradients across training perturbations might be considered more difficult. Other common heuristics for estimating sample difficulty include using training loss magnitude (Li et al., 2022), gradient norms [34], or predictive uncertainty (Aljuhani et al., 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[248810776 | Zhu et al. | 2022 | Citations: 6]": "\u2018\u2018Easy/hard sample\u201d is a popular parlance in machine learning. Learning difficulty of samples refers to how easy/hard a sample is during a learning procedure. An increasing need of measuring learning difficulty demonstrates its importance in machine learning (e.g., difficulty-based weighting learning strategies). Previous literature has proposed a number of learning difficulty measures. However, no comprehensive investigation for learning difficulty is available to date, resulting in that nearly all existing measures are heuristically defined without a rigorous theoretical foundation. This study attempts to conduct a pilot theoretical study for learning difficulty of samples. First, influential factors for learning difficulty are summarized. Under various situations conducted by summarized influential factors, correlations between learning difficulty and two vital criteria of machine learning, namely, generalization error and model complexity, are revealed. Second, a theoretical definition of learning difficulty is proposed on the basis of these two criteria. A practical measure of learning difficulty is proposed under the direction of the theoretical definition by importing the bias-variance trade-off theory. Subsequently, the rationality of theoretical definition and the practical measure are demonstrated, respectively, by analysis of several classical weighting methods and abundant experiments realized under all situations conducted by summarized influential factors. The mentioned weighting methods can be reasonably explained under the proposed theoretical definition and concerned propositions. The comparison in these experiments indicates that the proposed measure significantly outperforms the other measures throughout the experiments.",
                    "[244958599 | Li et al. | 2022 | Citations: 68]": "Imbalanced datasets are commonplace in modern machine learning problems. The presence of under-represented classes or groups with sensitive attributes results in concerns about generalization and fairness. Such concerns are further exacerbated by the fact that large capacity deep nets can perfectly fit the training data and appear to achieve perfect accuracy and fairness during training, but perform poorly during test. To address these challenges, we propose AutoBalance, a bi-level optimization framework that automatically designs a training loss function to optimize a blend of accuracy and fairness-seeking objectives. Specifically, a lower-level problem trains the model weights, and an upper-level problem tunes the loss function by monitoring and optimizing the desired objective over the validation data. Our loss design enables personalized treatment for classes/groups by employing a parametric cross-entropy loss and individualized data augmentation schemes. We evaluate the benefits and performance of our approach for the application scenarios of imbalanced and group-sensitive classification. Extensive empirical evaluations demonstrate the benefits of AutoBalance over state-of-the-art approaches. Our experimental findings are complemented with theoretical insights on loss function design and the benefits of train-validation split. All code is available open-source."
                },
                "metadata": [
                    {
                        "section_title": "C. Quantifying Sample Difficulty for Curriculum Learning",
                        "pdf_hash": "",
                        "start": 202,
                        "end": 992,
                        "sentence_offsets": [
                            {
                                "start": 202,
                                "end": 394
                            },
                            {
                                "start": 395,
                                "end": 451
                            },
                            {
                                "start": 452,
                                "end": 678
                            },
                            {
                                "start": 679,
                                "end": 838
                            },
                            {
                                "start": 839,
                                "end": 992
                            }
                        ],
                        "ref_mentions": [
                            "248810776",
                            "244958599",
                            "252369193"
                        ],
                        "quote": "While directly quantifying prompt difficulty for LLM alignment is relatively underexplored, insights can be drawn from methods measuring sample difficulty in broader machine learning contexts. These methods often inform adaptive training strategies. For instance, Zhu et al. (Zhu et al., 2022) provided a formal definition of sample learning difficulty inspired by the bias-variance trade-off in statistical learning theory, proposing a variance-based measure reflecting learning stability. Their work suggests that samples leading to higher variance in model predictions or gradients across training perturbations might be considered more difficult. Other common heuristics for estimating sample difficulty include using training loss magnitude (Li et al., 2022), gradient norms [34], or predictive uncertainty (Aljuhani et al., 2022)."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.033414,
        "cot": "When examining the user query about effective and recent techniques for measuring data difficulty in constructing curricula for large language model training, I need to organize the quotes into meaningful dimensions that will help craft a comprehensive answer.\n\nFirst, I'll need an Introduction/Background section to establish the foundational concepts of curriculum learning and data difficulty measurement for the reader.\n\nThen, I'll organize the quotes based on the following logical dimensions:\n\n1. \"Introduction and Background\" - This will provide context on what curriculum learning is and why measuring data difficulty is important for LLM training. While there are no specific quotes for this section, it's necessary to introduce the topic.\n\n2. \"Loss-based Difficulty Metrics\" - Loss appears frequently in the quotes as a common and fundamental way to measure data difficulty. Many quotes reference cross-entropy loss, instantaneous loss, average loss over training, and variants. This section should be a synthesis since it needs to explain how loss functions as a difficulty metric rather than just listing types.\n\n3. \"Uncertainty-based Difficulty Metrics\" - Several quotes mention uncertainty as a way to measure difficulty, including model uncertainty, prediction uncertainty, and entropy-based measures. This should be synthesis format to explain the relationship between uncertainty and difficulty.\n\n4. \"Training Dynamics Metrics\" - Many quotes discuss metrics derived from observing model behavior during training, such as confidence, variability, gradient-based measures, and learning percentage. This should be a synthesis to explain how training behavior indicates difficulty.\n\n5. \"Predefined Heuristic Metrics\" - Several quotes mention metrics based on data characteristics rather than model behavior, such as sentence length, word frequency, etc. This should be a list format since it's essentially enumerating different static metrics.\n\n6. \"Combined and Novel Approaches\" - Some quotes discuss innovative combinations of metrics or new approaches for measuring difficulty. This should be synthesis to explain how these approaches integrate multiple signals.\n\n7. \"Automatic Difficulty Measurement\" - Quotes mention automatic approaches that don't require manual definition of difficulty. This should be synthesis to explain how these approaches work.\n\n8. \"Applications and Effectiveness in LLM Training\" - Quotes that specifically discuss the effectiveness of different difficulty metrics in LLM training. This should be synthesis to draw conclusions about which approaches work best specifically for LLMs.\n\nThis organization provides a comprehensive overview of the different approaches to measuring data difficulty for curriculum learning in LLM training, moving from fundamental concepts to more advanced and combined approaches, and finally to their specific applications in LLMs.",
        "plan": {
            "Introduction and Background (synthesis)": [],
            "Loss-based Difficulty Metrics (synthesis)": [
                1,
                6,
                7,
                8,
                9,
                10,
                13,
                17,
                19,
                22
            ],
            "Uncertainty-based Difficulty Metrics (synthesis)": [
                0,
                1,
                7,
                8,
                9,
                12,
                17,
                25
            ],
            "Training Dynamics Metrics (synthesis)": [
                0,
                1,
                6,
                8,
                9,
                18,
                20,
                25,
                28
            ],
            "Predefined Heuristic Metrics (list)": [
                2,
                3,
                14,
                15,
                19,
                21,
                22,
                24
            ],
            "Combined and Novel Approaches (synthesis)": [
                4,
                11,
                16,
                20,
                22,
                26
            ],
            "Automatic Difficulty Measurement (synthesis)": [
                3,
                5,
                7,
                10,
                26
            ],
            "Applications and Effectiveness in LLM Training (synthesis)": [
                5,
                21,
                22,
                23,
                27
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction and Background",
                "tldr": "Data difficulty measurement is crucial for curriculum learning in LLM training, where models learn from progressively more challenging examples. Recent techniques range from simple heuristics to sophisticated training dynamics measures that help optimize the learning process. (LLM Memory)",
                "text": "\nCurriculum learning in Large Language Models (LLMs) involves training models on data ordered by difficulty, starting with simpler examples and gradually introducing more complex ones. This approach mimics human learning and has shown potential to improve model performance, convergence speed, and generalization. Measuring data difficulty accurately is therefore a critical component in constructing effective curricula for LLM training.\n\nResearchers have developed various techniques to quantify the difficulty of training examples. Traditional approaches often relied on simple heuristics like sentence length or vocabulary rarity. However, recent advances have introduced more sophisticated metrics that better capture the true learning challenge of each example. These metrics can be broadly categorized into loss-based measures, uncertainty estimates, training dynamics indicators, and predefined heuristics.\n\nThe effectiveness of curriculum learning depends heavily on how accurately we can measure example difficulty. Recent research has shown that dynamic difficulty assessment\u2014where the difficulty is measured during training rather than predetermined\u2014can be particularly effective for large language models. This approach recognizes that difficulty is not an inherent static property of the data but depends on the current state of the model and its learning progress. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nModern curriculum learning strategies for LLMs often employ automatic techniques to measure and adapt to difficulty levels during training. These approaches aim to optimize the presentation order of training examples to maximize learning efficiency and effectiveness. By carefully constructing curricula based on measured difficulty, researchers have demonstrated improvements in final model performance, training efficiency, and capability on complex reasoning tasks. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Loss-based Difficulty Metrics",
                "tldr": "Loss-based metrics quantify training data difficulty by measuring how challenging it is for a model to learn specific examples, typically using cross-entropy or other loss functions. These metrics enable curriculum learning approaches where models first learn from low-loss \"easy\" examples before progressing to high-loss \"difficult\" ones. (15 sources)",
                "text": "\nLoss-based difficulty metrics are among the most common and straightforward approaches to quantify data difficulty for curriculum learning in LLM training. These metrics leverage the model's training output to assess how challenging each example is for the model to learn. The core assumption is that samples with higher loss values are more difficult for the model to process and learn from <Paper corpusId=\"248810776\" paperTitle=\"(Zhu et al., 2022)\" isShortName></Paper>.\n\nCross-entropy loss is particularly prevalent in this category, serving as a direct indicator of the discrepancy between predicted and actual outcomes. When this loss is high, it indicates that the model struggles to correctly predict the expected output, suggesting greater difficulty <Paper corpusId=\"269756933\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>. Many implementations use this instantaneous loss value from a single training epoch as the difficulty measure, with higher loss corresponding to increased difficulty <Paper corpusId=\"257937538\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>.\n\nSelf-paced learning (SPL), a popular curriculum learning approach, directly employs loss values to determine which examples to prioritize during training. In traditional SPL, the model first learns from low-loss \"easy\" examples, and as training progresses, gradually incorporates higher-loss \"difficult\" examples <Paper corpusId=\"266741778\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"94785766\" paperTitle=\"(Gong et al., 2019)\" isShortName></Paper>. This approach is intuitive and has proven effective across various learning tasks <Paper corpusId=\"248810776\" paperTitle=\"(Zhu et al., 2022)\" isShortName></Paper>.\n\nHowever, relying solely on instantaneous loss values can be problematic. Training loss can provide noisy estimates of sample difficulty due to the phenomenon of data memorization in neural networks <Paper corpusId=\"264819795\" paperTitle=\"(Elgaar et al., 2023)\" isShortName></Paper> <Paper corpusId=\"11455421\" paperTitle=\"(Arpit et al., 2017)\" isShortName></Paper> <Paper corpusId=\"6212000\" paperTitle=\"(Zhang et al., 2016)\" isShortName></Paper>. To address this limitation, researchers have explored variations on basic loss-based metrics:\n\n1. **Average Loss**: Instead of using a single epoch's loss, some approaches calculate the average loss over multiple training epochs or the entire training process to provide a more stable difficulty assessment <Paper corpusId=\"248810776\" paperTitle=\"(Zhu et al., 2022)\" isShortName></Paper>.\n\n2. **Moving Average Loss**: The exponential moving average (EMA) of loss values over time can better capture the temporal aspects of learning difficulty <Paper corpusId=\"257937538\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"220047761\" paperTitle=\"(Zhou et al., 2020)\" isShortName></Paper>.\n\n3. **Loss Change Rate**: Some methods measure the rate of change in loss values between consecutive training iterations, identifying examples that cause significant fluctuations as particularly difficult <Paper corpusId=\"257937538\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>.\n\n4. **Annotation Entropy**: For data with multiple human annotations, the entropy of annotator labels can serve as a proxy for difficulty. Higher entropy (more disagreement among annotators) correlates with examples that models also find difficult <Paper corpusId=\"259370648\" paperTitle=\"(Elgaar et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"222177851\" paperTitle=\"(Nie et al., 2020)\" isShortName></Paper>.\n\n5. **Combined Loss Metrics**: Recent research suggests that using both bias (average loss) and variance terms provides a more comprehensive difficulty measure than using loss alone <Paper corpusId=\"255749213\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper>.\n\nFor automatic curriculum learning, these loss-based metrics can be integrated into training through various weighting schemes. Some approaches divide data into discrete difficulty buckets based on loss thresholds <Paper corpusId=\"259593617\" paperTitle=\"(Zeng et al., 2023)\" isShortName></Paper>, while others implement continuous weighting systems based on normalized loss values <Paper corpusId=\"256460262\" paperTitle=\"(Xu et al., 2023)\" isShortName></Paper>.\n\nDespite their limitations, loss-based metrics remain popular due to their simplicity and effectiveness. They provide a model-centric perspective on difficulty that directly relates to the learning process, making them particularly well-suited for curriculum learning in LLM training <Paper corpusId=\"244119160\" paperTitle=\"(Zhu et al., 2021)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Zhu et al., 2022)",
                        "snippets": [
                            "The partition of training data into different subsets according to their learning difficulties and adoption of separate learning schemes (e.g., weighting) are proven to be useful in many learning tasks [21,43,46,68]. The learning difficulty investigated in this study refers to the degrees of easy or hard to learn of training samples in a given learning task.\n\nNumerous methods are proposed to measure the learning difficulty of a training sample. The most common practice is to leverage the training output (e.g., loss and the predicted value on the true category) of a sample to construct the measurements. In Self-paced Learning (SPL) [26,68], the training loss is used to determine whether a sample is easy or not, and easy samples are first learned. We assume that p i,yi is the prediction on the ground-truth category for a training sample x i . In object detection, the value of (1 \u2212 p i,yi ) is used to indicate the learning difficulty for x i [43] . Given that the training output in an epoch may be unreliable, some methods utilize the average training output of a sample during the training to measure the difficulty. Huang et al. [33] designed a cyclic training procedure, and the model is trained from underfitting to over-fitting in one cycle. The average training loss in the whole cyclic procedure is used as the noisy indicator for a training sample. Feng et al. [20] utilized the magnitude of the loss gradient to measure the learning difficulty of a training sample. A large gradient magnitude indicates a high degree of difficulty.\n\nLearning difficulty is considered as an intrinsic property of data in machine learning [46,75]. Existing measurements are usually based on heuristic cues or inspirations, and they can be divided into the following main categories: \n\n\u2022 Loss-based measurement. This category directly uses the loss as the measure. Most measures fall into this category because it is simple yet effective in various learning tasks. Some methods [68] directly utilize the loss in one epoch as the degree of difficulty. Accordingly, the degrees for the same samples vary in different epochs. Some others utilize the average loss [45] during the partial or whole training procedure for measurement. \u2022 Cross-validation-based measurement. This category adopts a crossvalidation strategy [65]. For example, five-fold cross-validation is performed, and the whole cross-validation is repeated ten times. Consequently, each ELDD training sample receives ten predictions. The value of error predictions is used as the indicator of difficulty. \u2022 Uncertainty-based measurement. This category uses the (model) uncertainty of a sample to measure the difficulty. Aguilar et al. [3] identified hard samples based on the epistemic uncertainty (also known as the model uncertainty). They leveraged the Bayesian Neural Network [67] to infer the epistemic uncertainty. \u2022 Margin-based measurement. This category uses the margin (distance) of a sample to the underlying decision surface as the measurement. The rationale is that a small margin denotes a large difficulty [42,68]. \u2022 Gradient-based measurement. This category uses the loss gradient of a sample to measure the difficulty. Agarwal and Hooker [2] proposed the variance of gradients (VOG) across different epochs to rank data from difficult to easy. They considered that samples with high VOG values are far more difficult for the model to learn. Santiago et al. [54] applied the norm of the gradients to measure the difficulty, and high norms indicate large difficulty for learning."
                        ],
                        "paper": {
                            "corpus_id": 248810776,
                            "title": "Exploring the Learning Difficulty of Data: Theory and Measure",
                            "authors": [
                                {
                                    "authorId": "2152350103",
                                    "name": "Weiyao Zhu"
                                },
                                {
                                    "authorId": "2061463125",
                                    "name": "Ou Wu"
                                },
                                {
                                    "authorId": "2165378509",
                                    "name": "Fengguang Su"
                                },
                                {
                                    "authorId": "2165450358",
                                    "name": "Yingjun Deng"
                                }
                            ],
                            "year": 2022,
                            "venue": "ACM Transactions on Knowledge Discovery from Data",
                            "n_citations": 6
                        },
                        "score": 0.89306640625
                    },
                    {
                        "id": "(Kim et al., 2024)",
                        "snippets": [
                            "In the development of training data for curriculum learning, accurately measuring data difficulty is crucial. Specifically, for LLMs (Large Language Models), determining the degree of data difficulty is challenging. Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021; Nagatsuka et al., 2023). However, these metrics may not fully reflect the complexity of a dataset. It is essential to assess data difficulty from the model's perspective, rather than relying solely on data-specific metrics. Our research proposes a new approach to calculate the degree of difficulty based on a model-centric perspective. By organizing the training dataset according to difficulty using our new metric, we aim to improve the model's performance compared to random shuffling.\n\n3 Methods for Quantitative Difficulty Measurement\n\nIn this section, we introduce a novel methodology for training models that begins with easier tasks and methodically progresses to more challenging ones. This approach requires organizing data by its level of complexity, for which we have established three principal criteria. By arranging the data in an order that goes from less to more difficult, this approach establishes a structured progression for learning.\n\n## Loss\nA high loss signifies a substantial discrepancy between the predicted and actual outcomes, indicating that the model perceives the data as challenging.\n\nWe establish a direct correlation between high loss values and increased difficulty. Therefore, prompts that generate higher loss are considered more challenging for the model to learn. This method computes the cross-entropy loss associated with the answer segment of the prompt."
                        ],
                        "paper": {
                            "corpus_id": 269756933,
                            "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "2301165169",
                                    "name": "Jisu Kim"
                                },
                                {
                                    "authorId": "2301167177",
                                    "name": "Juhwan Lee"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.88037109375
                    },
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "In the process of model training, samples with larger losses are harder to learn for the current stage of the model. Conversely, samples with smaller losses prove that the model has been able to correctly predict or classify that sample, which should reduce the sampling probability of that sample. Using sample loss (Gan et al., 2021)[29] as a difficulty evaluation metric, such as Negative Log Likelihood loss [79], square loss (Tang et al., 2019), and cross-entropy loss (Shu et al., 2019)110,(Guo et al., 2018) as difficulty scores, is widely used in self-paced learning (Kumar et al., 2010) and its variant methods (Li et al., 2016)(Fan et al., 2016)",
                            "Cross entropy is used as a measure of transferability (Shu et al., 2019), domain relevance (Zhan et al., 2021), uncertainty (Zhou et al., 2020), and representativeness (Tang et al., 2019)[114], as in Shu et al. (Shu et al., 2019), where cross entropy loss is used as a measure of sample transferability for solving sample noises of the source domain and distribution shift across domains",
                            "Rather than focusing on the instantaneous loss values of a sample, some studies have focused on its loss value during training, calculating the change in model loss over two consecutive training iterations (Zhou et al., 2020) as a difficulty metric, proving that a sample is very difficult when its loss fluctuates between maximum and minimum values during the sequence. Zhou et al. (Zhou et al., 2021) proposed the exponential moving average (EMA) method for the detection of clean and pseudo-labeled samples."
                        ],
                        "paper": {
                            "corpus_id": 257937538,
                            "title": "A Review of the Evaluation System for Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "144102068",
                                    "name": "Fengchun Liu"
                                },
                                {
                                    "authorId": "2213622721",
                                    "name": "Tong Zhang"
                                },
                                {
                                    "authorId": "1806923",
                                    "name": "Chunying Zhang"
                                },
                                {
                                    "authorId": "145412331",
                                    "name": "Lu Liu"
                                },
                                {
                                    "authorId": "2108947838",
                                    "name": "Liya Wang"
                                },
                                {
                                    "authorId": "2213836973",
                                    "name": "Bin Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Electronics",
                            "n_citations": 6
                        },
                        "score": 0.79248046875
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "In the case of self-paced CL (SPL, where difficulty is measured during training), the loss function is often used as the difficulty measure (Kumar et al., 2010)- (Gong et al., 2019). Initially, during training, samples with higher losses are excluded from optimization. As training advances, the threshold is gradually increased to incorporate more complex data into the optimization process. However, relying solely on loss might not accurately represent the inherent difficulty of data, as difficulty is an intrinsic attribute of samples and should be independent of ground truth labels. To overcome this limitation, we propose employing uncertainty as the difficulty measure for SPL."
                        ],
                        "paper": {
                            "corpus_id": 266741778,
                            "title": "Answering from Sure to Uncertain: Uncertainty-Aware Curriculum Learning for Video Question Answering",
                            "authors": [
                                {
                                    "authorId": "2266086772",
                                    "name": "Haopeng Li"
                                },
                                {
                                    "authorId": "2066160726",
                                    "name": "Qiuhong Ke"
                                },
                                {
                                    "authorId": "2146826373",
                                    "name": "Mingming Gong"
                                },
                                {
                                    "authorId": "2277600596",
                                    "name": "Tom Drummond"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.7392578125
                    },
                    {
                        "id": "(Gong et al., 2019)",
                        "snippets": [
                            "Self-paced learning (SPL) is a recently proposed paradigm to imitate the learning process of humans/animals. SPL involves easier samples into training at first and then gradually takes more complex ones into consideration. Current SPL regimes incorporate a self-paced (SP) regularizer into the learning objective with a gradually increasing pace parameter. Therefore, it is difficult to obtain the solution path of the SPL regime and determine where to optimally stop this increasing process. In this paper, a multiobjective SPL method is proposed to optimize the loss function and the SP regularizer simultaneously. A decomposition-based multiobjective particle swarm optimization algorithm is used to simultaneously optimize the two objectives for obtaining the solutions. In the proposed method, a polynomial soft weighting regularizer is proposed to penalize the loss. Theoretical studies are conducted to show that the previous regularizers are roughly particular cases of the proposed polynomial soft weighting regularizer family. Then an implicit decomposition method is proposed to search the solutions with respect to the sample number involved into training. A set of solutions can be obtained by the proposed method and naturally constitute the solution path of the SPL regime. Then a satisfactory solution can be naturally obtained from these solutions by utilizing some effective tools in evolutionary multiobjective optimization. Experiments on matrix factorization and classification problems demonstrate the effectiveness of the proposed technique."
                        ],
                        "paper": {
                            "corpus_id": 94785766,
                            "title": "Decomposition-Based Evolutionary Multiobjective Optimization to Self-Paced Learning",
                            "authors": [
                                {
                                    "authorId": "144605807",
                                    "name": "Maoguo Gong"
                                },
                                {
                                    "authorId": "2152297783",
                                    "name": "Hao Li"
                                },
                                {
                                    "authorId": "1803714",
                                    "name": "Deyu Meng"
                                },
                                {
                                    "authorId": "144145103",
                                    "name": "Q. Miao"
                                },
                                {
                                    "authorId": "39280862",
                                    "name": "Jia Liu"
                                }
                            ],
                            "year": 2019,
                            "venue": "IEEE Transactions on Evolutionary Computation",
                            "n_citations": 54
                        },
                        "score": 0
                    },
                    {
                        "id": "(Elgaar et al., 2023)",
                        "snippets": [
                            "Second, existing approaches quantify the difficulty of data based on instantaneous training loss. However, training loss provides noisy estimates of sample difficulty due to data memorization (Zhang et al., 2016)(Arpit et al., 2017) in neural models."
                        ],
                        "paper": {
                            "corpus_id": 264819795,
                            "title": "Ling-CL: Understanding NLP Models through Linguistic Curricula",
                            "authors": [
                                {
                                    "authorId": "1659451954",
                                    "name": "Mohamed Elgaar"
                                },
                                {
                                    "authorId": "143656058",
                                    "name": "Hadi Amiri"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 2
                        },
                        "score": 0.76025390625
                    },
                    {
                        "id": "(Arpit et al., 2017)",
                        "snippets": [
                            "We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization."
                        ],
                        "paper": {
                            "corpus_id": 11455421,
                            "title": "A Closer Look at Memorization in Deep Networks",
                            "authors": [
                                {
                                    "authorId": "2309967",
                                    "name": "Devansh Arpit"
                                },
                                {
                                    "authorId": "40569328",
                                    "name": "Stanislaw Jastrzebski"
                                },
                                {
                                    "authorId": "2482072",
                                    "name": "Nicolas Ballas"
                                },
                                {
                                    "authorId": "145055042",
                                    "name": "David Krueger"
                                },
                                {
                                    "authorId": "2286872375",
                                    "name": "Emmanuel Bengio"
                                },
                                {
                                    "authorId": "19308176",
                                    "name": "Maxinder S. Kanwal"
                                },
                                {
                                    "authorId": "3422058",
                                    "name": "Tegan Maharaj"
                                },
                                {
                                    "authorId": "35988982",
                                    "name": "Asja Fischer"
                                },
                                {
                                    "authorId": "1760871",
                                    "name": "Aaron C. Courville"
                                },
                                {
                                    "authorId": "1751762",
                                    "name": "Yoshua Bengio"
                                },
                                {
                                    "authorId": "1388317459",
                                    "name": "Simon Lacoste-Julien"
                                }
                            ],
                            "year": 2017,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 1827
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2016)",
                        "snippets": [
                            "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. \nThrough extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. \nWe interpret our experimental findings by comparison with traditional models."
                        ],
                        "paper": {
                            "corpus_id": 6212000,
                            "title": "Understanding deep learning requires rethinking generalization",
                            "authors": [
                                {
                                    "authorId": "151505981",
                                    "name": "Chiyuan Zhang"
                                },
                                {
                                    "authorId": "1751569",
                                    "name": "Samy Bengio"
                                },
                                {
                                    "authorId": "1775622",
                                    "name": "Moritz Hardt"
                                },
                                {
                                    "authorId": "9229182",
                                    "name": "B. Recht"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                }
                            ],
                            "year": 2016,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 4637
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhou et al., 2020)",
                        "snippets": [
                            "Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule."
                        ],
                        "paper": {
                            "corpus_id": 220047761,
                            "title": "Uncertainty-Aware Curriculum Learning for Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "2110349143",
                                    "name": "Yikai Zhou"
                                },
                                {
                                    "authorId": "21299583",
                                    "name": "Baosong Yang"
                                },
                                {
                                    "authorId": "1758353",
                                    "name": "Derek F. Wong"
                                },
                                {
                                    "authorId": "153379180",
                                    "name": "Yu Wan"
                                },
                                {
                                    "authorId": "1774304",
                                    "name": "Lidia S. Chao"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 96
                        },
                        "score": 0
                    },
                    {
                        "id": "(Elgaar et al._1, 2023)",
                        "snippets": [
                            "Annotation entropy is a natural measure of difficulty (for humans) and may serve as a reliable difficulty metric for models. Entropy of each sample x i is calculated as \u2212 l p c log p c (Shannon, 1948), where c is a class category and p c is the fraction of annotators who chose label c for the sample. The use of entropy is supported in (Nie et al., 2020), reporting a consistent positive correlation between model accuracy and level of human agreement.\n\nFurthermore, moving average of a sample's instantaneous loss is a good metric for difficulty (Zhou et al., 2020). Using a baseline model trained with no curriculum and with default hyperparameters, we collect the loss values of all training instances at intervals of 0.5 epochs and use the average loss as prior knowledge about sample difficulty. We obtain twenty observations of the loss and compute the average for each sample."
                        ],
                        "paper": {
                            "corpus_id": 259370648,
                            "title": "HuCurl: Human-induced Curriculum Discovery",
                            "authors": [
                                {
                                    "authorId": "1659451954",
                                    "name": "Mohamed Elgaar"
                                },
                                {
                                    "authorId": "143656058",
                                    "name": "Hadi Amiri"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 0
                        },
                        "score": 0.7119140625
                    },
                    {
                        "id": "(Nie et al., 2020)",
                        "snippets": [
                            "Despite the subjective nature of many NLP tasks, most NLU evaluations have focused on using the majority label with presumably high agreement as the ground truth. Less attention has been paid to the distribution of human opinions. We collect ChaosNLI, a dataset with a total of 464,500 annotations to study Collective HumAn OpinionS in oft-used NLI evaluation sets. This dataset is created by collecting 100 annotations per example for 3,113 examples in SNLI and MNLI and 1,532 examples in Abductive-NLI. Analysis reveals that: (1) high human disagreement exists in a noticeable amount of examples in these datasets; (2) the state-of-the-art models lack the ability to recover the distribution over human labels; (3) models achieve near-perfect accuracy on the subset of data with a high level of human agreement, whereas they can barely beat a random guess on the data with low levels of human agreement, which compose most of the common errors made by state-of-the-art models on the evaluation sets. This questions the validity of improving model performance on old metrics for the low-agreement part of evaluation datasets. Hence, we argue for a detailed examination of human agreement in future data collection efforts, and evaluating model outputs against the distribution over collective human opinions. The ChaosNLI dataset and experimental scripts are available at this https URL"
                        ],
                        "paper": {
                            "corpus_id": 222177851,
                            "title": "What Can We Learn from Collective Human Opinions on Natural Language Inference Data?",
                            "authors": [
                                {
                                    "authorId": "40383658",
                                    "name": "Yixin Nie"
                                },
                                {
                                    "authorId": "144401309",
                                    "name": "Xiang Zhou"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 138
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhou et al., 2023)",
                        "snippets": [
                            "The commonly used difficulty measures, such as loss (Kumar et al., 2010) and gradient norm (Santiago et al., 2021), are mainly related to the bias term. Shin et al. [27] emphasized that only using loss as the measurement cannot distinguish clean and noisy samples, especially for uniform label noise. There are also a few existing studies that use variance (Chang et al., 2017)[29]. For instance, Agarwal et al. [30] applied the variance of gradient norms as the difficulty measure. Indeed, both the variance and bias terms should not be underestimated when measuring the samples' learning difficulty. Our theoretical analyses support that generalization error including both the two terms can capture four main factors influencing the samples' learning difficulty. Thus, the error can be leveraged as a universal measure that is more reasonable than existing measures. Existing studies generally apply the K-fold cross-validation method (Yang et al., 2020) to calculate the generalization error."
                        ],
                        "paper": {
                            "corpus_id": 255749213,
                            "title": "Understanding Difficulty-based Sample Weighting with a Universal Difficulty Measure",
                            "authors": [
                                {
                                    "authorId": "48667407",
                                    "name": "Xiaoling Zhou"
                                },
                                {
                                    "authorId": "2061463107",
                                    "name": "Ou Wu"
                                },
                                {
                                    "authorId": "2152350103",
                                    "name": "Weiyao Zhu"
                                },
                                {
                                    "authorId": "2200527528",
                                    "name": "Ziyang Liang"
                                }
                            ],
                            "year": 2023,
                            "venue": "ECML/PKDD",
                            "n_citations": 5
                        },
                        "score": 0.62255859375
                    },
                    {
                        "id": "(Zeng et al., 2023)",
                        "snippets": [
                            "Automatic. Though pre-defined strategies have been demonstrated effective in various application scenarios, they are often plagued by their strong reliance on human expertise to define an appropriate difficulty measurer and an extensive search for effective combinations of difficulty measurer and training scheduler. Therefore, in addition to the four pre-defined CL strategies described above, we further designed an automatic difficulty measurer to dynamically select data samples based on instance-wise training loss and enable a more flexible training process. Specifically, the automatic difficulty measurer used in this study characterizes data samples as either easy and difficult, which represents the samples whose ground truth scores are correctly or incorrectly predicted by a model in a training epoch."
                        ],
                        "paper": {
                            "corpus_id": 259593617,
                            "title": "On the Effectiveness of Curriculum Learning in Educational Text Scoring",
                            "authors": [
                                {
                                    "authorId": "8647899",
                                    "name": "Zijie Zeng"
                                },
                                {
                                    "authorId": "65953975",
                                    "name": "D. Ga\u0161evi\u0107"
                                },
                                {
                                    "authorId": "49901492",
                                    "name": "Guangliang Chen"
                                }
                            ],
                            "year": 2023,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 6
                        },
                        "score": 0.623046875
                    },
                    {
                        "id": "(Xu et al., 2023)",
                        "snippets": [
                            "To alleviate these problems, various automatic difficulty measurers have been developed and explored in the literature, including self-paced learning (Kumar et al., 2010), transfer teacher [69], reinforcement learning teacher (Graves et al., 2017), and other automatic difficulty measurers (Jiang et al., 2017)(Sanger, 1994)[54]. Inspired by these methods, we modify difficulty scores automatically with prediction errors, which are critical indicators of CPS uncertainty. Substantial work has been conducted in the literature, demonstrating the importance of handling uncertainties in CPS security and safety [21,(Han et al., 2022)(Ma et al., 2019)(Xu et al., 2022)(Zhang et al., 2019)[68]",
                            "we define the following two types of automatic difficulty measurers: Hamming Distance-based Measurer (HDM, Definition 5.1) and Cross Entropy-based Measurer (CEM, Definition 5.2). Hamming distance is commonly used to calculate the difference of two strings of equal length, while cross-entropy loss estimates uncertainty by comparing real distribution and prediction."
                        ],
                        "paper": {
                            "corpus_id": 256460262,
                            "title": "Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems",
                            "authors": [
                                {
                                    "authorId": "2118221146",
                                    "name": "Qinghua Xu"
                                },
                                {
                                    "authorId": "47495965",
                                    "name": "Sajid Ali"
                                },
                                {
                                    "authorId": "145456500",
                                    "name": "T. Yue"
                                }
                            ],
                            "year": 2023,
                            "venue": "ACM Transactions on Software Engineering and Methodology",
                            "n_citations": 22
                        },
                        "score": 0.658203125
                    },
                    {
                        "id": "(Zhu et al., 2021)",
                        "snippets": [
                            "Curriculum learning, a machine training strategy that feeds training instances to the model from easy to hard, has been proven to facili-tate the dialogue generation task. Meanwhile, knowledge distillation, a knowledge transformation methodology among teachers and students networks can yield signi\ufb01cant performance boost for student models. Hence, in this paper, we introduce a combination of curriculum learning and knowledge distillation for ef\ufb01cient dialogue generation models, where curriculum learning can help knowledge distillation from data and model aspects. To start with, from the data aspect, we cluster the training cases according to their complexity, which is calculated by various types of features such as sentence length and coherence between dialog pairs. Furthermore, we employ an adversarial training strategy to identify the complexity of cases from model level. The intuition is that, if a discriminator can tell the generated response is from the teacher or the student, then the case is dif\ufb01cult that the student model has not adapted to yet. Finally, we use self-paced learning, which is an extension to curriculum learning to assign weights for distillation. In conclusion, we arrange a hierarchical curriculum based on the above two aspects for the student model under the guidance from the teacher model. Experimental results demon-strate that our methods achieve improvements compared with competitive baselines."
                        ],
                        "paper": {
                            "corpus_id": 244119160,
                            "title": "Combining Curriculum Learning and Knowledge Distillation for Dialogue Generation",
                            "authors": [
                                {
                                    "authorId": "2152206274",
                                    "name": "Qingqing Zhu"
                                },
                                {
                                    "authorId": "2116950235",
                                    "name": "Xiuying Chen"
                                },
                                {
                                    "authorId": "2111192980",
                                    "name": "Pengfei Wu"
                                },
                                {
                                    "authorId": "2108414477",
                                    "name": "Junfei Liu"
                                },
                                {
                                    "authorId": "144060462",
                                    "name": "Dongyan Zhao"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 27
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Uncertainty-based Difficulty Metrics",
                "tldr": "Uncertainty-based metrics assess training data difficulty by measuring model confidence and variability in predictions, capturing examples where the model exhibits high uncertainty or inconsistency across training epochs. (12 sources)",
                "text": "\nUncertainty-based difficulty metrics offer a different approach to measuring data complexity compared to loss-based metrics, focusing on the model's confidence and consistency in its predictions rather than just the magnitude of error. These metrics aim to identify samples where the model exhibits high uncertainty, suggesting they may be more challenging to learn.\n\nOne primary uncertainty metric is **prediction variability**, which measures the standard deviation of probabilities assigned to the gold label across multiple training epochs <Paper corpusId=\"247694098\" paperTitle=\"(Christopoulou et al., 2022)\" isShortName></Paper>. High variability values indicate greater uncertainty for a training example, suggesting that the model's confidence fluctuates significantly during training. While low variability scores alone don't provide clear information about difficulty, high scores reliably identify uncertain examples that may require special attention in curriculum learning.\n\nAnother widely used metric is **prediction confidence**, calculated as the average probability assigned to the gold label by a model across multiple epochs <Paper corpusId=\"247694098\" paperTitle=\"(Christopoulou et al., 2022)\" isShortName></Paper>. This is a continuous metric where higher values correspond to easier examples, providing a direct measure of how confidently the model can identify the correct label. Both confidence and variability can be used in combination, with confidence serving as a primary metric and variability as an auxiliary one.\n\nThe **entropy of model predictions** offers another perspective on uncertainty <Paper corpusId=\"273098749\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper> <Paper corpusId=\"264451551\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>. Higher entropy in the prediction vector indicates the model is distributing probability mass across multiple possible outputs rather than confidently predicting a single class, suggesting the example is more difficult or ambiguous. Coleman et al. developed entropy-based metrics that have proven effective for data selection tasks <Paper corpusId=\"273098749\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>.\n\nIn addition to model-based uncertainty, some approaches leverage **annotation entropy** as a proxy for difficulty <Paper corpusId=\"259370648\" paperTitle=\"(Elgaar et al._1, 2023)\" isShortName></Paper>. This metric calculates the entropy of human annotator labels, with higher entropy (more disagreement among annotators) indicating samples that are likely more difficult for models as well. Research by Nie et al. has shown a consistent positive correlation between model accuracy and the level of human agreement, supporting annotation entropy as a reliable difficulty metric <Paper corpusId=\"259370648\" paperTitle=\"(Elgaar et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"222177851\" paperTitle=\"(Nie et al., 2020)\" isShortName></Paper>.\n\nFor more sophisticated uncertainty estimation, some methods employ **Bayesian Neural Networks** to infer epistemic uncertainty (also known as model uncertainty) <Paper corpusId=\"248810776\" paperTitle=\"(Zhu et al., 2022)\" isShortName></Paper>. Alternatively, the **variance of gradients** across different epochs can rank data from difficult to easy, with the assumption that samples with high variance of gradients are more challenging for the model to learn <Paper corpusId=\"248810776\" paperTitle=\"(Zhu et al., 2022)\" isShortName></Paper>.\n\nSome researchers have integrated uncertainty-based metrics into self-paced learning frameworks. Li et al. proposed using uncertainty as the difficulty measure for self-paced learning instead of relying solely on loss values, arguing that difficulty is an intrinsic attribute of samples and should be independent of ground truth labels <Paper corpusId=\"266741778\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. Similarly, Zhou et al. developed an uncertainty-aware curriculum learning approach for neural machine translation that uses model uncertainty to govern the learning schedule <Paper corpusId=\"257937538\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"220047761\" paperTitle=\"(Zhou et al., 2020)\" isShortName></Paper>.\n\nCross-entropy can also function as an uncertainty measure. Xu et al. defined a Cross Entropy-based Measurer (CEM) that estimates uncertainty by comparing real distribution and prediction <Paper corpusId=\"256460262\" paperTitle=\"(Xu et al., 2023)\" isShortName></Paper>. This approach can be particularly useful for measuring sample transferability and handling domain relevance in adaptation tasks <Paper corpusId=\"257937538\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"69357784\" paperTitle=\"(Shu et al., 2019)\" isShortName></Paper>.\n\nRecent research has also explored composite metrics that combine different aspects of uncertainty. Swayamdipta et al. introduced a framework that uses both the model's confidence in the true class and the variability of this confidence across epochs to characterize datasets, revealing distinct regions in the data that contribute differently to model learning <Paper corpusId=\"273098749\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper> <Paper corpusId=\"221856637\" paperTitle=\"(Swayamdipta et al., 2020)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Christopoulou et al., 2022)",
                        "snippets": [
                            "In this work, instead, we employ CL for NLU by taking advantage of training dynamics as difficulty metrics, i.e., statistics that measure the behavior of the model at hand on specific task-data instances during training and propose modifications of existing CL schedulers based on these statistics.\n\nWe list three difficulty metrics that use statistics during the course of a model's training, as follows: CONFIDENCE (CONF) of an example x i is the average probability assigned to the gold label y i by a model with parameters \u03b8 across a number of epochs E. This is a continuous metric with higher values corresponding to easier examples. \n\nVARIABILITY (VAR) of an example x i is the standard deviation of the probabilities assigned to the gold label y i across E epochs. It is a continuous metric with higher values indicating greater uncertainty for a training example. \n\nConfidence and correctness are the primary metrics that we use in our curricula since low and high values correspond to hard and easy examples respectively. On the other hand, variability is used as an auxiliary metric since only high scores clearly represent uncertain examples while low scores offer no important information on their own."
                        ],
                        "paper": {
                            "corpus_id": 247694098,
                            "title": "Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU",
                            "authors": [
                                {
                                    "authorId": "48810605",
                                    "name": "Fenia Christopoulou"
                                },
                                {
                                    "authorId": "2346538",
                                    "name": "Gerasimos Lampouras"
                                },
                                {
                                    "authorId": "2676143",
                                    "name": "Ignacio Iacobacci"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 4
                        },
                        "score": 0.72509765625
                    },
                    {
                        "id": "(Xie et al., 2024)",
                        "snippets": [
                            "Metrics that measure model confidence include the entropy of the prediction vector (Coleman et al., 2019) and the variance of the predicted probabilities across training epochs (Swayamdipta et al., 2020). Metrics that measure model error include EL2N (Paul et al., 2021), which calculates the L 2 norm of the error vector, and the Forgetting score (Toneva et al., 2018), which tracks the frequency of misclassifications after initial correct classifications. AUM (Pleiss et al., 2020) combines both perspectives by measuring the confidence for correct classifications and the error for misclassifications."
                        ],
                        "paper": {
                            "corpus_id": 273098749,
                            "title": "Structural-Entropy-Based Sample Selection for Efficient and Effective Learning",
                            "authors": [
                                {
                                    "authorId": "2324006090",
                                    "name": "Tianchi Xie"
                                },
                                {
                                    "authorId": "2316675548",
                                    "name": "Jiangning Zhu"
                                },
                                {
                                    "authorId": "2324704341",
                                    "name": "Guozu Ma"
                                },
                                {
                                    "authorId": "2324761714",
                                    "name": "Minzhi Lin"
                                },
                                {
                                    "authorId": "2324226156",
                                    "name": "Wei Chen"
                                },
                                {
                                    "authorId": "1604645934",
                                    "name": "Weikai Yang"
                                },
                                {
                                    "authorId": "2257322120",
                                    "name": "Shixia Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.75048828125
                    },
                    {
                        "id": "(Wu et al., 2023)",
                        "snippets": [
                            "As shown in Fig. 7, there are five major manners to measure learning difficulty of samples, namely, loss-based, gradient-based, uncertainty-based, multi-training-based, and distance-based",
                            "Model uncertainty can be calculated based on information entropy of the DNN prediction or the variance of multiple predictions output by a DNN with the dropout trick (Kendall et al., 2017). \u2022 Diversity. This type aims to identify the diversity of a subset of training samples. The subset is usually a category. The measurement for subset diversity is useful in the design of data augmentation strategy for the subset (Kumar et al., 2019) and data selection (Su et al., 2022). Friedman and Dieng [80] leveraged the exponential of the Shannon entropy of the eigenvalues of a similarity matrix, namely, vendi score to measure diversity."
                        ],
                        "paper": {
                            "corpus_id": 264451551,
                            "title": "Data Optimization in Deep Learning: A Survey",
                            "authors": [
                                {
                                    "authorId": "2261492235",
                                    "name": "Ou Wu"
                                },
                                {
                                    "authorId": "7304193",
                                    "name": "Rujing Yao"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Transactions on Knowledge and Data Engineering",
                            "n_citations": 1
                        },
                        "score": 0.85693359375
                    },
                    {
                        "id": "(Elgaar et al._1, 2023)",
                        "snippets": [
                            "Annotation entropy is a natural measure of difficulty (for humans) and may serve as a reliable difficulty metric for models. Entropy of each sample x i is calculated as \u2212 l p c log p c (Shannon, 1948), where c is a class category and p c is the fraction of annotators who chose label c for the sample. The use of entropy is supported in (Nie et al., 2020), reporting a consistent positive correlation between model accuracy and level of human agreement.\n\nFurthermore, moving average of a sample's instantaneous loss is a good metric for difficulty (Zhou et al., 2020). Using a baseline model trained with no curriculum and with default hyperparameters, we collect the loss values of all training instances at intervals of 0.5 epochs and use the average loss as prior knowledge about sample difficulty. We obtain twenty observations of the loss and compute the average for each sample."
                        ],
                        "paper": {
                            "corpus_id": 259370648,
                            "title": "HuCurl: Human-induced Curriculum Discovery",
                            "authors": [
                                {
                                    "authorId": "1659451954",
                                    "name": "Mohamed Elgaar"
                                },
                                {
                                    "authorId": "143656058",
                                    "name": "Hadi Amiri"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 0
                        },
                        "score": 0.7119140625
                    },
                    {
                        "id": "(Nie et al., 2020)",
                        "snippets": [
                            "Despite the subjective nature of many NLP tasks, most NLU evaluations have focused on using the majority label with presumably high agreement as the ground truth. Less attention has been paid to the distribution of human opinions. We collect ChaosNLI, a dataset with a total of 464,500 annotations to study Collective HumAn OpinionS in oft-used NLI evaluation sets. This dataset is created by collecting 100 annotations per example for 3,113 examples in SNLI and MNLI and 1,532 examples in Abductive-NLI. Analysis reveals that: (1) high human disagreement exists in a noticeable amount of examples in these datasets; (2) the state-of-the-art models lack the ability to recover the distribution over human labels; (3) models achieve near-perfect accuracy on the subset of data with a high level of human agreement, whereas they can barely beat a random guess on the data with low levels of human agreement, which compose most of the common errors made by state-of-the-art models on the evaluation sets. This questions the validity of improving model performance on old metrics for the low-agreement part of evaluation datasets. Hence, we argue for a detailed examination of human agreement in future data collection efforts, and evaluating model outputs against the distribution over collective human opinions. The ChaosNLI dataset and experimental scripts are available at this https URL"
                        ],
                        "paper": {
                            "corpus_id": 222177851,
                            "title": "What Can We Learn from Collective Human Opinions on Natural Language Inference Data?",
                            "authors": [
                                {
                                    "authorId": "40383658",
                                    "name": "Yixin Nie"
                                },
                                {
                                    "authorId": "144401309",
                                    "name": "Xiang Zhou"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 138
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhu et al., 2022)",
                        "snippets": [
                            "The partition of training data into different subsets according to their learning difficulties and adoption of separate learning schemes (e.g., weighting) are proven to be useful in many learning tasks [21,43,46,68]. The learning difficulty investigated in this study refers to the degrees of easy or hard to learn of training samples in a given learning task.\n\nNumerous methods are proposed to measure the learning difficulty of a training sample. The most common practice is to leverage the training output (e.g., loss and the predicted value on the true category) of a sample to construct the measurements. In Self-paced Learning (SPL) [26,68], the training loss is used to determine whether a sample is easy or not, and easy samples are first learned. We assume that p i,yi is the prediction on the ground-truth category for a training sample x i . In object detection, the value of (1 \u2212 p i,yi ) is used to indicate the learning difficulty for x i [43] . Given that the training output in an epoch may be unreliable, some methods utilize the average training output of a sample during the training to measure the difficulty. Huang et al. [33] designed a cyclic training procedure, and the model is trained from underfitting to over-fitting in one cycle. The average training loss in the whole cyclic procedure is used as the noisy indicator for a training sample. Feng et al. [20] utilized the magnitude of the loss gradient to measure the learning difficulty of a training sample. A large gradient magnitude indicates a high degree of difficulty.\n\nLearning difficulty is considered as an intrinsic property of data in machine learning [46,75]. Existing measurements are usually based on heuristic cues or inspirations, and they can be divided into the following main categories: \n\n\u2022 Loss-based measurement. This category directly uses the loss as the measure. Most measures fall into this category because it is simple yet effective in various learning tasks. Some methods [68] directly utilize the loss in one epoch as the degree of difficulty. Accordingly, the degrees for the same samples vary in different epochs. Some others utilize the average loss [45] during the partial or whole training procedure for measurement. \u2022 Cross-validation-based measurement. This category adopts a crossvalidation strategy [65]. For example, five-fold cross-validation is performed, and the whole cross-validation is repeated ten times. Consequently, each ELDD training sample receives ten predictions. The value of error predictions is used as the indicator of difficulty. \u2022 Uncertainty-based measurement. This category uses the (model) uncertainty of a sample to measure the difficulty. Aguilar et al. [3] identified hard samples based on the epistemic uncertainty (also known as the model uncertainty). They leveraged the Bayesian Neural Network [67] to infer the epistemic uncertainty. \u2022 Margin-based measurement. This category uses the margin (distance) of a sample to the underlying decision surface as the measurement. The rationale is that a small margin denotes a large difficulty [42,68]. \u2022 Gradient-based measurement. This category uses the loss gradient of a sample to measure the difficulty. Agarwal and Hooker [2] proposed the variance of gradients (VOG) across different epochs to rank data from difficult to easy. They considered that samples with high VOG values are far more difficult for the model to learn. Santiago et al. [54] applied the norm of the gradients to measure the difficulty, and high norms indicate large difficulty for learning."
                        ],
                        "paper": {
                            "corpus_id": 248810776,
                            "title": "Exploring the Learning Difficulty of Data: Theory and Measure",
                            "authors": [
                                {
                                    "authorId": "2152350103",
                                    "name": "Weiyao Zhu"
                                },
                                {
                                    "authorId": "2061463125",
                                    "name": "Ou Wu"
                                },
                                {
                                    "authorId": "2165378509",
                                    "name": "Fengguang Su"
                                },
                                {
                                    "authorId": "2165450358",
                                    "name": "Yingjun Deng"
                                }
                            ],
                            "year": 2022,
                            "venue": "ACM Transactions on Knowledge Discovery from Data",
                            "n_citations": 6
                        },
                        "score": 0.89306640625
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "In the case of self-paced CL (SPL, where difficulty is measured during training), the loss function is often used as the difficulty measure (Kumar et al., 2010)- (Gong et al., 2019). Initially, during training, samples with higher losses are excluded from optimization. As training advances, the threshold is gradually increased to incorporate more complex data into the optimization process. However, relying solely on loss might not accurately represent the inherent difficulty of data, as difficulty is an intrinsic attribute of samples and should be independent of ground truth labels. To overcome this limitation, we propose employing uncertainty as the difficulty measure for SPL."
                        ],
                        "paper": {
                            "corpus_id": 266741778,
                            "title": "Answering from Sure to Uncertain: Uncertainty-Aware Curriculum Learning for Video Question Answering",
                            "authors": [
                                {
                                    "authorId": "2266086772",
                                    "name": "Haopeng Li"
                                },
                                {
                                    "authorId": "2066160726",
                                    "name": "Qiuhong Ke"
                                },
                                {
                                    "authorId": "2146826373",
                                    "name": "Mingming Gong"
                                },
                                {
                                    "authorId": "2277600596",
                                    "name": "Tom Drummond"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.7392578125
                    },
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "In the process of model training, samples with larger losses are harder to learn for the current stage of the model. Conversely, samples with smaller losses prove that the model has been able to correctly predict or classify that sample, which should reduce the sampling probability of that sample. Using sample loss (Gan et al., 2021)[29] as a difficulty evaluation metric, such as Negative Log Likelihood loss [79], square loss (Tang et al., 2019), and cross-entropy loss (Shu et al., 2019)110,(Guo et al., 2018) as difficulty scores, is widely used in self-paced learning (Kumar et al., 2010) and its variant methods (Li et al., 2016)(Fan et al., 2016)",
                            "Cross entropy is used as a measure of transferability (Shu et al., 2019), domain relevance (Zhan et al., 2021), uncertainty (Zhou et al., 2020), and representativeness (Tang et al., 2019)[114], as in Shu et al. (Shu et al., 2019), where cross entropy loss is used as a measure of sample transferability for solving sample noises of the source domain and distribution shift across domains",
                            "Rather than focusing on the instantaneous loss values of a sample, some studies have focused on its loss value during training, calculating the change in model loss over two consecutive training iterations (Zhou et al., 2020) as a difficulty metric, proving that a sample is very difficult when its loss fluctuates between maximum and minimum values during the sequence. Zhou et al. (Zhou et al., 2021) proposed the exponential moving average (EMA) method for the detection of clean and pseudo-labeled samples."
                        ],
                        "paper": {
                            "corpus_id": 257937538,
                            "title": "A Review of the Evaluation System for Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "144102068",
                                    "name": "Fengchun Liu"
                                },
                                {
                                    "authorId": "2213622721",
                                    "name": "Tong Zhang"
                                },
                                {
                                    "authorId": "1806923",
                                    "name": "Chunying Zhang"
                                },
                                {
                                    "authorId": "145412331",
                                    "name": "Lu Liu"
                                },
                                {
                                    "authorId": "2108947838",
                                    "name": "Liya Wang"
                                },
                                {
                                    "authorId": "2213836973",
                                    "name": "Bin Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Electronics",
                            "n_citations": 6
                        },
                        "score": 0.79248046875
                    },
                    {
                        "id": "(Zhou et al., 2020)",
                        "snippets": [
                            "Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule."
                        ],
                        "paper": {
                            "corpus_id": 220047761,
                            "title": "Uncertainty-Aware Curriculum Learning for Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "2110349143",
                                    "name": "Yikai Zhou"
                                },
                                {
                                    "authorId": "21299583",
                                    "name": "Baosong Yang"
                                },
                                {
                                    "authorId": "1758353",
                                    "name": "Derek F. Wong"
                                },
                                {
                                    "authorId": "153379180",
                                    "name": "Yu Wan"
                                },
                                {
                                    "authorId": "1774304",
                                    "name": "Lidia S. Chao"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 96
                        },
                        "score": 0
                    },
                    {
                        "id": "(Xu et al., 2023)",
                        "snippets": [
                            "To alleviate these problems, various automatic difficulty measurers have been developed and explored in the literature, including self-paced learning (Kumar et al., 2010), transfer teacher [69], reinforcement learning teacher (Graves et al., 2017), and other automatic difficulty measurers (Jiang et al., 2017)(Sanger, 1994)[54]. Inspired by these methods, we modify difficulty scores automatically with prediction errors, which are critical indicators of CPS uncertainty. Substantial work has been conducted in the literature, demonstrating the importance of handling uncertainties in CPS security and safety [21,(Han et al., 2022)(Ma et al., 2019)(Xu et al., 2022)(Zhang et al., 2019)[68]",
                            "we define the following two types of automatic difficulty measurers: Hamming Distance-based Measurer (HDM, Definition 5.1) and Cross Entropy-based Measurer (CEM, Definition 5.2). Hamming distance is commonly used to calculate the difference of two strings of equal length, while cross-entropy loss estimates uncertainty by comparing real distribution and prediction."
                        ],
                        "paper": {
                            "corpus_id": 256460262,
                            "title": "Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems",
                            "authors": [
                                {
                                    "authorId": "2118221146",
                                    "name": "Qinghua Xu"
                                },
                                {
                                    "authorId": "47495965",
                                    "name": "Sajid Ali"
                                },
                                {
                                    "authorId": "145456500",
                                    "name": "T. Yue"
                                }
                            ],
                            "year": 2023,
                            "venue": "ACM Transactions on Software Engineering and Methodology",
                            "n_citations": 22
                        },
                        "score": 0.658203125
                    },
                    {
                        "id": "(Shu et al., 2019)",
                        "snippets": [
                            "Domain adaptation improves a target task by knowledge transfer from a source domain with rich annotations. It is not uncommon that \"source-domain engineering\" becomes a cumbersome process in domain adaptation: the high-quality source domains highly related to the target domain are hardly available. Thus, weakly-supervised domain adaptation has been introduced to address this difficulty, where we can tolerate the source domain with noises in labels, features, or both. As such, for a particular target task, we simply collect the source domain with coarse labeling or corrupted data. In this paper, we try to address two entangled challenges of weaklysupervised domain adaptation: sample noises of the source domain and distribution shift across domains. To disentangle these challenges, a Transferable Curriculum Learning (TCL) approach is proposed to train the deep networks, guided by a transferable curriculum informing which of the source examples are noiseless and transferable. The approach enhances positive transfer from clean source examples to the target and mitigates negative transfer of noisy source examples. A thorough evaluation shows that our approach significantly outperforms the state-of-the-art on weakly-supervised domain adaptation tasks."
                        ],
                        "paper": {
                            "corpus_id": 69357784,
                            "title": "Transferable Curriculum for Weakly-Supervised Domain Adaptation",
                            "authors": [
                                {
                                    "authorId": "2066476988",
                                    "name": "Yang Shu"
                                },
                                {
                                    "authorId": "3451430",
                                    "name": "Zhangjie Cao"
                                },
                                {
                                    "authorId": "35776445",
                                    "name": "Mingsheng Long"
                                },
                                {
                                    "authorId": "2144499343",
                                    "name": "Jianmin Wang"
                                }
                            ],
                            "year": 2019,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 103
                        },
                        "score": 0
                    },
                    {
                        "id": "(Swayamdipta et al., 2020)",
                        "snippets": [
                            "Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps---a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example---the model's confidence in the true class, and the variability of this confidence across epochs---obtained in a single run of training. Experiments across four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of \"ambiguous\" regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are \"easy to learn\" for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds \"hard to learn\"; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization."
                        ],
                        "paper": {
                            "corpus_id": 221856637,
                            "title": "Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics",
                            "authors": [
                                {
                                    "authorId": "2705113",
                                    "name": "Swabha Swayamdipta"
                                },
                                {
                                    "authorId": "2279023325",
                                    "name": "Roy Schwartz"
                                },
                                {
                                    "authorId": "35219984",
                                    "name": "Nicholas Lourie"
                                },
                                {
                                    "authorId": "1705260",
                                    "name": "Yizhong Wang"
                                },
                                {
                                    "authorId": "2548384",
                                    "name": "Hannaneh Hajishirzi"
                                },
                                {
                                    "authorId": "144365875",
                                    "name": "Noah A. Smith"
                                },
                                {
                                    "authorId": "1699545",
                                    "name": "Yejin Choi"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 452
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Training Dynamics Metrics",
                "tldr": "Training dynamics metrics measure model behavior on specific examples throughout the training process, capturing how consistently and confidently models learn from data. These metrics\u2014including confidence, variability, loss change rate, and gradient-based measures\u2014provide insights into learning difficulty that static or instantaneous metrics cannot. (14 sources)",
                "text": "\nTraining dynamics metrics offer a sophisticated approach to measuring data difficulty by tracking how a model's behavior on specific examples evolves during training. Unlike static heuristics or instantaneous loss values, these metrics provide insights into learning patterns over time, making them particularly valuable for constructing effective curricula for LLM training.\n\nA foundational concept in training dynamics is **confidence**, defined as the average probability assigned to the gold label by a model across multiple training epochs <Paper corpusId=\"247694098\" paperTitle=\"(Christopoulou et al., 2022)\" isShortName></Paper>. Higher confidence values generally indicate easier examples for the model. Complementing this is **variability**, which measures the standard deviation of probabilities assigned to the gold label across epochs, with higher values revealing examples that create greater uncertainty <Paper corpusId=\"247694098\" paperTitle=\"(Christopoulou et al., 2022)\" isShortName></Paper>. These metrics are particularly valuable in curriculum learning, with confidence serving as a primary metric for difficulty assessment while variability helps identify uncertain examples.\n\nThe **change rate of loss** during training provides another valuable perspective on difficulty. Zhou et al. proposed tracking loss fluctuations between training iterations, suggesting that samples whose loss values fluctuate dramatically between maximum and minimum values are particularly difficult <Paper corpusId=\"220047761\" paperTitle=\"(Zhou et al., 2020)\" isShortName></Paper>. Building on this approach, the exponential moving average (EMA) of loss values over time can offer a more stable assessment of difficulty by smoothing out temporary fluctuations <Paper corpusId=\"244958599\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>.\n\n**Learning percentage** represents a novel approach for self-ranking training data. This metric assumes that easier samples show more learning progress in earlier epochs, with the model using this information to prioritize examples accordingly <Paper corpusId=\"267740312\" paperTitle=\"(Mekala et al., 2024)\" isShortName></Paper> <Paper corpusId=\"249060677\" paperTitle=\"(Mekala et al., 2022)\" isShortName></Paper>. This concept aligns with the observation that neural networks tend to learn clean examples before noisy ones, making learning order a valuable signal for difficulty assessment.\n\nGradient-based metrics have emerged as another powerful tool for measuring data difficulty. These include:\n\n1. **Gradient magnitude**: Samples with large gradient magnitudes are considered more difficult for the model to learn <Paper corpusId=\"248810776\" paperTitle=\"(Zhu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"268696658\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>.\n\n2. **Variance of gradients (VOG)**: This metric ranks data from difficult to easy by measuring the variance of gradients across different training epochs, with high variance indicating examples that are more challenging for the model <Paper corpusId=\"248810776\" paperTitle=\"(Zhu et al., 2022)\" isShortName></Paper>.\n\n3. **Gradient-based Difficulty Measure (GDM)**: This approach evaluates example difficulty through dynamic measurement of gradient magnitude with respect to the example itself, offering more fine-grained insights than training loss alone, particularly when different examples yield identical losses <Paper corpusId=\"268696658\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>.\n\nThe bias-variance trade-off from statistical learning theory has informed newer difficulty metrics. Zhu et al. argue that both bias (related to average loss) and variance terms should be considered when measuring sample difficulty <Paper corpusId=\"248810776\" paperTitle=\"(Zhu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"255749213\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper>. Their theoretical analysis suggests that generalization error, which incorporates both terms, can serve as a more comprehensive difficulty measure than approaches focusing solely on bias or variance.\n\nSeveral metrics combine model confidence and error measurements. Coleman et al. developed entropy-based metrics for data selection <Paper corpusId=\"195750622\" paperTitle=\"(Coleman et al., 2019)\" isShortName></Paper>, while Swayamdipta et al. introduced a framework using both confidence in true class predictions and variability of this confidence across epochs to characterize datasets <Paper corpusId=\"221856637\" paperTitle=\"(Swayamdipta et al., 2020)\" isShortName></Paper>. Other notable methods include EL2N (Error L2-Norm), which calculates the L2 norm of the error vector <Paper corpusId=\"235898952\" paperTitle=\"(Paul et al., 2021)\" isShortName></Paper>, and the Forgetting score, which tracks the frequency of misclassifications after initial correct classifications <Paper corpusId=\"55481903\" paperTitle=\"(Toneva et al., 2018)\" isShortName></Paper>.\n\nFinally, **annotation entropy**\u2014measuring human annotator disagreement\u2014serves as an external difficulty metric that often correlates well with model difficulty <Paper corpusId=\"259370648\" paperTitle=\"(Elgaar et al._1, 2023)\" isShortName></Paper>. Research by Nie et al. has established a consistent positive correlation between model accuracy and the level of human agreement, supporting annotation entropy as a reliable difficulty metric <Paper corpusId=\"222177851\" paperTitle=\"(Nie et al., 2020)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Christopoulou et al., 2022)",
                        "snippets": [
                            "In this work, instead, we employ CL for NLU by taking advantage of training dynamics as difficulty metrics, i.e., statistics that measure the behavior of the model at hand on specific task-data instances during training and propose modifications of existing CL schedulers based on these statistics.\n\nWe list three difficulty metrics that use statistics during the course of a model's training, as follows: CONFIDENCE (CONF) of an example x i is the average probability assigned to the gold label y i by a model with parameters \u03b8 across a number of epochs E. This is a continuous metric with higher values corresponding to easier examples. \n\nVARIABILITY (VAR) of an example x i is the standard deviation of the probabilities assigned to the gold label y i across E epochs. It is a continuous metric with higher values indicating greater uncertainty for a training example. \n\nConfidence and correctness are the primary metrics that we use in our curricula since low and high values correspond to hard and easy examples respectively. On the other hand, variability is used as an auxiliary metric since only high scores clearly represent uncertain examples while low scores offer no important information on their own."
                        ],
                        "paper": {
                            "corpus_id": 247694098,
                            "title": "Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU",
                            "authors": [
                                {
                                    "authorId": "48810605",
                                    "name": "Fenia Christopoulou"
                                },
                                {
                                    "authorId": "2346538",
                                    "name": "Gerasimos Lampouras"
                                },
                                {
                                    "authorId": "2676143",
                                    "name": "Ignacio Iacobacci"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 4
                        },
                        "score": 0.72509765625
                    },
                    {
                        "id": "(Zhou et al., 2020)",
                        "snippets": [
                            "Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule."
                        ],
                        "paper": {
                            "corpus_id": 220047761,
                            "title": "Uncertainty-Aware Curriculum Learning for Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "2110349143",
                                    "name": "Yikai Zhou"
                                },
                                {
                                    "authorId": "21299583",
                                    "name": "Baosong Yang"
                                },
                                {
                                    "authorId": "1758353",
                                    "name": "Derek F. Wong"
                                },
                                {
                                    "authorId": "153379180",
                                    "name": "Yu Wan"
                                },
                                {
                                    "authorId": "1774304",
                                    "name": "Lidia S. Chao"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 96
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2022)",
                        "snippets": [
                            "Imbalanced datasets are commonplace in modern machine learning problems. The presence of under-represented classes or groups with sensitive attributes results in concerns about generalization and fairness. Such concerns are further exacerbated by the fact that large capacity deep nets can perfectly fit the training data and appear to achieve perfect accuracy and fairness during training, but perform poorly during test. To address these challenges, we propose AutoBalance, a bi-level optimization framework that automatically designs a training loss function to optimize a blend of accuracy and fairness-seeking objectives. Specifically, a lower-level problem trains the model weights, and an upper-level problem tunes the loss function by monitoring and optimizing the desired objective over the validation data. Our loss design enables personalized treatment for classes/groups by employing a parametric cross-entropy loss and individualized data augmentation schemes. We evaluate the benefits and performance of our approach for the application scenarios of imbalanced and group-sensitive classification. Extensive empirical evaluations demonstrate the benefits of AutoBalance over state-of-the-art approaches. Our experimental findings are complemented with theoretical insights on loss function design and the benefits of train-validation split. All code is available open-source."
                        ],
                        "paper": {
                            "corpus_id": 244958599,
                            "title": "AutoBalance: Optimized Loss Functions for Imbalanced Data",
                            "authors": [
                                {
                                    "authorId": "47629178",
                                    "name": "Mingchen Li"
                                },
                                {
                                    "authorId": "2108029216",
                                    "name": "Xuechen Zhang"
                                },
                                {
                                    "authorId": "2751682",
                                    "name": "Christos Thrampoulidis"
                                },
                                {
                                    "authorId": "2281075331",
                                    "name": "Jiasi Chen"
                                },
                                {
                                    "authorId": "3103394",
                                    "name": "Samet Oymak"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 68
                        },
                        "score": 0
                    },
                    {
                        "id": "(Mekala et al., 2024)",
                        "snippets": [
                            "Drawing inspiration from the learning order metric in (Mekala et al., 2022), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered."
                        ],
                        "paper": {
                            "corpus_id": 267740312,
                            "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
                            "authors": [
                                {
                                    "authorId": "7565696",
                                    "name": "Dheeraj Mekala"
                                },
                                {
                                    "authorId": "2284673632",
                                    "name": "Alex Nguyen"
                                },
                                {
                                    "authorId": "2284595153",
                                    "name": "Jingbo Shang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 21
                        },
                        "score": 0.861328125
                    },
                    {
                        "id": "(Mekala et al., 2022)",
                        "snippets": [
                            "Weakly supervised text classification methods typically train a deep neural classifier based on pseudo-labels. The quality of pseudo-labels is crucial to final performance but they are inevitably noisy due to their heuristic nature, so selecting the correct ones has a huge potential for performance boost. One straightforward solution is to select samples based on the softmax probability scores in the neural classifier corresponding to their pseudo-labels. However, we show through our experiments that such solutions are ineffective and unstable due to the erroneously high-confidence predictions from poorly calibrated models. Recent studies on the memorization effects of deep neural models suggest that these models first memorize training samples with clean labels and then those with noisy labels. Inspired by this observation, we propose a novel pseudo-label selection method LOPS that takes learning order of samples into consideration. We hypothesize that the learning order reflects the probability of wrong annotation in terms of ranking, and therefore, propose to select the samples that are learnt earlier. LOPS can be viewed as a strong performance-boost plug-in to most of existing weakly-supervised text classification methods, as confirmed in extensive experiments on four real-world datasets."
                        ],
                        "paper": {
                            "corpus_id": 249060677,
                            "title": "LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification",
                            "authors": [
                                {
                                    "authorId": "7565696",
                                    "name": "Dheeraj Mekala"
                                },
                                {
                                    "authorId": "2113540861",
                                    "name": "Chengyu Dong"
                                },
                                {
                                    "authorId": "2884976",
                                    "name": "Jingbo Shang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 20
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhu et al., 2022)",
                        "snippets": [
                            "The partition of training data into different subsets according to their learning difficulties and adoption of separate learning schemes (e.g., weighting) are proven to be useful in many learning tasks [21,43,46,68]. The learning difficulty investigated in this study refers to the degrees of easy or hard to learn of training samples in a given learning task.\n\nNumerous methods are proposed to measure the learning difficulty of a training sample. The most common practice is to leverage the training output (e.g., loss and the predicted value on the true category) of a sample to construct the measurements. In Self-paced Learning (SPL) [26,68], the training loss is used to determine whether a sample is easy or not, and easy samples are first learned. We assume that p i,yi is the prediction on the ground-truth category for a training sample x i . In object detection, the value of (1 \u2212 p i,yi ) is used to indicate the learning difficulty for x i [43] . Given that the training output in an epoch may be unreliable, some methods utilize the average training output of a sample during the training to measure the difficulty. Huang et al. [33] designed a cyclic training procedure, and the model is trained from underfitting to over-fitting in one cycle. The average training loss in the whole cyclic procedure is used as the noisy indicator for a training sample. Feng et al. [20] utilized the magnitude of the loss gradient to measure the learning difficulty of a training sample. A large gradient magnitude indicates a high degree of difficulty.\n\nLearning difficulty is considered as an intrinsic property of data in machine learning [46,75]. Existing measurements are usually based on heuristic cues or inspirations, and they can be divided into the following main categories: \n\n\u2022 Loss-based measurement. This category directly uses the loss as the measure. Most measures fall into this category because it is simple yet effective in various learning tasks. Some methods [68] directly utilize the loss in one epoch as the degree of difficulty. Accordingly, the degrees for the same samples vary in different epochs. Some others utilize the average loss [45] during the partial or whole training procedure for measurement. \u2022 Cross-validation-based measurement. This category adopts a crossvalidation strategy [65]. For example, five-fold cross-validation is performed, and the whole cross-validation is repeated ten times. Consequently, each ELDD training sample receives ten predictions. The value of error predictions is used as the indicator of difficulty. \u2022 Uncertainty-based measurement. This category uses the (model) uncertainty of a sample to measure the difficulty. Aguilar et al. [3] identified hard samples based on the epistemic uncertainty (also known as the model uncertainty). They leveraged the Bayesian Neural Network [67] to infer the epistemic uncertainty. \u2022 Margin-based measurement. This category uses the margin (distance) of a sample to the underlying decision surface as the measurement. The rationale is that a small margin denotes a large difficulty [42,68]. \u2022 Gradient-based measurement. This category uses the loss gradient of a sample to measure the difficulty. Agarwal and Hooker [2] proposed the variance of gradients (VOG) across different epochs to rank data from difficult to easy. They considered that samples with high VOG values are far more difficult for the model to learn. Santiago et al. [54] applied the norm of the gradients to measure the difficulty, and high norms indicate large difficulty for learning."
                        ],
                        "paper": {
                            "corpus_id": 248810776,
                            "title": "Exploring the Learning Difficulty of Data: Theory and Measure",
                            "authors": [
                                {
                                    "authorId": "2152350103",
                                    "name": "Weiyao Zhu"
                                },
                                {
                                    "authorId": "2061463125",
                                    "name": "Ou Wu"
                                },
                                {
                                    "authorId": "2165378509",
                                    "name": "Fengguang Su"
                                },
                                {
                                    "authorId": "2165450358",
                                    "name": "Yingjun Deng"
                                }
                            ],
                            "year": 2022,
                            "venue": "ACM Transactions on Knowledge Discovery from Data",
                            "n_citations": 6
                        },
                        "score": 0.89306640625
                    },
                    {
                        "id": "(Zhao et al., 2024)",
                        "snippets": [
                            "Current methods evaluate example difficulty through predefined metrics, such as sentence length, or dynamically update it based on training loss, such as cross-entropy loss. However, predefined difficulty measures do not integrate the model's feedback, and training loss, focusing only on the difference between predictions and ground truth, raises an inaccurate difficulty measurement issue when different examples yield identical training losses.\n\nTo address these limitations, we propose the Gradientbased Difficulty Measure (GDM), which evaluates example difficulty through dynamic measurement of the gradient magnitude with respect to the example itself. Unlike training loss, the gradient avoids inaccurate difficulty assessment by taking input features into account. As a result, even if examples yield the same loss, their gradients can differ. Additionally, loss landscapes can encompass plateaus or saddle points, where training loss remains relatively stable even with substantial shifts in model parameters. In these scenarios, utilizing training loss for evaluating example difficulty can be misleading. Conversely, the gradient provides finergrained insights into changes in model parameters, making the gradient magnitude a more informative approach for evaluating difficulty."
                        ],
                        "paper": {
                            "corpus_id": 268696658,
                            "title": "Symmetric Self-Paced Learning for Domain Generalization",
                            "authors": [
                                {
                                    "authorId": "2110600983",
                                    "name": "Di Zhao"
                                },
                                {
                                    "authorId": "34930533",
                                    "name": "Yun Sing Koh"
                                },
                                {
                                    "authorId": "2276344048",
                                    "name": "Gillian Dobbie"
                                },
                                {
                                    "authorId": "2293565669",
                                    "name": "Hongsheng Hu"
                                },
                                {
                                    "authorId": "2293453267",
                                    "name": "Philippe Fournier-Viger"
                                }
                            ],
                            "year": 2024,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 5
                        },
                        "score": 0.7685546875
                    },
                    {
                        "id": "(Zhou et al., 2023)",
                        "snippets": [
                            "The commonly used difficulty measures, such as loss (Kumar et al., 2010) and gradient norm (Santiago et al., 2021), are mainly related to the bias term. Shin et al. [27] emphasized that only using loss as the measurement cannot distinguish clean and noisy samples, especially for uniform label noise. There are also a few existing studies that use variance (Chang et al., 2017)[29]. For instance, Agarwal et al. [30] applied the variance of gradient norms as the difficulty measure. Indeed, both the variance and bias terms should not be underestimated when measuring the samples' learning difficulty. Our theoretical analyses support that generalization error including both the two terms can capture four main factors influencing the samples' learning difficulty. Thus, the error can be leveraged as a universal measure that is more reasonable than existing measures. Existing studies generally apply the K-fold cross-validation method (Yang et al., 2020) to calculate the generalization error."
                        ],
                        "paper": {
                            "corpus_id": 255749213,
                            "title": "Understanding Difficulty-based Sample Weighting with a Universal Difficulty Measure",
                            "authors": [
                                {
                                    "authorId": "48667407",
                                    "name": "Xiaoling Zhou"
                                },
                                {
                                    "authorId": "2061463107",
                                    "name": "Ou Wu"
                                },
                                {
                                    "authorId": "2152350103",
                                    "name": "Weiyao Zhu"
                                },
                                {
                                    "authorId": "2200527528",
                                    "name": "Ziyang Liang"
                                }
                            ],
                            "year": 2023,
                            "venue": "ECML/PKDD",
                            "n_citations": 5
                        },
                        "score": 0.62255859375
                    },
                    {
                        "id": "(Coleman et al., 2019)",
                        "snippets": [
                            "Data selection methods such as active learning and core-set selection are useful tools for machine learning on large datasets, but they can be prohibitively expensive to apply in deep learning. Unlike in other areas of machine learning, the feature representations that these techniques depend on are learned in deep learning rather than given, which takes a substantial amount of training time. In this work, we show that we can significantly improve the computational efficiency of data selection in deep learning by using a much smaller proxy model to perform data selection for tasks that will eventually require a large target model (e.g., selecting data points to label for active learning). In deep learning, we can scale down models by removing hidden layers or reducing their dimension to create proxies that are an order of magnitude faster. Although these small proxy models have significantly higher error, we find that they empirically provide useful rankings for data selection that have a high correlation with those of larger models. We evaluate this \"selection via proxy\" (SVP) approach on several data selection tasks. For active learning, applying SVP to Sener and Savarese [2018]'s recent method for active learning in deep learning gives a 4x improvement in execution time while yielding the same model accuracy. For core-set selection, we show that a proxy model that trains 10x faster than a target ResNet164 model on CIFAR10 can be used to remove 50% of the training data without compromising the accuracy of the target model, making end-to-end training time improvements via core-set selection possible."
                        ],
                        "paper": {
                            "corpus_id": 195750622,
                            "title": "Selection Via Proxy: Efficient Data Selection For Deep Learning",
                            "authors": [
                                {
                                    "authorId": "2866919",
                                    "name": "Cody A. Coleman"
                                },
                                {
                                    "authorId": "144088780",
                                    "name": "Christopher Yeh"
                                },
                                {
                                    "authorId": "1776721",
                                    "name": "Stephen Mussmann"
                                },
                                {
                                    "authorId": "2389094",
                                    "name": "Baharan Mirzasoleiman"
                                },
                                {
                                    "authorId": "2740804",
                                    "name": "Peter Bailis"
                                },
                                {
                                    "authorId": "145419642",
                                    "name": "Percy Liang"
                                },
                                {
                                    "authorId": "1702139",
                                    "name": "J. Leskovec"
                                },
                                {
                                    "authorId": "143834867",
                                    "name": "M. Zaharia"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 349
                        },
                        "score": 0
                    },
                    {
                        "id": "(Swayamdipta et al., 2020)",
                        "snippets": [
                            "Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps---a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example---the model's confidence in the true class, and the variability of this confidence across epochs---obtained in a single run of training. Experiments across four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of \"ambiguous\" regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are \"easy to learn\" for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds \"hard to learn\"; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization."
                        ],
                        "paper": {
                            "corpus_id": 221856637,
                            "title": "Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics",
                            "authors": [
                                {
                                    "authorId": "2705113",
                                    "name": "Swabha Swayamdipta"
                                },
                                {
                                    "authorId": "2279023325",
                                    "name": "Roy Schwartz"
                                },
                                {
                                    "authorId": "35219984",
                                    "name": "Nicholas Lourie"
                                },
                                {
                                    "authorId": "1705260",
                                    "name": "Yizhong Wang"
                                },
                                {
                                    "authorId": "2548384",
                                    "name": "Hannaneh Hajishirzi"
                                },
                                {
                                    "authorId": "144365875",
                                    "name": "Noah A. Smith"
                                },
                                {
                                    "authorId": "1699545",
                                    "name": "Yejin Choi"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 452
                        },
                        "score": 0
                    },
                    {
                        "id": "(Paul et al., 2021)",
                        "snippets": [
                            "Recent success in deep learning has partially been driven by training increasingly overparametrized networks on ever larger datasets. It is therefore natural to ask: how much of the data is superfluous, which examples are important for generalization, and how do we find them? In this work, we make the striking observation that, in standard vision datasets, simple scores averaged over several weight initializations can be used to identify important examples very early in training. We propose two such scores -- the Gradient Normed (GraNd) and the Error L2-Norm (EL2N) scores -- and demonstrate their efficacy on a range of architectures and datasets by pruning significant fractions of training data without sacrificing test accuracy. In fact, using EL2N scores calculated a few epochs into training, we can prune half of the CIFAR10 training set while slightly improving test accuracy. Furthermore, for a given dataset, EL2N scores from one architecture or hyperparameter configuration generalize to other configurations. Compared to recent work that prunes data by discarding examples that are rarely forgotten over the course of training, our scores use only local information early in training. We also use our scores to detect noisy examples and study training dynamics through the lens of important examples -- we investigate how the data distribution shapes the loss surface and identify subspaces of the model's data representation that are relatively stable over training."
                        ],
                        "paper": {
                            "corpus_id": 235898952,
                            "title": "Deep Learning on a Data Diet: Finding Important Examples Early in Training",
                            "authors": [
                                {
                                    "authorId": "1690452",
                                    "name": "Mansheej Paul"
                                },
                                {
                                    "authorId": "25769960",
                                    "name": "S. Ganguli"
                                },
                                {
                                    "authorId": "2533850",
                                    "name": "G. Dziugaite"
                                }
                            ],
                            "year": 2021,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 461
                        },
                        "score": 0
                    },
                    {
                        "id": "(Toneva et al., 2018)",
                        "snippets": [
                            "Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift. We define a \"forgetting event\" to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning. Across several benchmark data sets, we find that: (i) certain examples are forgotten with high frequency, and some not at all; (ii) a data set\u2019s (un)forgettable examples generalize across neural architectures; and (iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance."
                        ],
                        "paper": {
                            "corpus_id": 55481903,
                            "title": "An Empirical Study of Example Forgetting during Deep Neural Network Learning",
                            "authors": [
                                {
                                    "authorId": "2822168",
                                    "name": "Mariya Toneva"
                                },
                                {
                                    "authorId": "2041695",
                                    "name": "Alessandro Sordoni"
                                },
                                {
                                    "authorId": "15032777",
                                    "name": "R\u00e9mi Tachet des Combes"
                                },
                                {
                                    "authorId": "3382568",
                                    "name": "Adam Trischler"
                                },
                                {
                                    "authorId": "1751762",
                                    "name": "Yoshua Bengio"
                                },
                                {
                                    "authorId": "21889436",
                                    "name": "Geoffrey J. Gordon"
                                }
                            ],
                            "year": 2018,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 741
                        },
                        "score": 0
                    },
                    {
                        "id": "(Elgaar et al._1, 2023)",
                        "snippets": [
                            "Annotation entropy is a natural measure of difficulty (for humans) and may serve as a reliable difficulty metric for models. Entropy of each sample x i is calculated as \u2212 l p c log p c (Shannon, 1948), where c is a class category and p c is the fraction of annotators who chose label c for the sample. The use of entropy is supported in (Nie et al., 2020), reporting a consistent positive correlation between model accuracy and level of human agreement.\n\nFurthermore, moving average of a sample's instantaneous loss is a good metric for difficulty (Zhou et al., 2020). Using a baseline model trained with no curriculum and with default hyperparameters, we collect the loss values of all training instances at intervals of 0.5 epochs and use the average loss as prior knowledge about sample difficulty. We obtain twenty observations of the loss and compute the average for each sample."
                        ],
                        "paper": {
                            "corpus_id": 259370648,
                            "title": "HuCurl: Human-induced Curriculum Discovery",
                            "authors": [
                                {
                                    "authorId": "1659451954",
                                    "name": "Mohamed Elgaar"
                                },
                                {
                                    "authorId": "143656058",
                                    "name": "Hadi Amiri"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 0
                        },
                        "score": 0.7119140625
                    },
                    {
                        "id": "(Nie et al., 2020)",
                        "snippets": [
                            "Despite the subjective nature of many NLP tasks, most NLU evaluations have focused on using the majority label with presumably high agreement as the ground truth. Less attention has been paid to the distribution of human opinions. We collect ChaosNLI, a dataset with a total of 464,500 annotations to study Collective HumAn OpinionS in oft-used NLI evaluation sets. This dataset is created by collecting 100 annotations per example for 3,113 examples in SNLI and MNLI and 1,532 examples in Abductive-NLI. Analysis reveals that: (1) high human disagreement exists in a noticeable amount of examples in these datasets; (2) the state-of-the-art models lack the ability to recover the distribution over human labels; (3) models achieve near-perfect accuracy on the subset of data with a high level of human agreement, whereas they can barely beat a random guess on the data with low levels of human agreement, which compose most of the common errors made by state-of-the-art models on the evaluation sets. This questions the validity of improving model performance on old metrics for the low-agreement part of evaluation datasets. Hence, we argue for a detailed examination of human agreement in future data collection efforts, and evaluating model outputs against the distribution over collective human opinions. The ChaosNLI dataset and experimental scripts are available at this https URL"
                        ],
                        "paper": {
                            "corpus_id": 222177851,
                            "title": "What Can We Learn from Collective Human Opinions on Natural Language Inference Data?",
                            "authors": [
                                {
                                    "authorId": "40383658",
                                    "name": "Yixin Nie"
                                },
                                {
                                    "authorId": "144401309",
                                    "name": "Xiang Zhou"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 138
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Predefined Heuristic Metrics",
                "tldr": "Predefined heuristic metrics use intuitive data characteristics to estimate training example difficulty without model-based computation. Common metrics include text length, word rarity, sentence complexity, and annotation agreement, which can be applied across various domains. (17 sources)",
                "text": "\nPredefined heuristic metrics offer straightforward, computationally efficient approaches to measure data difficulty for curriculum learning in LLM training. Unlike model-based metrics that require training or inference passes, these metrics rely on inherent properties of the data that can be calculated prior to training.\n\nIn natural language processing, the most widely used heuristic metrics include:\n\n1. **Text Length**: Sentence or sequence length is perhaps the most common difficulty metric, based on the assumption that longer texts are more challenging for models to process <Paper corpusId=\"26468344\" paperTitle=\"(Kocmi et al., 2017)\" isShortName></Paper> <Paper corpusId=\"166228313\" paperTitle=\"(Tay et al., 2019)\" isShortName></Paper> <Paper corpusId=\"233433844\" paperTitle=\"(Zhang et al., 2021)\" isShortName></Paper>. This metric has been extensively used in machine translation and reading comprehension tasks <Paper corpusId=\"265552007\" paperTitle=\"(Ding et al., 2023)\" isShortName></Paper> <Paper corpusId=\"269741199\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>.\n\n2. **Word Rarity/Frequency**: Examples containing less frequent words from the training vocabulary are considered more difficult <Paper corpusId=\"51606954\" paperTitle=\"(Liu et al., 2018)\" isShortName></Paper> <Paper corpusId=\"248887633\" paperTitle=\"(Croitoru et al., 2022)\" isShortName></Paper>. Wang et al. (2022) used word frequency as a complexity metric for curriculum-based pre-training of language models <Paper corpusId=\"269741199\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>.\n\n3. **Linguistic Complexity**: More sophisticated linguistic features such as syntactic complexity, grammatical structures, and text coherence can serve as difficulty indicators <Paper corpusId=\"265351926\" paperTitle=\"(Vakil et al., 2023)\" isShortName></Paper>.\n\n4. **Object Size and Count**: In computer vision, images containing fewer and larger objects are often considered easier to process <Paper corpusId=\"6954583\" paperTitle=\"(Shi et al., 2016)\" isShortName></Paper> <Paper corpusId=\"271051051\" paperTitle=\"(Jarca et al., 2024)\" isShortName></Paper>.\n\n5. **Annotation Agreement**: For datasets with multiple human annotations, the degree of annotator agreement can serve as a proxy for example difficulty <Paper corpusId=\"221995570\" paperTitle=\"(Wei et al., 2020)\" isShortName></Paper>. Higher disagreement among annotators often correlates with examples that models also find difficult <Paper corpusId=\"204539326\" paperTitle=\"(Jimenez-Sanchez et al., 2019)\" isShortName></Paper>.\n\n6. **Perplexity**: Pre-computed perplexity scores from auxiliary language models can estimate text complexity without requiring full model training <Paper corpusId=\"265351926\" paperTitle=\"(Vakil et al., 2023)\" isShortName></Paper>.\n\n7. **Domain-Specific Metrics**: Some approaches develop task-specific heuristics, such as using similarity of parallel sentence pairs in machine translation <Paper corpusId=\"267770178\" paperTitle=\"(Tang et al., 2024)\" isShortName></Paper>.\n\n8. **Comprehensive Complexity Measures**: More recent work has proposed combined metrics that incorporate multiple factors. For example, Ranaldi et al. developed a complexity measure based on length, rarity, and comprehensibility of samples, which outperformed existing curriculum learning methods when used for pre-training BERT and RoBERTa from scratch <Paper corpusId=\"269741199\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper> <Paper corpusId=\"265068175\" paperTitle=\"(Ranaldi et al., 2023)\" isShortName></Paper>.\n\n9. **C-Score**: This metric measures how consistently a particular example is learned across various models, indicating its inherent difficulty without relying on loss values that might lead to overfitting <Paper corpusId=\"250408174\" paperTitle=\"(Raymond-Saez et al., 2022)\" isShortName></Paper>.\n\nWhile predefined heuristic metrics are computationally efficient and intuitive, they may not always align with the actual learning difficulty experienced by models <Paper corpusId=\"248887633\" paperTitle=\"(Croitoru et al., 2022)\" isShortName></Paper>. This limitation has led researchers to explore more sophisticated model-centric approaches that can better capture the true learning challenge of each example <Paper corpusId=\"269756933\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>.\n\nThe effectiveness of these heuristic metrics varies across tasks and domains, with some researchers finding that combined approaches or task-specific metrics yield better results than general-purpose metrics <Paper corpusId=\"243766208\" paperTitle=\"(Zhang et al._1, 2021)\" isShortName></Paper>. The selection of appropriate difficulty metrics remains an active area of research in curriculum learning for LLM training.",
                "citations": [
                    {
                        "id": "(Kocmi et al., 2017)",
                        "snippets": [
                            "We examine the effects of particular orderings of sentence pairs on the on-line training of neural machine translation (NMT). We focus on two types of such orderings: (1) ensuring that each minibatch contains sentences similar in some aspect and (2) gradual inclusion of some sentence types as the training progresses (so called \"curriculum learning\"). In our English-to-Czech experiments, the internal homogeneity of minibatches has no effect on the training but some of our \"curricula\" achieve a small improvement over the baseline."
                        ],
                        "paper": {
                            "corpus_id": 26468344,
                            "title": "Curriculum Learning and Minibatch Bucketing in Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "3452584",
                                    "name": "Tom Kocmi"
                                },
                                {
                                    "authorId": "143832874",
                                    "name": "Ondrej Bojar"
                                }
                            ],
                            "year": 2017,
                            "venue": "Recent Advances in Natural Language Processing",
                            "n_citations": 141
                        },
                        "score": 0
                    },
                    {
                        "id": "(Tay et al., 2019)",
                        "snippets": [
                            "This paper tackles the problem of reading comprehension over long narratives where documents easily span over thousands of tokens. We propose a curriculum learning (CL) based Pointer-Generator framework for reading/sampling over large documents, enabling diverse training of the neural model based on the notion of alternating contextual difficulty. This can be interpreted as a form of domain randomization and/or generative pretraining during training. To this end, the usage of the Pointer-Generator softens the requirement of having the answer within the context, enabling us to construct diverse training samples for learning. Additionally, we propose a new Introspective Alignment Layer (IAL), which reasons over decomposed alignments using block-based self-attention. We evaluate our proposed method on the NarrativeQA reading comprehension benchmark, achieving state-of-the-art performance, improving existing baselines by 51% relative improvement on BLEU-4 and 17% relative improvement on Rouge-L. Extensive ablations confirm the effectiveness of our proposed IAL and CL components."
                        ],
                        "paper": {
                            "corpus_id": 166228313,
                            "title": "Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives",
                            "authors": [
                                {
                                    "authorId": "144447820",
                                    "name": "Yi Tay"
                                },
                                {
                                    "authorId": "2992833",
                                    "name": "Shuohang Wang"
                                },
                                {
                                    "authorId": "26336902",
                                    "name": "Anh Tuan Luu"
                                },
                                {
                                    "authorId": "49252800",
                                    "name": "Jie Fu"
                                },
                                {
                                    "authorId": "22209930",
                                    "name": "Minh C. Phan"
                                },
                                {
                                    "authorId": "2854297",
                                    "name": "Xingdi Yuan"
                                },
                                {
                                    "authorId": "30586030",
                                    "name": "J. Rao"
                                },
                                {
                                    "authorId": "144194328",
                                    "name": "S. Hui"
                                },
                                {
                                    "authorId": "2085709",
                                    "name": "Aston Zhang"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 110
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2021)",
                        "snippets": [
                            "BERT [1] is very computationally expensive, which is a hurdle for its training and deployment. This work focuses on removing the unnecessary computation due to input padding in BERT. The input of BERT consists of two concatenated sentences. If the length of the two concatenated sentences is shorter than the maximum sequence length, padding must be added to the end of the sentences to fill the empty slots in the input. Because the lengths of sentences vary greatly, there can be a large amount of padding in input. For the English Wikipedia & BooksCorpus dataset, the percentage of padding among all the input tokens is 17% and 48%, respectively, when the max sequence length is set to 128 and 512. For the Chinese Wikipedia dataset, this percentage is 35% and 79%, respectively, when the max sequence length is 128 and 512. For SQuAD-v1.1 [2], padding accounts for 54% of the total input tokens when the max sequence length is 384. Thus, there is a lot of wasted computation on padding, which significantly increases the training and inference time."
                        ],
                        "paper": {
                            "corpus_id": 233433844,
                            "title": "Reducing BERT Computation by Padding Removal and Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "144142217",
                                    "name": "Wei Zhang"
                                },
                                {
                                    "authorId": "2149192554",
                                    "name": "Wei Wei"
                                },
                                {
                                    "authorId": "2108908267",
                                    "name": "Wen Wang"
                                },
                                {
                                    "authorId": "2152164167",
                                    "name": "Lingling Jin"
                                },
                                {
                                    "authorId": "144332880",
                                    "name": "Zheng Cao"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE International Symposium on Performance Analysis of Systems and Software",
                            "n_citations": 19
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ding et al., 2023)",
                        "snippets": [
                            "Difficulty Metric. In the realm of curriculum learning for natural language processing, the most widely used difficulty metric is perhaps the sequence length (Geiping et al., 2022)157,(Li et al., 2021). The underlying assumption is that processing longer sentences poses greater challenges than shorter ones. Another prevalent metric is vocabulary rarity [285](Zhao et al., 2020), based on the intuition that sentences with less frequently used words in the training set are inherently more complex to comprehend. In addition, this metric could be measured by the uncertainty sampling principle in active learning, where the uncertainty indicated by other pre-trained models could serves as a gauge of difficulty as well."
                        ],
                        "paper": {
                            "corpus_id": 265552007,
                            "title": "The Efficiency Spectrum of Large Language Models: An Algorithmic Survey",
                            "authors": [
                                {
                                    "authorId": "2257191230",
                                    "name": "Tianyu Ding"
                                },
                                {
                                    "authorId": "2257252148",
                                    "name": "Tianyi Chen"
                                },
                                {
                                    "authorId": "2268495701",
                                    "name": "Haidong Zhu"
                                },
                                {
                                    "authorId": "2257278916",
                                    "name": "Jiachen Jiang"
                                },
                                {
                                    "authorId": "2269696935",
                                    "name": "Yiqi Zhong"
                                },
                                {
                                    "authorId": "2257235854",
                                    "name": "Jinxin Zhou"
                                },
                                {
                                    "authorId": "2269161996",
                                    "name": "Guangzhi Wang"
                                },
                                {
                                    "authorId": "2269697742",
                                    "name": "Zhihui Zhu"
                                },
                                {
                                    "authorId": "15623770",
                                    "name": "Ilya Zharkov"
                                },
                                {
                                    "authorId": "46225943",
                                    "name": "Luming Liang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 23
                        },
                        "score": 0.80517578125
                    },
                    {
                        "id": "(Lee et al., 2024)",
                        "snippets": [
                            "Curriculum Learning It is observed that deep learning model training can benefit from the implementation of Curriculum Learning (CL), i.e., using data samples sorted based on a curriculum versus training on the randomly shuffled data (Soviany et al., 2021). Recently, CL methods have been developed and deployed for the LMs as well, at pre-training and post-training stages using a variety of linguistically motivated curricula such as sentence length or term frequency complexity measure based ranking (Liu et al., 2018)(Zhang et al., 2021)Campos, 2021;Weber et al., 2023)",
                            "Li et al., 2021 implemented a CL at the pre-training of LMs using the sequence length as the difficulty metric with the curriculum of starting from the shorter sequence training data toward the longer sequence",
                            "(Ranaldi et al., 2023) proposed a new complexity measure based on the length, rarity, and comprehensibility of the samples and sorted the corpus according to the proposed complexity measure during the pre-training stage and showed that their CL approach led to better performance in downstream tasks",
                            "Wang et al., 2022 used the frequency of words as the complexity metric for the curriculum-based pre-training of LMs."
                        ],
                        "paper": {
                            "corpus_id": 269741199,
                            "title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation",
                            "authors": [
                                {
                                    "authorId": "2116659852",
                                    "name": "Joonho Lee"
                                },
                                {
                                    "authorId": "2301016207",
                                    "name": "Jae Oh Woo"
                                },
                                {
                                    "authorId": "2301014929",
                                    "name": "Juree Seok"
                                },
                                {
                                    "authorId": "2301015562",
                                    "name": "Parisa Hassanzadeh"
                                },
                                {
                                    "authorId": "2301015224",
                                    "name": "Wooseok Jang"
                                },
                                {
                                    "authorId": "2301016428",
                                    "name": "JuYoun Son"
                                },
                                {
                                    "authorId": "91748824",
                                    "name": "Sima Didari"
                                },
                                {
                                    "authorId": "2301014892",
                                    "name": "Baruch Gutow"
                                },
                                {
                                    "authorId": "2065513368",
                                    "name": "Heng Hao"
                                },
                                {
                                    "authorId": "2301015932",
                                    "name": "Hankyu Moon"
                                },
                                {
                                    "authorId": "2301166595",
                                    "name": "Wenjun Hu"
                                },
                                {
                                    "authorId": "2301413463",
                                    "name": "Yeong-Dae Kwon"
                                },
                                {
                                    "authorId": "2301133161",
                                    "name": "Taehee Lee"
                                },
                                {
                                    "authorId": "2301015935",
                                    "name": "Seungjai Min"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 2
                        },
                        "score": 0.6982421875
                    },
                    {
                        "id": "(Liu et al., 2018)",
                        "snippets": [
                            "By reason of being able to obtain natural language responses, natural answers are more favored in real-world Question Answering (QA) systems. Generative models learn to automatically generate natural answers from large-scale question answer pairs (QA-pairs). However, they are suffering from the uncontrollable and uneven quality of QA-pairs crawled from the Internet. To address this problem, we propose a curriculum learning based framework for natural answer generation (CL-NAG), which is able to take full advantage of the valuable learning data from a noisy and uneven-quality corpus. Specifically, we employ two practical measures to automatically measure the quality (complexity) of QA-pairs. Based on the measurements, CL-NAG firstly utilizes simple and low-quality QA-pairs to learn a basic model, and then gradually learns to produce better answers with richer contents and more complete syntaxes based on more complex and higher-quality QA-pairs. In this way, all valuable information in the noisy and uneven-quality corpus could be fully exploited. Experiments demonstrate that CL-NAG outperforms the state-of-the-arts, which increases 6.8% and 8.7% in the accuracy for simple and complex questions, respectively."
                        ],
                        "paper": {
                            "corpus_id": 51606954,
                            "title": "Curriculum Learning for Natural Answer Generation",
                            "authors": [
                                {
                                    "authorId": "49046540",
                                    "name": "Cao Liu"
                                },
                                {
                                    "authorId": "1954845",
                                    "name": "Shizhu He"
                                },
                                {
                                    "authorId": "2200096",
                                    "name": "Kang Liu"
                                },
                                {
                                    "authorId": "1390572170",
                                    "name": "Jun Zhao"
                                }
                            ],
                            "year": 2018,
                            "venue": "International Joint Conference on Artificial Intelligence",
                            "n_citations": 85
                        },
                        "score": 0
                    },
                    {
                        "id": "(Croitoru et al., 2022)",
                        "snippets": [
                            "The main challenge for the methods that build the curriculum at the data level is measuring the difficulty of the data samples, which is required to order the samples from easy to hard. Most studies have addressed the problem with human input (Pentina et al., 2014)(Jim\u00e9nez-S\u00e1nchez et al., 2019)(Wei et al., 2020) or metrics based on domain-specific heuristics. For instance, the text length (Kocmi et al., 2017)[44](Tay et al., 2019)(Zhang et al., 2021) and the word frequency [1,(Liu et al., 2018) have been employed in natural language processing. In computer vision, the samples containing fewer and larger objects have been considered to be easier in some works (Shi et al., 2016)[33]. Other solutions employed difficulty estimators (Ionescu et al., 2016) or even the confidence level of the predictions made by the neural network (Gong et al., 2016)(Hacohen et al., 2019) to approximate the complexity of the data samples. Other studies (Khan et al., 2024)(Khan et al., 2022)(Khan et al., 2023) used the error of a previously trained model to estimate the difficulty of each sample."
                        ],
                        "paper": {
                            "corpus_id": 248887633,
                            "title": "LeRaC: Learning Rate Curriculum",
                            "authors": [
                                {
                                    "authorId": "2154573729",
                                    "name": "Florinel-Alin Croitoru"
                                },
                                {
                                    "authorId": "103931166",
                                    "name": "Nicolae-C\u0103t\u0103lin Ristea"
                                },
                                {
                                    "authorId": "1817759",
                                    "name": "Radu Tudor Ionescu"
                                },
                                {
                                    "authorId": "1703601",
                                    "name": "N. Sebe"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Journal of Computer Vision",
                            "n_citations": 9
                        },
                        "score": 0.869140625
                    },
                    {
                        "id": "(Vakil et al., 2023)",
                        "snippets": [
                            "Current curriculum learning methodologies in NLP rely on data properties, e.g., sentence length, word rarity, or syntactic features (Platanios et al., 2019)(Liu et al., 2022), or annotation disagreement (Elgaar et al., 2023); as well as model properties such as training loss and its variations (Graves et al., 2017)(Amiri et al., 2017) to sequence data samples for training. (Elgaar et al., 2023) developed a curriculum discovery framework based on prior knowledge of sample difficulty, utilized annotation entropy and loss values."
                        ],
                        "paper": {
                            "corpus_id": 265351926,
                            "title": "Complexity-Guided Curriculum Learning for Text Graphs",
                            "authors": [
                                {
                                    "authorId": "2165570025",
                                    "name": "Nidhi Vakil"
                                },
                                {
                                    "authorId": "2267488904",
                                    "name": "Hadi Amiri"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 3
                        },
                        "score": 0.63427734375
                    },
                    {
                        "id": "(Shi et al., 2016)",
                        "snippets": [
                            "We present a technique for weakly supervised object localization (WSOL), building on the observation that WSOL algorithms usually work better on images with bigger objects. Instead of training the object detector on the entire training set at the same time, we propose a curriculum learning strategy to feed training images into the WSOL learning loop in an order from images containing bigger objects down to smaller ones. To automatically determine the order, we train a regressor to estimate the size of the object given the whole image as input. Furthermore, we use these size estimates to further improve the re-localization step of WSOL by assigning weights to object proposals according to how close their size matches the estimated object size. We demonstrate the effectiveness of using size order and size weighting on the challenging PASCAL VOC 2007 dataset, where we achieve a significant improvement over existing state-of-the-art WSOL techniques."
                        ],
                        "paper": {
                            "corpus_id": 6954583,
                            "title": "Weakly Supervised Object Localization Using Size Estimates",
                            "authors": [
                                {
                                    "authorId": "8003001",
                                    "name": "Miaojing Shi"
                                },
                                {
                                    "authorId": "143865718",
                                    "name": "V. Ferrari"
                                }
                            ],
                            "year": 2016,
                            "venue": "European Conference on Computer Vision",
                            "n_citations": 81
                        },
                        "score": 0
                    },
                    {
                        "id": "(Jarca et al., 2024)",
                        "snippets": [
                            "Researchers have made significant strides in finding improved metrics for various domains and tasks.For instance, images containing fewer and larger objects in computer vision are deemed easier than other images (Shi et al., 2016)(Soviany et al., 2019).In natural language processing, word frequency (Bengio et al., 2009)(Liu et al., 2018) and sequence length [8](Kocmi et al., 2017)(Tay et al., 2019)(Zhang et al., 2021) are utilized to assess the sample difficulty.In some cases, researchers have also integrated human feedback into their metric design (Jim\u00e9nez-S\u00e1nchez et al., 2019)(Pentina et al., 2014)(Wei et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 271051051,
                            "title": "CBM: Curriculum by Masking",
                            "authors": [
                                {
                                    "authorId": "2310341007",
                                    "name": "Andrei Jarca"
                                },
                                {
                                    "authorId": "2154573729",
                                    "name": "Florinel-Alin Croitoru"
                                },
                                {
                                    "authorId": "2249763264",
                                    "name": "R. Ionescu"
                                }
                            ],
                            "year": 2024,
                            "venue": "European Conference on Artificial Intelligence",
                            "n_citations": 0
                        },
                        "score": 0.69384765625
                    },
                    {
                        "id": "(Wei et al., 2020)",
                        "snippets": [
                            "Applying curriculum learning requires both a range of difficulty in data and a method for determining the difficulty of examples. In many tasks, however, satisfying these requirements can be a formidable challenge.In this paper, we contend that histopathology image classification is a compelling use case for curriculum learning. Based on the nature of histopathology images, a range of difficulty inherently exists among examples, and, since medical datasets are often labeled by multiple annotators, annotator agreement can be used as a natural proxy for the difficulty of a given example. Hence, we propose a simple curriculum learning method that trains on progressively-harder images as determined by annotator agreement.We evaluate our hypothesis on the challenging and clinically-important task of colorectal polyp classification. Whereas vanilla training achieves an AUC of 83.7% for this task, a model trained with our proposed curriculum learning approach achieves an AUC of 88.2%, an improvement of 4.5%. Our work aims to inspire researchers to think more creatively and rigorously when choosing contexts for applying curriculum learning."
                        ],
                        "paper": {
                            "corpus_id": 221995570,
                            "title": "Learn like a Pathologist: Curriculum Learning by Annotator Agreement for Histopathology Image Classification",
                            "authors": [
                                {
                                    "authorId": "47807379",
                                    "name": "Jerry W. Wei"
                                },
                                {
                                    "authorId": "3583138",
                                    "name": "A. Suriawinata"
                                },
                                {
                                    "authorId": "2064646056",
                                    "name": "Bing Ren"
                                },
                                {
                                    "authorId": "49543754",
                                    "name": "Xiaoying Liu"
                                },
                                {
                                    "authorId": "7013728",
                                    "name": "Mikhail Lisovsky"
                                },
                                {
                                    "authorId": "37530422",
                                    "name": "L. Vaickus"
                                },
                                {
                                    "authorId": "2111163034",
                                    "name": "Charles Brown"
                                },
                                {
                                    "authorId": "2067799679",
                                    "name": "Michael Baker"
                                },
                                {
                                    "authorId": "1659771244",
                                    "name": "Mustafa Nasir-Moin"
                                },
                                {
                                    "authorId": "47136049",
                                    "name": "Naofumi Tomita"
                                },
                                {
                                    "authorId": "1732879",
                                    "name": "L. Torresani"
                                },
                                {
                                    "authorId": "144026731",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "145945685",
                                    "name": "S. Hassanpour"
                                }
                            ],
                            "year": 2020,
                            "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                            "n_citations": 49
                        },
                        "score": 0
                    },
                    {
                        "id": "(Jimenez-Sanchez et al., 2019)",
                        "snippets": [
                            "Current deep-learning based methods do not easily integrate to clinical protocols, neither take full advantage of medical knowledge. In this work, we propose and compare several strategies relying on curriculum learning, to support the classification of proximal femur fracture from X-ray images, a challenging problem as reflected by existing intra- and inter-expert disagreement. Our strategies are derived from knowledge such as medical decision trees and inconsistencies in the annotations of multiple experts, which allows us to assign a degree of difficulty to each training sample. We demonstrate that if we start learning \"easy\" examples and move towards \"hard\", the model can reach a better performance, even with fewer data. The evaluation is performed on the classification of a clinical dataset of about 1000 X-ray images. Our results show that, compared to class-uniform and random strategies, the proposed medical knowledge-based curriculum, performs up to 15% better in terms of accuracy, achieving the performance of experienced trauma surgeons."
                        ],
                        "paper": {
                            "corpus_id": 204539326,
                            "title": "Medical-based Deep Curriculum Learning for Improved Fracture Classification",
                            "authors": [
                                {
                                    "authorId": "1404288037",
                                    "name": "Amelia Jim\u00e9nez-S\u00e1nchez"
                                },
                                {
                                    "authorId": "1704770",
                                    "name": "D. Mateus"
                                },
                                {
                                    "authorId": "79884102",
                                    "name": "S. Kirchhoff"
                                },
                                {
                                    "authorId": "4858301",
                                    "name": "C. Kirchhoff"
                                },
                                {
                                    "authorId": "5140452",
                                    "name": "P. Biberthaler"
                                },
                                {
                                    "authorId": "145587209",
                                    "name": "N. Navab"
                                },
                                {
                                    "authorId": "49463953",
                                    "name": "M. Ballester"
                                },
                                {
                                    "authorId": "50186507",
                                    "name": "G. Piella"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                            "n_citations": 48
                        },
                        "score": 0
                    },
                    {
                        "id": "(Tang et al., 2024)",
                        "snippets": [
                            "For instance, Mohiuddin et al. (2022) developed three deterministic scoring methods for machine translation that leverage the similarity of a parallel sentence pairs and cross-entropy. (Zhu et al., 2021) evaluated sample difficulty in dialogue generation based on model loss. Lobov et al. (2022) utilized metrics such as sentence length, average confidence, and perplexity value to assess difficulty in named entity recognition (NER)."
                        ],
                        "paper": {
                            "corpus_id": 267770178,
                            "title": "An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling",
                            "authors": [
                                {
                                    "authorId": "50295874",
                                    "name": "Xuemei Tang"
                                },
                                {
                                    "authorId": "2284863194",
                                    "name": "Qi Su"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.8076171875
                    },
                    {
                        "id": "(Ranaldi et al., 2023)",
                        "snippets": [
                            "Directly learning from complex examples is generally problematic for humans and machines. Indeed, a better strategy is exposing learners to examples in a reasonable, pedagogically-motivated order. Curriculum Learning (CL) has been proposed to import this strategy when training machine learning models. In this paper, building on Curriculum Learning, we propose a novel, linguistically motivated measure to determine example complexity for organizing examples during learning. Our complexity measure - LRC- is based on length, rarity, and comprehensibility. Our resulting learning model is CL-LRC, that is, CL with LRC. Experiments on downstream tasks show that CL-LRC outperforms existing CL and non-CL methods for training BERT and RoBERTa from scratch. Furthermore, we analyzed different measures, including perplexity, loss, and learning curve of different models pre-trained from scratch, showing that CL-LRC performs better than the state-of-the-art."
                        ],
                        "paper": {
                            "corpus_id": 265068175,
                            "title": "Modeling Easiness for Training Transformers with Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "2008183566",
                                    "name": "Leonardo Ranaldi"
                                },
                                {
                                    "authorId": "2199247500",
                                    "name": "Giulia Pucci"
                                },
                                {
                                    "authorId": "103839825",
                                    "name": "F. M. Zanzotto"
                                }
                            ],
                            "year": 2023,
                            "venue": "Recent Advances in Natural Language Processing",
                            "n_citations": 13
                        },
                        "score": 0
                    },
                    {
                        "id": "(Raymond-Saez et al., 2022)",
                        "snippets": [
                            "Curriculum Learning (CL) strategies are based on providing a model with an increasingly difficult selection of examples, similar to how children learn in school. CL strategies are popular in Reinforcement Learning -where a curriculum of tasks is progressively learned -and Natural Language Processing applications -where commonly sentence length has been a proxy for difficulty.\n\nThe problem, however, lies in how to determine the difficulty of samples. Typically, an ad hoc difficulty measure needs to be crafted by hand for a specific problem, which limits their application in practice. Even worse, for some applications like image classification, sometimes there's no natural difficulty ordering that can be created. This has led to the development of more general measures of difficulty: Self Paced Learning (Kumar et al., 2010) utilizes the current loss of an example as a proxy for difficulty; however, this requires extra computation and is prone to overfitting (Jiang et al., 2015). As an alternative, a metric called C-Score was proposed (Jiang et al., 2021). This metric measures how consistently a particular example is learned across various models, that is, it measures the proportion of iterations where a sample is successfully classified."
                        ],
                        "paper": {
                            "corpus_id": 250408174,
                            "title": "A Study on the Predictability of Sample Learning Consistency",
                            "authors": [
                                {
                                    "authorId": "2110753585",
                                    "name": "Alain Raymond-S\u00e1ez"
                                },
                                {
                                    "authorId": "2064859104",
                                    "name": "J. Hurtado"
                                },
                                {
                                    "authorId": "2052429926",
                                    "name": "\u00c1lvaro Soto"
                                }
                            ],
                            "year": 2022,
                            "venue": "LatinX in AI at International Conference on Machine Learning 2022",
                            "n_citations": 0
                        },
                        "score": 0.6787109375
                    },
                    {
                        "id": "(Kim et al., 2024)",
                        "snippets": [
                            "In the development of training data for curriculum learning, accurately measuring data difficulty is crucial. Specifically, for LLMs (Large Language Models), determining the degree of data difficulty is challenging. Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021; Nagatsuka et al., 2023). However, these metrics may not fully reflect the complexity of a dataset. It is essential to assess data difficulty from the model's perspective, rather than relying solely on data-specific metrics. Our research proposes a new approach to calculate the degree of difficulty based on a model-centric perspective. By organizing the training dataset according to difficulty using our new metric, we aim to improve the model's performance compared to random shuffling.\n\n3 Methods for Quantitative Difficulty Measurement\n\nIn this section, we introduce a novel methodology for training models that begins with easier tasks and methodically progresses to more challenging ones. This approach requires organizing data by its level of complexity, for which we have established three principal criteria. By arranging the data in an order that goes from less to more difficult, this approach establishes a structured progression for learning.\n\n## Loss\nA high loss signifies a substantial discrepancy between the predicted and actual outcomes, indicating that the model perceives the data as challenging.\n\nWe establish a direct correlation between high loss values and increased difficulty. Therefore, prompts that generate higher loss are considered more challenging for the model to learn. This method computes the cross-entropy loss associated with the answer segment of the prompt."
                        ],
                        "paper": {
                            "corpus_id": 269756933,
                            "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "2301165169",
                                    "name": "Jisu Kim"
                                },
                                {
                                    "authorId": "2301167177",
                                    "name": "Juhwan Lee"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.88037109375
                    },
                    {
                        "id": "(Zhang et al._1, 2021)",
                        "snippets": [
                            "With the notable success of pretrained language models, the pretraining-fine-tuning paradigm has become a dominant solution for natural language understanding (NLU) tasks. Typically, the training instances of a target NLU task are introduced in a completely random order and treated equally at the fine-tuning stage. However, these instances can vary greatly in difficulty, and similar to human learning procedures, language models can benefit from an easy-to-difficult curriculum. Based on this concept, we propose a curriculum learning (CL) framework. Our framework consists of two stages, Review and Arrange, targeting the two main challenges in curriculum learning, i.e., how to define the difficulty of instances and how to arrange a curriculum based on the difficulty, respectively. In the first stage, we devise a cross-review (CR) method to train several teacher models first and then review the training set in a crossed manner to distinguish easy instances from difficult instances. In the second stage, two sampling algorithms, a coarse-grained arrangement (CGA) and a fine-grained arrangement (FGA), are proposed to arrange a curriculum for language models in which the learning materials start from the easiest instances, and more difficult instances are gradually added into the training procedure. Compared to previous heuristic CL methods, our framework can avoid the errors caused by a gap in difficulty between humans and machines and has strong generalization ability. We conduct comprehensive experiments, and the results show that our curriculum learning framework, without any manual model architecture design or use of external data, obtains significant and universal performance improvements on a wide range of NLU tasks in different languages."
                        ],
                        "paper": {
                            "corpus_id": 243766208,
                            "title": "Review and Arrange: Curriculum Learning for Natural Language Understanding",
                            "authors": [
                                {
                                    "authorId": "48378753",
                                    "name": "L. Zhang"
                                },
                                {
                                    "authorId": "1855978",
                                    "name": "Zhendong Mao"
                                },
                                {
                                    "authorId": "1754285124",
                                    "name": "Benfeng Xu"
                                },
                                {
                                    "authorId": "143906199",
                                    "name": "Quan Wang"
                                },
                                {
                                    "authorId": "1699819",
                                    "name": "Yongdong Zhang"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
                            "n_citations": 14
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Combined and Novel Approaches",
                "tldr": "Recent research combines multiple difficulty metrics to create more robust measures of training data complexity. These hybrid approaches integrate model-centric metrics with data characteristics to provide comprehensive difficulty assessments that better support curriculum learning in LLM training. (10 sources)",
                "text": "\nRecognizing the limitations of individual difficulty metrics, researchers have developed innovative combined approaches that leverage multiple signals to measure training data difficulty more comprehensively. These hybrid methods aim to capture different aspects of learning difficulty and provide more nuanced assessments for curriculum learning in LLM training.\n\nOne notable approach combines representation distance with perplexity scores to create a more robust difficulty measure <Paper corpusId=\"266166227\" paperTitle=\"(Zhou et al._1, 2023)\" isShortName></Paper>. This integration leverages both semantic understanding and linguistic complexity to better assess example difficulty from multiple perspectives. Similarly, other researchers have explored ensemble-based methods that measure example difficulty through agreement across predictions from multiple model components, either from final layers or intermediate representations <Paper corpusId=\"250626773\" paperTitle=\"(Peng et al., 2022)\" isShortName></Paper> <Paper corpusId=\"235458180\" paperTitle=\"(Baldock et al., 2021)\" isShortName></Paper>.\n\nThe Gradient-based Difficulty Measure (GDM) represents another innovative approach that evaluates example difficulty through dynamic measurement of gradient magnitude with respect to the example itself. Unlike training loss, which only considers the difference between predictions and ground truth, GDM takes input features into account, allowing for more fine-grained assessment even when different examples yield identical loss values. This is particularly valuable in loss landscapes with plateaus or saddle points, where training loss might remain stable despite substantial shifts in model parameters <Paper corpusId=\"268696658\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>.\n\nRecent work has also explored model-centric difficulty metrics that move beyond data-specific characteristics. Kim et al. propose calculating difficulty from the model's perspective rather than relying solely on traditional metrics like text length or word rarity <Paper corpusId=\"269756933\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>. This approach recognizes that true learning difficulty depends on the interaction between the model and the data, not just inherent data properties.\n\nThe Confidence-guided Curriculum Learning (CCL) framework represents another innovative approach that integrates difficulty measurement with proficiency estimation based on curriculum learning losses observed during both training and validation phases. This method eliminates dependency on task-specific metrics, making implementation more straightforward across different domains <Paper corpusId=\"273901573\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nSeveral researchers have adopted the general curriculum learning framework of \"difficulty measure + training scheduler\" <Paper corpusId=\"261557260\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper> <Paper corpusId=\"232362223\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>. In this approach, the difficulty measure determines the relative complexity of each data sample, while the training scheduler decides the sequence of data subsets throughout the training process. This framework has proven effective not only for optimizing learning but also for data cleaning, with studies showing performance improvements on denoised data <Paper corpusId=\"261557260\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper> <Paper corpusId=\"1054586\" paperTitle=\"(Wang et al., 2017)\" isShortName></Paper> <Paper corpusId=\"174798125\" paperTitle=\"(Wang et al., 2019)\" isShortName></Paper>.\n\nThese combined and novel approaches to measuring data difficulty represent a significant advancement over single-metric methods. By integrating multiple signals and considering both data characteristics and model behavior, these techniques provide more comprehensive assessments of training example complexity, enabling more effective curriculum construction for large language model training.",
                "citations": [
                    {
                        "id": "(Zhou et al._1, 2023)",
                        "snippets": [
                            "Therefore, we use a combination of the representation distance and perplexity score as a measure of examples' difficulty."
                        ],
                        "paper": {
                            "corpus_id": 266166227,
                            "title": "Non-compositional Expression Generation Based on Curriculum Learning and Continual Learning",
                            "authors": [
                                {
                                    "authorId": "102489044",
                                    "name": "Jianing Zhou"
                                },
                                {
                                    "authorId": "41048608",
                                    "name": "Ziheng Zeng"
                                },
                                {
                                    "authorId": "2008458",
                                    "name": "Hongyu Gong"
                                },
                                {
                                    "authorId": "2263637139",
                                    "name": "Suma Bhat"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 2
                        },
                        "score": 0.69091796875
                    },
                    {
                        "id": "(Peng et al., 2022)",
                        "snippets": [
                            "Very recently, deep ensemble methods (Baldock et al., 2021)[18] measure example difficulty with agreement either from last layers' predictions or from intermediate layers' predictions. Reducing estimation uncertainty with ensembling requires selected treatments and controls such as architectures, number of submodels, and number of data splits. In this work, we show faithful image difficulty can be efficiently estimated by deep metric learning. Hyperspherical learning [26], a weakly supervised learning framework, groups instances of the same concept together and pushes instances of different concepts apart by enforcing angular discrimination during training."
                        ],
                        "paper": {
                            "corpus_id": 250626773,
                            "title": "Angular Gap: Reducing the Uncertainty of Image Difficulty through Model Calibration",
                            "authors": [
                                {
                                    "authorId": "2147421343",
                                    "name": "Bohua Peng"
                                },
                                {
                                    "authorId": "145481136",
                                    "name": "Mobarakol Islam"
                                },
                                {
                                    "authorId": "2067051679",
                                    "name": "Mei Tu"
                                }
                            ],
                            "year": 2022,
                            "venue": "ACM Multimedia",
                            "n_citations": 9
                        },
                        "score": 0.6455078125
                    },
                    {
                        "id": "(Baldock et al., 2021)",
                        "snippets": [
                            "Existing work on understanding deep learning often employs measures that compress all data-dependent information into a few numbers. In this work, we adopt a perspective based on the role of individual examples. We introduce a measure of the computational difficulty of making a prediction for a given input: the (effective) prediction depth. Our extensive investigation reveals surprising yet simple relationships between the prediction depth of a given input and the model's uncertainty, confidence, accuracy and speed of learning for that data point. We further categorize difficult examples into three interpretable groups, demonstrate how these groups are processed differently inside deep models and showcase how this understanding allows us to improve prediction accuracy. Insights from our study lead to a coherent view of a number of separately reported phenomena in the literature: early layers generalize while later layers memorize; early layers converge faster and networks learn easy data and simple functions first."
                        ],
                        "paper": {
                            "corpus_id": 235458180,
                            "title": "Deep Learning Through the Lens of Example Difficulty",
                            "authors": [
                                {
                                    "authorId": "37680581",
                                    "name": "R. Baldock"
                                },
                                {
                                    "authorId": "50153692",
                                    "name": "Hartmut Maennel"
                                },
                                {
                                    "authorId": "3007442",
                                    "name": "Behnam Neyshabur"
                                }
                            ],
                            "year": 2021,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 161
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhao et al., 2024)",
                        "snippets": [
                            "Current methods evaluate example difficulty through predefined metrics, such as sentence length, or dynamically update it based on training loss, such as cross-entropy loss. However, predefined difficulty measures do not integrate the model's feedback, and training loss, focusing only on the difference between predictions and ground truth, raises an inaccurate difficulty measurement issue when different examples yield identical training losses.\n\nTo address these limitations, we propose the Gradientbased Difficulty Measure (GDM), which evaluates example difficulty through dynamic measurement of the gradient magnitude with respect to the example itself. Unlike training loss, the gradient avoids inaccurate difficulty assessment by taking input features into account. As a result, even if examples yield the same loss, their gradients can differ. Additionally, loss landscapes can encompass plateaus or saddle points, where training loss remains relatively stable even with substantial shifts in model parameters. In these scenarios, utilizing training loss for evaluating example difficulty can be misleading. Conversely, the gradient provides finergrained insights into changes in model parameters, making the gradient magnitude a more informative approach for evaluating difficulty."
                        ],
                        "paper": {
                            "corpus_id": 268696658,
                            "title": "Symmetric Self-Paced Learning for Domain Generalization",
                            "authors": [
                                {
                                    "authorId": "2110600983",
                                    "name": "Di Zhao"
                                },
                                {
                                    "authorId": "34930533",
                                    "name": "Yun Sing Koh"
                                },
                                {
                                    "authorId": "2276344048",
                                    "name": "Gillian Dobbie"
                                },
                                {
                                    "authorId": "2293565669",
                                    "name": "Hongsheng Hu"
                                },
                                {
                                    "authorId": "2293453267",
                                    "name": "Philippe Fournier-Viger"
                                }
                            ],
                            "year": 2024,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 5
                        },
                        "score": 0.7685546875
                    },
                    {
                        "id": "(Kim et al., 2024)",
                        "snippets": [
                            "In the development of training data for curriculum learning, accurately measuring data difficulty is crucial. Specifically, for LLMs (Large Language Models), determining the degree of data difficulty is challenging. Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021; Nagatsuka et al., 2023). However, these metrics may not fully reflect the complexity of a dataset. It is essential to assess data difficulty from the model's perspective, rather than relying solely on data-specific metrics. Our research proposes a new approach to calculate the degree of difficulty based on a model-centric perspective. By organizing the training dataset according to difficulty using our new metric, we aim to improve the model's performance compared to random shuffling.\n\n3 Methods for Quantitative Difficulty Measurement\n\nIn this section, we introduce a novel methodology for training models that begins with easier tasks and methodically progresses to more challenging ones. This approach requires organizing data by its level of complexity, for which we have established three principal criteria. By arranging the data in an order that goes from less to more difficult, this approach establishes a structured progression for learning.\n\n## Loss\nA high loss signifies a substantial discrepancy between the predicted and actual outcomes, indicating that the model perceives the data as challenging.\n\nWe establish a direct correlation between high loss values and increased difficulty. Therefore, prompts that generate higher loss are considered more challenging for the model to learn. This method computes the cross-entropy loss associated with the answer segment of the prompt."
                        ],
                        "paper": {
                            "corpus_id": 269756933,
                            "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "2301165169",
                                    "name": "Jisu Kim"
                                },
                                {
                                    "authorId": "2301167177",
                                    "name": "Juhwan Lee"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.88037109375
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "CCL integrates a difficulty measure and a proficiency estimator directly derived from the CL losses observed in both the training and validation phases. This innovation eliminates the dependency on task-specific metrics, streamlining the implementation process."
                        ],
                        "paper": {
                            "corpus_id": 273901573,
                            "title": "Curriculum Consistency Learning for Conditional Sentence Generation",
                            "authors": [
                                {
                                    "authorId": "2287881971",
                                    "name": "Liangxin Liu"
                                },
                                {
                                    "authorId": "2256344322",
                                    "name": "Xuebo Liu"
                                },
                                {
                                    "authorId": "2324576203",
                                    "name": "Lian Lian"
                                },
                                {
                                    "authorId": "2324857773",
                                    "name": "Shengjun Cheng"
                                },
                                {
                                    "authorId": "2142545243",
                                    "name": "Jun Rao"
                                },
                                {
                                    "authorId": "2273704740",
                                    "name": "Tengfei Yu"
                                },
                                {
                                    "authorId": "2193388247",
                                    "name": "Hexuan Deng"
                                },
                                {
                                    "authorId": "2269805934",
                                    "name": "Min Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 1
                        },
                        "score": 0.62451171875
                    },
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "We follow the general curriculum learning framework of difficulty measure and training scheduler (Wang et al., 2021), where difficulty measure determines the relative \"difficulty\" of each data sample and training scheduler decides the sequence of data subsets throughout the training process. Additionally, curriculum learning has been approved as an effective method for data cleaning, and many works have shown that the performance can be improved on denoised data (Wang et al., 2017)(Wang et al., 2019)."
                        ],
                        "paper": {
                            "corpus_id": 261557260,
                            "title": "Epi-Curriculum: Episodic Curriculum Learning for Low-Resource Domain Adaptation in Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "2238164778",
                                    "name": "Keyu Chen"
                                },
                                {
                                    "authorId": "2237989461",
                                    "name": "Zhuang Di"
                                },
                                {
                                    "authorId": "2260916568",
                                    "name": "Mingchen Li"
                                },
                                {
                                    "authorId": "2334755606",
                                    "name": "J. Morris Chang"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Transactions on Artificial Intelligence",
                            "n_citations": 3
                        },
                        "score": 0.57958984375
                    },
                    {
                        "id": "(Wang et al., 2021)",
                        "snippets": [
                            "Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of <italic>Difficulty Measurer <inline-formula><tex-math notation=\"LaTeX\">$+$</tex-math><alternatives><mml:math><mml:mo>+</mml:mo></mml:math><inline-graphic xlink:href=\"wang-ieq1-3069908.gif\"/></alternatives></inline-formula> Training Scheduler</italic> and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. We also analyze principles to select different CL designs that may benefit practical applications. Finally, we present our insights on the relationships connecting CL and other machine learning concepts including transfer learning, meta-learning, continual learning and active learning, etc., then point out challenges in CL as well as potential future research directions deserving further investigations."
                        ],
                        "paper": {
                            "corpus_id": 232362223,
                            "title": "A Survey on Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "2153687490",
                                    "name": "Xin Wang"
                                },
                                {
                                    "authorId": "51310474",
                                    "name": "Yudong Chen"
                                },
                                {
                                    "authorId": "145583986",
                                    "name": "Wenwu Zhu"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                            "n_citations": 611
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2017)",
                        "snippets": [
                            "Instance weighting has been widely applied to phrase-based machine translation domain adaptation. However, it is challenging to be applied to Neural Machine Translation (NMT) directly, because NMT is not a linear model. In this paper, two instance weighting technologies, i.e., sentence weighting and domain weighting with a dynamic weight learning strategy, are proposed for NMT domain adaptation. Empirical results on the IWSLT English-German/French tasks show that the proposed methods can substantially improve NMT performance by up to 2.7-6.7 BLEU points, outperforming the existing baselines by up to 1.6-3.6 BLEU points."
                        ],
                        "paper": {
                            "corpus_id": 1054586,
                            "title": "Instance Weighting for Neural Machine Translation Domain Adaptation",
                            "authors": [
                                {
                                    "authorId": "108085542",
                                    "name": "Rui Wang"
                                },
                                {
                                    "authorId": "1802277",
                                    "name": "M. Utiyama"
                                },
                                {
                                    "authorId": "2978364",
                                    "name": "Lemao Liu"
                                },
                                {
                                    "authorId": "2849740",
                                    "name": "Kehai Chen"
                                },
                                {
                                    "authorId": "1698363",
                                    "name": "E. Sumita"
                                }
                            ],
                            "year": 2017,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 132
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2019)",
                        "snippets": [
                            "Noise and domain are important aspects of data quality for neural machine translation. Existing research focus separately on domain-data selection, clean-data selection, or their static combination, leaving the dynamic interaction across them not explicitly examined. This paper introduces a \"co-curricular learning\" method to compose dynamic domain-data selection with dynamic clean-data selection, for transfer learning across both capabilities. We apply an EM-style optimization procedure to further refine the \"co-curriculum\". Experiment results and analysis with two domains demonstrate the effectiveness of the method and the properties of data scheduled by the co-curriculum."
                        ],
                        "paper": {
                            "corpus_id": 174798125,
                            "title": "Dynamically Composing Domain-Data Selection with Clean-Data Selection by \u201cCo-Curricular Learning\u201d for Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "40397893",
                                    "name": "Wei Wang"
                                },
                                {
                                    "authorId": "73769795",
                                    "name": "Isaac Caswell"
                                },
                                {
                                    "authorId": "1802969",
                                    "name": "Ciprian Chelba"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 59
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Automatic Difficulty Measurement",
                "tldr": "Automatic difficulty measurement techniques dynamically assess training example complexity without requiring predefined heuristics or human expertise. These approaches leverage real-time model feedback such as prediction errors, training loss, and cross-entropy to automatically determine data difficulty during the training process. (6 sources)",
                "text": "\nAutomatic difficulty measurement addresses a fundamental limitation of curriculum learning: the traditional requirement for manually crafted, task-specific difficulty metrics. Rather than relying on predefined heuristics that may not generalize well across domains, automatic approaches determine sample difficulty dynamically during training without extensive human intervention <Paper corpusId=\"250408174\" paperTitle=\"(Raymond-Saez et al., 2022)\" isShortName></Paper>.\n\nSelf-paced learning (SPL) represents one of the earliest automatic difficulty measurement techniques, using the current loss of an example as a proxy for difficulty. However, this approach has known limitations, including vulnerability to overfitting and the requirement for additional computation <Paper corpusId=\"250408174\" paperTitle=\"(Raymond-Saez et al., 2022)\" isShortName></Paper>. To address these issues, researchers have developed alternative automatic metrics such as C-Score, which measures how consistently a particular example is learned across various models\u2014essentially capturing the proportion of iterations where a sample is successfully classified <Paper corpusId=\"250408174\" paperTitle=\"(Raymond-Saez et al., 2022)\" isShortName></Paper>.\n\nMore sophisticated automatic approaches modify difficulty scores based on prediction errors, which serve as critical indicators of model uncertainty. For instance, Xu et al. introduced two automatic difficulty measurers: the Hamming Distance-based Measurer (HDM) and the Cross Entropy-based Measurer (CEM). While hamming distance calculates the difference between two equal-length strings, cross-entropy estimates uncertainty by comparing real distribution and prediction <Paper corpusId=\"256460262\" paperTitle=\"(Xu et al., 2023)\" isShortName></Paper>. These methods build upon earlier work in automatic difficulty measurement, including transfer teacher approaches and reinforcement learning-based curriculum selection <Paper corpusId=\"256460262\" paperTitle=\"(Xu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"11137059\" paperTitle=\"(Graves et al., 2017)\" isShortName></Paper>.\n\nReinforcement learning has proven particularly valuable for automatically selecting the optimal curriculum path. Graves et al. demonstrated a method that uses learning progress signals\u2014such as increases in prediction accuracy or network complexity\u2014as rewards for a multi-armed bandit algorithm that determines a stochastic syllabus <Paper corpusId=\"256460262\" paperTitle=\"(Xu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"11137059\" paperTitle=\"(Graves et al., 2017)\" isShortName></Paper>. This approach has shown significant acceleration in learning efficiency, in some cases reducing training time by half.\n\nRecent research has further refined automatic difficulty measurement for practical applications. Zeng et al. designed an automatic difficulty measurer that dynamically categorizes samples as either \"easy\" or \"difficult\" based on whether their ground truth scores are correctly predicted during a training epoch <Paper corpusId=\"259593617\" paperTitle=\"(Zeng et al., 2023)\" isShortName></Paper>. This binary classification approach enables more flexible training processes without relying on complex difficulty scales.\n\nThe Confidence-guided Curriculum Learning (CCL) framework represents another advancement in automatic difficulty measurement. This approach integrates difficulty measurement with proficiency estimation derived directly from curriculum learning losses observed during both training and validation phases. By eliminating dependency on task-specific metrics, CCL streamlines implementation across different domains <Paper corpusId=\"273901573\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nThese automatic techniques have proven particularly valuable for large-scale pretraining tasks, where they improve both data and training efficiency <Paper corpusId=\"254366532\" paperTitle=\"(Li et al._1, 2022)\" isShortName></Paper>. By dynamically adjusting to the model's current state and learning progress, automatic difficulty measurers provide a more adaptive approach to curriculum construction than static, predefined metrics, making them especially well-suited for the complex training dynamics of large language models.",
                "citations": [
                    {
                        "id": "(Raymond-Saez et al., 2022)",
                        "snippets": [
                            "Curriculum Learning (CL) strategies are based on providing a model with an increasingly difficult selection of examples, similar to how children learn in school. CL strategies are popular in Reinforcement Learning -where a curriculum of tasks is progressively learned -and Natural Language Processing applications -where commonly sentence length has been a proxy for difficulty.\n\nThe problem, however, lies in how to determine the difficulty of samples. Typically, an ad hoc difficulty measure needs to be crafted by hand for a specific problem, which limits their application in practice. Even worse, for some applications like image classification, sometimes there's no natural difficulty ordering that can be created. This has led to the development of more general measures of difficulty: Self Paced Learning (Kumar et al., 2010) utilizes the current loss of an example as a proxy for difficulty; however, this requires extra computation and is prone to overfitting (Jiang et al., 2015). As an alternative, a metric called C-Score was proposed (Jiang et al., 2021). This metric measures how consistently a particular example is learned across various models, that is, it measures the proportion of iterations where a sample is successfully classified."
                        ],
                        "paper": {
                            "corpus_id": 250408174,
                            "title": "A Study on the Predictability of Sample Learning Consistency",
                            "authors": [
                                {
                                    "authorId": "2110753585",
                                    "name": "Alain Raymond-S\u00e1ez"
                                },
                                {
                                    "authorId": "2064859104",
                                    "name": "J. Hurtado"
                                },
                                {
                                    "authorId": "2052429926",
                                    "name": "\u00c1lvaro Soto"
                                }
                            ],
                            "year": 2022,
                            "venue": "LatinX in AI at International Conference on Machine Learning 2022",
                            "n_citations": 0
                        },
                        "score": 0.6787109375
                    },
                    {
                        "id": "(Xu et al., 2023)",
                        "snippets": [
                            "To alleviate these problems, various automatic difficulty measurers have been developed and explored in the literature, including self-paced learning (Kumar et al., 2010), transfer teacher [69], reinforcement learning teacher (Graves et al., 2017), and other automatic difficulty measurers (Jiang et al., 2017)(Sanger, 1994)[54]. Inspired by these methods, we modify difficulty scores automatically with prediction errors, which are critical indicators of CPS uncertainty. Substantial work has been conducted in the literature, demonstrating the importance of handling uncertainties in CPS security and safety [21,(Han et al., 2022)(Ma et al., 2019)(Xu et al., 2022)(Zhang et al., 2019)[68]",
                            "we define the following two types of automatic difficulty measurers: Hamming Distance-based Measurer (HDM, Definition 5.1) and Cross Entropy-based Measurer (CEM, Definition 5.2). Hamming distance is commonly used to calculate the difference of two strings of equal length, while cross-entropy loss estimates uncertainty by comparing real distribution and prediction."
                        ],
                        "paper": {
                            "corpus_id": 256460262,
                            "title": "Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems",
                            "authors": [
                                {
                                    "authorId": "2118221146",
                                    "name": "Qinghua Xu"
                                },
                                {
                                    "authorId": "47495965",
                                    "name": "Sajid Ali"
                                },
                                {
                                    "authorId": "145456500",
                                    "name": "T. Yue"
                                }
                            ],
                            "year": 2023,
                            "venue": "ACM Transactions on Software Engineering and Methodology",
                            "n_citations": 22
                        },
                        "score": 0.658203125
                    },
                    {
                        "id": "(Graves et al., 2017)",
                        "snippets": [
                            "We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multi-armed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level."
                        ],
                        "paper": {
                            "corpus_id": 11137059,
                            "title": "Automated Curriculum Learning for Neural Networks",
                            "authors": [
                                {
                                    "authorId": "1753223",
                                    "name": "Alex Graves"
                                },
                                {
                                    "authorId": "1792298",
                                    "name": "Marc G. Bellemare"
                                },
                                {
                                    "authorId": "10698483",
                                    "name": "Jacob Menick"
                                },
                                {
                                    "authorId": "1708654",
                                    "name": "R. Munos"
                                },
                                {
                                    "authorId": "2645384",
                                    "name": "K. Kavukcuoglu"
                                }
                            ],
                            "year": 2017,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 530
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zeng et al., 2023)",
                        "snippets": [
                            "Automatic. Though pre-defined strategies have been demonstrated effective in various application scenarios, they are often plagued by their strong reliance on human expertise to define an appropriate difficulty measurer and an extensive search for effective combinations of difficulty measurer and training scheduler. Therefore, in addition to the four pre-defined CL strategies described above, we further designed an automatic difficulty measurer to dynamically select data samples based on instance-wise training loss and enable a more flexible training process. Specifically, the automatic difficulty measurer used in this study characterizes data samples as either easy and difficult, which represents the samples whose ground truth scores are correctly or incorrectly predicted by a model in a training epoch."
                        ],
                        "paper": {
                            "corpus_id": 259593617,
                            "title": "On the Effectiveness of Curriculum Learning in Educational Text Scoring",
                            "authors": [
                                {
                                    "authorId": "8647899",
                                    "name": "Zijie Zeng"
                                },
                                {
                                    "authorId": "65953975",
                                    "name": "D. Ga\u0161evi\u0107"
                                },
                                {
                                    "authorId": "49901492",
                                    "name": "Guangliang Chen"
                                }
                            ],
                            "year": 2023,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 6
                        },
                        "score": 0.623046875
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "CCL integrates a difficulty measure and a proficiency estimator directly derived from the CL losses observed in both the training and validation phases. This innovation eliminates the dependency on task-specific metrics, streamlining the implementation process."
                        ],
                        "paper": {
                            "corpus_id": 273901573,
                            "title": "Curriculum Consistency Learning for Conditional Sentence Generation",
                            "authors": [
                                {
                                    "authorId": "2287881971",
                                    "name": "Liangxin Liu"
                                },
                                {
                                    "authorId": "2256344322",
                                    "name": "Xuebo Liu"
                                },
                                {
                                    "authorId": "2324576203",
                                    "name": "Lian Lian"
                                },
                                {
                                    "authorId": "2324857773",
                                    "name": "Shengjun Cheng"
                                },
                                {
                                    "authorId": "2142545243",
                                    "name": "Jun Rao"
                                },
                                {
                                    "authorId": "2273704740",
                                    "name": "Tengfei Yu"
                                },
                                {
                                    "authorId": "2193388247",
                                    "name": "Hexuan Deng"
                                },
                                {
                                    "authorId": "2269805934",
                                    "name": "Min Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 1
                        },
                        "score": 0.62451171875
                    },
                    {
                        "id": "(Li et al._1, 2022)",
                        "snippets": [
                            "These techniques improve data and training efficiency, but existing solutions have several limitations: \u2022 Techniques like curriculum learning (CL) improve data efficiency by indexing and sampling training data based on certain difficulty metric (Bengio et al., 2009), and it has recently proved effective on large-scale pretraining tasks (Li, Zhang, and He 2022)."
                        ],
                        "paper": {
                            "corpus_id": 254366532,
                            "title": "DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing",
                            "authors": [
                                {
                                    "authorId": "2609325",
                                    "name": "Conglong Li"
                                },
                                {
                                    "authorId": "9088433",
                                    "name": "Z. Yao"
                                },
                                {
                                    "authorId": "2129511744",
                                    "name": "Xiaoxia Wu"
                                },
                                {
                                    "authorId": "67016465",
                                    "name": "Minjia Zhang"
                                },
                                {
                                    "authorId": "2145020341",
                                    "name": "Yuxiong He"
                                }
                            ],
                            "year": 2022,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 25
                        },
                        "score": 0.57861328125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Applications and Effectiveness in LLM Training",
                "tldr": "Curriculum learning strategies have shown considerable effectiveness in LLM training across both pre-training and fine-tuning stages, leading to improved model performance and training efficiency. Research demonstrates that organizing training data by difficulty\u2014whether using traditional metrics like sequence length and word frequency or newer model-centric approaches\u2014yields better results than random shuffling. (8 sources)",
                "text": "\nCurriculum learning (CL) has emerged as a powerful approach for enhancing the training of large language models. By organizing training data from easier to more difficult examples, CL has proven particularly effective for large-scale pre-training tasks, improving both data and training efficiency <Paper corpusId=\"254366532\" paperTitle=\"(Li et al._1, 2022)\" isShortName></Paper>. This approach mimics human learning processes, providing a more structured path for model development than traditional random shuffling methods.\n\nRecent research has implemented curriculum learning at various stages of LLM development. At the pre-training stage, Li et al. employed sequence length as a difficulty metric, starting with shorter sequences and gradually incorporating longer ones <Paper corpusId=\"269741199\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>. Similarly, Wang et al. used word frequency as a complexity metric for curriculum-based pre-training, with less frequent words considered more difficult <Paper corpusId=\"269741199\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>. These linguistically motivated curricula have demonstrated tangible benefits for model performance.\n\nA particularly notable advancement comes from Ranaldi et al., who developed a complexity measure based on length, rarity, and comprehensibility of samples (LRC). When applied to pre-training BERT and RoBERTa from scratch, their curriculum learning approach led to improved performance on downstream tasks compared to existing methods <Paper corpusId=\"269741199\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper> <Paper corpusId=\"265068175\" paperTitle=\"(Ranaldi et al., 2023)\" isShortName></Paper>. This suggests that more sophisticated, multi-dimensional difficulty metrics can yield even stronger results than single-factor approaches.\n\nFor fine-tuning stages, curriculum learning has also shown significant benefits. Zhang et al. developed a framework consisting of \"Review\" and \"Arrange\" stages to address the challenges of defining instance difficulty and arranging an appropriate curriculum. Their approach yielded substantial performance improvements across various natural language understanding tasks in different languages <Paper corpusId=\"269741199\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper> <Paper corpusId=\"243766208\" paperTitle=\"(Zhang et al._1, 2021)\" isShortName></Paper>.\n\nBeyond traditional metrics, researchers have explored more model-centric approaches to measuring data difficulty. Kim et al. proposed calculating difficulty from the model's perspective rather than relying solely on data-specific characteristics like text length or word rarity. Their method uses cross-entropy loss as a direct indicator of how challenging the model finds particular examples, with higher loss values correlating with increased difficulty <Paper corpusId=\"269756933\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>. This approach acknowledges that true learning difficulty depends on the interaction between the model and the data, not just inherent data properties.\n\nIn data pruning applications, Marion et al. investigated multiple neural heuristics of sample difficulty and concluded that the perplexity of a sample under a reference language model serves as the best pruning metric <Paper corpusId=\"270199394\" paperTitle=\"(Ankner et al., 2024)\" isShortName></Paper>. This finding highlights the value of model-based difficulty metrics for optimizing training data selection.\n\nCurriculum learning principles have also been applied to knowledge distillation processes. Adaptive knowledge distillation techniques adjust the weight of each sample's loss based on its difficulty or use adaptive temperature in the softmax function to control the smoothness of the teacher model's predictions. This targeted approach emphasizes harder samples that the student model struggles with, providing more detailed knowledge transfer from the teacher model and improving performance more efficiently than uniform distillation methods <Paper corpusId=\"277398866\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper>.\n\nIn scenarios with noisy or uneven-quality data, curriculum learning offers additional benefits. Liu et al. demonstrated that a curriculum learning framework for natural answer generation (CL-NAG) could effectively exploit valuable information from noisy corpora by first utilizing simple, low-quality question-answer pairs to learn a basic model, then gradually incorporating more complex, higher-quality examples <Paper corpusId=\"269741199\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper> <Paper corpusId=\"51606954\" paperTitle=\"(Liu et al., 2018)\" isShortName></Paper>. This approach increased accuracy for both simple and complex questions compared to state-of-the-art methods.",
                "citations": [
                    {
                        "id": "(Li et al._1, 2022)",
                        "snippets": [
                            "These techniques improve data and training efficiency, but existing solutions have several limitations: \u2022 Techniques like curriculum learning (CL) improve data efficiency by indexing and sampling training data based on certain difficulty metric (Bengio et al., 2009), and it has recently proved effective on large-scale pretraining tasks (Li, Zhang, and He 2022)."
                        ],
                        "paper": {
                            "corpus_id": 254366532,
                            "title": "DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing",
                            "authors": [
                                {
                                    "authorId": "2609325",
                                    "name": "Conglong Li"
                                },
                                {
                                    "authorId": "9088433",
                                    "name": "Z. Yao"
                                },
                                {
                                    "authorId": "2129511744",
                                    "name": "Xiaoxia Wu"
                                },
                                {
                                    "authorId": "67016465",
                                    "name": "Minjia Zhang"
                                },
                                {
                                    "authorId": "2145020341",
                                    "name": "Yuxiong He"
                                }
                            ],
                            "year": 2022,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 25
                        },
                        "score": 0.57861328125
                    },
                    {
                        "id": "(Lee et al., 2024)",
                        "snippets": [
                            "Curriculum Learning It is observed that deep learning model training can benefit from the implementation of Curriculum Learning (CL), i.e., using data samples sorted based on a curriculum versus training on the randomly shuffled data (Soviany et al., 2021). Recently, CL methods have been developed and deployed for the LMs as well, at pre-training and post-training stages using a variety of linguistically motivated curricula such as sentence length or term frequency complexity measure based ranking (Liu et al., 2018)(Zhang et al., 2021)Campos, 2021;Weber et al., 2023)",
                            "Li et al., 2021 implemented a CL at the pre-training of LMs using the sequence length as the difficulty metric with the curriculum of starting from the shorter sequence training data toward the longer sequence",
                            "(Ranaldi et al., 2023) proposed a new complexity measure based on the length, rarity, and comprehensibility of the samples and sorted the corpus according to the proposed complexity measure during the pre-training stage and showed that their CL approach led to better performance in downstream tasks",
                            "Wang et al., 2022 used the frequency of words as the complexity metric for the curriculum-based pre-training of LMs."
                        ],
                        "paper": {
                            "corpus_id": 269741199,
                            "title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation",
                            "authors": [
                                {
                                    "authorId": "2116659852",
                                    "name": "Joonho Lee"
                                },
                                {
                                    "authorId": "2301016207",
                                    "name": "Jae Oh Woo"
                                },
                                {
                                    "authorId": "2301014929",
                                    "name": "Juree Seok"
                                },
                                {
                                    "authorId": "2301015562",
                                    "name": "Parisa Hassanzadeh"
                                },
                                {
                                    "authorId": "2301015224",
                                    "name": "Wooseok Jang"
                                },
                                {
                                    "authorId": "2301016428",
                                    "name": "JuYoun Son"
                                },
                                {
                                    "authorId": "91748824",
                                    "name": "Sima Didari"
                                },
                                {
                                    "authorId": "2301014892",
                                    "name": "Baruch Gutow"
                                },
                                {
                                    "authorId": "2065513368",
                                    "name": "Heng Hao"
                                },
                                {
                                    "authorId": "2301015932",
                                    "name": "Hankyu Moon"
                                },
                                {
                                    "authorId": "2301166595",
                                    "name": "Wenjun Hu"
                                },
                                {
                                    "authorId": "2301413463",
                                    "name": "Yeong-Dae Kwon"
                                },
                                {
                                    "authorId": "2301133161",
                                    "name": "Taehee Lee"
                                },
                                {
                                    "authorId": "2301015935",
                                    "name": "Seungjai Min"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 2
                        },
                        "score": 0.6982421875
                    },
                    {
                        "id": "(Ranaldi et al., 2023)",
                        "snippets": [
                            "Directly learning from complex examples is generally problematic for humans and machines. Indeed, a better strategy is exposing learners to examples in a reasonable, pedagogically-motivated order. Curriculum Learning (CL) has been proposed to import this strategy when training machine learning models. In this paper, building on Curriculum Learning, we propose a novel, linguistically motivated measure to determine example complexity for organizing examples during learning. Our complexity measure - LRC- is based on length, rarity, and comprehensibility. Our resulting learning model is CL-LRC, that is, CL with LRC. Experiments on downstream tasks show that CL-LRC outperforms existing CL and non-CL methods for training BERT and RoBERTa from scratch. Furthermore, we analyzed different measures, including perplexity, loss, and learning curve of different models pre-trained from scratch, showing that CL-LRC performs better than the state-of-the-art."
                        ],
                        "paper": {
                            "corpus_id": 265068175,
                            "title": "Modeling Easiness for Training Transformers with Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "2008183566",
                                    "name": "Leonardo Ranaldi"
                                },
                                {
                                    "authorId": "2199247500",
                                    "name": "Giulia Pucci"
                                },
                                {
                                    "authorId": "103839825",
                                    "name": "F. M. Zanzotto"
                                }
                            ],
                            "year": 2023,
                            "venue": "Recent Advances in Natural Language Processing",
                            "n_citations": 13
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al._1, 2021)",
                        "snippets": [
                            "With the notable success of pretrained language models, the pretraining-fine-tuning paradigm has become a dominant solution for natural language understanding (NLU) tasks. Typically, the training instances of a target NLU task are introduced in a completely random order and treated equally at the fine-tuning stage. However, these instances can vary greatly in difficulty, and similar to human learning procedures, language models can benefit from an easy-to-difficult curriculum. Based on this concept, we propose a curriculum learning (CL) framework. Our framework consists of two stages, Review and Arrange, targeting the two main challenges in curriculum learning, i.e., how to define the difficulty of instances and how to arrange a curriculum based on the difficulty, respectively. In the first stage, we devise a cross-review (CR) method to train several teacher models first and then review the training set in a crossed manner to distinguish easy instances from difficult instances. In the second stage, two sampling algorithms, a coarse-grained arrangement (CGA) and a fine-grained arrangement (FGA), are proposed to arrange a curriculum for language models in which the learning materials start from the easiest instances, and more difficult instances are gradually added into the training procedure. Compared to previous heuristic CL methods, our framework can avoid the errors caused by a gap in difficulty between humans and machines and has strong generalization ability. We conduct comprehensive experiments, and the results show that our curriculum learning framework, without any manual model architecture design or use of external data, obtains significant and universal performance improvements on a wide range of NLU tasks in different languages."
                        ],
                        "paper": {
                            "corpus_id": 243766208,
                            "title": "Review and Arrange: Curriculum Learning for Natural Language Understanding",
                            "authors": [
                                {
                                    "authorId": "48378753",
                                    "name": "L. Zhang"
                                },
                                {
                                    "authorId": "1855978",
                                    "name": "Zhendong Mao"
                                },
                                {
                                    "authorId": "1754285124",
                                    "name": "Benfeng Xu"
                                },
                                {
                                    "authorId": "143906199",
                                    "name": "Quan Wang"
                                },
                                {
                                    "authorId": "1699819",
                                    "name": "Yongdong Zhang"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
                            "n_citations": 14
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kim et al., 2024)",
                        "snippets": [
                            "In the development of training data for curriculum learning, accurately measuring data difficulty is crucial. Specifically, for LLMs (Large Language Models), determining the degree of data difficulty is challenging. Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021; Nagatsuka et al., 2023). However, these metrics may not fully reflect the complexity of a dataset. It is essential to assess data difficulty from the model's perspective, rather than relying solely on data-specific metrics. Our research proposes a new approach to calculate the degree of difficulty based on a model-centric perspective. By organizing the training dataset according to difficulty using our new metric, we aim to improve the model's performance compared to random shuffling.\n\n3 Methods for Quantitative Difficulty Measurement\n\nIn this section, we introduce a novel methodology for training models that begins with easier tasks and methodically progresses to more challenging ones. This approach requires organizing data by its level of complexity, for which we have established three principal criteria. By arranging the data in an order that goes from less to more difficult, this approach establishes a structured progression for learning.\n\n## Loss\nA high loss signifies a substantial discrepancy between the predicted and actual outcomes, indicating that the model perceives the data as challenging.\n\nWe establish a direct correlation between high loss values and increased difficulty. Therefore, prompts that generate higher loss are considered more challenging for the model to learn. This method computes the cross-entropy loss associated with the answer segment of the prompt."
                        ],
                        "paper": {
                            "corpus_id": 269756933,
                            "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "2301165169",
                                    "name": "Jisu Kim"
                                },
                                {
                                    "authorId": "2301167177",
                                    "name": "Juhwan Lee"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.88037109375
                    },
                    {
                        "id": "(Ankner et al., 2024)",
                        "snippets": [
                            "Marion et al. (2023) investigate data pruning based on multiple neural heuristics of sample difficulty, ultimately concluding that the perplexity of a sample under a reference language model is the best pruning metric."
                        ],
                        "paper": {
                            "corpus_id": 270199394,
                            "title": "Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models",
                            "authors": [
                                {
                                    "authorId": "2172356226",
                                    "name": "Zachary Ankner"
                                },
                                {
                                    "authorId": "73066558",
                                    "name": "Cody Blakeney"
                                },
                                {
                                    "authorId": "34824875",
                                    "name": "Kartik K. Sreenivasan"
                                },
                                {
                                    "authorId": "2304322161",
                                    "name": "Max Marion"
                                },
                                {
                                    "authorId": "2028252288",
                                    "name": "Matthew L. Leavitt"
                                },
                                {
                                    "authorId": "1690452",
                                    "name": "Mansheej Paul"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 34
                        },
                        "score": 0.775390625
                    },
                    {
                        "id": "(Zhang et al., 2025)",
                        "snippets": [
                            "For instance, if the student model performs well on easier samples but struggles with harder ones, the distillation process will emphasize these harder samples, providing more detailed and nuanced knowledge transfer from the teacher model. This targeted training helps in improving the student model's performance more efficiently compared to uniform distillation methods. Adaptive KD can be implemented using various techniques, such as adjusting the weight of each sample's loss based on its difficulty or using an adaptive temperature in the softmax function to control the smoothness of the teacher model's predictions."
                        ],
                        "paper": {
                            "corpus_id": 277398866,
                            "title": "Distilling Wisdom: A Review on Optimizing Learning From Massive Language Models",
                            "authors": [
                                {
                                    "authorId": "2326763710",
                                    "name": "Dingzong Zhang"
                                },
                                {
                                    "authorId": "2345374431",
                                    "name": "Devi Listiyani"
                                },
                                {
                                    "authorId": "2302811064",
                                    "name": "Priyanka Singh"
                                },
                                {
                                    "authorId": "2309099465",
                                    "name": "Manoranjan Mohanty"
                                }
                            ],
                            "year": 2025,
                            "venue": "IEEE Access",
                            "n_citations": 0
                        },
                        "score": 0.69091796875
                    },
                    {
                        "id": "(Liu et al., 2018)",
                        "snippets": [
                            "By reason of being able to obtain natural language responses, natural answers are more favored in real-world Question Answering (QA) systems. Generative models learn to automatically generate natural answers from large-scale question answer pairs (QA-pairs). However, they are suffering from the uncontrollable and uneven quality of QA-pairs crawled from the Internet. To address this problem, we propose a curriculum learning based framework for natural answer generation (CL-NAG), which is able to take full advantage of the valuable learning data from a noisy and uneven-quality corpus. Specifically, we employ two practical measures to automatically measure the quality (complexity) of QA-pairs. Based on the measurements, CL-NAG firstly utilizes simple and low-quality QA-pairs to learn a basic model, and then gradually learns to produce better answers with richer contents and more complete syntaxes based on more complex and higher-quality QA-pairs. In this way, all valuable information in the noisy and uneven-quality corpus could be fully exploited. Experiments demonstrate that CL-NAG outperforms the state-of-the-arts, which increases 6.8% and 8.7% in the accuracy for simple and complex questions, respectively."
                        ],
                        "paper": {
                            "corpus_id": 51606954,
                            "title": "Curriculum Learning for Natural Answer Generation",
                            "authors": [
                                {
                                    "authorId": "49046540",
                                    "name": "Cao Liu"
                                },
                                {
                                    "authorId": "1954845",
                                    "name": "Shizhu He"
                                },
                                {
                                    "authorId": "2200096",
                                    "name": "Kang Liu"
                                },
                                {
                                    "authorId": "1390572170",
                                    "name": "Jun Zhao"
                                }
                            ],
                            "year": 2018,
                            "venue": "International Joint Conference on Artificial Intelligence",
                            "n_citations": 85
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.308826
    }
}