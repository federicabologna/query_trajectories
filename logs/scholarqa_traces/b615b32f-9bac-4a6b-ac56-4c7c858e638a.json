{
    "query": "What are the most effective methods for improving the calibration of language model confidence scores, such as multicalibration or softmax-based estimation, and how do these techniques compare in practice?",
    "user_id": "lib_user",
    "task_id": "b615b32f-9bac-4a6b-ac56-4c7c858e638a",
    "timestamp": "2025-06-24T00:46:14.373244",
    "n_retrieval": 256,
    "n_retrieved": 255,
    "n_candidates": 38,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.5538839999999999,
    "decomposed_query": {
        "rewritten_query": "Most effective methods for improving the calibration of language model confidence scores, such as multicalibration or softmax-based estimation, and how these techniques compare in practice.",
        "keyword_query": "language model confidence calibration multicalibration softmax estimation techniques comparison",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009993,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Confidence Estimation For LLM-Based Dialogue State Tracking",
            "venue": "Spoken Language Technology Workshop",
            "year": 2024,
            "reference_count": 31,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.09629, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2321493889",
                    "name": "Yi-Jyun Sun"
                },
                {
                    "authorId": "29832722",
                    "name": "Suvodip Dey"
                },
                {
                    "authorId": "2365041900",
                    "name": "Dilek Hakkani-Tur"
                },
                {
                    "authorId": "5108268",
                    "name": "Gokhan Tur"
                }
            ],
            "abstract": "Estimation of a model\u2019s confidence on its outputs is critical for Conversational AI systems based on large language models (LLMs), especially for reducing hallucination and preventing over-reliance. In this work, we provide an exhaustive exploration of methods, including approaches proposed for open- and closed-weight LLMs, aimed at quantifying and leveraging model uncertainty to improve the reliability of LLM-generated responses, specifically focusing on dialogue state tracking (DST) in task-oriented dialogue systems (TODS). Regardless of the model type, well-calibrated confidence scores are essential to handle uncertainties, thereby improving model performance. We evaluate four methods for estimating confidence scores based on softmax, raw token scores, verbalized confidences, and a combination of these methods, using the area under the curve (AUC) metric to assess calibration, with higher AUC indicating better calibration. We also enhance these with a self-probing mechanism, proposed for closed models. Furthermore, we assess these methods using an open-weight model fine-tuned for the task of DST, achieving superior joint goal accuracy (JGA). Our findings also suggest that fine-tuning open-weight LLMs can result in enhanced AUC performance, indicating better confidence score calibration.",
            "corpus_id": 272689376,
            "sentences": [
                {
                    "corpus_id": "272689376",
                    "title": "Confidence Estimation For LLM-Based Dialogue State Tracking",
                    "text": "Estimation of a model\u2019s confidence on its outputs is critical for Conversational AI systems based on large language models (LLMs), especially for reducing hallucination and preventing over-reliance. In this work, we provide an exhaustive exploration of methods, including approaches proposed for open- and closed-weight LLMs, aimed at quantifying and leveraging model uncertainty to improve the reliability of LLM-generated responses, specifically focusing on dialogue state tracking (DST) in task-oriented dialogue systems (TODS). Regardless of the model type, well-calibrated confidence scores are essential to handle uncertainties, thereby improving model performance. We evaluate four methods for estimating confidence scores based on softmax, raw token scores, verbalized confidences, and a combination of these methods, using the area under the curve (AUC) metric to assess calibration, with higher AUC indicating better calibration. We also enhance these with a self-probing mechanism, proposed for closed models. Furthermore, we assess these methods using an open-weight model fine-tuned for the task of DST, achieving superior joint goal accuracy (JGA). Our findings also suggest that fine-tuning open-weight LLMs can result in enhanced AUC performance, indicating better confidence score calibration.",
                    "score": 0.45739263470743674,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9814453125
                }
            ],
            "relevance_judgement": 0.9814453125,
            "relevance_judgment_input_expanded": "# Title: Confidence Estimation For LLM-Based Dialogue State Tracking\n# Venue: Spoken Language Technology Workshop\n# Authors: Yi-Jyun Sun, Suvodip Dey, Dilek Hakkani-Tur, Gokhan Tur\n## Abstract\nEstimation of a model\u2019s confidence on its outputs is critical for Conversational AI systems based on large language models (LLMs), especially for reducing hallucination and preventing over-reliance. In this work, we provide an exhaustive exploration of methods, including approaches proposed for open- and closed-weight LLMs, aimed at quantifying and leveraging model uncertainty to improve the reliability of LLM-generated responses, specifically focusing on dialogue state tracking (DST) in task-oriented dialogue systems (TODS). Regardless of the model type, well-calibrated confidence scores are essential to handle uncertainties, thereby improving model performance. We evaluate four methods for estimating confidence scores based on softmax, raw token scores, verbalized confidences, and a combination of these methods, using the area under the curve (AUC) metric to assess calibration, with higher AUC indicating better calibration. We also enhance these with a self-probing mechanism, proposed for closed models. Furthermore, we assess these methods using an open-weight model fine-tuned for the task of DST, achieving superior joint goal accuracy (JGA). Our findings also suggest that fine-tuning open-weight LLMs can result in enhanced AUC performance, indicating better confidence score calibration.\n",
            "reference_string": "[272689376 | Sun et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Improving Automatic Text Recognition with Language Models in the PyLaia Open-Source Library",
            "venue": "IEEE International Conference on Document Analysis and Recognition",
            "year": 2024,
            "reference_count": 31,
            "citation_count": 4,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.18722, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "116700157",
                    "name": "Sol\u00e8ne Tarride"
                },
                {
                    "authorId": "2298900821",
                    "name": "Yoann Schneider"
                },
                {
                    "authorId": "2298903215",
                    "name": "Marie Generali-Lince"
                },
                {
                    "authorId": "1380222130",
                    "name": "M\u00e9lodie Boillet"
                },
                {
                    "authorId": "1491425010",
                    "name": "Bastien Abadie"
                },
                {
                    "authorId": "2285800930",
                    "name": "Christopher Kermorvant"
                }
            ],
            "abstract": "PyLaia is one of the most popular open-source software for Automatic Text Recognition (ATR), delivering strong performance in terms of speed and accuracy. In this paper, we outline our recent contributions to the PyLaia library, focusing on the incorporation of reliable confidence scores and the integration of statistical language modeling during decoding. Our implementation provides an easy way to combine PyLaia with n-grams language models at different levels. One of the highlights of this work is that language models are completely auto-tuned: they can be built and used easily without any expert knowledge, and without requiring any additional data. To demonstrate the significance of our contribution, we evaluate PyLaia's performance on twelve datasets, both with and without language modelling. The results show that decoding with small language models improves the Word Error Rate by 13% and the Character Error Rate by 12% in average. Additionally, we conduct an analysis of confidence scores and highlight the importance of calibration techniques. Our implementation is publicly available in the official PyLaia repository at https://gitlab.teklia.com/atr/pylaia, and twelve open-source models are released on Hugging Face.",
            "corpus_id": 269449363,
            "sentences": [
                {
                    "corpus_id": "269449363",
                    "title": "Improving Automatic Text Recognition with Language Models in the PyLaia Open-Source Library",
                    "text": "First, we compare different confidence estimation methods by measuring the correlation between confidence scores and recognition rate.Then, we discuss the benefit of confidence calibration using temperature scaling.\n\nBest method for confidence estimation Table 2 presents the correlation between different computation methods for confidence scores and the recognition rate, defined as 1 \u2212 CER.Both Spearman (rank correlation) and Pearson (linear correlation) coefficients are computed to capture different aspects of the correlation.Unfortunately, we find that the Monte Carlo dropout approach leads to a degradation of recognition quality in addition to high computational cost.Therefore, this method is excluded from our analysis.\n\nThe results show that the most effective method is to take the softmax value for each token after the CTC decoding step and average these values for each line to obtain line-level confidence scores.This approach not only ensures computational efficiency, but also yields a strong correlation with the recognition rate.As a result of these findings, we have incorporated this calculation method into PyLaia.\n\nConfidence score calibration The optimal temperature T is determined using a grid search algorithm on the validation set (from 1 to 6 with a step of 0.5).Once T is found, we compute the correlation between confidence scores and the recognition rate on the test set, with and without temperature scaling.\n\nTable 3 provides insight into the impact of model calibration on the linear correlation between confidence scores and recognition rate on the test set of different datasets.\n\nTable 3: Impact of model calibration using the optimal temperature T on the correlation between confidence scores and the recognition rate.In all experiments, the p-value is always well below 0.05, indicating that the linear correlation is always significant.The results show significant improvements in the linear correlation between confidence scores and recognition rates when temperature scaling is applied.Across different datasets, optimal temperatures ranging from 1.5 to 4.0 yield consistently higher correlations compared to scenarios without scaling.Since temperature scaling only scales the logits without changing the underlying ranking of the predictions, it does not affect the rank correlation.\n\nOur analysis suggests that the default value of 2.0 is a robust choice for calibration, even for datasets where the optimal temperature is not equal to 2.0.",
                    "score": 0.3975211698930668,
                    "section_title": "Confidence score analysis",
                    "char_start_offset": 19580,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 134
                        },
                        {
                            "start": 134,
                            "end": 215
                        },
                        {
                            "start": 217,
                            "end": 393
                        },
                        {
                            "start": 393,
                            "end": 533
                        },
                        {
                            "start": 533,
                            "end": 679
                        },
                        {
                            "start": 679,
                            "end": 732
                        },
                        {
                            "start": 734,
                            "end": 932
                        },
                        {
                            "start": 932,
                            "end": 1052
                        },
                        {
                            "start": 1052,
                            "end": 1140
                        },
                        {
                            "start": 1142,
                            "end": 1296
                        },
                        {
                            "start": 1296,
                            "end": 1445
                        },
                        {
                            "start": 1447,
                            "end": 1620
                        },
                        {
                            "start": 1622,
                            "end": 1761
                        },
                        {
                            "start": 1761,
                            "end": 1881
                        },
                        {
                            "start": 1881,
                            "end": 2033
                        },
                        {
                            "start": 2033,
                            "end": 2182
                        },
                        {
                            "start": 2182,
                            "end": 2331
                        },
                        {
                            "start": 2333,
                            "end": 2489
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97314453125
                }
            ],
            "relevance_judgement": 0.97314453125,
            "relevance_judgment_input_expanded": "# Title: Improving Automatic Text Recognition with Language Models in the PyLaia Open-Source Library\n# Venue: IEEE International Conference on Document Analysis and Recognition\n# Authors: Sol\u00e8ne Tarride, Yoann Schneider, Marie Generali-Lince, M\u00e9lodie Boillet, Bastien Abadie, Christopher Kermorvant\n## Abstract\nPyLaia is one of the most popular open-source software for Automatic Text Recognition (ATR), delivering strong performance in terms of speed and accuracy. In this paper, we outline our recent contributions to the PyLaia library, focusing on the incorporation of reliable confidence scores and the integration of statistical language modeling during decoding. Our implementation provides an easy way to combine PyLaia with n-grams language models at different levels. One of the highlights of this work is that language models are completely auto-tuned: they can be built and used easily without any expert knowledge, and without requiring any additional data. To demonstrate the significance of our contribution, we evaluate PyLaia's performance on twelve datasets, both with and without language modelling. The results show that decoding with small language models improves the Word Error Rate by 13% and the Character Error Rate by 12% in average. Additionally, we conduct an analysis of confidence scores and highlight the importance of calibration techniques. Our implementation is publicly available in the official PyLaia repository at https://gitlab.teklia.com/atr/pylaia, and twelve open-source models are released on Hugging Face.\n## Confidence score analysis\nFirst, we compare different confidence estimation methods by measuring the correlation between confidence scores and recognition rate.Then, we discuss the benefit of confidence calibration using temperature scaling.\n\nBest method for confidence estimation Table 2 presents the correlation between different computation methods for confidence scores and the recognition rate, defined as 1 \u2212 CER.Both Spearman (rank correlation) and Pearson (linear correlation) coefficients are computed to capture different aspects of the correlation.Unfortunately, we find that the Monte Carlo dropout approach leads to a degradation of recognition quality in addition to high computational cost.Therefore, this method is excluded from our analysis.\n\nThe results show that the most effective method is to take the softmax value for each token after the CTC decoding step and average these values for each line to obtain line-level confidence scores.This approach not only ensures computational efficiency, but also yields a strong correlation with the recognition rate.As a result of these findings, we have incorporated this calculation method into PyLaia.\n\nConfidence score calibration The optimal temperature T is determined using a grid search algorithm on the validation set (from 1 to 6 with a step of 0.5).Once T is found, we compute the correlation between confidence scores and the recognition rate on the test set, with and without temperature scaling.\n\nTable 3 provides insight into the impact of model calibration on the linear correlation between confidence scores and recognition rate on the test set of different datasets.\n\nTable 3: Impact of model calibration using the optimal temperature T on the correlation between confidence scores and the recognition rate.In all experiments, the p-value is always well below 0.05, indicating that the linear correlation is always significant.The results show significant improvements in the linear correlation between confidence scores and recognition rates when temperature scaling is applied.Across different datasets, optimal temperatures ranging from 1.5 to 4.0 yield consistently higher correlations compared to scenarios without scaling.Since temperature scaling only scales the logits without changing the underlying ranking of the predictions, it does not affect the rank correlation.\n\nOur analysis suggests that the default value of 2.0 is a robust choice for calibration, even for datasets where the optimal temperature is not equal to 2.0.",
            "reference_string": "[269449363 | Tarride et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Multicalibration for Confidence Scoring in LLMs",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 69,
            "citation_count": 19,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.04689, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2295667267",
                    "name": "Gianluca Detommaso"
                },
                {
                    "authorId": "2295665717",
                    "name": "Martin Bertran"
                },
                {
                    "authorId": "2295664744",
                    "name": "Riccardo Fogliato"
                },
                {
                    "authorId": "2295665299",
                    "name": "Aaron Roth"
                }
            ],
            "abstract": "This paper proposes the use of\"multicalibration\"to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data. We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and\"self-annotation\"- querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods.",
            "corpus_id": 269004786,
            "sentences": [
                {
                    "corpus_id": "269004786",
                    "title": "Multicalibration for Confidence Scoring in LLMs",
                    "text": "This paper proposes the use of\"multicalibration\"to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data. We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and\"self-annotation\"- querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods.",
                    "score": 0.5635416005167772,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.970703125
                },
                {
                    "corpus_id": "269004786",
                    "title": "Multicalibration for Confidence Scoring in LLMs",
                    "text": "In this paper we introduce multicalibration to confidence scoring for LLMs, and develop several new techniques for both generating groups of prompts to multicalibrate with respect to, as well as new multicalibration algorithms which have improved practical performance.We show that when applied to existing scoring functions from the literature, our methods substantially improve both the error and calibration of the scores.What we have presented is an extensible framework, and so there is a clear pathway to improvement via new grouping strategies that are both semantically meaningful and correlated with LLM performance.",
                    "score": 0.4487285233628986,
                    "section_title": "Conclusions",
                    "char_start_offset": 31535,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 269
                        },
                        {
                            "start": 269,
                            "end": 425
                        },
                        {
                            "start": 425,
                            "end": 625
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9482421875
                }
            ],
            "relevance_judgement": 0.970703125,
            "relevance_judgment_input_expanded": "# Title: Multicalibration for Confidence Scoring in LLMs\n# Venue: International Conference on Machine Learning\n# Authors: Gianluca Detommaso, Martin Bertran, Riccardo Fogliato, Aaron Roth\n## Abstract\nThis paper proposes the use of\"multicalibration\"to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data. We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and\"self-annotation\"- querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods.\n## Conclusions\nIn this paper we introduce multicalibration to confidence scoring for LLMs, and develop several new techniques for both generating groups of prompts to multicalibrate with respect to, as well as new multicalibration algorithms which have improved practical performance.We show that when applied to existing scoring functions from the literature, our methods substantially improve both the error and calibration of the scores.What we have presented is an extensible framework, and so there is a clear pathway to improvement via new grouping strategies that are both semantically meaningful and correlated with LLM performance.",
            "reference_string": "[269004786 | Detommaso et al. | 2024 | Citations: 19]"
        },
        {
            "title": "Few-Shot Recalibration of Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 38,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.18286, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2293910776",
                    "name": "Xiang Lisa Li"
                },
                {
                    "authorId": "3030219",
                    "name": "Urvashi Khandelwal"
                },
                {
                    "authorId": "2091768",
                    "name": "Kelvin Guu"
                }
            ],
            "abstract": "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain. Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.",
            "corpus_id": 268723623,
            "sentences": [
                {
                    "corpus_id": "268723623",
                    "title": "Few-Shot Recalibration of Language Models",
                    "text": "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain. Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.",
                    "score": 0.42743692167850017,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.966796875
                }
            ],
            "relevance_judgement": 0.966796875,
            "relevance_judgment_input_expanded": "# Title: Few-Shot Recalibration of Language Models\n# Venue: arXiv.org\n# Authors: Xiang Lisa Li, Urvashi Khandelwal, Kelvin Guu\n## Abstract\nRecent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain. Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.\n",
            "reference_string": "[268723623 | Li et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Bag of Tricks for In-Distribution Calibration of Pretrained Transformers",
            "venue": "Findings",
            "year": 2023,
            "reference_count": 43,
            "citation_count": 5,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2302.06690",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.06690, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1590802650",
                    "name": "Jaeyoung Kim"
                },
                {
                    "authorId": "2059662364",
                    "name": "Dongbin Na"
                },
                {
                    "authorId": "7236231",
                    "name": "Sungchul Choi"
                },
                {
                    "authorId": "26381055",
                    "name": "Sungbin Lim"
                }
            ],
            "abstract": "While pre-trained language models (PLMs) have become a de-facto standard promoting the accuracy of text classification tasks, recent studies find that PLMs often predict over-confidently.Although calibration methods have been proposed, such as ensemble learning and data augmentation, most of the methods have been verified in computer vision benchmarks rather than in PLM-based text classification tasks. In this paper, we present an empirical study on confidence calibration for PLMs, addressing three categories, including confidence penalty losses, data augmentations, and ensemble methods. We find that the ensemble model overfitted to the training set shows sub-par calibration performance and also observe that PLMs trained with confidence penalty loss have a trade-off between calibration and accuracy. Building on these observations, we propose the Calibrated PLM (CALL), a combination of calibration techniques. The CALL complements shortcomings that may occur when utilizing a calibration method individually and boosts both classification and calibration accuracy. Design choices in CALL\u2019s training procedures are extensively studied, and we provide a detailed analysis of how calibration techniques affect the calibration performance of PLMs.",
            "corpus_id": 256846523,
            "sentences": [
                {
                    "corpus_id": "256846523",
                    "title": "Bag of Tricks for In-Distribution Calibration of Pretrained Transformers",
                    "text": "While pre-trained language models (PLMs) have become a de-facto standard promoting the accuracy of text classification tasks, recent studies find that PLMs often predict over-confidently.Although calibration methods have been proposed, such as ensemble learning and data augmentation, most of the methods have been verified in computer vision benchmarks rather than in PLM-based text classification tasks. In this paper, we present an empirical study on confidence calibration for PLMs, addressing three categories, including confidence penalty losses, data augmentations, and ensemble methods. We find that the ensemble model overfitted to the training set shows sub-par calibration performance and also observe that PLMs trained with confidence penalty loss have a trade-off between calibration and accuracy. Building on these observations, we propose the Calibrated PLM (CALL), a combination of calibration techniques. The CALL complements shortcomings that may occur when utilizing a calibration method individually and boosts both classification and calibration accuracy. Design choices in CALL\u2019s training procedures are extensively studied, and we provide a detailed analysis of how calibration techniques affect the calibration performance of PLMs.",
                    "score": 0.47390604669222347,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96484375
                }
            ],
            "relevance_judgement": 0.96484375,
            "relevance_judgment_input_expanded": "# Title: Bag of Tricks for In-Distribution Calibration of Pretrained Transformers\n# Venue: Findings\n# Authors: Jaeyoung Kim, Dongbin Na, Sungchul Choi, Sungbin Lim\n## Abstract\nWhile pre-trained language models (PLMs) have become a de-facto standard promoting the accuracy of text classification tasks, recent studies find that PLMs often predict over-confidently.Although calibration methods have been proposed, such as ensemble learning and data augmentation, most of the methods have been verified in computer vision benchmarks rather than in PLM-based text classification tasks. In this paper, we present an empirical study on confidence calibration for PLMs, addressing three categories, including confidence penalty losses, data augmentations, and ensemble methods. We find that the ensemble model overfitted to the training set shows sub-par calibration performance and also observe that PLMs trained with confidence penalty loss have a trade-off between calibration and accuracy. Building on these observations, we propose the Calibrated PLM (CALL), a combination of calibration techniques. The CALL complements shortcomings that may occur when utilizing a calibration method individually and boosts both classification and calibration accuracy. Design choices in CALL\u2019s training procedures are extensively studied, and we provide a detailed analysis of how calibration techniques affect the calibration performance of PLMs.\n",
            "reference_string": "[256846523 | Kim et al. | 2023 | Citations: 5]"
        },
        {
            "title": "Graph-based Confidence Calibration for Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 52,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.02454, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2329041193",
                    "name": "Yukun Li"
                },
                {
                    "authorId": "2238971842",
                    "name": "Sijia Wang"
                },
                {
                    "authorId": "2301269508",
                    "name": "Lifu Huang"
                },
                {
                    "authorId": "2294764328",
                    "name": "Li-Ping Liu"
                }
            ],
            "abstract": "Reliable confidence estimation is essential for enhancing the trustworthiness of large language models (LLMs), especially in high-stakes scenarios. Despite its importance, accurately estimating confidence in LLM responses remains a significant challenge. In this work, we propose using an auxiliary learning model to assess response correctness based on the self-consistency of multiple outputs generated by the LLM. Our method builds a consistency graph to represent the agreement among multiple responses and uses a graph neural network (GNN) to estimate the likelihood that each response is correct. Experiments demonstrate that this method has strong calibration performance on various benchmark datasets and generalizes well to out-of-domain cases.",
            "corpus_id": 273821704,
            "sentences": [
                {
                    "corpus_id": "273821704",
                    "title": "Graph-based Confidence Calibration for Large Language Models",
                    "text": "One important approach to improving the reliability of large language models (LLMs) is to provide accurate confidence estimations regarding the correctness of their answers. However, developing a well-calibrated confidence estimation model is challenging, as mistakes made by LLMs can be difficult to detect. We propose a novel method combining the LLM's self-consistency with labeled data and training an auxiliary model to estimate the correctness of its responses to questions. This auxiliary model predicts the correctness of responses based solely on their consistent information. To set up the learning problem, we use a weighted graph to represent the consistency among the LLM's multiple responses to a question. Correctness labels are assigned to these responses based on their similarity to the correct answer. We then train a graph neural network to estimate the probability of correct responses. Experiments demonstrate that the proposed approach substantially outperforms several of the most recent methods in confidence calibration across multiple widely adopted benchmark datasets. Furthermore, the proposed approach significantly improves the generalization capability of confidence calibration on out-of-domain (OOD) data.",
                    "score": 0.4043211166024709,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.962890625
                },
                {
                    "corpus_id": "273821704",
                    "title": "Graph-based Confidence Calibration for Large Language Models",
                    "text": "For the Llama3 model, the confidence calibration performance on TriviaQA is shown in Table 1. For the TriviaQA dataset, it can be observed that the likelihood-based method performs poorly on the calibration error (ECE and Brier Score) and AUROC due to unreliable model prediction probability (Zhang et al., 2024). Platt scaling improves the ECE post-calibration and enhances the model's discriminative ability, resulting in higher AUROC results. However, this method cannot capture the semantic equivalence among answers, leading to sub-optimal performance. The Verbalized and Verbalized Qual prompts LLM to output confidence for their answers, improving AUROC by 3 \u2212 5% compared with the likelihood baseline. However, it faces the overconfidence issue; thus, the calibration errors are still high. The GraphSpectral method can produce good confidence estimations, but its calibration performance is poor. Even with the addition of techniques such as Isotonic Calibration or Platt Scaling, this issue can only be partially mitigated. The auxiliary DeBERTa method combines the LLM outputs, Chain-of-Thoughts (CoT) outputs, and verbalized confidence to fine-tune the DeBERTa model for predicting confidence. Our method captures the prediction confidence based on the graph structure of LLM's responses in semantic space and achieves better ECE results. The ECE is reduced from 0.07 to 0.03 and improves the AUROC from 0.72 to 0.86 compared with the baseline calibration methods. The experiment results on TruthfulQA, HotpotQA and CoQA for the Llama3 model are shown in Table 1. These results show a similar trend, with our model achieving superior performance in confidence calibration compared to the baseline methods. Furthermore, we also compare the confidence calibration performance for the Vicuna model on the TriviaQA, CoQA, TruthfulQA and HotpotQA datasets. The results are summarized in Table 2. Our model consistently improves the calibration error compared to the baseline methods. Both GraphSpectral and our method have a similar assumption that the consistency level between responses indicates the confidence levels of these responses.",
                    "score": 0.42755302941991663,
                    "section_title": "Experiment results",
                    "char_start_offset": 23332,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 93
                        },
                        {
                            "start": 94,
                            "end": 313
                        },
                        {
                            "start": 314,
                            "end": 445
                        },
                        {
                            "start": 446,
                            "end": 557
                        },
                        {
                            "start": 558,
                            "end": 709
                        },
                        {
                            "start": 710,
                            "end": 798
                        },
                        {
                            "start": 799,
                            "end": 905
                        },
                        {
                            "start": 906,
                            "end": 1033
                        },
                        {
                            "start": 1034,
                            "end": 1205
                        },
                        {
                            "start": 1206,
                            "end": 1350
                        },
                        {
                            "start": 1351,
                            "end": 1476
                        },
                        {
                            "start": 1477,
                            "end": 1575
                        },
                        {
                            "start": 1576,
                            "end": 1717
                        },
                        {
                            "start": 1718,
                            "end": 1863
                        },
                        {
                            "start": 1864,
                            "end": 1990
                        },
                        {
                            "start": 1991,
                            "end": 2147
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8955078125
                },
                {
                    "corpus_id": "273821704",
                    "title": "Graph-based Confidence Calibration for Large Language Models",
                    "text": "However, manually designed features are limited in their ability to capture the full extent of self-consistency among LLM responses, leading to poor calibration performance. \n\nTo better calibrate the confidence estimation, some methods directly use correctness labels in their calibration procedures. Mielke et al. (2022) train a calibrator to predict the correctness of a response for a given question. With a similar idea, Ulmer et al. (2024) train a language model (e.g., DeBERTa) based on question-response pairs to predict the probability of responses' correctness. Based on SelfCheckGPT (Manakul et al., 2023) and JAFC (Tian et al., 2023), Chen et al. (2024) train supervised models to reduce grouping losses and improve the confidence estimation. The method by Liu et al. (2024) uses an LLM's latent representations to predict the correctness of responses. Detommaso et al. (2024) use the \"multicalibration\" technique to calibrate the probability of correctness. Fadeeva et al. (2023) offer a detailed comparative study of various confidence estimation methods, providing empirical evidence on their effectiveness across different tasks. However, these studies have not sufficiently exploited response consistency to predict the probabilities of the responses being correct.",
                    "score": 0.5564202991937732,
                    "section_title": "Related Work",
                    "char_start_offset": 7867,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 173
                        },
                        {
                            "start": 176,
                            "end": 300
                        },
                        {
                            "start": 301,
                            "end": 403
                        },
                        {
                            "start": 404,
                            "end": 570
                        },
                        {
                            "start": 571,
                            "end": 753
                        },
                        {
                            "start": 754,
                            "end": 863
                        },
                        {
                            "start": 864,
                            "end": 969
                        },
                        {
                            "start": 970,
                            "end": 1144
                        },
                        {
                            "start": 1145,
                            "end": 1281
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 301,
                            "end": 321,
                            "matchedPaperCorpusId": "250073258"
                        },
                        {
                            "start": 593,
                            "end": 615,
                            "matchedPaperCorpusId": "257557820"
                        },
                        {
                            "start": 625,
                            "end": 644,
                            "matchedPaperCorpusId": "258865733"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.890625
                }
            ],
            "relevance_judgement": 0.962890625,
            "relevance_judgment_input_expanded": "# Title: Graph-based Confidence Calibration for Large Language Models\n# Venue: arXiv.org\n# Authors: Yukun Li, Sijia Wang, Lifu Huang, Li-Ping Liu\n## Abstract\nReliable confidence estimation is essential for enhancing the trustworthiness of large language models (LLMs), especially in high-stakes scenarios. Despite its importance, accurately estimating confidence in LLM responses remains a significant challenge. In this work, we propose using an auxiliary learning model to assess response correctness based on the self-consistency of multiple outputs generated by the LLM. Our method builds a consistency graph to represent the agreement among multiple responses and uses a graph neural network (GNN) to estimate the likelihood that each response is correct. Experiments demonstrate that this method has strong calibration performance on various benchmark datasets and generalizes well to out-of-domain cases.\n## Related Work\nHowever, manually designed features are limited in their ability to capture the full extent of self-consistency among LLM responses, leading to poor calibration performance. \n\nTo better calibrate the confidence estimation, some methods directly use correctness labels in their calibration procedures. Mielke et al. (2022) train a calibrator to predict the correctness of a response for a given question. With a similar idea, Ulmer et al. (2024) train a language model (e.g., DeBERTa) based on question-response pairs to predict the probability of responses' correctness. Based on SelfCheckGPT (Manakul et al., 2023) and JAFC (Tian et al., 2023), Chen et al. (2024) train supervised models to reduce grouping losses and improve the confidence estimation. The method by Liu et al. (2024) uses an LLM's latent representations to predict the correctness of responses. Detommaso et al. (2024) use the \"multicalibration\" technique to calibrate the probability of correctness. Fadeeva et al. (2023) offer a detailed comparative study of various confidence estimation methods, providing empirical evidence on their effectiveness across different tasks. However, these studies have not sufficiently exploited response consistency to predict the probabilities of the responses being correct.\n\n## Experiment results\nFor the Llama3 model, the confidence calibration performance on TriviaQA is shown in Table 1. For the TriviaQA dataset, it can be observed that the likelihood-based method performs poorly on the calibration error (ECE and Brier Score) and AUROC due to unreliable model prediction probability (Zhang et al., 2024). Platt scaling improves the ECE post-calibration and enhances the model's discriminative ability, resulting in higher AUROC results. However, this method cannot capture the semantic equivalence among answers, leading to sub-optimal performance. The Verbalized and Verbalized Qual prompts LLM to output confidence for their answers, improving AUROC by 3 \u2212 5% compared with the likelihood baseline. However, it faces the overconfidence issue; thus, the calibration errors are still high. The GraphSpectral method can produce good confidence estimations, but its calibration performance is poor. Even with the addition of techniques such as Isotonic Calibration or Platt Scaling, this issue can only be partially mitigated. The auxiliary DeBERTa method combines the LLM outputs, Chain-of-Thoughts (CoT) outputs, and verbalized confidence to fine-tune the DeBERTa model for predicting confidence. Our method captures the prediction confidence based on the graph structure of LLM's responses in semantic space and achieves better ECE results. The ECE is reduced from 0.07 to 0.03 and improves the AUROC from 0.72 to 0.86 compared with the baseline calibration methods. The experiment results on TruthfulQA, HotpotQA and CoQA for the Llama3 model are shown in Table 1. These results show a similar trend, with our model achieving superior performance in confidence calibration compared to the baseline methods. Furthermore, we also compare the confidence calibration performance for the Vicuna model on the TriviaQA, CoQA, TruthfulQA and HotpotQA datasets. The results are summarized in Table 2. Our model consistently improves the calibration error compared to the baseline methods. Both GraphSpectral and our method have a similar assumption that the consistency level between responses indicates the confidence levels of these responses.",
            "reference_string": "[273821704 | Li et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Calibrating Long-form Generations from Large Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 50,
            "citation_count": 12,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.06544, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2145952181",
                    "name": "Yukun Huang"
                },
                {
                    "authorId": "2283783652",
                    "name": "Yixin Liu"
                },
                {
                    "authorId": "2283771339",
                    "name": "Raghuveer Thirukovalluru"
                },
                {
                    "authorId": "2266838179",
                    "name": "Arman Cohan"
                },
                {
                    "authorId": "2060730422",
                    "name": "Bhuwan Dhingra"
                }
            ],
            "abstract": "To enhance Large Language Models' (LLMs) reliability, calibration is essential -- the model's assessed confidence scores should align with the actual likelihood of its responses being correct. However, current confidence elicitation methods and calibration metrics typically rely on a binary true/false assessment of response correctness. This approach does not apply to long-form generation, where an answer can be partially correct. Addressing this gap, we introduce a unified calibration framework, in which both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores. Within this framework, we develop three metrics to precisely evaluate LLM calibration and further propose two confidence elicitation methods based on self-consistency and self-evaluation. Our experiments, which include long-form QA and summarization tasks, demonstrate that larger models don't necessarily guarantee better calibration, that calibration performance is found to be metric-dependent, and that self-consistency methods excel in factoid datasets. We also find that calibration can be enhanced through techniques such as fine-tuning, integrating relevant source documents, scaling the temperature, and combining self-consistency with self-evaluation. Lastly, we showcase a practical application of our system: selecting and cascading open-source models and ChatGPT to optimize correctness given a limited API budget. This research not only challenges existing notions of LLM calibration but also offers practical methodologies for improving trustworthiness in long-form generation.",
            "corpus_id": 267617073,
            "sentences": [
                {
                    "corpus_id": "267617073",
                    "title": "Calibrating Long-form Generations from Large Language Models",
                    "text": "To enhance Large Language Models' (LLMs) reliability, calibration is essential -- the model's assessed confidence scores should align with the actual likelihood of its responses being correct. However, current confidence elicitation methods and calibration metrics typically rely on a binary true/false assessment of response correctness. This approach does not apply to long-form generation, where an answer can be partially correct. Addressing this gap, we introduce a unified calibration framework, in which both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores. Within this framework, we develop three metrics to precisely evaluate LLM calibration and further propose two confidence elicitation methods based on self-consistency and self-evaluation. Our experiments, which include long-form QA and summarization tasks, demonstrate that larger models don't necessarily guarantee better calibration, that calibration performance is found to be metric-dependent, and that self-consistency methods excel in factoid datasets. We also find that calibration can be enhanced through techniques such as fine-tuning, integrating relevant source documents, scaling the temperature, and combining self-consistency with self-evaluation. Lastly, we showcase a practical application of our system: selecting and cascading open-source models and ChatGPT to optimize correctness given a limited API budget. This research not only challenges existing notions of LLM calibration but also offers practical methodologies for improving trustworthiness in long-form generation.",
                    "score": 0.4768309723492106,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9609375
                },
                {
                    "corpus_id": "267617073",
                    "title": "Calibrating Long-form Generations from Large Language Models",
                    "text": "\u2022 A cost-effective model usage strategy, illustrating the practicality of long-form calibration in optimizing LLM deployment. \n\nMeasuring Calibration Calibration (Guo et al., 2017, Minderer et al., 2021)has been widely studied in language models, whose probabilities derived from logits are generally found to not be calibrated (Jiang et al., 2020, Kadavath et al., 2022, Chen et al., 2023d). Standard metrics to measure the calibration include Expected Calibration Error (ECE) for confidence-accuracy disparity (Naeini et al., 2015), Brier Score for mean squared prediction-outcome differences, and AUROC for assessing confidence-based correct answer identification (Boyd et al., 2013, Kuhn et al., 2023). Selective Accuracy@Coverage measures accuracy within the model's most confident predictions (Liang et al., 2023, Cole et al., 2023). However, these metrics, rooted in a binary notion of correctness, fall short for long-form tasks where correctness spans a range, suggesting a distribution-based approach is more apt. \n\nImproving Calibration Traditional calibration methods focus on post-processing logits (Guo et al., 2017), but with LLMs generating unbounded text, logits could fall short. Thus, extracting better confidence scores (i.e., confidence elicitation) has become crucial for improving calibration. These include: verbalization, which directly asks the model to output its confidence (Lin et al., 2022), consistency, which uses the uniformity of multiple responses to gauge confidence (Kadavath et al., 2022, Kuhn et al., 2023, Cole et al., 2023, Chen et al., 2023c, Tian et al., 2023a, Lin et al., 2023), and the hybrid of both (Xiong et al., 2023, Tian et al., 2023b, Chen and Mueller, 2023).",
                    "score": 0.5504831140346489,
                    "section_title": "Introduction",
                    "char_start_offset": 5549,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 125
                        },
                        {
                            "start": 128,
                            "end": 392
                        },
                        {
                            "start": 393,
                            "end": 706
                        },
                        {
                            "start": 707,
                            "end": 839
                        },
                        {
                            "start": 840,
                            "end": 1023
                        },
                        {
                            "start": 1026,
                            "end": 1197
                        },
                        {
                            "start": 1198,
                            "end": 1316
                        },
                        {
                            "start": 1317,
                            "end": 1712
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 162,
                            "end": 179,
                            "matchedPaperCorpusId": "28671436"
                        },
                        {
                            "start": 370,
                            "end": 391,
                            "matchedPaperCorpusId": "253244504"
                        },
                        {
                            "start": 512,
                            "end": 533,
                            "matchedPaperCorpusId": "6292807"
                        },
                        {
                            "start": 667,
                            "end": 685,
                            "matchedPaperCorpusId": "37409855"
                        },
                        {
                            "start": 799,
                            "end": 818,
                            "matchedPaperCorpusId": "253553585"
                        },
                        {
                            "start": 1112,
                            "end": 1130,
                            "matchedPaperCorpusId": "28671436"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89697265625
                }
            ],
            "relevance_judgement": 0.9609375,
            "relevance_judgment_input_expanded": "# Title: Calibrating Long-form Generations from Large Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Yukun Huang, Yixin Liu, Raghuveer Thirukovalluru, Arman Cohan, Bhuwan Dhingra\n## Abstract\nTo enhance Large Language Models' (LLMs) reliability, calibration is essential -- the model's assessed confidence scores should align with the actual likelihood of its responses being correct. However, current confidence elicitation methods and calibration metrics typically rely on a binary true/false assessment of response correctness. This approach does not apply to long-form generation, where an answer can be partially correct. Addressing this gap, we introduce a unified calibration framework, in which both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores. Within this framework, we develop three metrics to precisely evaluate LLM calibration and further propose two confidence elicitation methods based on self-consistency and self-evaluation. Our experiments, which include long-form QA and summarization tasks, demonstrate that larger models don't necessarily guarantee better calibration, that calibration performance is found to be metric-dependent, and that self-consistency methods excel in factoid datasets. We also find that calibration can be enhanced through techniques such as fine-tuning, integrating relevant source documents, scaling the temperature, and combining self-consistency with self-evaluation. Lastly, we showcase a practical application of our system: selecting and cascading open-source models and ChatGPT to optimize correctness given a limited API budget. This research not only challenges existing notions of LLM calibration but also offers practical methodologies for improving trustworthiness in long-form generation.\n## Introduction\n\u2022 A cost-effective model usage strategy, illustrating the practicality of long-form calibration in optimizing LLM deployment. \n\nMeasuring Calibration Calibration (Guo et al., 2017, Minderer et al., 2021)has been widely studied in language models, whose probabilities derived from logits are generally found to not be calibrated (Jiang et al., 2020, Kadavath et al., 2022, Chen et al., 2023d). Standard metrics to measure the calibration include Expected Calibration Error (ECE) for confidence-accuracy disparity (Naeini et al., 2015), Brier Score for mean squared prediction-outcome differences, and AUROC for assessing confidence-based correct answer identification (Boyd et al., 2013, Kuhn et al., 2023). Selective Accuracy@Coverage measures accuracy within the model's most confident predictions (Liang et al., 2023, Cole et al., 2023). However, these metrics, rooted in a binary notion of correctness, fall short for long-form tasks where correctness spans a range, suggesting a distribution-based approach is more apt. \n\nImproving Calibration Traditional calibration methods focus on post-processing logits (Guo et al., 2017), but with LLMs generating unbounded text, logits could fall short. Thus, extracting better confidence scores (i.e., confidence elicitation) has become crucial for improving calibration. These include: verbalization, which directly asks the model to output its confidence (Lin et al., 2022), consistency, which uses the uniformity of multiple responses to gauge confidence (Kadavath et al., 2022, Kuhn et al., 2023, Cole et al., 2023, Chen et al., 2023c, Tian et al., 2023a, Lin et al., 2023), and the hybrid of both (Xiong et al., 2023, Tian et al., 2023b, Chen and Mueller, 2023).",
            "reference_string": "[267617073 | Huang et al. | 2024 | Citations: 12]"
        },
        {
            "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
            "venue": "Transactions of the Association for Computational Linguistics",
            "year": 2020,
            "reference_count": 77,
            "citation_count": 436,
            "influential_citation_count": 32,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00407/1962628/tacl_a_00407.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.00955, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2669515",
                    "name": "Zhengbao Jiang"
                },
                {
                    "authorId": "50007145",
                    "name": "J. Araki"
                },
                {
                    "authorId": "47929135",
                    "name": "Haibo Ding"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                }
            ],
            "abstract": "Abstract Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, \u201cHow can we know when language models know, with confidence, the answer to a particular query?\u201d We examine this question from the point of view of calibration, the property of a probabilistic model\u2019s predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models\u2014T5, BART, and GPT-2\u2014and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github.com/jzbjyb/lm-calibration.",
            "corpus_id": 235078802,
            "sentences": [
                {
                    "corpus_id": "235078802",
                    "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
                    "text": "This is especially true if these models are deployed to safety-critical domains such as healthcare and finance, where mistaken answers can have serious consequences. 1 n this paper, we ask the question \"how can we know when language models know, with confidence, the answer to a particular knowledge-based query?\" Specifically, we examine this from the point of view of calibration, whether the model's probability estimates are well-aligned with the actual probability of the answer being correct. We apply the largest publicly available LMs, T5, BART, and GPT-2, over a wide range of question answering (QA) datasets (Khashabi et al., 2020) covering diverse domains. We first observe that despite the models' high performance (e.g. T5 eclipses other alternatives such as GPT-3 on some datasets), the models tend to not be well calibrated; their probability estimates over candidates have far-from-perfect correspondence with the actual probability that the answer they provide is correct. Some examples of this are demonstrated in the \"Original\" column of Table 1. \n\nTo alleviate this problem, we propose methods to make LMs' confidence scores correlate better with the likelihood of model prediction being correct. We examined both fine-tuning methods that modify LMs' parameters and post-hoc methods that keep LMs fixed and only manipulate the confidence values or inputs. Specifically, we fine-tune the LM using softmax-or marginbased objective functions based on multiple candidate answers. For post-hoc calibration, we examined temperature-based scaling and feature-based decision trees that take prediction probability and input-related features as input and produce calibrated confidence (Jagannatha and Yu, 2020;Desai and Durrett, 2020;Kamath et al., 2020). We also study the sensitivity of LMs' confidence estimation with respect to language variation by paraphrasing candidate answers and augmenting questions using retrieved context. \n\nExperimental results demonstrate that both finetuning and post-hoc methods can improve calibration performance without sacrificing accuracy. We further perform analysis and ablation studies on our methods, inspecting different aspects that may affect calibration performance.",
                    "score": 0.4851874149954394,
                    "section_title": "Introduction",
                    "char_start_offset": 1723,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 167
                        },
                        {
                            "start": 168,
                            "end": 313
                        },
                        {
                            "start": 314,
                            "end": 498
                        },
                        {
                            "start": 499,
                            "end": 668
                        },
                        {
                            "start": 669,
                            "end": 733
                        },
                        {
                            "start": 734,
                            "end": 990
                        },
                        {
                            "start": 991,
                            "end": 1066
                        },
                        {
                            "start": 1069,
                            "end": 1217
                        },
                        {
                            "start": 1218,
                            "end": 1376
                        },
                        {
                            "start": 1377,
                            "end": 1496
                        },
                        {
                            "start": 1497,
                            "end": 1767
                        },
                        {
                            "start": 1768,
                            "end": 1946
                        },
                        {
                            "start": 1949,
                            "end": 2089
                        },
                        {
                            "start": 2090,
                            "end": 2224
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1697,
                            "end": 1722,
                            "matchedPaperCorpusId": "215548393"
                        },
                        {
                            "start": 1746,
                            "end": 1766,
                            "matchedPaperCorpusId": "219721462"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95947265625
                },
                {
                    "corpus_id": "235078802",
                    "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
                    "text": "In this section, we discuss each method in detail and analyze why they can improve calibration performance. \n\nObjective Function Matters. The original Uni-fiedQA model is fine-tuned based on MLE, which maximizes the probability of the gold answer given the question. Both softmax-based and margin-based fine-tuning, which explicitly compare and adjust the probability of candidate answers, can further improve ECE on multiplechoice datasets. We argue that the softmax-based and margin-based objective functions are better suited for questions with potential candidates. \n\nPost-processing Confidence is Effective Universally. Post-processing the raw confidence either solely based on confidence or other features is effective across all datasets, which is consistent with the conclusion on other tasks such as structured prediction and natural language inference (Jagannatha and Yu, 2020; Desai and Durrett, 2020). We demonstrate the histogram of confidence before and after applying temperature-based scaling or feature-based decision trees in Figure 2. LMs tend to be over-confident, with most predictions having either extremely high or low confidence. Both methods can successfully re-scale the confidence to reasonable ranges, thus improving the calibration performance. \n\nParaphrasing Answers and Input Augmentation can Improve Confidence Estimation. The improvement brought by using paraphrasing is significant on multiple-choice datasets, demonstrating that using diverse expressions can indeed improve confidence estimation. To better understand under what circumstances paraphrasing works, we group candidate answers into two categories: the first group includes candidate answers that become better calibrated using paraphrases; the second group includes candidate answers whose confidence remains the same using paraphrases. We say that a candidate becomes better calibrated if its confidence increases/decreases by 20% if it is a correct or incorrect answer respectively. We found that the average length of questions for better calibrated candidates ( 187) is much shorter than that of candidates without improvement (320), indicating that paraphrasing is useful mainly for short questions. We also compute the diversity of word usage in paraphrases using the number of unique words divided by the total length of paraphrases.",
                    "score": 0.5523821564947221,
                    "section_title": "Analysis of Individual Calibration Methods",
                    "char_start_offset": 25002,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 107
                        },
                        {
                            "start": 110,
                            "end": 137
                        },
                        {
                            "start": 138,
                            "end": 266
                        },
                        {
                            "start": 267,
                            "end": 441
                        },
                        {
                            "start": 442,
                            "end": 569
                        },
                        {
                            "start": 572,
                            "end": 624
                        },
                        {
                            "start": 625,
                            "end": 913
                        },
                        {
                            "start": 914,
                            "end": 1154
                        },
                        {
                            "start": 1155,
                            "end": 1274
                        },
                        {
                            "start": 1277,
                            "end": 1355
                        },
                        {
                            "start": 1356,
                            "end": 1532
                        },
                        {
                            "start": 1533,
                            "end": 1835
                        },
                        {
                            "start": 1836,
                            "end": 1983
                        },
                        {
                            "start": 1984,
                            "end": 2203
                        },
                        {
                            "start": 2204,
                            "end": 2339
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.951171875
                }
            ],
            "relevance_judgement": 0.95947265625,
            "relevance_judgment_input_expanded": "# Title: How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering\n# Venue: Transactions of the Association for Computational Linguistics\n# Authors: Zhengbao Jiang, J. Araki, Haibo Ding, Graham Neubig\n## Abstract\nAbstract Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, \u201cHow can we know when language models know, with confidence, the answer to a particular query?\u201d We examine this question from the point of view of calibration, the property of a probabilistic model\u2019s predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models\u2014T5, BART, and GPT-2\u2014and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github.com/jzbjyb/lm-calibration.\n## Introduction\nThis is especially true if these models are deployed to safety-critical domains such as healthcare and finance, where mistaken answers can have serious consequences. 1 n this paper, we ask the question \"how can we know when language models know, with confidence, the answer to a particular knowledge-based query?\" Specifically, we examine this from the point of view of calibration, whether the model's probability estimates are well-aligned with the actual probability of the answer being correct. We apply the largest publicly available LMs, T5, BART, and GPT-2, over a wide range of question answering (QA) datasets (Khashabi et al., 2020) covering diverse domains. We first observe that despite the models' high performance (e.g. T5 eclipses other alternatives such as GPT-3 on some datasets), the models tend to not be well calibrated; their probability estimates over candidates have far-from-perfect correspondence with the actual probability that the answer they provide is correct. Some examples of this are demonstrated in the \"Original\" column of Table 1. \n\nTo alleviate this problem, we propose methods to make LMs' confidence scores correlate better with the likelihood of model prediction being correct. We examined both fine-tuning methods that modify LMs' parameters and post-hoc methods that keep LMs fixed and only manipulate the confidence values or inputs. Specifically, we fine-tune the LM using softmax-or marginbased objective functions based on multiple candidate answers. For post-hoc calibration, we examined temperature-based scaling and feature-based decision trees that take prediction probability and input-related features as input and produce calibrated confidence (Jagannatha and Yu, 2020;Desai and Durrett, 2020;Kamath et al., 2020). We also study the sensitivity of LMs' confidence estimation with respect to language variation by paraphrasing candidate answers and augmenting questions using retrieved context. \n\nExperimental results demonstrate that both finetuning and post-hoc methods can improve calibration performance without sacrificing accuracy. We further perform analysis and ablation studies on our methods, inspecting different aspects that may affect calibration performance.\n\n## Analysis of Individual Calibration Methods\nIn this section, we discuss each method in detail and analyze why they can improve calibration performance. \n\nObjective Function Matters. The original Uni-fiedQA model is fine-tuned based on MLE, which maximizes the probability of the gold answer given the question. Both softmax-based and margin-based fine-tuning, which explicitly compare and adjust the probability of candidate answers, can further improve ECE on multiplechoice datasets. We argue that the softmax-based and margin-based objective functions are better suited for questions with potential candidates. \n\nPost-processing Confidence is Effective Universally. Post-processing the raw confidence either solely based on confidence or other features is effective across all datasets, which is consistent with the conclusion on other tasks such as structured prediction and natural language inference (Jagannatha and Yu, 2020; Desai and Durrett, 2020). We demonstrate the histogram of confidence before and after applying temperature-based scaling or feature-based decision trees in Figure 2. LMs tend to be over-confident, with most predictions having either extremely high or low confidence. Both methods can successfully re-scale the confidence to reasonable ranges, thus improving the calibration performance. \n\nParaphrasing Answers and Input Augmentation can Improve Confidence Estimation. The improvement brought by using paraphrasing is significant on multiple-choice datasets, demonstrating that using diverse expressions can indeed improve confidence estimation. To better understand under what circumstances paraphrasing works, we group candidate answers into two categories: the first group includes candidate answers that become better calibrated using paraphrases; the second group includes candidate answers whose confidence remains the same using paraphrases. We say that a candidate becomes better calibrated if its confidence increases/decreases by 20% if it is a correct or incorrect answer respectively. We found that the average length of questions for better calibrated candidates ( 187) is much shorter than that of candidates without improvement (320), indicating that paraphrasing is useful mainly for short questions. We also compute the diversity of word usage in paraphrases using the number of unique words divided by the total length of paraphrases.",
            "reference_string": "[235078802 | Jiang et al. | 2020 | Citations: 436]"
        },
        {
            "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 226,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.18346, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2357723383",
                    "name": "Toghrul Abbasli"
                },
                {
                    "authorId": "2311579070",
                    "name": "Kentaroh Toyoda"
                },
                {
                    "authorId": "2285070153",
                    "name": "Yuan Wang"
                },
                {
                    "authorId": "2115471566",
                    "name": "Leon Witt"
                },
                {
                    "authorId": "2358256611",
                    "name": "Muhammad Asif Ali"
                },
                {
                    "authorId": "2239095099",
                    "name": "Yukai Miao"
                },
                {
                    "authorId": "2302856161",
                    "name": "Dan Li"
                },
                {
                    "authorId": "2285078682",
                    "name": "Qingsong Wei"
                }
            ],
            "abstract": "Large Language Models (LLMs) have been transformative across many domains. However, hallucination -- confidently outputting incorrect information -- remains one of the leading challenges for LLMs. This raises the question of how to accurately assess and quantify the uncertainty of LLMs. Extensive literature on traditional models has explored Uncertainty Quantification (UQ) to measure uncertainty and employed calibration techniques to address the misalignment between uncertainty and accuracy. While some of these methods have been adapted for LLMs, the literature lacks an in-depth analysis of their effectiveness and does not offer a comprehensive benchmark to enable insightful comparison among existing solutions. In this work, we fill this gap via a systematic survey of representative prior works on UQ and calibration for LLMs and introduce a rigorous benchmark. Using two widely used reliability datasets, we empirically evaluate six related methods, which justify the significant findings of our review. Finally, we provide outlooks for key future directions and outline open challenges. To the best of our knowledge, this survey is the first dedicated study to review the calibration methods and relevant metrics for LLMs.",
            "corpus_id": 278129791,
            "sentences": [
                {
                    "corpus_id": "278129791",
                    "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review",
                    "text": "Calibration, initially developed for forecasting [92]- [94], is a technique to assess and improve the quality of uncertainty estimates by ensuring that models provide more transparent and reliable predictive probabilities [25]. Since modern deep learning models frequently rely on softmax output probabilities to estimate uncertainty, improper calibration can lead to misleading confidence scores, which is particularly problematic in high-stakes decision-making tasks. To achieve proper calibration, a model's predicted confidence level (e.g., the softmax score \u03b1) should align with its actual accuracy [41]. In other words, among all instances where the model assigns a confidence of \u03b1, the proportion of correct predictions should approximate \u03b1. This alignment can be evaluated through techniques such as reliability diagrams, expected calibration error (ECE), and temperature scaling (TS) [28]. A classification model can be considered calibrated if: \n\nwhere f (X) function that maps the feature space X and label space Y , while pf represents the predicted confidence \u03b1 of f . To assess calibration, we can partition this confidence into intervals and construct reliability diagrams [14], which visualize the relationship between predicted confidence and actual accuracy within each interval. These diagrams help identify whether the model is under-confident, over-confident, or wellcalibrated, providing insights into model reliability based on bin-wise accuracy and confidence. While achieving perfect accuracy in real-world scenarios is often infeasible [29], perfect calibration of uncertainty remains an attainable goal [95], [96]. \n\nPost-hoc or post-training methods, such as TS [28], Platt scaling [97], histogram binning [98], and training with label smoothing [99], are classical methods of calibration. Mixup training with learning the complexity of samples was proposed to calibrate the pre-trained language models [100]. We note that open-box methods of calibration can be complex and difficult to apply to LLMs, but they can be more informative of inner states than post-hoc methods [101].",
                    "score": 0.47003931892317397,
                    "section_title": "A. Model Calibration",
                    "char_start_offset": 18857,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 227
                        },
                        {
                            "start": 228,
                            "end": 469
                        },
                        {
                            "start": 470,
                            "end": 609
                        },
                        {
                            "start": 610,
                            "end": 748
                        },
                        {
                            "start": 749,
                            "end": 898
                        },
                        {
                            "start": 899,
                            "end": 954
                        },
                        {
                            "start": 957,
                            "end": 1081
                        },
                        {
                            "start": 1082,
                            "end": 1297
                        },
                        {
                            "start": 1298,
                            "end": 1484
                        },
                        {
                            "start": 1485,
                            "end": 1641
                        },
                        {
                            "start": 1644,
                            "end": 1817
                        },
                        {
                            "start": 1818,
                            "end": 1937
                        },
                        {
                            "start": 1938,
                            "end": 2107
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 49,
                            "end": 53,
                            "matchedPaperCorpusId": "127203550"
                        },
                        {
                            "start": 55,
                            "end": 59,
                            "matchedPaperCorpusId": "109884250"
                        },
                        {
                            "start": 604,
                            "end": 608,
                            "matchedPaperCorpusId": "202718866"
                        },
                        {
                            "start": 893,
                            "end": 897,
                            "matchedPaperCorpusId": "28671436"
                        },
                        {
                            "start": 1188,
                            "end": 1192,
                            "matchedPaperCorpusId": "207158152"
                        },
                        {
                            "start": 1690,
                            "end": 1694,
                            "matchedPaperCorpusId": "28671436"
                        },
                        {
                            "start": 1710,
                            "end": 1714,
                            "matchedPaperCorpusId": "56563878"
                        },
                        {
                            "start": 1734,
                            "end": 1738,
                            "matchedPaperCorpusId": "9594071"
                        },
                        {
                            "start": 1774,
                            "end": 1778,
                            "matchedPaperCorpusId": "174802983"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.953125
                }
            ],
            "relevance_judgement": 0.953125,
            "relevance_judgment_input_expanded": "# Title: Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review\n# Venue: arXiv.org\n# Authors: Toghrul Abbasli, Kentaroh Toyoda, Yuan Wang, Leon Witt, Muhammad Asif Ali, Yukai Miao, Dan Li, Qingsong Wei\n## Abstract\nLarge Language Models (LLMs) have been transformative across many domains. However, hallucination -- confidently outputting incorrect information -- remains one of the leading challenges for LLMs. This raises the question of how to accurately assess and quantify the uncertainty of LLMs. Extensive literature on traditional models has explored Uncertainty Quantification (UQ) to measure uncertainty and employed calibration techniques to address the misalignment between uncertainty and accuracy. While some of these methods have been adapted for LLMs, the literature lacks an in-depth analysis of their effectiveness and does not offer a comprehensive benchmark to enable insightful comparison among existing solutions. In this work, we fill this gap via a systematic survey of representative prior works on UQ and calibration for LLMs and introduce a rigorous benchmark. Using two widely used reliability datasets, we empirically evaluate six related methods, which justify the significant findings of our review. Finally, we provide outlooks for key future directions and outline open challenges. To the best of our knowledge, this survey is the first dedicated study to review the calibration methods and relevant metrics for LLMs.\n## A. Model Calibration\nCalibration, initially developed for forecasting [92]- [94], is a technique to assess and improve the quality of uncertainty estimates by ensuring that models provide more transparent and reliable predictive probabilities [25]. Since modern deep learning models frequently rely on softmax output probabilities to estimate uncertainty, improper calibration can lead to misleading confidence scores, which is particularly problematic in high-stakes decision-making tasks. To achieve proper calibration, a model's predicted confidence level (e.g., the softmax score \u03b1) should align with its actual accuracy [41]. In other words, among all instances where the model assigns a confidence of \u03b1, the proportion of correct predictions should approximate \u03b1. This alignment can be evaluated through techniques such as reliability diagrams, expected calibration error (ECE), and temperature scaling (TS) [28]. A classification model can be considered calibrated if: \n\nwhere f (X) function that maps the feature space X and label space Y , while pf represents the predicted confidence \u03b1 of f . To assess calibration, we can partition this confidence into intervals and construct reliability diagrams [14], which visualize the relationship between predicted confidence and actual accuracy within each interval. These diagrams help identify whether the model is under-confident, over-confident, or wellcalibrated, providing insights into model reliability based on bin-wise accuracy and confidence. While achieving perfect accuracy in real-world scenarios is often infeasible [29], perfect calibration of uncertainty remains an attainable goal [95], [96]. \n\nPost-hoc or post-training methods, such as TS [28], Platt scaling [97], histogram binning [98], and training with label smoothing [99], are classical methods of calibration. Mixup training with learning the complexity of samples was proposed to calibrate the pre-trained language models [100]. We note that open-box methods of calibration can be complex and difficult to apply to LLMs, but they can be more informative of inner states than post-hoc methods [101].",
            "reference_string": "[278129791 | Abbasli et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Calibrating Large Language Models with Sample Consistency",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 29,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.13904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1904906987",
                    "name": "Qing Lyu"
                },
                {
                    "authorId": "2266467511",
                    "name": "Kumar Shridhar"
                },
                {
                    "authorId": "8805254",
                    "name": "Chaitanya Malaviya"
                },
                {
                    "authorId": "2258934061",
                    "name": "Li Zhang"
                },
                {
                    "authorId": "51131518",
                    "name": "Yanai Elazar"
                },
                {
                    "authorId": "2261389843",
                    "name": "Niket Tandon"
                },
                {
                    "authorId": "2817917",
                    "name": "Marianna Apidianaki"
                },
                {
                    "authorId": "2790926",
                    "name": "Mrinmaya Sachan"
                },
                {
                    "authorId": "1763608",
                    "name": "Chris Callison-Burch"
                }
            ],
            "abstract": "Accurately gauging the confidence level of Large Language Models' (LLMs) predictions is pivotal for their reliable application. However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale. In this work, we explore the potential of deriving confidence from the distribution of multiple randomly sampled model generations, via three measures of consistency. We perform an extensive evaluation across various open and closed-source models on nine reasoning datasets. Results show that consistency-based calibration methods outperform existing post-hoc approaches. Meanwhile, we find that factors such as intermediate explanations, model scaling, and larger sample sizes enhance calibration, while instruction-tuning makes calibration more difficult. Moreover, confidence scores obtained from consistency have the potential to enhance model performance. Finally, we offer practical guidance on choosing suitable consistency metrics for calibration, tailored to the characteristics of various LMs.",
            "corpus_id": 267770526,
            "sentences": [
                {
                    "corpus_id": "267770526",
                    "title": "Calibrating Large Language Models with Sample Consistency",
                    "text": "Accurately gauging the confidence level of Large Language Models' (LLMs) predictions is pivotal for their reliable application. However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale. In this work, we explore the potential of deriving confidence from the distribution of multiple randomly sampled model generations, via three measures of consistency. We perform an extensive evaluation across various open and closed-source models on nine reasoning datasets. Results show that consistency-based calibration methods outperform existing post-hoc approaches. Meanwhile, we find that factors such as intermediate explanations, model scaling, and larger sample sizes enhance calibration, while instruction-tuning makes calibration more difficult. Moreover, confidence scores obtained from consistency have the potential to enhance model performance. Finally, we offer practical guidance on choosing suitable consistency metrics for calibration, tailored to the characteristics of various LMs.",
                    "score": 0.43437488186114714,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94921875
                }
            ],
            "relevance_judgement": 0.94921875,
            "relevance_judgment_input_expanded": "# Title: Calibrating Large Language Models with Sample Consistency\n# Venue: arXiv.org\n# Authors: Qing Lyu, Kumar Shridhar, Chaitanya Malaviya, Li Zhang, Yanai Elazar, Niket Tandon, Marianna Apidianaki, Mrinmaya Sachan, Chris Callison-Burch\n## Abstract\nAccurately gauging the confidence level of Large Language Models' (LLMs) predictions is pivotal for their reliable application. However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale. In this work, we explore the potential of deriving confidence from the distribution of multiple randomly sampled model generations, via three measures of consistency. We perform an extensive evaluation across various open and closed-source models on nine reasoning datasets. Results show that consistency-based calibration methods outperform existing post-hoc approaches. Meanwhile, we find that factors such as intermediate explanations, model scaling, and larger sample sizes enhance calibration, while instruction-tuning makes calibration more difficult. Moreover, confidence scores obtained from consistency have the potential to enhance model performance. Finally, we offer practical guidance on choosing suitable consistency metrics for calibration, tailored to the characteristics of various LMs.\n",
            "reference_string": "[267770526 | Lyu et al. | 2024 | Citations: 29]"
        },
        {
            "title": "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 28,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.13230, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2120099874",
                    "name": "Xin Liu"
                },
                {
                    "authorId": "2131675694",
                    "name": "Farima Fatahi Bayat"
                },
                {
                    "authorId": "2299208178",
                    "name": "Lu Wang"
                }
            ],
            "abstract": "Calibrating language models (LMs) aligns their generation confidence with the actual likelihood of answer correctness, which can inform users about LMs\u2019 reliability and mitigate hallucinated content. However, prior calibration methods, such as self-consistency-based and logit-based approaches, are either limited in inference-time efficiency or fall short of providing informative signals. Moreover, simply filtering out low-confidence responses reduces the LM\u2019s helpfulness when the answers are correct. Therefore, effectively using calibration techniques to enhance an LM\u2019s factuality remains an unsolved challenge. In this paper, we first propose an activation-based calibration method, ActCab, which trains a linear layer on top of the LM\u2019s last-layer activations that can better capture the representations of knowledge. Built on top of ActCab, we further propose CoDec, a confidence-guided decoding strategy to elicit truthful answers with high confidence from LMs. By evaluating on five popular QA benchmarks, ActCab achieves superior calibration performance than all competitive baselines, e.g., by reducing the average expected calibration error (ECE) score by up to 39%. Further experiments on CoDec show consistent improvements in several LMs\u2019 factuality on challenging QA datasets, such as TruthfulQA, highlighting the value of confidence signals in enhancing the factuality.",
            "corpus_id": 270620078,
            "sentences": [
                {
                    "corpus_id": "270620078",
                    "title": "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding",
                    "text": "Calibrating language models (LMs) aligns their generation confidence with the actual likelihood of answer correctness, which can inform users about LMs\u2019 reliability and mitigate hallucinated content. However, prior calibration methods, such as self-consistency-based and logit-based approaches, are either limited in inference-time efficiency or fall short of providing informative signals. Moreover, simply filtering out low-confidence responses reduces the LM\u2019s helpfulness when the answers are correct. Therefore, effectively using calibration techniques to enhance an LM\u2019s factuality remains an unsolved challenge. In this paper, we first propose an activation-based calibration method, ActCab, which trains a linear layer on top of the LM\u2019s last-layer activations that can better capture the representations of knowledge. Built on top of ActCab, we further propose CoDec, a confidence-guided decoding strategy to elicit truthful answers with high confidence from LMs. By evaluating on five popular QA benchmarks, ActCab achieves superior calibration performance than all competitive baselines, e.g., by reducing the average expected calibration error (ECE) score by up to 39%. Further experiments on CoDec show consistent improvements in several LMs\u2019 factuality on challenging QA datasets, such as TruthfulQA, highlighting the value of confidence signals in enhancing the factuality.",
                    "score": 0.45925154507088684,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9462890625
                },
                {
                    "corpus_id": "270620078",
                    "title": "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding",
                    "text": "Despite their impressive language understanding and generation capabilities, language models (LMs) still produce hallucinated content (Zhang et al., 2023), undermining their trustworthiness.One mitigation is to calibrate LMs' confidence in their outputs to align with the actual likelihood of their responses being correct (Jiang et al., 2021;Liu et al., 2024).Well-calibrated confidence scores allow users to discern the reliability of LMs' responses, enabling them not only to determine whether to trust the model's outputs but also to decide when further verification is needed.\n\nPopular LM calibration methods include training-free, inference-only approaches through verbalization (Tian et al., 2023a) or selfconsistency measurements (Wang et al., 2023) and training-based methods, such as tuning temperature parameters (Liang et al., 2018) or learning uncertainty estimations from LMs' logits (Liu et al., 2024).Despite their potential compatibility with many LMs, training-free methods are limited by the models' instruction-following capabilities and can be computationally expensive during inference.\n\nTraining-based methods, on the other hand, directly learn to output model's uncertainty.For instance, Kuhn et al. (2023) leverages the token logits predicted by the model to estimate the confidence of the entire response.Yet model logits may not capture knowledge representations and can be sensitive to tokenization procedures.Moreover, model confidence has been naively used by setting a threshold to filter out low-probability responses (Ren et al., 2023a;Zablotskaia et al., 2023), leaving these queries unanswered and reducing model's helpfulness.Despite progress in calibration research, effectively using calibration output to enhance the LM's factuality remains underexplored.\n\nIn this work, we make two primary contributions: (i) a lightweight and effective LM calibration technique, and (ii) a decoding strategy that uses calibrated confidence to elicit correct responses from LMs. Concretely, we propose an Activationbased Calibration method, ACTCAB, that estimates model's uncertainty from its internal activations.",
                    "score": 0.45774727624697287,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 190
                        },
                        {
                            "start": 190,
                            "end": 361
                        },
                        {
                            "start": 361,
                            "end": 581
                        },
                        {
                            "start": 583,
                            "end": 917
                        },
                        {
                            "start": 917,
                            "end": 1108
                        },
                        {
                            "start": 1110,
                            "end": 1198
                        },
                        {
                            "start": 1198,
                            "end": 1331
                        },
                        {
                            "start": 1331,
                            "end": 1438
                        },
                        {
                            "start": 1438,
                            "end": 1662
                        },
                        {
                            "start": 1662,
                            "end": 1794
                        },
                        {
                            "start": 1796,
                            "end": 2137
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 323,
                            "end": 343,
                            "matchedPaperCorpusId": "235078802"
                        },
                        {
                            "start": 343,
                            "end": 360,
                            "matchedPaperCorpusId": "264811211"
                        },
                        {
                            "start": 738,
                            "end": 757,
                            "matchedPaperCorpusId": "247595263"
                        },
                        {
                            "start": 824,
                            "end": 844,
                            "matchedPaperCorpusId": "3526391"
                        },
                        {
                            "start": 898,
                            "end": 916,
                            "matchedPaperCorpusId": "264811211"
                        },
                        {
                            "start": 1212,
                            "end": 1230,
                            "matchedPaperCorpusId": "257039062"
                        },
                        {
                            "start": 1550,
                            "end": 1569,
                            "matchedPaperCorpusId": "252668614"
                        },
                        {
                            "start": 1569,
                            "end": 1594,
                            "matchedPaperCorpusId": "258187125"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9443359375
                }
            ],
            "relevance_judgement": 0.9462890625,
            "relevance_judgment_input_expanded": "# Title: Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Xin Liu, Farima Fatahi Bayat, Lu Wang\n## Abstract\nCalibrating language models (LMs) aligns their generation confidence with the actual likelihood of answer correctness, which can inform users about LMs\u2019 reliability and mitigate hallucinated content. However, prior calibration methods, such as self-consistency-based and logit-based approaches, are either limited in inference-time efficiency or fall short of providing informative signals. Moreover, simply filtering out low-confidence responses reduces the LM\u2019s helpfulness when the answers are correct. Therefore, effectively using calibration techniques to enhance an LM\u2019s factuality remains an unsolved challenge. In this paper, we first propose an activation-based calibration method, ActCab, which trains a linear layer on top of the LM\u2019s last-layer activations that can better capture the representations of knowledge. Built on top of ActCab, we further propose CoDec, a confidence-guided decoding strategy to elicit truthful answers with high confidence from LMs. By evaluating on five popular QA benchmarks, ActCab achieves superior calibration performance than all competitive baselines, e.g., by reducing the average expected calibration error (ECE) score by up to 39%. Further experiments on CoDec show consistent improvements in several LMs\u2019 factuality on challenging QA datasets, such as TruthfulQA, highlighting the value of confidence signals in enhancing the factuality.\n## Introduction\nDespite their impressive language understanding and generation capabilities, language models (LMs) still produce hallucinated content (Zhang et al., 2023), undermining their trustworthiness.One mitigation is to calibrate LMs' confidence in their outputs to align with the actual likelihood of their responses being correct (Jiang et al., 2021;Liu et al., 2024).Well-calibrated confidence scores allow users to discern the reliability of LMs' responses, enabling them not only to determine whether to trust the model's outputs but also to decide when further verification is needed.\n\nPopular LM calibration methods include training-free, inference-only approaches through verbalization (Tian et al., 2023a) or selfconsistency measurements (Wang et al., 2023) and training-based methods, such as tuning temperature parameters (Liang et al., 2018) or learning uncertainty estimations from LMs' logits (Liu et al., 2024).Despite their potential compatibility with many LMs, training-free methods are limited by the models' instruction-following capabilities and can be computationally expensive during inference.\n\nTraining-based methods, on the other hand, directly learn to output model's uncertainty.For instance, Kuhn et al. (2023) leverages the token logits predicted by the model to estimate the confidence of the entire response.Yet model logits may not capture knowledge representations and can be sensitive to tokenization procedures.Moreover, model confidence has been naively used by setting a threshold to filter out low-probability responses (Ren et al., 2023a;Zablotskaia et al., 2023), leaving these queries unanswered and reducing model's helpfulness.Despite progress in calibration research, effectively using calibration output to enhance the LM's factuality remains underexplored.\n\nIn this work, we make two primary contributions: (i) a lightweight and effective LM calibration technique, and (ii) a decoding strategy that uses calibrated confidence to elicit correct responses from LMs. Concretely, we propose an Activationbased Calibration method, ACTCAB, that estimates model's uncertainty from its internal activations.",
            "reference_string": "[270620078 | Liu et al. | 2024 | Citations: 6]"
        },
        {
            "title": "A Close Look into the Calibration of Pre-trained Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2022,
            "reference_count": 69,
            "citation_count": 51,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2211.00151",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.00151, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "123331686",
                    "name": "Yangyi Chen"
                },
                {
                    "authorId": "2152195191",
                    "name": "Lifan Yuan"
                },
                {
                    "authorId": "52297757",
                    "name": "Ganqu Cui"
                },
                {
                    "authorId": "2141313179",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "2072975661",
                    "name": "Heng Ji"
                }
            ],
            "abstract": "Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs\u2019 calibration performance in training. We consider six factors as control variables, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. We observe a consistent change in calibration performance across six factors. We find that PLMs don\u2019t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not. We highlight that our finding somewhat contradicts two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining improves model calibration. Next, we study the effectiveness of existing calibration methods in mitigating the overconfidence issue. Besides unlearnable calibration methods (e.g., label smoothing), we adapt and extend two recently proposed learnable methods that directly collect data to train models to have reasonable confidence estimations. Experimental results show that learnable methods significantly reduce PLMs\u2019 confidence in wrong predictions.",
            "corpus_id": 253244504,
            "sentences": [
                {
                    "corpus_id": "253244504",
                    "title": "A Close Look into the Calibration of Pre-trained Language Models",
                    "text": "Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs\u2019 calibration performance in training. We consider six factors as control variables, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. We observe a consistent change in calibration performance across six factors. We find that PLMs don\u2019t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not. We highlight that our finding somewhat contradicts two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining improves model calibration. Next, we study the effectiveness of existing calibration methods in mitigating the overconfidence issue. Besides unlearnable calibration methods (e.g., label smoothing), we adapt and extend two recently proposed learnable methods that directly collect data to train models to have reasonable confidence estimations. Experimental results show that learnable methods significantly reduce PLMs\u2019 confidence in wrong predictions.",
                    "score": 0.4985230420499696,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94189453125
                },
                {
                    "corpus_id": "253244504",
                    "title": "A Close Look into the Calibration of Pre-trained Language Models",
                    "text": "Pre-trained language models (PLMs) are successful in many downstream tasks regarding performance (Wang et al., 2019). But for a reliable deployment in practice, the calibration performance should also be carefully examined (Vaicenavicius et al., 2019). Well-calibrated models assign appropriate confidence scores that truly reflect the outcome probability of their predictions. However, the confidence scores of existing deep neural networks cannot serve as reliable estimates of their uncertainty (Guo et al., 2017), and a deep understanding of PLMs calibration is lacking.\n\nIn this paper, we give a systematical analysis of PLMs calibration. We consider two questions about PLMs calibration: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? We first introduce the metrics we adopt for calibration performance evaluation. The most widely used calibration metric ECE (Naeini et al., 2015) is considered. It measures the difference between confidence and accuracy by portioning samples into various confidence zones. To give a more comprehensive and practical calibration evaluation, we provide an application-driven perspective, describing two undesirable situations in practice: (1) For the first question, we consider the influence of six factors that have influence on PLMs' calibration performance, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. Some of them are overlooked in previous empirical studies (Snoek et al., 2019;Nixon et al., 2019;Minderer et al., 2021). We motivate to conduct fine-grained control experiments to study the dynamic change in PLMs' calibration performance in training through manipulating controlling variables. We empirically observe a consistent change in calibration performance across six factors. All six factors influence PLMs' fitness on the training distribution. This results in two states of PLMs considering calibration performance, namely under-fitted and over-fitted states (see Fig. 1). In the under-fitted state, PLMs' performance and confidence increase at different speeds when more fitted on the training distribution. In the over-fitting state, PLMs' confidence continues to",
                    "score": 0.480202632707609,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 97,
                            "end": 116,
                            "matchedPaperCorpusId": "5034059"
                        },
                        {
                            "start": 223,
                            "end": 251,
                            "matchedPaperCorpusId": "67749814"
                        },
                        {
                            "start": 498,
                            "end": 516,
                            "matchedPaperCorpusId": "28671436"
                        },
                        {
                            "start": 1566,
                            "end": 1586,
                            "matchedPaperCorpusId": "174803437"
                        },
                        {
                            "start": 1586,
                            "end": 1605,
                            "matchedPaperCorpusId": "102486060"
                        },
                        {
                            "start": 1605,
                            "end": 1627,
                            "matchedPaperCorpusId": "235435823"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92822265625
                }
            ],
            "relevance_judgement": 0.94189453125,
            "relevance_judgment_input_expanded": "# Title: A Close Look into the Calibration of Pre-trained Language Models\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Yangyi Chen, Lifan Yuan, Ganqu Cui, Zhiyuan Liu, Heng Ji\n## Abstract\nPre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs\u2019 calibration performance in training. We consider six factors as control variables, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. We observe a consistent change in calibration performance across six factors. We find that PLMs don\u2019t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not. We highlight that our finding somewhat contradicts two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining improves model calibration. Next, we study the effectiveness of existing calibration methods in mitigating the overconfidence issue. Besides unlearnable calibration methods (e.g., label smoothing), we adapt and extend two recently proposed learnable methods that directly collect data to train models to have reasonable confidence estimations. Experimental results show that learnable methods significantly reduce PLMs\u2019 confidence in wrong predictions.\n## INTRODUCTION\nPre-trained language models (PLMs) are successful in many downstream tasks regarding performance (Wang et al., 2019). But for a reliable deployment in practice, the calibration performance should also be carefully examined (Vaicenavicius et al., 2019). Well-calibrated models assign appropriate confidence scores that truly reflect the outcome probability of their predictions. However, the confidence scores of existing deep neural networks cannot serve as reliable estimates of their uncertainty (Guo et al., 2017), and a deep understanding of PLMs calibration is lacking.\n\nIn this paper, we give a systematical analysis of PLMs calibration. We consider two questions about PLMs calibration: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? We first introduce the metrics we adopt for calibration performance evaluation. The most widely used calibration metric ECE (Naeini et al., 2015) is considered. It measures the difference between confidence and accuracy by portioning samples into various confidence zones. To give a more comprehensive and practical calibration evaluation, we provide an application-driven perspective, describing two undesirable situations in practice: (1) For the first question, we consider the influence of six factors that have influence on PLMs' calibration performance, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. Some of them are overlooked in previous empirical studies (Snoek et al., 2019;Nixon et al., 2019;Minderer et al., 2021). We motivate to conduct fine-grained control experiments to study the dynamic change in PLMs' calibration performance in training through manipulating controlling variables. We empirically observe a consistent change in calibration performance across six factors. All six factors influence PLMs' fitness on the training distribution. This results in two states of PLMs considering calibration performance, namely under-fitted and over-fitted states (see Fig. 1). In the under-fitted state, PLMs' performance and confidence increase at different speeds when more fitted on the training distribution. In the over-fitting state, PLMs' confidence continues to",
            "reference_string": "[253244504 | Chen et al. | 2022 | Citations: 51]"
        },
        {
            "title": "Influences on LLM Calibration: A Study of Response Agreement, Loss Functions, and Prompt Styles",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.03991, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2291031833",
                    "name": "Yuxi Xia"
                },
                {
                    "authorId": "152746845",
                    "name": "Pedro Henrique Luz de Araujo"
                },
                {
                    "authorId": "46214177",
                    "name": "Klim Zaporojets"
                },
                {
                    "authorId": "2266756543",
                    "name": "Benjamin Roth"
                }
            ],
            "abstract": "Calibration, the alignment between model confidence and prediction accuracy, is critical for the reliable deployment of large language models (LLMs). Existing works neglect to measure the generalization of their methods to other prompt styles and different sizes of LLMs. To address this, we define a controlled experimental setting covering 12 LLMs and four prompt styles. We additionally investigate if incorporating the response agreement of multiple LLMs and an appropriate loss function can improve calibration performance. Concretely, we build Calib-n, a novel framework that trains an auxiliary model for confidence estimation that aggregates responses from multiple LLMs to capture inter-model agreement. To optimize calibration, we integrate focal and AUC surrogate losses alongside binary cross-entropy. Experiments across four datasets demonstrate that both response agreement and focal loss improve calibration from baselines. We find that few-shot prompts are the most effective for auxiliary model-based methods, and auxiliary models demonstrate robust calibration performance across accuracy variations, outperforming LLMs' internal probabilities and verbalized confidences. These insights deepen the understanding of influence factors in LLM calibration, supporting their reliable deployment in diverse applications.",
            "corpus_id": 275342783,
            "sentences": [
                {
                    "corpus_id": "275342783",
                    "title": "Influences on LLM Calibration: A Study of Response Agreement, Loss Functions, and Prompt Styles",
                    "text": "Calibration, the alignment between model confidence and prediction accuracy, is critical for the reliable deployment of large language models (LLMs). Existing works neglect to measure the generalization of their methods to other prompt styles and different sizes of LLMs. To address this, we define a controlled experimental setting covering 12 LLMs and four prompt styles. We additionally investigate if incorporating the response agreement of multiple LLMs and an appropriate loss function can improve calibration performance. Concretely, we build Calib-n, a novel framework that trains an auxiliary model for confidence estimation that aggregates responses from multiple LLMs to capture inter-model agreement. To optimize calibration, we integrate focal and AUC surrogate losses alongside binary cross-entropy. Experiments across four datasets demonstrate that both response agreement and focal loss improve calibration from baselines. We find that few-shot prompts are the most effective for auxiliary model-based methods, and auxiliary models demonstrate robust calibration performance across accuracy variations, outperforming LLMs' internal probabilities and verbalized confidences. These insights deepen the understanding of influence factors in LLM calibration, supporting their reliable deployment in diverse applications.",
                    "score": 0.4917232245953574,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93994140625
                }
            ],
            "relevance_judgement": 0.93994140625,
            "relevance_judgment_input_expanded": "# Title: Influences on LLM Calibration: A Study of Response Agreement, Loss Functions, and Prompt Styles\n# Venue: arXiv.org\n# Authors: Yuxi Xia, Pedro Henrique Luz de Araujo, Klim Zaporojets, Benjamin Roth\n## Abstract\nCalibration, the alignment between model confidence and prediction accuracy, is critical for the reliable deployment of large language models (LLMs). Existing works neglect to measure the generalization of their methods to other prompt styles and different sizes of LLMs. To address this, we define a controlled experimental setting covering 12 LLMs and four prompt styles. We additionally investigate if incorporating the response agreement of multiple LLMs and an appropriate loss function can improve calibration performance. Concretely, we build Calib-n, a novel framework that trains an auxiliary model for confidence estimation that aggregates responses from multiple LLMs to capture inter-model agreement. To optimize calibration, we integrate focal and AUC surrogate losses alongside binary cross-entropy. Experiments across four datasets demonstrate that both response agreement and focal loss improve calibration from baselines. We find that few-shot prompts are the most effective for auxiliary model-based methods, and auxiliary models demonstrate robust calibration performance across accuracy variations, outperforming LLMs' internal probabilities and verbalized confidences. These insights deepen the understanding of influence factors in LLM calibration, supporting their reliable deployment in diverse applications.\n",
            "reference_string": "[275342783 | Xia et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Calibrating Language Models with Adaptive Temperature Scaling",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 32,
            "citation_count": 17,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.19817, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2285068366",
                    "name": "Johnathan Xie"
                },
                {
                    "authorId": "2111073657",
                    "name": "Annie S. Chen"
                },
                {
                    "authorId": "2260289273",
                    "name": "Yoonho Lee"
                },
                {
                    "authorId": "2260106796",
                    "name": "Eric Mitchell"
                },
                {
                    "authorId": "2279838067",
                    "name": "Chelsea Finn"
                }
            ],
            "abstract": "The effectiveness of large language models (LLMs) is not only measured by their ability to generate accurate outputs but also by their calibration\u2014how well their confidence scores reflect the probability of their outputs being correct. While unsupervised pre-training has been shown to yield LLMs with well-calibrated conditional probabilities, recent studies have shown that after fine-tuning with reinforcement learning from human feedback (RLHF), the calibration of these models degrades significantly. In this work, we introduce Adaptive Temperature Scaling (ATS), a post-hoc calibration method that predicts a temperature scaling parameter for each token prediction. The predicted temperature values adapt based on token-level features and are fit over a standard supervised fine-tuning (SFT) dataset. The adaptive nature of ATS addresses the varying degrees of calibration shift that can occur after RLHF fine-tuning. ATS improves calibration by over 10-50% across three downstream natural language evaluation benchmarks compared to prior calibration methods and does not impede performance improvements from RLHF.",
            "corpus_id": 272987064,
            "sentences": [
                {
                    "corpus_id": "272987064",
                    "title": "Calibrating Language Models with Adaptive Temperature Scaling",
                    "text": "Large language models (LLMs) have become a cornerstone of modern artificial intelligence, offering impressive capabilities in natural language processing tasks. However, the reliability of LLMs is intertwined with their ability to generate confidence scores that accurately reflect the likelihood of their outputs being correct. This calibration, aligning a model's confidence with its accuracy, is essential, especially when LLMs are deployed in real-world scenarios where decisions based on incorrect outputs can have significant consequences. \n\nWhile unsupervised pre-training methods have shown success in producing well-calibrated LLMs, * Equal contribution. a challenge arises when these models undergo finetuning through reinforcement learning from human feedback (RLHF). While RLHF fine-tuning is effective in enhancing model performance on specific tasks and aligning outputs with human preferences, recent studies indicate a notable degradation in the calibration of LLMs post-RLHF finetuning (Achiam et al., 2023;Tian et al., 2023;Kadavath et al., 2022). This degradation compromises the model's ability to provide reliable confidence scores, an issue that becomes critical when these models are applied to tasks requiring high levels of trust and accuracy. An important question arises: how can we maintain the performance gains achieved through RLHF fine-tuning while ensuring that the model's confidence scores remain reliable? \n\nTo address this challenge, our work introduces Adaptive Temperature Scaling (ATS), a post-hoc calibration technique that predicts a temperature scaling parameter for each token prediction based on a language model's hidden features. Basic temperature scaling is a widely-used calibration method that applies a single temperature parameter across all outputs of a model. This technique, while effective in some contexts, assumes uniform calibration needs across all inputs, which is often not the case for complex models like LLMs. ATS, in contrast, predicts a unique temperature scaling parameter for each set of token predictions. This input-specific approach allows ATS to refine the calibration process, addressing the varying degrees of calibration shift that can occur after RLHF fine-tuning.",
                    "score": 0.38573729386420696,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 160
                        },
                        {
                            "start": 161,
                            "end": 328
                        },
                        {
                            "start": 329,
                            "end": 545
                        },
                        {
                            "start": 548,
                            "end": 663
                        },
                        {
                            "start": 664,
                            "end": 778
                        },
                        {
                            "start": 779,
                            "end": 1065
                        },
                        {
                            "start": 1066,
                            "end": 1268
                        },
                        {
                            "start": 1269,
                            "end": 1441
                        },
                        {
                            "start": 1444,
                            "end": 1676
                        },
                        {
                            "start": 1677,
                            "end": 1813
                        },
                        {
                            "start": 1814,
                            "end": 1974
                        },
                        {
                            "start": 1975,
                            "end": 2075
                        },
                        {
                            "start": 2076,
                            "end": 2241
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1024,
                            "end": 1042,
                            "matchedPaperCorpusId": "258865733"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93994140625
                }
            ],
            "relevance_judgement": 0.93994140625,
            "relevance_judgment_input_expanded": "# Title: Calibrating Language Models with Adaptive Temperature Scaling\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Johnathan Xie, Annie S. Chen, Yoonho Lee, Eric Mitchell, Chelsea Finn\n## Abstract\nThe effectiveness of large language models (LLMs) is not only measured by their ability to generate accurate outputs but also by their calibration\u2014how well their confidence scores reflect the probability of their outputs being correct. While unsupervised pre-training has been shown to yield LLMs with well-calibrated conditional probabilities, recent studies have shown that after fine-tuning with reinforcement learning from human feedback (RLHF), the calibration of these models degrades significantly. In this work, we introduce Adaptive Temperature Scaling (ATS), a post-hoc calibration method that predicts a temperature scaling parameter for each token prediction. The predicted temperature values adapt based on token-level features and are fit over a standard supervised fine-tuning (SFT) dataset. The adaptive nature of ATS addresses the varying degrees of calibration shift that can occur after RLHF fine-tuning. ATS improves calibration by over 10-50% across three downstream natural language evaluation benchmarks compared to prior calibration methods and does not impede performance improvements from RLHF.\n## Introduction\nLarge language models (LLMs) have become a cornerstone of modern artificial intelligence, offering impressive capabilities in natural language processing tasks. However, the reliability of LLMs is intertwined with their ability to generate confidence scores that accurately reflect the likelihood of their outputs being correct. This calibration, aligning a model's confidence with its accuracy, is essential, especially when LLMs are deployed in real-world scenarios where decisions based on incorrect outputs can have significant consequences. \n\nWhile unsupervised pre-training methods have shown success in producing well-calibrated LLMs, * Equal contribution. a challenge arises when these models undergo finetuning through reinforcement learning from human feedback (RLHF). While RLHF fine-tuning is effective in enhancing model performance on specific tasks and aligning outputs with human preferences, recent studies indicate a notable degradation in the calibration of LLMs post-RLHF finetuning (Achiam et al., 2023;Tian et al., 2023;Kadavath et al., 2022). This degradation compromises the model's ability to provide reliable confidence scores, an issue that becomes critical when these models are applied to tasks requiring high levels of trust and accuracy. An important question arises: how can we maintain the performance gains achieved through RLHF fine-tuning while ensuring that the model's confidence scores remain reliable? \n\nTo address this challenge, our work introduces Adaptive Temperature Scaling (ATS), a post-hoc calibration technique that predicts a temperature scaling parameter for each token prediction based on a language model's hidden features. Basic temperature scaling is a widely-used calibration method that applies a single temperature parameter across all outputs of a model. This technique, while effective in some contexts, assumes uniform calibration needs across all inputs, which is often not the case for complex models like LLMs. ATS, in contrast, predicts a unique temperature scaling parameter for each set of token predictions. This input-specific approach allows ATS to refine the calibration process, addressing the varying degrees of calibration shift that can occur after RLHF fine-tuning.",
            "reference_string": "[272987064 | Xie et al. | 2024 | Citations: 17]"
        },
        {
            "title": "A Survey of Calibration Process for Black-Box LLMs",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 128,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.12767, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2336030822",
                    "name": "Liangru Xie"
                },
                {
                    "authorId": "2326538488",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "2276423058",
                    "name": "Jingying Zeng"
                },
                {
                    "authorId": "2313288320",
                    "name": "Xianfeng Tang"
                },
                {
                    "authorId": "2336079495",
                    "name": "Yan Han"
                },
                {
                    "authorId": "2305051640",
                    "name": "Chen Luo"
                },
                {
                    "authorId": "2336079074",
                    "name": "Jing Huang"
                },
                {
                    "authorId": "2313299851",
                    "name": "Zhen Li"
                },
                {
                    "authorId": "2257362558",
                    "name": "Suhang Wang"
                },
                {
                    "authorId": "2315065019",
                    "name": "Qi He"
                }
            ],
            "abstract": "Large Language Models (LLMs) demonstrate remarkable performance in semantic understanding and generation, yet accurately assessing their output reliability remains a significant challenge. While numerous studies have explored calibration techniques, they primarily focus on White-Box LLMs with accessible parameters. Black-Box LLMs, despite their superior performance, pose heightened requirements for calibration techniques due to their API-only interaction constraints. Although recent researches have achieved breakthroughs in black-box LLMs calibration, a systematic survey of these methodologies is still lacking. To bridge this gap, we presents the first comprehensive survey on calibration techniques for black-box LLMs. We first define the Calibration Process of LLMs as comprising two interrelated key steps: Confidence Estimation and Calibration. Second, we conduct a systematic review of applicable methods within black-box settings, and provide insights on the unique challenges and connections in implementing these key steps. Furthermore, we explore typical applications of Calibration Process in black-box LLMs and outline promising future research directions, providing new perspectives for enhancing reliability and human-machine alignment. This is our GitHub link: https://github.com/LiangruXie/Calibration-Process-in-Black-Box-LLMs",
            "corpus_id": 274789494,
            "sentences": [
                {
                    "corpus_id": "274789494",
                    "title": "A Survey of Calibration Process for Black-Box LLMs",
                    "text": "Calibration aims to align confidence scores with correctness. Commonly used calibration methods include temperature scaling (Guo et al., 2017), label smoothing (Szegedy et al., 2016), and logistic calibration (Kull et al., 2017). However, methods like label smoothing and logistic calibration are unsuitable for black-box LLMs, as they require internal model access or fine-tuning, whereas temperature scaling, which operates on the logits of model outputs, is better suited for gray-box models. In black-box LLMs, common calibration methods include post-processing or transforming the model into a gray-box. Due to the near-well-calibrated nature of Confidence Estimation in black-box LLMs, calibration pressure is reduced. This section discusses the subsequent steps required to achieve well-calibration following Confidence Estimation. \n\nIn post-processing methods, popular techniques include Histogram Binning (Zadrozny and Elkan, 2001) and Isotonic Regression (Jiang et al., 2011). Histogram Binning (Zadrozny and Elkan, 2001) adjusts confidence values within different intervals by assessing the correctness of predictions in each interval, providing a simple but potentially limited calibration when confidence distributions are uneven. Isotonic Regression (Jiang et al., 2011), on the other hand, seeks an optimal monotonically increasing function f (c) to map the model's confidence c 1 , c 2 , . . . , c n to correctness. f (c) need to satisfy the condition in Formula 2, ensuring that the confidence ranking aligns with the correctness. \n\nThough flexible, Isotonic Regression is less effective in classification tasks as it doesn't directly modify the decision boundary. \n\nTo address more complex scenarios, some methods introduce additional constraints. For instance, H\u00e9bert-Johnson et al. ( 2018) introduced a multicalibration method, ensuring not only overall alignment but also calibration across multiple subgroups S j through iterative alignment. The overall goal can be expressed as Formula 3. \n\n(3) \n\nWhile this method promotes fairness, it is computationally expensive and depends heavily on subgrouping strategies, usually relying on known features.",
                    "score": 0.468478230837678,
                    "section_title": "Calibration Methods",
                    "char_start_offset": 18690,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 61
                        },
                        {
                            "start": 62,
                            "end": 229
                        },
                        {
                            "start": 230,
                            "end": 495
                        },
                        {
                            "start": 496,
                            "end": 608
                        },
                        {
                            "start": 609,
                            "end": 724
                        },
                        {
                            "start": 725,
                            "end": 838
                        },
                        {
                            "start": 841,
                            "end": 986
                        },
                        {
                            "start": 987,
                            "end": 1243
                        },
                        {
                            "start": 1244,
                            "end": 1409
                        },
                        {
                            "start": 1410,
                            "end": 1431
                        },
                        {
                            "start": 1432,
                            "end": 1547
                        },
                        {
                            "start": 1550,
                            "end": 1681
                        },
                        {
                            "start": 1684,
                            "end": 1765
                        },
                        {
                            "start": 1766,
                            "end": 1963
                        },
                        {
                            "start": 1964,
                            "end": 2011
                        },
                        {
                            "start": 2014,
                            "end": 2017
                        },
                        {
                            "start": 2020,
                            "end": 2170
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 124,
                            "end": 142,
                            "matchedPaperCorpusId": "28671436"
                        },
                        {
                            "start": 160,
                            "end": 182,
                            "matchedPaperCorpusId": "206593880"
                        },
                        {
                            "start": 209,
                            "end": 228,
                            "matchedPaperCorpusId": "217264968"
                        },
                        {
                            "start": 914,
                            "end": 940,
                            "matchedPaperCorpusId": "3094990"
                        },
                        {
                            "start": 965,
                            "end": 985,
                            "matchedPaperCorpusId": "14711865"
                        },
                        {
                            "start": 1005,
                            "end": 1031,
                            "matchedPaperCorpusId": "3094990"
                        },
                        {
                            "start": 1264,
                            "end": 1284,
                            "matchedPaperCorpusId": "14711865"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93408203125
                },
                {
                    "corpus_id": "274789494",
                    "title": "A Survey of Calibration Process for Black-Box LLMs",
                    "text": "LLMs. Bias detection and mitigation remain critical challenges in black-box LLMs. The inaccessibility of model internals makes it difficult to identify and address bias, imposing inherent limitations on both confidence estimation and calibration. This often leads to disproportionately high or low calibration errors for specific groups or tasks, undermining the model's reliability and fairness. Traditional methods have limited effectiveness in addressing bias within black-box settings, underscoring the necessity for tailored solutions. Addressing these challenges requires a focus on improving output transparency, integrating external calibration mechanisms, and leveraging user feedback for dynamic confidence adjustment. These strategies not only tackle the unique obstacles of black-box settings but also enhance the calibration performance and practical utility of LLMs, enabling more reliable and interpretable applications across diverse real-world scenarios. \n\nCalibration for Long-form Text. Calibrating long-form text poses significant challenges for both white-box and black-box LLMs. As the demand for LLMs grows, long texts in QA and other generative tasks often encompass multiple claims and facts, making calibration increasingly complex. Assessing whether confidence scores are well-calibrated becomes particularly difficult when only portions of the text are correct, and quality evaluation is further complicated by its inherently subjective nature. Addressing these challenges requires effective methods that incorporate human perception as a metric for correctness. Additionally, it is crucial to develop approaches tailored to black-box settings, where model parameters remain inaccessible.",
                    "score": 0.40649313789892605,
                    "section_title": "Bias Detection and Mitigation for black-box",
                    "char_start_offset": 29097,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 5
                        },
                        {
                            "start": 6,
                            "end": 81
                        },
                        {
                            "start": 82,
                            "end": 246
                        },
                        {
                            "start": 247,
                            "end": 396
                        },
                        {
                            "start": 397,
                            "end": 540
                        },
                        {
                            "start": 541,
                            "end": 728
                        },
                        {
                            "start": 729,
                            "end": 971
                        },
                        {
                            "start": 974,
                            "end": 1005
                        },
                        {
                            "start": 1006,
                            "end": 1100
                        },
                        {
                            "start": 1101,
                            "end": 1258
                        },
                        {
                            "start": 1259,
                            "end": 1472
                        },
                        {
                            "start": 1473,
                            "end": 1590
                        },
                        {
                            "start": 1591,
                            "end": 1716
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89404296875
                },
                {
                    "corpus_id": "274789494",
                    "title": "A Survey of Calibration Process for Black-Box LLMs",
                    "text": "Given these limitations and the high computational costs of traditional methods, Confidence Estimation and Calibration Methods have emerged as promising alternatives (Ni et al., 2024;Mielke et al., 2022), offering cost-effective solutions applicable to both white-box and black-box models while maintaining scalability to large-scale deployments. \n\nIn recent years, Confidence Estimation and Calibration have frequently been discussed together, as the estimation of confidence is often influenced by the uncertainty in the model or data, and calibration methods help the model recognize its own knowledge limitations (Jiang et al., 2021;Shrivastava et al., 2023). Calibration allows LLMs to adjust their confidence to more accurately reflect the quality of their outputs (Kuhn et al., 2023;Duan et al., 2023). For example, in the context of diagnosing rare diseases, LLMs may estimate a 95% confidence score to an incorrect response, while the accurate confidence, due to a lack of domain knowledge, should be closer to 40%. Calibration can identify this discrepancy and adjust the model's confidence to more accurately reflect the quality of the response, preventing overconfidence in generated responses (Ren et al., 2023;Geng et al., 2024;Tian et al., 2023). This process of Confidence Estimation, and Calibration to achieve well-calibrated confidence is referred to as the Calibration Process in this survey. \n\nA growing number of surveys have reviewed factors influencing LLMs Calibration Process. For instance, some studies have summarized how model parameters, training stages, or training data influence confidence and calibration (Zhu et al., 2023), while others investigate which confidence representations are more beneficial for calibration during the RLHF phase (Tian et al., 2023). Additionally, many studies have reviewed various confidence estimation and calibration methods on Language Models(LMs) and LLMs (Geng et al., 2024).",
                    "score": 0.5419711294723611,
                    "section_title": "Introduction",
                    "char_start_offset": 1309,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 346
                        },
                        {
                            "start": 349,
                            "end": 663
                        },
                        {
                            "start": 664,
                            "end": 809
                        },
                        {
                            "start": 810,
                            "end": 1024
                        },
                        {
                            "start": 1025,
                            "end": 1261
                        },
                        {
                            "start": 1262,
                            "end": 1412
                        },
                        {
                            "start": 1415,
                            "end": 1502
                        },
                        {
                            "start": 1503,
                            "end": 1795
                        },
                        {
                            "start": 1796,
                            "end": 1944
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 183,
                            "end": 203,
                            "matchedPaperCorpusId": "250073258"
                        },
                        {
                            "start": 617,
                            "end": 637,
                            "matchedPaperCorpusId": "235078802"
                        },
                        {
                            "start": 1206,
                            "end": 1224,
                            "matchedPaperCorpusId": "266335508"
                        },
                        {
                            "start": 1224,
                            "end": 1242,
                            "matchedPaperCorpusId": "265157516"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88525390625
                }
            ],
            "relevance_judgement": 0.93408203125,
            "relevance_judgment_input_expanded": "# Title: A Survey of Calibration Process for Black-Box LLMs\n# Venue: arXiv.org\n# Authors: Liangru Xie, Hui Liu, Jingying Zeng, Xianfeng Tang, Yan Han, Chen Luo, Jing Huang, Zhen Li, Suhang Wang, Qi He\n## Abstract\nLarge Language Models (LLMs) demonstrate remarkable performance in semantic understanding and generation, yet accurately assessing their output reliability remains a significant challenge. While numerous studies have explored calibration techniques, they primarily focus on White-Box LLMs with accessible parameters. Black-Box LLMs, despite their superior performance, pose heightened requirements for calibration techniques due to their API-only interaction constraints. Although recent researches have achieved breakthroughs in black-box LLMs calibration, a systematic survey of these methodologies is still lacking. To bridge this gap, we presents the first comprehensive survey on calibration techniques for black-box LLMs. We first define the Calibration Process of LLMs as comprising two interrelated key steps: Confidence Estimation and Calibration. Second, we conduct a systematic review of applicable methods within black-box settings, and provide insights on the unique challenges and connections in implementing these key steps. Furthermore, we explore typical applications of Calibration Process in black-box LLMs and outline promising future research directions, providing new perspectives for enhancing reliability and human-machine alignment. This is our GitHub link: https://github.com/LiangruXie/Calibration-Process-in-Black-Box-LLMs\n## Introduction\nGiven these limitations and the high computational costs of traditional methods, Confidence Estimation and Calibration Methods have emerged as promising alternatives (Ni et al., 2024;Mielke et al., 2022), offering cost-effective solutions applicable to both white-box and black-box models while maintaining scalability to large-scale deployments. \n\nIn recent years, Confidence Estimation and Calibration have frequently been discussed together, as the estimation of confidence is often influenced by the uncertainty in the model or data, and calibration methods help the model recognize its own knowledge limitations (Jiang et al., 2021;Shrivastava et al., 2023). Calibration allows LLMs to adjust their confidence to more accurately reflect the quality of their outputs (Kuhn et al., 2023;Duan et al., 2023). For example, in the context of diagnosing rare diseases, LLMs may estimate a 95% confidence score to an incorrect response, while the accurate confidence, due to a lack of domain knowledge, should be closer to 40%. Calibration can identify this discrepancy and adjust the model's confidence to more accurately reflect the quality of the response, preventing overconfidence in generated responses (Ren et al., 2023;Geng et al., 2024;Tian et al., 2023). This process of Confidence Estimation, and Calibration to achieve well-calibrated confidence is referred to as the Calibration Process in this survey. \n\nA growing number of surveys have reviewed factors influencing LLMs Calibration Process. For instance, some studies have summarized how model parameters, training stages, or training data influence confidence and calibration (Zhu et al., 2023), while others investigate which confidence representations are more beneficial for calibration during the RLHF phase (Tian et al., 2023). Additionally, many studies have reviewed various confidence estimation and calibration methods on Language Models(LMs) and LLMs (Geng et al., 2024).\n\n## Calibration Methods\nCalibration aims to align confidence scores with correctness. Commonly used calibration methods include temperature scaling (Guo et al., 2017), label smoothing (Szegedy et al., 2016), and logistic calibration (Kull et al., 2017). However, methods like label smoothing and logistic calibration are unsuitable for black-box LLMs, as they require internal model access or fine-tuning, whereas temperature scaling, which operates on the logits of model outputs, is better suited for gray-box models. In black-box LLMs, common calibration methods include post-processing or transforming the model into a gray-box. Due to the near-well-calibrated nature of Confidence Estimation in black-box LLMs, calibration pressure is reduced. This section discusses the subsequent steps required to achieve well-calibration following Confidence Estimation. \n\nIn post-processing methods, popular techniques include Histogram Binning (Zadrozny and Elkan, 2001) and Isotonic Regression (Jiang et al., 2011). Histogram Binning (Zadrozny and Elkan, 2001) adjusts confidence values within different intervals by assessing the correctness of predictions in each interval, providing a simple but potentially limited calibration when confidence distributions are uneven. Isotonic Regression (Jiang et al., 2011), on the other hand, seeks an optimal monotonically increasing function f (c) to map the model's confidence c 1 , c 2 , . . . , c n to correctness. f (c) need to satisfy the condition in Formula 2, ensuring that the confidence ranking aligns with the correctness. \n\nThough flexible, Isotonic Regression is less effective in classification tasks as it doesn't directly modify the decision boundary. \n\nTo address more complex scenarios, some methods introduce additional constraints. For instance, H\u00e9bert-Johnson et al. ( 2018) introduced a multicalibration method, ensuring not only overall alignment but also calibration across multiple subgroups S j through iterative alignment. The overall goal can be expressed as Formula 3. \n\n(3) \n\nWhile this method promotes fairness, it is computationally expensive and depends heavily on subgrouping strategies, usually relying on known features.\n\n## Bias Detection and Mitigation for black-box\nLLMs. Bias detection and mitigation remain critical challenges in black-box LLMs. The inaccessibility of model internals makes it difficult to identify and address bias, imposing inherent limitations on both confidence estimation and calibration. This often leads to disproportionately high or low calibration errors for specific groups or tasks, undermining the model's reliability and fairness. Traditional methods have limited effectiveness in addressing bias within black-box settings, underscoring the necessity for tailored solutions. Addressing these challenges requires a focus on improving output transparency, integrating external calibration mechanisms, and leveraging user feedback for dynamic confidence adjustment. These strategies not only tackle the unique obstacles of black-box settings but also enhance the calibration performance and practical utility of LLMs, enabling more reliable and interpretable applications across diverse real-world scenarios. \n\nCalibration for Long-form Text. Calibrating long-form text poses significant challenges for both white-box and black-box LLMs. As the demand for LLMs grows, long texts in QA and other generative tasks often encompass multiple claims and facts, making calibration increasingly complex. Assessing whether confidence scores are well-calibrated becomes particularly difficult when only portions of the text are correct, and quality evaluation is further complicated by its inherently subjective nature. Addressing these challenges requires effective methods that incorporate human perception as a metric for correctness. Additionally, it is crucial to develop approaches tailored to black-box settings, where model parameters remain inaccessible.",
            "reference_string": "[274789494 | Xie et al. | 2024 | Citations: 4]"
        },
        {
            "title": "C3: Confidence Calibration Model Cascade for Inference-Efficient Cross-Lingual Natural Language Understanding",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 54,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.15991, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2276964586",
                    "name": "Taixi Lu"
                },
                {
                    "authorId": "51225422",
                    "name": "Haoyu Wang"
                },
                {
                    "authorId": "2286886107",
                    "name": "Huajie Shao"
                },
                {
                    "authorId": "2284861474",
                    "name": "Jing Gao"
                },
                {
                    "authorId": "2273953218",
                    "name": "Huaxiu Yao"
                }
            ],
            "abstract": "Cross-lingual natural language understanding (NLU) is a critical task in natural language processing (NLP). Recent advancements have seen multilingual pre-trained language models (mPLMs) significantly enhance the performance of these tasks. However, mPLMs necessitate substantial resources and incur high computational costs during inference, posing challenges for deployment in real-world and real-time systems. Existing model cascade methods seek to enhance inference efficiency by greedily selecting the lightest model capable of processing the current input from a variety of models, based on model confidence scores. Nonetheless, deep models tend to exhibit overconfidence, and confidence distributions vary across languages. This leads to the emission of confident but incorrect predictions by smaller models, hindering their ability to generalize effectively across test languages. In this study, we introduce a confidence calibration model cascade ($C^3$) method. This approach, simple yet effective, involves calibration prior to cascade inference, thereby enhancing cascade accuracy through more reliable predictions. Extensive experiments conducted on three cross-lingual benchmarks demonstrate that $C^3$ significantly outperforms all state-of-the-art baselines.",
            "corpus_id": 267938751,
            "sentences": [
                {
                    "corpus_id": "267938751",
                    "title": "C3: Confidence Calibration Model Cascade for Inference-Efficient Cross-Lingual Natural Language Understanding",
                    "text": "In this paper, we propose a simple, yet effective method, Confidence Calibration Cascade (C 3 ) that enhances cross-lingual inference accuracy through more reliable confidence scores. We introduce calibration methods for both Language Model cascade and Large Language Model cascade. We conduct extensive experiment on cross-lingual benchmarks. By comparing with state-of-the-art methods, the results demonstrates the effectiveness of C 3 . Furthermore, C 3 also demonstrates strong calibration results compared to vanilla cascade methods.",
                    "score": 0.4642747588997227,
                    "section_title": "Conclusion",
                    "char_start_offset": 23824,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 183
                        },
                        {
                            "start": 184,
                            "end": 282
                        },
                        {
                            "start": 283,
                            "end": 343
                        },
                        {
                            "start": 344,
                            "end": 439
                        },
                        {
                            "start": 440,
                            "end": 538
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93359375
                }
            ],
            "relevance_judgement": 0.93359375,
            "relevance_judgment_input_expanded": "# Title: C3: Confidence Calibration Model Cascade for Inference-Efficient Cross-Lingual Natural Language Understanding\n# Venue: arXiv.org\n# Authors: Taixi Lu, Haoyu Wang, Huajie Shao, Jing Gao, Huaxiu Yao\n## Abstract\nCross-lingual natural language understanding (NLU) is a critical task in natural language processing (NLP). Recent advancements have seen multilingual pre-trained language models (mPLMs) significantly enhance the performance of these tasks. However, mPLMs necessitate substantial resources and incur high computational costs during inference, posing challenges for deployment in real-world and real-time systems. Existing model cascade methods seek to enhance inference efficiency by greedily selecting the lightest model capable of processing the current input from a variety of models, based on model confidence scores. Nonetheless, deep models tend to exhibit overconfidence, and confidence distributions vary across languages. This leads to the emission of confident but incorrect predictions by smaller models, hindering their ability to generalize effectively across test languages. In this study, we introduce a confidence calibration model cascade ($C^3$) method. This approach, simple yet effective, involves calibration prior to cascade inference, thereby enhancing cascade accuracy through more reliable predictions. Extensive experiments conducted on three cross-lingual benchmarks demonstrate that $C^3$ significantly outperforms all state-of-the-art baselines.\n## Conclusion\nIn this paper, we propose a simple, yet effective method, Confidence Calibration Cascade (C 3 ) that enhances cross-lingual inference accuracy through more reliable confidence scores. We introduce calibration methods for both Language Model cascade and Large Language Model cascade. We conduct extensive experiment on cross-lingual benchmarks. By comparing with state-of-the-art methods, the results demonstrates the effectiveness of C 3 . Furthermore, C 3 also demonstrates strong calibration results compared to vanilla cascade methods.",
            "reference_string": "[267938751 | Lu et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 59,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.02902, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2332445319",
                    "name": "Liangjie Huang"
                },
                {
                    "authorId": "2329323230",
                    "name": "Dawei Li"
                },
                {
                    "authorId": "2354590799",
                    "name": "Huan Liu"
                },
                {
                    "authorId": "2329855611",
                    "name": "Lu Cheng"
                }
            ],
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable self-improvement capabilities, whereby models iteratively revise their outputs through self-generated feedback. While this reflective mechanism has shown promise in enhancing task performance, recent studies suggest that it may also introduce undesirable biases-most notably, self-bias, or the tendency of LLMs to favor their own prior outputs. In this work, we extend this line of inquiry by investigating the impact on confidence estimation. We evaluate three representative self-improvement paradigms-basic prompting, Chain-of-Thought (CoT) prompting, and tuning-based methods and find that iterative self-improvement can lead to systematic overconfidence, as evidenced by a steadily increasing Expected Calibration Error (ECE) and lower accuracy with high confidence. We then further explore the integration of confidence calibration techniques with self-improvement. Specifically, we compare three strategies: (1) applying calibration after multiple rounds of self-improvement, (2) calibrating before self-improvement, and (3) applying calibration iteratively at each self-improvement step. Our results show that iterative calibration is most effective in reducing ECE, yielding improved calibration. Our work pioneers the study of self-improving LLMs from a calibration perspective, offering valuable insights into balancing model performance and reliability.",
            "corpus_id": 277596537,
            "sentences": [
                {
                    "corpus_id": "277596537",
                    "title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
                    "text": "Large Language Models (LLMs) have demonstrated remarkable self-improvement capabilities, whereby models iteratively revise their outputs through self-generated feedback. While this reflective mechanism has shown promise in enhancing task performance, recent studies suggest that it may also introduce undesirable biases-most notably, self-bias, or the tendency of LLMs to favor their own prior outputs. In this work, we extend this line of inquiry by investigating the impact on confidence estimation. We evaluate three representative self-improvement paradigms-basic prompting, Chain-of-Thought (CoT) prompting, and tuning-based methods and find that iterative self-improvement can lead to systematic overconfidence, as evidenced by a steadily increasing Expected Calibration Error (ECE) and lower accuracy with high confidence. We then further explore the integration of confidence calibration techniques with self-improvement. Specifically, we compare three strategies: (1) applying calibration after multiple rounds of self-improvement, (2) calibrating before self-improvement, and (3) applying calibration iteratively at each self-improvement step. Our results show that iterative calibration is most effective in reducing ECE, yielding improved calibration. Our work pioneers the study of self-improving LLMs from a calibration perspective, offering valuable insights into balancing model performance and reliability.",
                    "score": 0.44890418804697785,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92822265625
                }
            ],
            "relevance_judgement": 0.92822265625,
            "relevance_judgment_input_expanded": "# Title: Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models\n# Venue: arXiv.org\n# Authors: Liangjie Huang, Dawei Li, Huan Liu, Lu Cheng\n## Abstract\nLarge Language Models (LLMs) have demonstrated remarkable self-improvement capabilities, whereby models iteratively revise their outputs through self-generated feedback. While this reflective mechanism has shown promise in enhancing task performance, recent studies suggest that it may also introduce undesirable biases-most notably, self-bias, or the tendency of LLMs to favor their own prior outputs. In this work, we extend this line of inquiry by investigating the impact on confidence estimation. We evaluate three representative self-improvement paradigms-basic prompting, Chain-of-Thought (CoT) prompting, and tuning-based methods and find that iterative self-improvement can lead to systematic overconfidence, as evidenced by a steadily increasing Expected Calibration Error (ECE) and lower accuracy with high confidence. We then further explore the integration of confidence calibration techniques with self-improvement. Specifically, we compare three strategies: (1) applying calibration after multiple rounds of self-improvement, (2) calibrating before self-improvement, and (3) applying calibration iteratively at each self-improvement step. Our results show that iterative calibration is most effective in reducing ECE, yielding improved calibration. Our work pioneers the study of self-improving LLMs from a calibration perspective, offering valuable insights into balancing model performance and reliability.\n",
            "reference_string": "[277596537 | Huang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Calibrating Verbalized Probabilities for Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 36,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.06707, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2214132154",
                    "name": "Cheng Wang"
                },
                {
                    "authorId": "72270481",
                    "name": "Gyuri Szarvas"
                },
                {
                    "authorId": "2325041176",
                    "name": "Georges Balazs"
                },
                {
                    "authorId": "70407105",
                    "name": "Pavel Danchenko"
                },
                {
                    "authorId": "2325095919",
                    "name": "Patrick Ernst"
                }
            ],
            "abstract": "Calibrating verbalized probabilities presents a novel approach for reliably assessing and leveraging outputs from black-box Large Language Models (LLMs). Recent methods have demonstrated improved calibration by applying techniques like Platt scaling or temperature scaling to the confidence scores generated by LLMs. In this paper, we explore the calibration of verbalized probability distributions for discriminative tasks. First, we investigate the capability of LLMs to generate probability distributions over categorical labels. We theoretically and empirically identify the issue of re-softmax arising from the scaling of verbalized probabilities, and propose using the invert softmax trick to approximate the\"logit\"by inverting verbalized probabilities. Through extensive evaluation on three public datasets, we demonstrate: (1) the robust capability of LLMs in generating class distributions, and (2) the effectiveness of the invert softmax trick in estimating logits, which, in turn, facilitates post-calibration adjustments.",
            "corpus_id": 273228940,
            "sentences": [
                {
                    "corpus_id": "273228940",
                    "title": "Calibrating Verbalized Probabilities for Large Language Models",
                    "text": "Calibrating verbalized probabilities presents a novel approach for reliably assessing and leveraging outputs from black-box Large Language Models (LLMs). Recent methods have demonstrated improved calibration by applying techniques like Platt scaling or temperature scaling to the confidence scores generated by LLMs. In this paper, we explore the calibration of verbalized probability distributions for discriminative tasks. First, we investigate the capability of LLMs to generate probability distributions over categorical labels. We theoretically and empirically identify the issue of re-softmax arising from the scaling of verbalized probabilities, and propose using the invert softmax trick to approximate the\"logit\"by inverting verbalized probabilities. Through extensive evaluation on three public datasets, we demonstrate: (1) the robust capability of LLMs in generating class distributions, and (2) the effectiveness of the invert softmax trick in estimating logits, which, in turn, facilitates post-calibration adjustments.",
                    "score": 0.3934583508282584,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.923828125
                }
            ],
            "relevance_judgement": 0.923828125,
            "relevance_judgment_input_expanded": "# Title: Calibrating Verbalized Probabilities for Large Language Models\n# Venue: arXiv.org\n# Authors: Cheng Wang, Gyuri Szarvas, Georges Balazs, Pavel Danchenko, Patrick Ernst\n## Abstract\nCalibrating verbalized probabilities presents a novel approach for reliably assessing and leveraging outputs from black-box Large Language Models (LLMs). Recent methods have demonstrated improved calibration by applying techniques like Platt scaling or temperature scaling to the confidence scores generated by LLMs. In this paper, we explore the calibration of verbalized probability distributions for discriminative tasks. First, we investigate the capability of LLMs to generate probability distributions over categorical labels. We theoretically and empirically identify the issue of re-softmax arising from the scaling of verbalized probabilities, and propose using the invert softmax trick to approximate the\"logit\"by inverting verbalized probabilities. Through extensive evaluation on three public datasets, we demonstrate: (1) the robust capability of LLMs in generating class distributions, and (2) the effectiveness of the invert softmax trick in estimating logits, which, in turn, facilitates post-calibration adjustments.\n",
            "reference_string": "[273228940 | Wang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Confidence Calibration of Classifiers with Many Classes",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 77,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.02988, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2329178942",
                    "name": "Adrien LeCoz"
                },
                {
                    "authorId": "2300096291",
                    "name": "St\u00e9phane Herbin"
                },
                {
                    "authorId": "7167973",
                    "name": "Faouzi Adjed"
                }
            ],
            "abstract": "For classification models based on neural networks, the maximum predicted class probability is often used as a confidence score. This score rarely predicts well the probability of making a correct prediction and requires a post-processing calibration step. However, many confidence calibration methods fail for problems with many classes. To address this issue, we transform the problem of calibrating a multiclass classifier into calibrating a single surrogate binary classifier. This approach allows for more efficient use of standard calibration methods. We evaluate our approach on numerous neural networks used for image or text classification and show that it significantly enhances existing calibration methods.",
            "corpus_id": 273821339,
            "sentences": [
                {
                    "corpus_id": "273821339",
                    "title": "Confidence Calibration of Classifiers with Many Classes",
                    "text": "The intent is that the confidence score accurately describes whether the prediction is correct, regardless of the class. We show that this novel approach, which we call Top-versus-All (TvA), significantly improves the performance of standard calibration methods: Temperature and Vector Scaling [17], Dirichlet Calibration [29], Histogram Binning [69], Isotonic Regression [70], Beta Calibration [28], and Bayesian Binning into Quantiles [46]. We also introduce a simple regularization for Vector Scaling or Dirichlet Calibration that mitigates overfitting when the number of classes is high relative to the calibration data size. We conduct experiments on multiple image and text classification datasets and many pre-trained models. \n\nOur main contributions are the following: \n\n\u2022 We discuss four issues of the standard approach to confidence calibration. \n\n\u2022 To solve these issues, we develop the Top-versus-All approach to confidence calibration of multiclass classifiers, transforming the problem into a single binary classifier's calibration. This straightforward reformulation enables more efficient use of existing calibration methods, achieved with minimal modifications to the methods' original algorithms. \n\n\u2022 Applied to scaling methods for calibration (such as Temperature Scaling), TvA allows the use of the binary cross-entropy loss, which is more efficient in decreasing the confidence of wrong predictions and leads to stronger gradients in the case of Temperature Scaling. Applied to binary methods for calibration (such as Histogram Binning), TvA significantly improves their performance and makes them accuracy-preserving. \n\n\u2022 We demonstrate our approach's scalability and generality with extensive experiments on image classification with state-of-the-art models for complex datasets and on text classification with Pre-trained Language Models (PLMs) and Large Language Models (LLMs).",
                    "score": 0.40237310729555487,
                    "section_title": "Introduction",
                    "char_start_offset": 4044,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 120
                        },
                        {
                            "start": 121,
                            "end": 442
                        },
                        {
                            "start": 443,
                            "end": 629
                        },
                        {
                            "start": 630,
                            "end": 732
                        },
                        {
                            "start": 735,
                            "end": 776
                        },
                        {
                            "start": 779,
                            "end": 855
                        },
                        {
                            "start": 858,
                            "end": 1046
                        },
                        {
                            "start": 1047,
                            "end": 1214
                        },
                        {
                            "start": 1217,
                            "end": 1487
                        },
                        {
                            "start": 1488,
                            "end": 1639
                        },
                        {
                            "start": 1642,
                            "end": 1902
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 294,
                            "end": 298,
                            "matchedPaperCorpusId": "28671436"
                        },
                        {
                            "start": 322,
                            "end": 326,
                            "matchedPaperCorpusId": "202773833"
                        },
                        {
                            "start": 346,
                            "end": 350,
                            "matchedPaperCorpusId": "9594071"
                        },
                        {
                            "start": 372,
                            "end": 376,
                            "matchedPaperCorpusId": "3349576"
                        },
                        {
                            "start": 395,
                            "end": 399,
                            "matchedPaperCorpusId": "85546658"
                        },
                        {
                            "start": 437,
                            "end": 441,
                            "matchedPaperCorpusId": "6292807"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92236328125
                }
            ],
            "relevance_judgement": 0.92236328125,
            "relevance_judgment_input_expanded": "# Title: Confidence Calibration of Classifiers with Many Classes\n# Venue: Neural Information Processing Systems\n# Authors: Adrien LeCoz, St\u00e9phane Herbin, Faouzi Adjed\n## Abstract\nFor classification models based on neural networks, the maximum predicted class probability is often used as a confidence score. This score rarely predicts well the probability of making a correct prediction and requires a post-processing calibration step. However, many confidence calibration methods fail for problems with many classes. To address this issue, we transform the problem of calibrating a multiclass classifier into calibrating a single surrogate binary classifier. This approach allows for more efficient use of standard calibration methods. We evaluate our approach on numerous neural networks used for image or text classification and show that it significantly enhances existing calibration methods.\n## Introduction\nThe intent is that the confidence score accurately describes whether the prediction is correct, regardless of the class. We show that this novel approach, which we call Top-versus-All (TvA), significantly improves the performance of standard calibration methods: Temperature and Vector Scaling [17], Dirichlet Calibration [29], Histogram Binning [69], Isotonic Regression [70], Beta Calibration [28], and Bayesian Binning into Quantiles [46]. We also introduce a simple regularization for Vector Scaling or Dirichlet Calibration that mitigates overfitting when the number of classes is high relative to the calibration data size. We conduct experiments on multiple image and text classification datasets and many pre-trained models. \n\nOur main contributions are the following: \n\n\u2022 We discuss four issues of the standard approach to confidence calibration. \n\n\u2022 To solve these issues, we develop the Top-versus-All approach to confidence calibration of multiclass classifiers, transforming the problem into a single binary classifier's calibration. This straightforward reformulation enables more efficient use of existing calibration methods, achieved with minimal modifications to the methods' original algorithms. \n\n\u2022 Applied to scaling methods for calibration (such as Temperature Scaling), TvA allows the use of the binary cross-entropy loss, which is more efficient in decreasing the confidence of wrong predictions and leads to stronger gradients in the case of Temperature Scaling. Applied to binary methods for calibration (such as Histogram Binning), TvA significantly improves their performance and makes them accuracy-preserving. \n\n\u2022 We demonstrate our approach's scalability and generality with extensive experiments on image classification with state-of-the-art models for complex datasets and on text classification with Pre-trained Language Models (PLMs) and Large Language Models (LLMs).",
            "reference_string": "[273821339 | LeCoz et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 41,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.11028, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1416534710",
                    "name": "P. Chhikara"
                }
            ],
            "abstract": "Large Language Models (LLMs) show remarkable proficiency in natural language tasks, yet their frequent overconfidence-misalignment between predicted confidence and true correctness-poses significant risks in critical decision-making applications. We present a comprehensive analysis on calibration in LLMs across nine LLMs and three factual Question-Answering (QA) datasets, systematically comparing standard free-generation settings against structured distractor-augmented prompts. Our evaluation reveals that explicitly incorporating distractors can substantially mitigate miscalibration, achieving relative accuracy improvements up to 460% and ECE reductions up to 90%. Despite general trends, we uncover nuanced findings: large RLHF-tuned models display inherent calibration strengths but can paradoxically suffer increased miscalibration on easier queries, whereas smaller models benefit disproportionately from distractor prompts but remain significantly miscalibrated. Through detailed analyses across question types, we identify persistent calibration failures, particularly in person-based queries. We conclude with concrete recommendations-targeted fine-tuning, structured prompting, and strategic model choice-to ensure reliable, trustworthy LLM deployments.",
            "corpus_id": 276408950,
            "sentences": [
                {
                    "corpus_id": "276408950",
                    "title": "Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models",
                    "text": "Large Language Models (LLMs) demonstrate impressive performance across diverse tasks, yet confidence calibration remains a challenge. Miscalibration - where models are overconfident or underconfident - poses risks, particularly in high-stakes applications. This paper presents an empirical study on LLM calibration, examining how model size, distractors, and question types affect confidence alignment. We introduce an evaluation framework to measure overconfidence and investigate whether multiple-choice formats mitigate or worsen miscalibration. Our findings show that while larger models (e.g., GPT-4o) are better calibrated overall, they are more prone to distraction, whereas smaller models benefit more from answer choices but struggle with uncertainty estimation. Unlike prior work, which primarily reports miscalibration trends, we provide actionable insights into failure modes and conditions that worsen overconfidence. These findings highlight the need for calibration-aware interventions and improved uncertainty estimation methods.",
                    "score": 0.45322880605223786,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.916015625
                }
            ],
            "relevance_judgement": 0.916015625,
            "relevance_judgment_input_expanded": "# Title: Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models\n# Venue: arXiv.org\n# Authors: P. Chhikara\n## Abstract\nLarge Language Models (LLMs) show remarkable proficiency in natural language tasks, yet their frequent overconfidence-misalignment between predicted confidence and true correctness-poses significant risks in critical decision-making applications. We present a comprehensive analysis on calibration in LLMs across nine LLMs and three factual Question-Answering (QA) datasets, systematically comparing standard free-generation settings against structured distractor-augmented prompts. Our evaluation reveals that explicitly incorporating distractors can substantially mitigate miscalibration, achieving relative accuracy improvements up to 460% and ECE reductions up to 90%. Despite general trends, we uncover nuanced findings: large RLHF-tuned models display inherent calibration strengths but can paradoxically suffer increased miscalibration on easier queries, whereas smaller models benefit disproportionately from distractor prompts but remain significantly miscalibrated. Through detailed analyses across question types, we identify persistent calibration failures, particularly in person-based queries. We conclude with concrete recommendations-targeted fine-tuning, structured prompting, and strategic model choice-to ensure reliable, trustworthy LLM deployments.\n",
            "reference_string": "[276408950 | Chhikara | 2025 | Citations: 3]"
        },
        {
            "title": "Calibrating Structured Output Predictors for Natural Language Processing",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2020,
            "reference_count": 34,
            "citation_count": 28,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.acl-main.188.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.04361, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3422909",
                    "name": "Abhyuday N. Jagannatha"
                },
                {
                    "authorId": "2119120474",
                    "name": "Hong Yu"
                }
            ],
            "abstract": "We address the problem of calibrating prediction confidence for output entities of interest in natural language processing (NLP) applications. It is important that NLP applications such as named entity recognition and question answering produce calibrated confidence scores for their predictions, especially if the applications are to be deployed in a safety-critical domain such as healthcare. However the output space of such structured prediction models are often too large to directly adapt binary or multi-class calibration methods. In this study, we propose a general calibration scheme for output entities of interest in neural network based structured prediction models. Our proposed method can be used with any binary class calibration scheme and a neural network model. Additionally, we show that our calibration method can also be used as an uncertainty-aware, entity-specific decoding step to improve the performance of the underlying model at no additional training cost or data requirements. We show that our method outperforms current calibration techniques for Named Entity Recognition, Part-of-speech tagging and Question Answering systems. We also observe an improvement in model performance from our decoding step across several tasks and benchmark datasets. Our method improves the calibration and model performance on out-of-domain test scenarios as well.",
            "corpus_id": 215548393,
            "sentences": [
                {
                    "corpus_id": "215548393",
                    "title": "Calibrating Structured Output Predictors for Natural Language Processing",
                    "text": "We address the problem of calibrating prediction confidence for output entities of interest in natural language processing (NLP) applications. It is important that NLP applications such as named entity recognition and question answering produce calibrated confidence scores for their predictions, especially if the applications are to be deployed in a safety-critical domain such as healthcare. However the output space of such structured prediction models are often too large to directly adapt binary or multi-class calibration methods. In this study, we propose a general calibration scheme for output entities of interest in neural network based structured prediction models. Our proposed method can be used with any binary class calibration scheme and a neural network model. Additionally, we show that our calibration method can also be used as an uncertainty-aware, entity-specific decoding step to improve the performance of the underlying model at no additional training cost or data requirements. We show that our method outperforms current calibration techniques for Named Entity Recognition, Part-of-speech tagging and Question Answering systems. We also observe an improvement in model performance from our decoding step across several tasks and benchmark datasets. Our method improves the calibration and model performance on out-of-domain test scenarios as well.",
                    "score": 0.46643136083894776,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9140625
                }
            ],
            "relevance_judgement": 0.9140625,
            "relevance_judgment_input_expanded": "# Title: Calibrating Structured Output Predictors for Natural Language Processing\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Abhyuday N. Jagannatha, Hong Yu\n## Abstract\nWe address the problem of calibrating prediction confidence for output entities of interest in natural language processing (NLP) applications. It is important that NLP applications such as named entity recognition and question answering produce calibrated confidence scores for their predictions, especially if the applications are to be deployed in a safety-critical domain such as healthcare. However the output space of such structured prediction models are often too large to directly adapt binary or multi-class calibration methods. In this study, we propose a general calibration scheme for output entities of interest in neural network based structured prediction models. Our proposed method can be used with any binary class calibration scheme and a neural network model. Additionally, we show that our calibration method can also be used as an uncertainty-aware, entity-specific decoding step to improve the performance of the underlying model at no additional training cost or data requirements. We show that our method outperforms current calibration techniques for Named Entity Recognition, Part-of-speech tagging and Question Answering systems. We also observe an improvement in model performance from our decoding step across several tasks and benchmark datasets. Our method improves the calibration and model performance on out-of-domain test scenarios as well.\n",
            "reference_string": "[215548393 | Jagannatha et al. | 2020 | Citations: 28]"
        },
        {
            "title": "Making Pre-trained Language Models both Task-solvers and Self-calibrators",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 62,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2307.11316",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.11316, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": null,
                    "name": "Yangyi Chen"
                },
                {
                    "authorId": "2144803999",
                    "name": "Xingyao Wang"
                },
                {
                    "authorId": "2072975661",
                    "name": "Heng Ji"
                }
            ],
            "abstract": "Pre-trained language models (PLMs) serve as backbones for various real-world systems. For high-stake applications, it's equally essential to have reasonable confidence estimations in predictions. While the vanilla confidence scores of PLMs can already be effectively utilized, PLMs consistently become overconfident in their wrong predictions, which is not desirable in practice. Previous work shows that introducing an extra calibration task can mitigate this issue. The basic idea involves acquiring additional data to train models in predicting the confidence of their initial predictions. However, it only demonstrates the feasibility of this kind of method, assuming that there are abundant extra available samples for the introduced calibration task. In this work, we consider the practical scenario that we need to effectively utilize training samples to make PLMs both task-solvers and self-calibrators. Three challenges are presented, including limited training samples, data imbalance, and distribution shifts. We first conduct pilot experiments to quantify various decisive factors in the calibration task. Based on the empirical analysis results, we propose a training algorithm LM-TOAST to tackle the challenges. Experimental results show that LM-TOAST can effectively utilize the training data to make PLMs have reasonable confidence estimations while maintaining the original task performance. Further, we consider three downstream applications, namely selective classification, adversarial defense, and model cascading, to show the practical usefulness of LM-TOAST. The code will be made public at \\url{https://github.com/Yangyi-Chen/LM-TOAST}.",
            "corpus_id": 259859030,
            "sentences": [
                {
                    "corpus_id": "259859030",
                    "title": "Making Pre-trained Language Models both Task-solvers and Self-calibrators",
                    "text": "Calibration methods. Typically, calibration methods rely on human intuitions or posterior adjustments to make the confidence estimations more accurate. Data augmentation (Hendrycks et al., 2020b;Wang et al., 2021) and model ensemble (Gal and Ghahramani, 2016;Lakshminarayanan et al., 2017) have been empirically proven to be successful in computer vision. However, they cannot bring the same benefits in NLP according to the empirical study in Chen et al. (2022b). So we don't consider them as baseline methods for comparison in this work, and two empirically effective methods are adopted. Temperature scaling (Platt et al., 1999;Guo et al., 2017) readjusts the output logits in a posterior way according to the calibration performance on a held-out set. Label smoothing (Szegedy et al., 2016) imposes a confidence regularization during the training process, discouraging models from being overconfident in their predictions.\n\nRecently, there is an emergent trend in NLP, motivating to directly collect data for training models to have reasonable confidence estimations. Kadavath et al. (2022) assume that the last hidden states of PLMs contain the uncertainty information, and directly apply a multi-layer perceptron on them to perform confidence estimations. Lin et al. (2022) also show that PLMs can be directly trained to give their confidence estimations by words. These two methods are proven to be successful in significantly reducing the overconfidence issue and are further extended to exploit the potential in this kind of methods (Chen et al., 2022b). Existing work demonstrates the feasibility and potential of this kind of method in ideal experimental settings that enough training data is given for the calibration task. We further consider the practical setting and propose an effective method in this work.\n\nApplications. The confidence scores have been widely utilized for various applications. A bunch of active learning methods relies on models' confidence to select the most informative samples to annotate (Zhang et al., 2022;Schr\u00f6der et al., 2022). Models' confidence can also be directly utilized for OOD and misclassification detection (Hendrycks and Gimp",
                    "score": 0.49410417568216125,
                    "section_title": "Related Work",
                    "char_start_offset": 22642,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 170,
                            "end": 195,
                            "matchedPaperCorpusId": "208637407"
                        },
                        {
                            "start": 195,
                            "end": 213,
                            "matchedPaperCorpusId": "239885364"
                        },
                        {
                            "start": 259,
                            "end": 289,
                            "matchedPaperCorpusId": "6294674"
                        },
                        {
                            "start": 611,
                            "end": 631,
                            "matchedPaperCorpusId": "56563878"
                        },
                        {
                            "start": 772,
                            "end": 794,
                            "matchedPaperCorpusId": "206593880"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9130859375
                }
            ],
            "relevance_judgement": 0.9130859375,
            "relevance_judgment_input_expanded": "# Title: Making Pre-trained Language Models both Task-solvers and Self-calibrators\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Yangyi Chen, Xingyao Wang, Heng Ji\n## Abstract\nPre-trained language models (PLMs) serve as backbones for various real-world systems. For high-stake applications, it's equally essential to have reasonable confidence estimations in predictions. While the vanilla confidence scores of PLMs can already be effectively utilized, PLMs consistently become overconfident in their wrong predictions, which is not desirable in practice. Previous work shows that introducing an extra calibration task can mitigate this issue. The basic idea involves acquiring additional data to train models in predicting the confidence of their initial predictions. However, it only demonstrates the feasibility of this kind of method, assuming that there are abundant extra available samples for the introduced calibration task. In this work, we consider the practical scenario that we need to effectively utilize training samples to make PLMs both task-solvers and self-calibrators. Three challenges are presented, including limited training samples, data imbalance, and distribution shifts. We first conduct pilot experiments to quantify various decisive factors in the calibration task. Based on the empirical analysis results, we propose a training algorithm LM-TOAST to tackle the challenges. Experimental results show that LM-TOAST can effectively utilize the training data to make PLMs have reasonable confidence estimations while maintaining the original task performance. Further, we consider three downstream applications, namely selective classification, adversarial defense, and model cascading, to show the practical usefulness of LM-TOAST. The code will be made public at \\url{https://github.com/Yangyi-Chen/LM-TOAST}.\n## Related Work\nCalibration methods. Typically, calibration methods rely on human intuitions or posterior adjustments to make the confidence estimations more accurate. Data augmentation (Hendrycks et al., 2020b;Wang et al., 2021) and model ensemble (Gal and Ghahramani, 2016;Lakshminarayanan et al., 2017) have been empirically proven to be successful in computer vision. However, they cannot bring the same benefits in NLP according to the empirical study in Chen et al. (2022b). So we don't consider them as baseline methods for comparison in this work, and two empirically effective methods are adopted. Temperature scaling (Platt et al., 1999;Guo et al., 2017) readjusts the output logits in a posterior way according to the calibration performance on a held-out set. Label smoothing (Szegedy et al., 2016) imposes a confidence regularization during the training process, discouraging models from being overconfident in their predictions.\n\nRecently, there is an emergent trend in NLP, motivating to directly collect data for training models to have reasonable confidence estimations. Kadavath et al. (2022) assume that the last hidden states of PLMs contain the uncertainty information, and directly apply a multi-layer perceptron on them to perform confidence estimations. Lin et al. (2022) also show that PLMs can be directly trained to give their confidence estimations by words. These two methods are proven to be successful in significantly reducing the overconfidence issue and are further extended to exploit the potential in this kind of methods (Chen et al., 2022b). Existing work demonstrates the feasibility and potential of this kind of method in ideal experimental settings that enough training data is given for the calibration task. We further consider the practical setting and propose an effective method in this work.\n\nApplications. The confidence scores have been widely utilized for various applications. A bunch of active learning methods relies on models' confidence to select the most informative samples to annotate (Zhang et al., 2022;Schr\u00f6der et al., 2022). Models' confidence can also be directly utilized for OOD and misclassification detection (Hendrycks and Gimp",
            "reference_string": "[259859030 | Chen et al. | 2023 | Citations: 0]"
        },
        {
            "title": "Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 47,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.17124, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1500662261",
                    "name": "Xinran Zhao"
                },
                {
                    "authorId": "2279761345",
                    "name": "Hongming Zhang"
                },
                {
                    "authorId": "2243367575",
                    "name": "Xiaoman Pan"
                },
                {
                    "authorId": "2087264100",
                    "name": "Wenlin Yao"
                },
                {
                    "authorId": "2256336899",
                    "name": "Dong Yu"
                },
                {
                    "authorId": "2287820683",
                    "name": "Tongshuang Wu"
                },
                {
                    "authorId": "2108276402",
                    "name": "Jianshu Chen"
                }
            ],
            "abstract": "For a LLM to be trustworthy, its confidence level should be well-calibrated with its actual performance. While it is now common sense that LLM performances are greatly impacted by prompts, the confidence calibration in prompting LLMs has yet to be thoroughly explored. In this paper, we explore how different prompting strategies influence LLM confidence calibration and how it could be improved. We conduct extensive experiments on six prompting methods in the question-answering context and we observe that, while these methods help improve the expected LLM calibration, they also trigger LLMs to be over-confident when responding to some instances. Inspired by human cognition, we propose Fact-and-Reflection (FaR) prompting, which improves the LLM calibration in two steps. First, FaR elicits the known\"facts\"that are relevant to the input prompt from the LLM. And then it asks the model to\"reflect\"over them to generate the final answer. Experiments show that FaR prompting achieves significantly better calibration; it lowers the Expected Calibration Error by 23.5% on our multi-purpose QA tasks. Notably, FaR prompting even elicits the capability of verbally expressing concerns in less confident scenarios, which helps trigger retrieval augmentation for solving these harder instances.",
            "corpus_id": 268032879,
            "sentences": [
                {
                    "corpus_id": "268032879",
                    "title": "Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models",
                    "text": "Prompting Large Language Models. Recent research (Brown et al., 2020;Kojima et al., 2023) on large language models shows that in-context learning (ICL) achieves great effectiveness in using models as few-shot or zero-shot reasoners. Different styles of prompting such as Knowledge prompting (Liu et al., 2022), Chain of Thought (CoT) prompting (Wei et al., 2022b), Self-Consistency prompting (Wang et al., 2023), Self-ask (Press et al., 2023), Tree-of-Thought prompting (Yao et al., 2023), andSkill-in-Context (Chen et al., 2023) are then proposed to guide the model to elicit its knowledge for reasoning in different ways. \n\nMost previous work mainly focuses on how such a prompting method influences the model performance on various tasks. In this paper, we compare how confidence calibration is influenced by different prompting methods. \n\nConfidence Calibration of LLMs. Extracting honest and constructive confidence scores of large language models is considered an important step towards building faithful and trustworthy AI systems (Desai and Durrett, 2020;Si et al., 2023). Many methods have been proposed recently to get reliable confidence scores with different suffix prompts added after outputting possible answers, such as a follow of True or False multiple choice question (Kadavath et al., 2022), asking models to describe the likelihood of the answer (Lin et al., 2022), and describing the likelihood that the answer is correct with demonstrations (Tian et al., 2023). However, it remains unclear how robust the methods are and how good they are comparatively. Our paper proposes FaR prompting as an orthogonal method to improve calibration and compare different extraction methods with our test bed. \n\nRecently, Yang et al. (2023) discuss the honesty problem of models as part of the alignment. Qian et al. ( 2023) study the confidence change when there is a conflict between in-context and model internal knowledge. Another line of work links model confidence with human confidence (Zhou et al., 2023;Steyvers et al., 2024;Zhou et al., 2024).",
                    "score": 0.4307074566937875,
                    "section_title": "Related Work",
                    "char_start_offset": 23915,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 32
                        },
                        {
                            "start": 33,
                            "end": 232
                        },
                        {
                            "start": 233,
                            "end": 623
                        },
                        {
                            "start": 626,
                            "end": 741
                        },
                        {
                            "start": 742,
                            "end": 840
                        },
                        {
                            "start": 843,
                            "end": 874
                        },
                        {
                            "start": 875,
                            "end": 1080
                        },
                        {
                            "start": 1081,
                            "end": 1483
                        },
                        {
                            "start": 1484,
                            "end": 1575
                        },
                        {
                            "start": 1576,
                            "end": 1715
                        },
                        {
                            "start": 1718,
                            "end": 1810
                        },
                        {
                            "start": 1811,
                            "end": 1932
                        },
                        {
                            "start": 1933,
                            "end": 2059
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 49,
                            "end": 69,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 291,
                            "end": 309,
                            "matchedPaperCorpusId": "239016123"
                        },
                        {
                            "start": 344,
                            "end": 363,
                            "matchedPaperCorpusId": "246411621"
                        },
                        {
                            "start": 1038,
                            "end": 1063,
                            "matchedPaperCorpusId": "212747810"
                        },
                        {
                            "start": 1063,
                            "end": 1079,
                            "matchedPaperCorpusId": "252917981"
                        },
                        {
                            "start": 1366,
                            "end": 1384,
                            "matchedPaperCorpusId": "249191391"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91015625
                }
            ],
            "relevance_judgement": 0.91015625,
            "relevance_judgment_input_expanded": "# Title: Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Xinran Zhao, Hongming Zhang, Xiaoman Pan, Wenlin Yao, Dong Yu, Tongshuang Wu, Jianshu Chen\n## Abstract\nFor a LLM to be trustworthy, its confidence level should be well-calibrated with its actual performance. While it is now common sense that LLM performances are greatly impacted by prompts, the confidence calibration in prompting LLMs has yet to be thoroughly explored. In this paper, we explore how different prompting strategies influence LLM confidence calibration and how it could be improved. We conduct extensive experiments on six prompting methods in the question-answering context and we observe that, while these methods help improve the expected LLM calibration, they also trigger LLMs to be over-confident when responding to some instances. Inspired by human cognition, we propose Fact-and-Reflection (FaR) prompting, which improves the LLM calibration in two steps. First, FaR elicits the known\"facts\"that are relevant to the input prompt from the LLM. And then it asks the model to\"reflect\"over them to generate the final answer. Experiments show that FaR prompting achieves significantly better calibration; it lowers the Expected Calibration Error by 23.5% on our multi-purpose QA tasks. Notably, FaR prompting even elicits the capability of verbally expressing concerns in less confident scenarios, which helps trigger retrieval augmentation for solving these harder instances.\n## Related Work\nPrompting Large Language Models. Recent research (Brown et al., 2020;Kojima et al., 2023) on large language models shows that in-context learning (ICL) achieves great effectiveness in using models as few-shot or zero-shot reasoners. Different styles of prompting such as Knowledge prompting (Liu et al., 2022), Chain of Thought (CoT) prompting (Wei et al., 2022b), Self-Consistency prompting (Wang et al., 2023), Self-ask (Press et al., 2023), Tree-of-Thought prompting (Yao et al., 2023), andSkill-in-Context (Chen et al., 2023) are then proposed to guide the model to elicit its knowledge for reasoning in different ways. \n\nMost previous work mainly focuses on how such a prompting method influences the model performance on various tasks. In this paper, we compare how confidence calibration is influenced by different prompting methods. \n\nConfidence Calibration of LLMs. Extracting honest and constructive confidence scores of large language models is considered an important step towards building faithful and trustworthy AI systems (Desai and Durrett, 2020;Si et al., 2023). Many methods have been proposed recently to get reliable confidence scores with different suffix prompts added after outputting possible answers, such as a follow of True or False multiple choice question (Kadavath et al., 2022), asking models to describe the likelihood of the answer (Lin et al., 2022), and describing the likelihood that the answer is correct with demonstrations (Tian et al., 2023). However, it remains unclear how robust the methods are and how good they are comparatively. Our paper proposes FaR prompting as an orthogonal method to improve calibration and compare different extraction methods with our test bed. \n\nRecently, Yang et al. (2023) discuss the honesty problem of models as part of the alignment. Qian et al. ( 2023) study the confidence change when there is a conflict between in-context and model internal knowledge. Another line of work links model confidence with human confidence (Zhou et al., 2023;Steyvers et al., 2024;Zhou et al., 2024).",
            "reference_string": "[268032879 | Zhao et al. | 2024 | Citations: 7]"
        },
        {
            "title": "Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 52,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.09127, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2297329510",
                    "name": "Ruixin Yang"
                },
                {
                    "authorId": "1801149",
                    "name": "Dheeraj Rajagopal"
                },
                {
                    "authorId": "2281641852",
                    "name": "S. Hayati"
                },
                {
                    "authorId": "2296769037",
                    "name": "Bin Hu"
                },
                {
                    "authorId": "48493368",
                    "name": "Dongyeop Kang"
                }
            ],
            "abstract": "Uncertainty estimation is a significant issue for current large language models (LLMs) that are generally poorly calibrated and over-confident, especially with reinforcement learning from human feedback (RLHF). Unlike humans, whose decisions and confidences not only stem from intrinsic beliefs but can also be adjusted through daily observations, existing calibration methods for LLMs focus on estimating or eliciting individual confidence without taking full advantage of the\"Collective Wisdom\": the interaction among multiple LLMs that can collectively improve both accuracy and calibration. In this work, we propose Collaborative Calibration, a post-hoc training-free calibration strategy that leverages the collaborative and expressive capabilities of multiple tool-augmented LLM agents in a simulated group deliberation process. We demonstrate the effectiveness of Collaborative Calibration on generative QA tasks across various domains, showing its potential in harnessing the rationalization of collectively calibrated confidence assessments and improving the reliability of model predictions.",
            "corpus_id": 269149545,
            "sentences": [
                {
                    "corpus_id": "269149545",
                    "title": "Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation",
                    "text": "Uncertainty estimation is a significant issue for current large language models (LLMs) that are generally poorly calibrated and over-confident, especially with reinforcement learning from human feedback (RLHF). Unlike humans, whose decisions and confidences not only stem from intrinsic beliefs but can also be adjusted through daily observations, existing calibration methods for LLMs focus on estimating or eliciting individual confidence without taking full advantage of the\"Collective Wisdom\": the interaction among multiple LLMs that can collectively improve both accuracy and calibration. In this work, we propose Collaborative Calibration, a post-hoc training-free calibration strategy that leverages the collaborative and expressive capabilities of multiple tool-augmented LLM agents in a simulated group deliberation process. We demonstrate the effectiveness of Collaborative Calibration on generative QA tasks across various domains, showing its potential in harnessing the rationalization of collectively calibrated confidence assessments and improving the reliability of model predictions.",
                    "score": 0.38920239527821626,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90673828125
                }
            ],
            "relevance_judgement": 0.90673828125,
            "relevance_judgment_input_expanded": "# Title: Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation\n# Venue: arXiv.org\n# Authors: Ruixin Yang, Dheeraj Rajagopal, S. Hayati, Bin Hu, Dongyeop Kang\n## Abstract\nUncertainty estimation is a significant issue for current large language models (LLMs) that are generally poorly calibrated and over-confident, especially with reinforcement learning from human feedback (RLHF). Unlike humans, whose decisions and confidences not only stem from intrinsic beliefs but can also be adjusted through daily observations, existing calibration methods for LLMs focus on estimating or eliciting individual confidence without taking full advantage of the\"Collective Wisdom\": the interaction among multiple LLMs that can collectively improve both accuracy and calibration. In this work, we propose Collaborative Calibration, a post-hoc training-free calibration strategy that leverages the collaborative and expressive capabilities of multiple tool-augmented LLM agents in a simulated group deliberation process. We demonstrate the effectiveness of Collaborative Calibration on generative QA tasks across various domains, showing its potential in harnessing the rationalization of collectively calibrated confidence assessments and improving the reliability of model predictions.\n",
            "reference_string": "[269149545 | Yang et al. | 2024 | Citations: 7]"
        },
        {
            "title": "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 26,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.03225, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319873250",
                    "name": "Jeremy Qin"
                },
                {
                    "authorId": "2320157307",
                    "name": "Bang Liu"
                },
                {
                    "authorId": "2319837437",
                    "name": "Quoc Dinh Nguyen"
                }
            ],
            "abstract": "Black-box large language models (LLMs) are increasingly deployed in various environments, making it essential for these models to effectively convey their confidence and uncertainty, especially in high-stakes settings. However, these models often exhibit overconfidence, leading to potential risks and misjudgments. Existing techniques for eliciting and calibrating LLM confidence have primarily focused on general reasoning datasets, yielding only modest improvements. Accurate calibration is crucial for informed decision-making and preventing adverse outcomes but remains challenging due to the complexity and variability of tasks these models perform. In this work, we investigate the miscalibration behavior of black-box LLMs within the healthcare setting. We propose a novel method, \\textit{Atypical Presentations Recalibration}, which leverages atypical presentations to adjust the model's confidence estimates. Our approach significantly improves calibration, reducing calibration errors by approximately 60\\% on three medical question answering datasets and outperforming existing methods such as vanilla verbalized confidence, CoT verbalized confidence and others. Additionally, we provide an in-depth analysis of the role of atypicality within the recalibration framework.",
            "corpus_id": 272423997,
            "sentences": [
                {
                    "corpus_id": "272423997",
                    "title": "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration",
                    "text": "Black-box large language models (LLMs) are increasingly deployed in various environments, making it essential for these models to effectively convey their confidence and uncertainty, especially in high-stakes settings. However, these models often exhibit overconfidence, leading to potential risks and misjudgments. Existing techniques for eliciting and calibrating LLM confidence have primarily focused on general reasoning datasets, yielding only modest improvements. Accurate calibration is crucial for informed decision-making and preventing adverse outcomes but remains challenging due to the complexity and variability of tasks these models perform. In this work, we investigate the miscalibration behavior of black-box LLMs within the healthcare setting. We propose a novel method, \\textit{Atypical Presentations Recalibration}, which leverages atypical presentations to adjust the model's confidence estimates. Our approach significantly improves calibration, reducing calibration errors by approximately 60\\% on three medical question answering datasets and outperforming existing methods such as vanilla verbalized confidence, CoT verbalized confidence and others. Additionally, we provide an in-depth analysis of the role of atypicality within the recalibration framework.",
                    "score": 0.4403055964948557,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89599609375
                }
            ],
            "relevance_judgement": 0.89599609375,
            "relevance_judgment_input_expanded": "# Title: Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Jeremy Qin, Bang Liu, Quoc Dinh Nguyen\n## Abstract\nBlack-box large language models (LLMs) are increasingly deployed in various environments, making it essential for these models to effectively convey their confidence and uncertainty, especially in high-stakes settings. However, these models often exhibit overconfidence, leading to potential risks and misjudgments. Existing techniques for eliciting and calibrating LLM confidence have primarily focused on general reasoning datasets, yielding only modest improvements. Accurate calibration is crucial for informed decision-making and preventing adverse outcomes but remains challenging due to the complexity and variability of tasks these models perform. In this work, we investigate the miscalibration behavior of black-box LLMs within the healthcare setting. We propose a novel method, \\textit{Atypical Presentations Recalibration}, which leverages atypical presentations to adjust the model's confidence estimates. Our approach significantly improves calibration, reducing calibration errors by approximately 60\\% on three medical question answering datasets and outperforming existing methods such as vanilla verbalized confidence, CoT verbalized confidence and others. Additionally, we provide an in-depth analysis of the role of atypicality within the recalibration framework.\n",
            "reference_string": "[272423997 | Qin et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 30,
            "citation_count": 355,
            "influential_citation_count": 57,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.14975",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14975, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2218407338",
                    "name": "Katherine Tian"
                },
                {
                    "authorId": "49688913",
                    "name": "E. Mitchell"
                },
                {
                    "authorId": "2064472884",
                    "name": "Allan Zhou"
                },
                {
                    "authorId": "50465276",
                    "name": "Archit Sharma"
                },
                {
                    "authorId": "102801230",
                    "name": "Rafael Rafailov"
                },
                {
                    "authorId": "18307037",
                    "name": "Huaxiu Yao"
                },
                {
                    "authorId": "46881670",
                    "name": "Chelsea Finn"
                },
                {
                    "authorId": "144783904",
                    "name": "Christopher D. Manning"
                }
            ],
            "abstract": "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%.",
            "corpus_id": 258865733,
            "sentences": [
                {
                    "corpus_id": "258865733",
                    "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
                    "text": "Real-world prediction systems invariably make errors. However, some mitigation of these errors is possible if the system produces well-calibrated 1 confidence estimates. In this case, the system's least confident predictions correspond to those that are most likely to be incorrect, potentially allowing these predictions to be skipped or overridden by a human. In the context of language models, one consequence of poor calibration may be hallucination, where a language model confidently asserts incorrect facts or reasoning. While the ability of very large LMs to absorb and synthesize knowledge about the outside world has gained significant Figure 1: Verbalized confidence scores (blue) are better-calibrated than log probabilities (orange) for gpt-3.5-turbo. Raw model probabilities (top-left) are consistently over-confident. Verbalized numerical probabilities (bottom) are better-calibrated. Considering more answer choices (bottom-right) further improves verbalized calibration (as in 'Considering the Opposite' in psychology; Lord et al. (1985)). Verbalized expressions of likelihood (top-right) also provide improved calibration. Bar height is average accuracy of predictions in bin. Darker bars mean more predictions fall in that confidence range. Results computed on SciQ. \n\nattention (Brown et al., 2020;Roberts et al., 2020;Bubeck et al., 2023), relatively little attention has been given to their well-calibratedness (Kadavath et al., 2022). Further, most existing analyses of the calibratedness of LLMs focus on models trained with maximum likelihood, while in practice, the most widely-used LLMs (such as ChatGPT) are fine-tuned using methods such as reinforcement learning from human feedback (Christiano et al., 2017). Some findings suggest that RLHF-LMs may sacrifice well-calibrated predictions for the sake of closer adherence to user instructions in dialogue (Kadavath et al., 2022;OpenAI, 2023), as the reinforcement learning objective encourages the model to allocate probability mass to the most preferred answer(s), rather than matching the relative frequency of possible answers.",
                    "score": 0.4257132300610431,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 53
                        },
                        {
                            "start": 54,
                            "end": 169
                        },
                        {
                            "start": 170,
                            "end": 361
                        },
                        {
                            "start": 362,
                            "end": 527
                        },
                        {
                            "start": 528,
                            "end": 764
                        },
                        {
                            "start": 765,
                            "end": 832
                        },
                        {
                            "start": 833,
                            "end": 899
                        },
                        {
                            "start": 900,
                            "end": 1056
                        },
                        {
                            "start": 1057,
                            "end": 1140
                        },
                        {
                            "start": 1141,
                            "end": 1194
                        },
                        {
                            "start": 1195,
                            "end": 1259
                        },
                        {
                            "start": 1260,
                            "end": 1285
                        },
                        {
                            "start": 1288,
                            "end": 1457
                        },
                        {
                            "start": 1458,
                            "end": 1738
                        },
                        {
                            "start": 1739,
                            "end": 2108
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1036,
                            "end": 1054,
                            "matchedPaperCorpusId": "24047702"
                        },
                        {
                            "start": 1298,
                            "end": 1318,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1318,
                            "end": 1339,
                            "matchedPaperCorpusId": "211205183"
                        },
                        {
                            "start": 1712,
                            "end": 1737,
                            "matchedPaperCorpusId": "4787508"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89501953125
                },
                {
                    "corpus_id": "258865733",
                    "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
                    "text": "We show that prompting a model to produce several answer choices before giving its confidence scores significantly improves calibration of verbalized probabilities. Combined with temperature scaling (Guo et al., 2017), this approach generally provides better calibration than model probabilities for ChatGPT2 , GPT-43 , and Claude 24 across three datasets, often reducing expected calibration error (ECE) by over 50%. Related Work. Several studies have examined the calibration of large LMs (Lin et al., 2022a;Park and Caragea, 2022;Kadavath et al., 2022;Xiao et al., 2022;Kuhn et al., 2023), finding that combining large pre-trained LMs with temperature scaling (Guo et al., 2017) produces very well-calibrated predictions (Kadavath et al., 2022;Xiao et al., 2022;Kuhn et al., 2023). Other work focuses on the tendency of language and dialogue models to use linguistic expressions of uncertainty in a well-calibrated manner (Zhou et al., 2023;Mielke et al., 2022). However, existing studies focus on LMs trained purely with unsupervised learning (although Kadavath et al. (2022) briefly examine RLHF-LMs), while widely used models in practice are fine-tuned with instruction-tuning or RLHF (Christiano et al., 2017). RLHF has been shown to effectively leverage annotations of human preferences to control sentiment (Ziegler et al., 2020), improve summarization or instruction-following quality (Stiennon et al., 2022;Ouyang et al., 2022), and inject behavioral priors of harmlessness (Bai et al., 2022b,a). However, recent work has raised the question of whether or not RLHF harms calibration (OpenAI, 2023).",
                    "score": 0.41523850837678006,
                    "section_title": "Introduction",
                    "char_start_offset": 3532,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 164
                        },
                        {
                            "start": 165,
                            "end": 417
                        },
                        {
                            "start": 418,
                            "end": 431
                        },
                        {
                            "start": 432,
                            "end": 784
                        },
                        {
                            "start": 785,
                            "end": 965
                        },
                        {
                            "start": 966,
                            "end": 1217
                        },
                        {
                            "start": 1218,
                            "end": 1507
                        },
                        {
                            "start": 1508,
                            "end": 1609
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 199,
                            "end": 217,
                            "matchedPaperCorpusId": "28671436"
                        },
                        {
                            "start": 491,
                            "end": 510,
                            "matchedPaperCorpusId": "249191391"
                        },
                        {
                            "start": 510,
                            "end": 533,
                            "matchedPaperCorpusId": "247450599"
                        },
                        {
                            "start": 555,
                            "end": 573,
                            "matchedPaperCorpusId": "247613322"
                        },
                        {
                            "start": 573,
                            "end": 591,
                            "matchedPaperCorpusId": "257039062"
                        },
                        {
                            "start": 663,
                            "end": 681,
                            "matchedPaperCorpusId": "28671436"
                        },
                        {
                            "start": 747,
                            "end": 765,
                            "matchedPaperCorpusId": "247613322"
                        },
                        {
                            "start": 765,
                            "end": 783,
                            "matchedPaperCorpusId": "257039062"
                        },
                        {
                            "start": 944,
                            "end": 964,
                            "matchedPaperCorpusId": "250073258"
                        },
                        {
                            "start": 1191,
                            "end": 1216,
                            "matchedPaperCorpusId": "4787508"
                        },
                        {
                            "start": 1316,
                            "end": 1338,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1395,
                            "end": 1418,
                            "matchedPaperCorpusId": "232335764"
                        },
                        {
                            "start": 1418,
                            "end": 1438,
                            "matchedPaperCorpusId": "246426909"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89404296875
                },
                {
                    "corpus_id": "258865733",
                    "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
                    "text": "Tables 1-5 show the results of evaluating various methods for extracting confidence from RLHF-LMs on gpt-3.5-turbo, gpt-4, claude-1, claude-2, and Llama-2-70b-chat, respectively. We distill several key conclusions from these experiments. 1. Large RLHF-LMs can often directly verbalize better-calibrated confidences (either a numerical confidence probability or an expression such as 'highly likely') than the models' conditional probabilities. 2. Among the methods for verbalizing probabilities directly, we observe that generating and evaluating multiple hypotheses improves calibration (see Figure 1), similarly to humans (Lord et al., 1985), and corroborating a similar finding in LMs (Kadavath et al., 2022). \n\n3. Language models can express their uncertainty with numerical probabilities as well or better than with words, which is surprising in light of longstanding difficulties in representing numbers in language models (Thawani et al., 2021). 4. Chainof-thought prompting does not improve verbalized calibration (see Appendix Figure 5 for additional CoT results). 5. The calibration of both Claude models' conditional probabilities roughly falls between gpt-3.5-turbo and gpt-4; however, while Claude 1 is much weaker at verbalizing its confidence, Claude 2 is generally a bit stronger than gpt-3.5-turbo at verbalizing. The verbal calibration of the open source model Llama-2-70b-chat is generally weaker than that of closed source models but still demonstrates improvement over its conditional probabilities by some metrics, and does so most clearly on TruthfulQA.",
                    "score": 0.3960769802229755,
                    "section_title": "Results",
                    "char_start_offset": 11595,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 115
                        },
                        {
                            "start": 116,
                            "end": 178
                        },
                        {
                            "start": 179,
                            "end": 237
                        },
                        {
                            "start": 238,
                            "end": 240
                        },
                        {
                            "start": 241,
                            "end": 443
                        },
                        {
                            "start": 444,
                            "end": 712
                        },
                        {
                            "start": 715,
                            "end": 952
                        },
                        {
                            "start": 953,
                            "end": 1073
                        },
                        {
                            "start": 1074,
                            "end": 1076
                        },
                        {
                            "start": 1077,
                            "end": 1314
                        },
                        {
                            "start": 1315,
                            "end": 1330
                        },
                        {
                            "start": 1331,
                            "end": 1576
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 624,
                            "end": 643,
                            "matchedPaperCorpusId": "24047702"
                        },
                        {
                            "start": 929,
                            "end": 951,
                            "matchedPaperCorpusId": "232335764"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86328125
                }
            ],
            "relevance_judgement": 0.89501953125,
            "relevance_judgment_input_expanded": "# Title: Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Katherine Tian, E. Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, Christopher D. Manning\n## Abstract\nA trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%.\n## Introduction\nReal-world prediction systems invariably make errors. However, some mitigation of these errors is possible if the system produces well-calibrated 1 confidence estimates. In this case, the system's least confident predictions correspond to those that are most likely to be incorrect, potentially allowing these predictions to be skipped or overridden by a human. In the context of language models, one consequence of poor calibration may be hallucination, where a language model confidently asserts incorrect facts or reasoning. While the ability of very large LMs to absorb and synthesize knowledge about the outside world has gained significant Figure 1: Verbalized confidence scores (blue) are better-calibrated than log probabilities (orange) for gpt-3.5-turbo. Raw model probabilities (top-left) are consistently over-confident. Verbalized numerical probabilities (bottom) are better-calibrated. Considering more answer choices (bottom-right) further improves verbalized calibration (as in 'Considering the Opposite' in psychology; Lord et al. (1985)). Verbalized expressions of likelihood (top-right) also provide improved calibration. Bar height is average accuracy of predictions in bin. Darker bars mean more predictions fall in that confidence range. Results computed on SciQ. \n\nattention (Brown et al., 2020;Roberts et al., 2020;Bubeck et al., 2023), relatively little attention has been given to their well-calibratedness (Kadavath et al., 2022). Further, most existing analyses of the calibratedness of LLMs focus on models trained with maximum likelihood, while in practice, the most widely-used LLMs (such as ChatGPT) are fine-tuned using methods such as reinforcement learning from human feedback (Christiano et al., 2017). Some findings suggest that RLHF-LMs may sacrifice well-calibrated predictions for the sake of closer adherence to user instructions in dialogue (Kadavath et al., 2022;OpenAI, 2023), as the reinforcement learning objective encourages the model to allocate probability mass to the most preferred answer(s), rather than matching the relative frequency of possible answers.\n...\nWe show that prompting a model to produce several answer choices before giving its confidence scores significantly improves calibration of verbalized probabilities. Combined with temperature scaling (Guo et al., 2017), this approach generally provides better calibration than model probabilities for ChatGPT2 , GPT-43 , and Claude 24 across three datasets, often reducing expected calibration error (ECE) by over 50%. Related Work. Several studies have examined the calibration of large LMs (Lin et al., 2022a;Park and Caragea, 2022;Kadavath et al., 2022;Xiao et al., 2022;Kuhn et al., 2023), finding that combining large pre-trained LMs with temperature scaling (Guo et al., 2017) produces very well-calibrated predictions (Kadavath et al., 2022;Xiao et al., 2022;Kuhn et al., 2023). Other work focuses on the tendency of language and dialogue models to use linguistic expressions of uncertainty in a well-calibrated manner (Zhou et al., 2023;Mielke et al., 2022). However, existing studies focus on LMs trained purely with unsupervised learning (although Kadavath et al. (2022) briefly examine RLHF-LMs), while widely used models in practice are fine-tuned with instruction-tuning or RLHF (Christiano et al., 2017). RLHF has been shown to effectively leverage annotations of human preferences to control sentiment (Ziegler et al., 2020), improve summarization or instruction-following quality (Stiennon et al., 2022;Ouyang et al., 2022), and inject behavioral priors of harmlessness (Bai et al., 2022b,a). However, recent work has raised the question of whether or not RLHF harms calibration (OpenAI, 2023).\n\n## Results\nTables 1-5 show the results of evaluating various methods for extracting confidence from RLHF-LMs on gpt-3.5-turbo, gpt-4, claude-1, claude-2, and Llama-2-70b-chat, respectively. We distill several key conclusions from these experiments. 1. Large RLHF-LMs can often directly verbalize better-calibrated confidences (either a numerical confidence probability or an expression such as 'highly likely') than the models' conditional probabilities. 2. Among the methods for verbalizing probabilities directly, we observe that generating and evaluating multiple hypotheses improves calibration (see Figure 1), similarly to humans (Lord et al., 1985), and corroborating a similar finding in LMs (Kadavath et al., 2022). \n\n3. Language models can express their uncertainty with numerical probabilities as well or better than with words, which is surprising in light of longstanding difficulties in representing numbers in language models (Thawani et al., 2021). 4. Chainof-thought prompting does not improve verbalized calibration (see Appendix Figure 5 for additional CoT results). 5. The calibration of both Claude models' conditional probabilities roughly falls between gpt-3.5-turbo and gpt-4; however, while Claude 1 is much weaker at verbalizing its confidence, Claude 2 is generally a bit stronger than gpt-3.5-turbo at verbalizing. The verbal calibration of the open source model Llama-2-70b-chat is generally weaker than that of closed source models but still demonstrates improvement over its conditional probabilities by some metrics, and does so most clearly on TruthfulQA.",
            "reference_string": "[258865733 | Tian et al. | 2023 | Citations: 355]"
        },
        {
            "title": "Calibration Meets Explanation: A Simple and Effective Approach for Model Confidence Estimates",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 42,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2211.03041",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.03041, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1664667501",
                    "name": "Dongfang Li"
                },
                {
                    "authorId": "33968873",
                    "name": "Baotian Hu"
                },
                {
                    "authorId": "144159781",
                    "name": "Qingcai Chen"
                }
            ],
            "abstract": "Calibration strengthens the trustworthiness of black-box models by producing better accurate confidence estimates on given examples. However, little is known about if model explanations can help confidence calibration. Intuitively, humans look at important features attributions and decide whether the model is trustworthy. Similarly, the explanations may tell us when the model might know and when it does not. Inspired by this, we propose a method named CME that leverages model explanations to make the model less confident with non-inductive attributions. The idea is that when the model is not highly confident, it is difficult to identify strong indications of any class, and the tokens accordingly do not have high attribution scores for any class and vice versa. We conduct extensive experiments on six datasets with two popular pre-trained language models in the in-domain and out-of-domain settings. The results show that CME improves calibration performance in all settings. The expected calibration errors are further reduced when combined with temperature scaling. Our findings highlight that model explanations can help calibrate posterior estimates.",
            "corpus_id": 253384402,
            "sentences": [
                {
                    "corpus_id": "253384402",
                    "title": "Calibration Meets Explanation: A Simple and Effective Approach for Model Confidence Estimates",
                    "text": "Calibration strengthens the trustworthiness of black-box models by producing better accurate confidence estimates on given examples. However, little is known about if model explanations can help confidence calibration. Intuitively, humans look at important features attributions and decide whether the model is trustworthy. Similarly, the explanations may tell us when the model might know and when it does not. Inspired by this, we propose a method named CME that leverages model explanations to make the model less confident with non-inductive attributions. The idea is that when the model is not highly confident, it is difficult to identify strong indications of any class, and the tokens accordingly do not have high attribution scores for any class and vice versa. We conduct extensive experiments on six datasets with two popular pre-trained language models in the in-domain and out-of-domain settings. The results show that CME improves calibration performance in all settings. The expected calibration errors are further reduced when combined with temperature scaling. Our findings highlight that model explanations can help calibrate posterior estimates.",
                    "score": 0.38762789913485074,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.890625
                }
            ],
            "relevance_judgement": 0.890625,
            "relevance_judgment_input_expanded": "# Title: Calibration Meets Explanation: A Simple and Effective Approach for Model Confidence Estimates\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Dongfang Li, Baotian Hu, Qingcai Chen\n## Abstract\nCalibration strengthens the trustworthiness of black-box models by producing better accurate confidence estimates on given examples. However, little is known about if model explanations can help confidence calibration. Intuitively, humans look at important features attributions and decide whether the model is trustworthy. Similarly, the explanations may tell us when the model might know and when it does not. Inspired by this, we propose a method named CME that leverages model explanations to make the model less confident with non-inductive attributions. The idea is that when the model is not highly confident, it is difficult to identify strong indications of any class, and the tokens accordingly do not have high attribution scores for any class and vice versa. We conduct extensive experiments on six datasets with two popular pre-trained language models in the in-domain and out-of-domain settings. The results show that CME improves calibration performance in all settings. The expected calibration errors are further reduced when combined with temperature scaling. Our findings highlight that model explanations can help calibrate posterior estimates.\n",
            "reference_string": "[253384402 | Li et al. | 2022 | Citations: 8]"
        },
        {
            "title": "QA-Calibration of Language Model Confidence Scores",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.06615, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2799707",
                    "name": "Putra Manggala"
                },
                {
                    "authorId": "1387253571",
                    "name": "Atalanti A. Mastakouri"
                },
                {
                    "authorId": "2319413371",
                    "name": "Elke Kirschbaum"
                },
                {
                    "authorId": "7993151",
                    "name": "S. Kasiviswanathan"
                },
                {
                    "authorId": "2556942",
                    "name": "Aaditya Ramdas"
                }
            ],
            "abstract": "To use generative question-and-answering (QA) systems for decision-making and in any critical application, these systems need to provide well-calibrated confidence scores that reflect the correctness of their answers. Existing calibration methods aim to ensure that the confidence score is, *on average*, indicative of the likelihood that the answer is correct. We argue, however, that this standard (average-case) notion of calibration is difficult to interpret for decision-making in generative QA. To address this, we generalize the standard notion of average calibration and introduce QA-calibration, which ensures calibration holds across different question-and-answer groups. We then propose discretized posthoc calibration schemes for achieving QA-calibration. We establish distribution-free guarantees on the performance of this method and validate our method on confidence scores returned by elicitation prompts across multiple QA benchmarks and large language models (LLMs).",
            "corpus_id": 273228151,
            "sentences": [
                {
                    "corpus_id": "273228151",
                    "title": "QA-Calibration of Language Model Confidence Scores",
                    "text": "Calibration for Language Models. Reinforcement learning from human feedback objective may prioritize adherence to user instructions in dialogue over producing well-calibrated predictions. (Kadavath et al., 2022). Lin et al. (2022) introduced the concept of verbalized confidence that prompts LMs to express confidence directly, focusing on fine-tuning, instead of zero-shot verbalized confidence. Mielke et al. (2022) uses an external calibrator for a white-box large language model. Other methods use consistency measures to improve LM calibration (Lyu et al., 2024). Our experimental setup closely relates to recent works in LM confidence elicitation (Tian et al., 2023;Xiong et al., 2024). These methods lack novel posthoc calibrators and do not offer the rigorous calibration guarantees that ours provide. Calibration has been shown to impact selective QA performance Kamath et al. ( 2020), but they focus on uncertainty quantification and assumes that the LM allows access to the model likelihood. \n\nGroup Notions of Calibration. Previous works highlight the limitations of average-case calibration. Group-wise calibration, which uses predefined groupings (Kleinberg et al., 2017;Pleiss et al., 2017), has been adapted for language models (LMs). Li et al. ( 2024) train a model that approximates the precision-threshold curve for a given group by using few-shot samples to predict the LM's empirical precision at various confidence thresholds. Ulmer et al. (2024) train an auxiliary model using accuracy per group as target to predict an LM's confidence based on textual input and output. Detommaso et al. (2024) achieves multicalibration -simultaneous calibration across various intersecting groupings of the data. Our work complements multicalibration, and our methods could extend to this by adapting Algorithm 3 in Detommaso et al. (2024). Luo et al. (2022) measure calibration over a set of similar predictions, quantified by a kernel function on feature space.",
                    "score": 0.6104855259619499,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 23662,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 32
                        },
                        {
                            "start": 33,
                            "end": 187
                        },
                        {
                            "start": 188,
                            "end": 212
                        },
                        {
                            "start": 213,
                            "end": 396
                        },
                        {
                            "start": 397,
                            "end": 483
                        },
                        {
                            "start": 484,
                            "end": 568
                        },
                        {
                            "start": 569,
                            "end": 692
                        },
                        {
                            "start": 693,
                            "end": 809
                        },
                        {
                            "start": 810,
                            "end": 1002
                        },
                        {
                            "start": 1005,
                            "end": 1034
                        },
                        {
                            "start": 1035,
                            "end": 1104
                        },
                        {
                            "start": 1105,
                            "end": 1250
                        },
                        {
                            "start": 1251,
                            "end": 1448
                        },
                        {
                            "start": 1449,
                            "end": 1593
                        },
                        {
                            "start": 1594,
                            "end": 1720
                        },
                        {
                            "start": 1721,
                            "end": 1848
                        },
                        {
                            "start": 1849,
                            "end": 1971
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 213,
                            "end": 230,
                            "matchedPaperCorpusId": "249191391"
                        },
                        {
                            "start": 397,
                            "end": 417,
                            "matchedPaperCorpusId": "250073258"
                        },
                        {
                            "start": 653,
                            "end": 672,
                            "matchedPaperCorpusId": "258865733"
                        },
                        {
                            "start": 672,
                            "end": 691,
                            "matchedPaperCorpusId": "259224389"
                        },
                        {
                            "start": 1161,
                            "end": 1185,
                            "matchedPaperCorpusId": "12845273"
                        },
                        {
                            "start": 1185,
                            "end": 1205,
                            "matchedPaperCorpusId": "75455"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8896484375
                }
            ],
            "relevance_judgement": 0.8896484375,
            "relevance_judgment_input_expanded": "# Title: QA-Calibration of Language Model Confidence Scores\n# Venue: International Conference on Learning Representations\n# Authors: Putra Manggala, Atalanti A. Mastakouri, Elke Kirschbaum, S. Kasiviswanathan, Aaditya Ramdas\n## Abstract\nTo use generative question-and-answering (QA) systems for decision-making and in any critical application, these systems need to provide well-calibrated confidence scores that reflect the correctness of their answers. Existing calibration methods aim to ensure that the confidence score is, *on average*, indicative of the likelihood that the answer is correct. We argue, however, that this standard (average-case) notion of calibration is difficult to interpret for decision-making in generative QA. To address this, we generalize the standard notion of average calibration and introduce QA-calibration, which ensures calibration holds across different question-and-answer groups. We then propose discretized posthoc calibration schemes for achieving QA-calibration. We establish distribution-free guarantees on the performance of this method and validate our method on confidence scores returned by elicitation prompts across multiple QA benchmarks and large language models (LLMs).\n## RELATED WORK\nCalibration for Language Models. Reinforcement learning from human feedback objective may prioritize adherence to user instructions in dialogue over producing well-calibrated predictions. (Kadavath et al., 2022). Lin et al. (2022) introduced the concept of verbalized confidence that prompts LMs to express confidence directly, focusing on fine-tuning, instead of zero-shot verbalized confidence. Mielke et al. (2022) uses an external calibrator for a white-box large language model. Other methods use consistency measures to improve LM calibration (Lyu et al., 2024). Our experimental setup closely relates to recent works in LM confidence elicitation (Tian et al., 2023;Xiong et al., 2024). These methods lack novel posthoc calibrators and do not offer the rigorous calibration guarantees that ours provide. Calibration has been shown to impact selective QA performance Kamath et al. ( 2020), but they focus on uncertainty quantification and assumes that the LM allows access to the model likelihood. \n\nGroup Notions of Calibration. Previous works highlight the limitations of average-case calibration. Group-wise calibration, which uses predefined groupings (Kleinberg et al., 2017;Pleiss et al., 2017), has been adapted for language models (LMs). Li et al. ( 2024) train a model that approximates the precision-threshold curve for a given group by using few-shot samples to predict the LM's empirical precision at various confidence thresholds. Ulmer et al. (2024) train an auxiliary model using accuracy per group as target to predict an LM's confidence based on textual input and output. Detommaso et al. (2024) achieves multicalibration -simultaneous calibration across various intersecting groupings of the data. Our work complements multicalibration, and our methods could extend to this by adapting Algorithm 3 in Detommaso et al. (2024). Luo et al. (2022) measure calibration over a set of similar predictions, quantified by a kernel function on feature space.",
            "reference_string": "[273228151 | Manggala et al. | 2024 | Citations: 3]"
        },
        {
            "title": "On the Calibration of Multilingual Question Answering LLMs",
            "venue": "",
            "year": 2023,
            "reference_count": 40,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.08669, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108877550",
                    "name": "Yahan Yang"
                },
                {
                    "authorId": "3420683",
                    "name": "Soham Dan"
                },
                {
                    "authorId": "2266750433",
                    "name": "Dan Roth"
                },
                {
                    "authorId": "2266807124",
                    "name": "Insup Lee"
                }
            ],
            "abstract": "Multilingual pre-trained Large Language Models (LLMs) are incredibly effective at Question Answering (QA), a core task in Natural Language Understanding, achieving high accuracies on several multilingual benchmarks. However, little is known about how well their confidences are calibrated. In this paper, we comprehensively benchmark the calibration of several multilingual LLMs (MLLMs) on a variety of QA tasks. We perform extensive experiments, spanning encoder-only, encoder-decoder, and decoder-only QA models (size varying from 110M to 7B parameters) and diverse languages, including both high- and low-resource ones. We study different dimensions of calibration in in-distribution, out-of-distribution, and cross-lingual transfer settings, and investigate strategies to improve it, including post-hoc methods and regularized fine-tuning. For decoder-only LLMs such as LlaMa2, we additionally find that in-context learning improves confidence calibration on multilingual data. We also conduct several ablation experiments to study the effect of language distances, language corpus size, and model size on calibration, and how multilingual models compare with their monolingual counterparts for diverse tasks and languages. Our experiments suggest that the multilingual QA models are poorly calibrated for languages other than English and incorporating a small set of cheaply translated multilingual samples during fine-tuning/calibration effectively enhances the calibration performance.",
            "corpus_id": 265212730,
            "sentences": [
                {
                    "corpus_id": "265212730",
                    "title": "On the Calibration of Multilingual Question Answering LLMs",
                    "text": "Multilingual pre-trained Large Language Models (LLMs) are incredibly effective at Question Answering (QA), a core task in Natural Language Understanding, achieving high accuracies on several multilingual benchmarks. However, little is known about how well their confidences are calibrated. In this paper, we comprehensively benchmark the calibration of several multilingual LLMs (MLLMs) on a variety of QA tasks. We perform extensive experiments, spanning encoder-only, encoder-decoder, and decoder-only QA models (size varying from 110M to 7B parameters) and diverse languages, including both high- and low-resource ones. We study different dimensions of calibration in in-distribution, out-of-distribution, and cross-lingual transfer settings, and investigate strategies to improve it, including post-hoc methods and regularized fine-tuning. For decoder-only LLMs such as LlaMa2, we additionally find that in-context learning improves confidence calibration on multilingual data. We also conduct several ablation experiments to study the effect of language distances, language corpus size, and model size on calibration, and how multilingual models compare with their monolingual counterparts for diverse tasks and languages. Our experiments suggest that the multilingual QA models are poorly calibrated for languages other than English and incorporating a small set of cheaply translated multilingual samples during fine-tuning/calibration effectively enhances the calibration performance.",
                    "score": 0.42758513383228963,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8818359375
                }
            ],
            "relevance_judgement": 0.8818359375,
            "relevance_judgment_input_expanded": "# Title: On the Calibration of Multilingual Question Answering LLMs\n# Venue: \n# Authors: Yahan Yang, Soham Dan, Dan Roth, Insup Lee\n## Abstract\nMultilingual pre-trained Large Language Models (LLMs) are incredibly effective at Question Answering (QA), a core task in Natural Language Understanding, achieving high accuracies on several multilingual benchmarks. However, little is known about how well their confidences are calibrated. In this paper, we comprehensively benchmark the calibration of several multilingual LLMs (MLLMs) on a variety of QA tasks. We perform extensive experiments, spanning encoder-only, encoder-decoder, and decoder-only QA models (size varying from 110M to 7B parameters) and diverse languages, including both high- and low-resource ones. We study different dimensions of calibration in in-distribution, out-of-distribution, and cross-lingual transfer settings, and investigate strategies to improve it, including post-hoc methods and regularized fine-tuning. For decoder-only LLMs such as LlaMa2, we additionally find that in-context learning improves confidence calibration on multilingual data. We also conduct several ablation experiments to study the effect of language distances, language corpus size, and model size on calibration, and how multilingual models compare with their monolingual counterparts for diverse tasks and languages. Our experiments suggest that the multilingual QA models are poorly calibrated for languages other than English and incorporating a small set of cheaply translated multilingual samples during fine-tuning/calibration effectively enhances the calibration performance.\n",
            "reference_string": "[265212730 | Yang et al. | 2023 | Citations: 2]"
        },
        {
            "title": "Improving Confidence Estimation on Out-of-Domain Data for End-to-End Speech Recognition",
            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "year": 2021,
            "reference_count": 38,
            "citation_count": 11,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2110.03327",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.03327, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1522347808",
                    "name": "Qiujia Li"
                },
                {
                    "authorId": "2153632494",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "150080713",
                    "name": "David Qiu"
                },
                {
                    "authorId": "2145999837",
                    "name": "Yanzhang He"
                },
                {
                    "authorId": "48749954",
                    "name": "Liangliang Cao"
                },
                {
                    "authorId": "1716393",
                    "name": "P. Woodland"
                }
            ],
            "abstract": "As end-to-end automatic speech recognition (ASR) models reach promising performance, various downstream tasks rely on good confidence estimators for these systems. Recent research has shown that model-based confidence estimators have a significant advantage over using the output softmax probabilities. If the input data to the speech recogniser is from mismatched acoustic and linguistic conditions, the ASR performance and the corresponding confidence estimators may exhibit severe degradation. Since confidence models are often trained on the same in-domain data as the ASR, generalising to out-of-domain (OOD) scenarios is challenging. By keeping the ASR model untouched, this paper proposes two approaches to improve the model-based confidence estimators on OOD data: using pseudo transcriptions and an additional OOD language model. With an ASR model trained on LibriSpeech, experiments show that the proposed methods can greatly improve the confidence metrics on TED-LIUM and Switchboard datasets while preserving in-domain performance. Furthermore, the improved confidence estimators are better calibrated on OOD data and can provide a much more reliable criterion for data selection.",
            "corpus_id": 238419596,
            "sentences": [
                {
                    "corpus_id": "238419596",
                    "title": "Improving Confidence Estimation on Out-of-Domain Data for End-to-End Speech Recognition",
                    "text": "Although model-based confidence scores are more reliable than softmax probabilities from end-to-end models as confidence estimators for both in-domain and OOD data, the performance on OOD data lags far behind the in-domain scenario. Using pseudo transcriptions to provide binary targets for training model-based confidence estimators and including additional features from an OOD LM are useful for improving the confidence scores on OOD datasets, even when pretraining uses OOD data. By exposing the CEM or R-EBM to OOD data, the calibration performance is also improved. Selecting OOD data using the improved confidence estimators is expected to considerably aid active or semi-supervised learning.",
                    "score": 0.4122072847212249,
                    "section_title": "CONCLUSION",
                    "char_start_offset": 18564,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 232
                        },
                        {
                            "start": 233,
                            "end": 483
                        },
                        {
                            "start": 484,
                            "end": 571
                        },
                        {
                            "start": 572,
                            "end": 699
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87939453125
                },
                {
                    "corpus_id": "238419596",
                    "title": "Improving Confidence Estimation on Out-of-Domain Data for End-to-End Speech Recognition",
                    "text": "As end-to-end automatic speech recognition (ASR) models reach promising performance, various downstream tasks rely on good confidence estimators for these systems. Recent research has shown that model-based confidence estimators have a significant advantage over using the output softmax probabilities. If the input data to the speech recogniser is from mismatched acoustic and linguistic conditions, the ASR performance and the corresponding confidence estimators may exhibit severe degradation. Since confidence models are often trained on the same in-domain data as the ASR, generalising to out-of-domain (OOD) scenarios is challenging. By keeping the ASR model untouched, this paper proposes two approaches to improve the model-based confidence estimators on OOD data: using pseudo transcriptions and an additional OOD language model. With an ASR model trained on LibriSpeech, experiments show that the proposed methods can greatly improve the confidence metrics on TED-LIUM and Switchboard datasets while preserving in-domain performance. Furthermore, the improved confidence estimators are better calibrated on OOD data and can provide a much more reliable criterion for data selection.",
                    "score": 0.40590371435536876,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85693359375
                }
            ],
            "relevance_judgement": 0.87939453125,
            "relevance_judgment_input_expanded": "# Title: Improving Confidence Estimation on Out-of-Domain Data for End-to-End Speech Recognition\n# Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing\n# Authors: Qiujia Li, Yu Zhang, David Qiu, Yanzhang He, Liangliang Cao, P. Woodland\n## Abstract\nAs end-to-end automatic speech recognition (ASR) models reach promising performance, various downstream tasks rely on good confidence estimators for these systems. Recent research has shown that model-based confidence estimators have a significant advantage over using the output softmax probabilities. If the input data to the speech recogniser is from mismatched acoustic and linguistic conditions, the ASR performance and the corresponding confidence estimators may exhibit severe degradation. Since confidence models are often trained on the same in-domain data as the ASR, generalising to out-of-domain (OOD) scenarios is challenging. By keeping the ASR model untouched, this paper proposes two approaches to improve the model-based confidence estimators on OOD data: using pseudo transcriptions and an additional OOD language model. With an ASR model trained on LibriSpeech, experiments show that the proposed methods can greatly improve the confidence metrics on TED-LIUM and Switchboard datasets while preserving in-domain performance. Furthermore, the improved confidence estimators are better calibrated on OOD data and can provide a much more reliable criterion for data selection.\n## CONCLUSION\nAlthough model-based confidence scores are more reliable than softmax probabilities from end-to-end models as confidence estimators for both in-domain and OOD data, the performance on OOD data lags far behind the in-domain scenario. Using pseudo transcriptions to provide binary targets for training model-based confidence estimators and including additional features from an OOD LM are useful for improving the confidence scores on OOD datasets, even when pretraining uses OOD data. By exposing the CEM or R-EBM to OOD data, the calibration performance is also improved. Selecting OOD data using the improved confidence estimators is expected to considerably aid active or semi-supervised learning.",
            "reference_string": "[238419596 | Li et al. | 2021 | Citations: 11]"
        },
        {
            "title": "Atomic Calibration of LLMs in Long-Form Generations",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 27,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.13246, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294313959",
                    "name": "Caiqi Zhang"
                },
                {
                    "authorId": "2287835207",
                    "name": "Ruihan Yang"
                },
                {
                    "authorId": "2326349647",
                    "name": "Zhisong Zhang"
                },
                {
                    "authorId": "2326648165",
                    "name": "Xinting Huang"
                },
                {
                    "authorId": "2326545246",
                    "name": "Sen Yang"
                },
                {
                    "authorId": "2326519013",
                    "name": "Dong Yu"
                },
                {
                    "authorId": "2326111107",
                    "name": "Nigel Collier"
                }
            ],
            "abstract": "Large language models (LLMs) often suffer from hallucinations, posing significant challenges for real-world applications. Confidence calibration, which estimates the underlying uncertainty of model predictions, is essential to enhance the LLMs' trustworthiness. Existing research on LLM calibration has primarily focused on short-form tasks, providing a single confidence score at the response level (macro calibration). However, this approach is insufficient for long-form generations, where responses often contain more complex statements and may include both accurate and inaccurate information. Therefore, we introduce atomic calibration, a novel approach that evaluates factuality calibration at a fine-grained level by breaking down long responses into atomic claims. We classify confidence elicitation methods into discriminative and generative types and demonstrate that their combination can enhance calibration. Our extensive experiments on various LLMs and datasets show that atomic calibration is well-suited for long-form generation and can also improve macro calibration results. Additionally, atomic calibration reveals insightful patterns in LLM confidence throughout the generation process.",
            "corpus_id": 273403321,
            "sentences": [
                {
                    "corpus_id": "273403321",
                    "title": "Atomic Calibration of LLMs in Long-Form Generations",
                    "text": "Large language models (LLMs) often suffer from hallucinations, posing significant challenges for real-world applications. Confidence calibration, which estimates the underlying uncertainty of model predictions, is essential to enhance the LLMs' trustworthiness. Existing research on LLM calibration has primarily focused on short-form tasks, providing a single confidence score at the response level (macro calibration). However, this approach is insufficient for long-form generations, where responses often contain more complex statements and may include both accurate and inaccurate information. Therefore, we introduce atomic calibration, a novel approach that evaluates factuality calibration at a fine-grained level by breaking down long responses into atomic claims. We classify confidence elicitation methods into discriminative and generative types and demonstrate that their combination can enhance calibration. Our extensive experiments on various LLMs and datasets show that atomic calibration is well-suited for long-form generation and can also improve macro calibration results. Additionally, atomic calibration reveals insightful patterns in LLM confidence throughout the generation process.",
                    "score": 0.39258058258641904,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87939453125
                }
            ],
            "relevance_judgement": 0.87939453125,
            "relevance_judgment_input_expanded": "# Title: Atomic Calibration of LLMs in Long-Form Generations\n# Venue: arXiv.org\n# Authors: Caiqi Zhang, Ruihan Yang, Zhisong Zhang, Xinting Huang, Sen Yang, Dong Yu, Nigel Collier\n## Abstract\nLarge language models (LLMs) often suffer from hallucinations, posing significant challenges for real-world applications. Confidence calibration, which estimates the underlying uncertainty of model predictions, is essential to enhance the LLMs' trustworthiness. Existing research on LLM calibration has primarily focused on short-form tasks, providing a single confidence score at the response level (macro calibration). However, this approach is insufficient for long-form generations, where responses often contain more complex statements and may include both accurate and inaccurate information. Therefore, we introduce atomic calibration, a novel approach that evaluates factuality calibration at a fine-grained level by breaking down long responses into atomic claims. We classify confidence elicitation methods into discriminative and generative types and demonstrate that their combination can enhance calibration. Our extensive experiments on various LLMs and datasets show that atomic calibration is well-suited for long-form generation and can also improve macro calibration results. Additionally, atomic calibration reveals insightful patterns in LLM confidence throughout the generation process.\n",
            "reference_string": "[273403321 | Zhang et al. | 2024 | Citations: 6]"
        },
        {
            "title": "On the Calibration of Large Language Models and Alignment",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 50,
            "citation_count": 44,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.13240, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2232117601",
                    "name": "Chiwei Zhu"
                },
                {
                    "authorId": "1754285124",
                    "name": "Benfeng Xu"
                },
                {
                    "authorId": "2261394849",
                    "name": "Quan Wang"
                },
                {
                    "authorId": "2261394984",
                    "name": "Yongdong Zhang"
                },
                {
                    "authorId": "2261422765",
                    "name": "Zhendong Mao"
                }
            ],
            "abstract": "As large language models attract increasing attention and find widespread application, concurrent challenges of reliability also arise at the same time. Confidence calibration, an effective analysis method for gauging the reliability of deep models, serves as a crucial tool for assessing and improving their reliability. However, such investigation has been comparatively underexplored. In this work, we conduct a systematic examination of the calibration of aligned language models throughout the entire construction process, including pretraining and alignment training. At each stage, we investigate how different training settings, such as parameter scales and training data, affect model calibration. To thoroughly assess model calibration, we evaluate models on three most concerned aspects: generation, factuality and understanding. Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration.",
            "corpus_id": 265351565,
            "sentences": [
                {
                    "corpus_id": "265351565",
                    "title": "On the Calibration of Large Language Models and Alignment",
                    "text": "As large language models attract increasing attention and find widespread application, concurrent challenges of reliability also arise at the same time. Confidence calibration, an effective analysis method for gauging the reliability of deep models, serves as a crucial tool for assessing and improving their reliability. However, such investigation has been comparatively underexplored. In this work, we conduct a systematic examination of the calibration of aligned language models throughout the entire construction process, including pretraining and alignment training. At each stage, we investigate how different training settings, such as parameter scales and training data, affect model calibration. To thoroughly assess model calibration, we evaluate models on three most concerned aspects: generation, factuality and understanding. Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration.",
                    "score": 0.5023206008642382,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.875
                }
            ],
            "relevance_judgement": 0.875,
            "relevance_judgment_input_expanded": "# Title: On the Calibration of Large Language Models and Alignment\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Chiwei Zhu, Benfeng Xu, Quan Wang, Yongdong Zhang, Zhendong Mao\n## Abstract\nAs large language models attract increasing attention and find widespread application, concurrent challenges of reliability also arise at the same time. Confidence calibration, an effective analysis method for gauging the reliability of deep models, serves as a crucial tool for assessing and improving their reliability. However, such investigation has been comparatively underexplored. In this work, we conduct a systematic examination of the calibration of aligned language models throughout the entire construction process, including pretraining and alignment training. At each stage, we investigate how different training settings, such as parameter scales and training data, affect model calibration. To thoroughly assess model calibration, we evaluate models on three most concerned aspects: generation, factuality and understanding. Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration.\n",
            "reference_string": "[265351565 | Zhu et al. | 2023 | Citations: 44]"
        },
        {
            "title": "On the Calibration of Massively Multilingual Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 44,
            "citation_count": 16,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2210.12265",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.12265, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "52154863",
                    "name": "Kabir Ahuja"
                },
                {
                    "authorId": "3010457",
                    "name": "Sunayana Sitaram"
                },
                {
                    "authorId": "34725175",
                    "name": "Sandipan Dandapat"
                },
                {
                    "authorId": "143990839",
                    "name": "M. Choudhury"
                }
            ],
            "abstract": "Massively Multilingual Language Models (MMLMs) have recently gained popularity due to their surprising effectiveness in cross-lingual transfer. While there has been much work in evaluating these models for their performance on a variety of tasks and languages, little attention has been paid on how well calibrated these models are with respect to the confidence in their predictions. We first investigate the calibration of MMLMs in the zero-shot setting and observe a clear case of miscalibration in low-resource languages or those which are typologically diverse from English. Next, we empirically show that calibration methods like temperature scaling and label smoothing do reasonably well in improving calibration in the zero-shot scenario. We also find that few-shot examples in the language can further help reduce calibration errors, often substantially. Overall, our work contributes towards building more reliable multilingual models by highlighting the issue of their miscalibration, understanding what language and model-specific factors influence it, and pointing out the strategies to improve the same.",
            "corpus_id": 253098773,
            "sentences": [
                {
                    "corpus_id": "253098773",
                    "title": "On the Calibration of Massively Multilingual Language Models",
                    "text": "We briefly review some commonly used methods for calibrating neural network based classifiers. 1. Temperature Scaling (TS and Self-TS) (Guo et al., 2017) is applied by scaling the output logits using a temperature parameter T before applying softmax i.e. : \n\n, where o k denotes the logits corresponding to the k th class. T is a learnable parameter obtained posttraining by maximizing the log-likelihood on the dev set while keeping other network parameters fixed. We experiment with two settings for improving calibration on a target language: using dev data in English to perform temperature scaling (TS) and using the target language's dev data (Self-TS). 2. Label Smoothing (LS) (Pereyra et al., 2017) is a regularization technique that penalizes low entropy distributions by using soft labels that are obtained by assigning a fixed probability q = 1 \u2212 \u03b1 to the true label (0 < \u03b1 < 1), and distributing the remaining probability mass uniformly across the remaining classes. Label smoothing has been empirically shown to be competitive with temperature scaling for calibration (M\u00fcller et al., 2019) especially in out of domain settings (Desai and Durrett, 2020) 3. Few-Shot Learning (FSL) We also investigate if fine-tuning the MMLM on a few examples in a target language in addition to the data in English, leads to any improvement in calibration as it does in the performance (Lauscher et al., 2020). Since these models are expected to be calibrated worse for out-of-domain data compared to in-domain data (Desai and Durrett, 2020), we try to improve calibration by reducing the domain shift through fewshot learning. \n\nApart from these, we also consider combinations of different calibration methods in our experiments, including Label Smoothing with Temperature Scaling (TS + LS or Self-TS + LS) and Few-Shot Learning with Label Smoothing (FSL + LS).",
                    "score": 0.3894575548324393,
                    "section_title": "Calibration Methods",
                    "char_start_offset": 4891,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 94
                        },
                        {
                            "start": 95,
                            "end": 256
                        },
                        {
                            "start": 259,
                            "end": 322
                        },
                        {
                            "start": 323,
                            "end": 465
                        },
                        {
                            "start": 466,
                            "end": 659
                        },
                        {
                            "start": 660,
                            "end": 978
                        },
                        {
                            "start": 979,
                            "end": 1406
                        },
                        {
                            "start": 1407,
                            "end": 1623
                        },
                        {
                            "start": 1626,
                            "end": 1858
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 135,
                            "end": 153,
                            "matchedPaperCorpusId": "28671436"
                        },
                        {
                            "start": 1081,
                            "end": 1102,
                            "matchedPaperCorpusId": "174802983"
                        },
                        {
                            "start": 1140,
                            "end": 1165,
                            "matchedPaperCorpusId": "212747810"
                        },
                        {
                            "start": 1382,
                            "end": 1405,
                            "matchedPaperCorpusId": "226262344"
                        },
                        {
                            "start": 1512,
                            "end": 1537,
                            "matchedPaperCorpusId": "212747810"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.875
                }
            ],
            "relevance_judgement": 0.875,
            "relevance_judgment_input_expanded": "# Title: On the Calibration of Massively Multilingual Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Kabir Ahuja, Sunayana Sitaram, Sandipan Dandapat, M. Choudhury\n## Abstract\nMassively Multilingual Language Models (MMLMs) have recently gained popularity due to their surprising effectiveness in cross-lingual transfer. While there has been much work in evaluating these models for their performance on a variety of tasks and languages, little attention has been paid on how well calibrated these models are with respect to the confidence in their predictions. We first investigate the calibration of MMLMs in the zero-shot setting and observe a clear case of miscalibration in low-resource languages or those which are typologically diverse from English. Next, we empirically show that calibration methods like temperature scaling and label smoothing do reasonably well in improving calibration in the zero-shot scenario. We also find that few-shot examples in the language can further help reduce calibration errors, often substantially. Overall, our work contributes towards building more reliable multilingual models by highlighting the issue of their miscalibration, understanding what language and model-specific factors influence it, and pointing out the strategies to improve the same.\n## Calibration Methods\nWe briefly review some commonly used methods for calibrating neural network based classifiers. 1. Temperature Scaling (TS and Self-TS) (Guo et al., 2017) is applied by scaling the output logits using a temperature parameter T before applying softmax i.e. : \n\n, where o k denotes the logits corresponding to the k th class. T is a learnable parameter obtained posttraining by maximizing the log-likelihood on the dev set while keeping other network parameters fixed. We experiment with two settings for improving calibration on a target language: using dev data in English to perform temperature scaling (TS) and using the target language's dev data (Self-TS). 2. Label Smoothing (LS) (Pereyra et al., 2017) is a regularization technique that penalizes low entropy distributions by using soft labels that are obtained by assigning a fixed probability q = 1 \u2212 \u03b1 to the true label (0 < \u03b1 < 1), and distributing the remaining probability mass uniformly across the remaining classes. Label smoothing has been empirically shown to be competitive with temperature scaling for calibration (M\u00fcller et al., 2019) especially in out of domain settings (Desai and Durrett, 2020) 3. Few-Shot Learning (FSL) We also investigate if fine-tuning the MMLM on a few examples in a target language in addition to the data in English, leads to any improvement in calibration as it does in the performance (Lauscher et al., 2020). Since these models are expected to be calibrated worse for out-of-domain data compared to in-domain data (Desai and Durrett, 2020), we try to improve calibration by reducing the domain shift through fewshot learning. \n\nApart from these, we also consider combinations of different calibration methods in our experiments, including Label Smoothing with Temperature Scaling (TS + LS or Self-TS + LS) and Few-Shot Learning with Label Smoothing (FSL + LS).",
            "reference_string": "[253098773 | Ahuja et al. | 2022 | Citations: 16]"
        },
        {
            "title": "Calibrating the Confidence of Large Language Models by Eliciting Fidelity",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 36,
            "citation_count": 13,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.02655, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2254324716",
                    "name": "Mozhi Zhang"
                },
                {
                    "authorId": "2255220422",
                    "name": "Mianqiu Huang"
                },
                {
                    "authorId": "2294723713",
                    "name": "Rundong Shi"
                },
                {
                    "authorId": "2295100870",
                    "name": "Linsen Guo"
                },
                {
                    "authorId": "2294823462",
                    "name": "Chong Peng"
                },
                {
                    "authorId": "2296600123",
                    "name": "Peng Yan"
                },
                {
                    "authorId": "2278545938",
                    "name": "Yaqian Zhou"
                },
                {
                    "authorId": "2294872225",
                    "name": "Xipeng Qiu"
                }
            ],
            "abstract": "Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate. In this paper, we decompose the language model confidence into the Uncertainty about the question and the Fidelity to the answer generated by language models. Then, we propose a plug-and-play method, UF Calibration, to estimate the confidence of language models. Our method has shown good calibration performance by conducting experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on Truly Well-Calibrated Confidence for large language models. Our method could serve as a strong baseline, and we hope that this work will provide some insights into the model confidence calibration.",
            "corpus_id": 268876453,
            "sentences": [
                {
                    "corpus_id": "268876453",
                    "title": "Calibrating the Confidence of Large Language Models by Eliciting Fidelity",
                    "text": "Recent work has focused on LLM calibration (Lin et al., 2022a;Kadavath et al., 2022;OpenAI, 2023). \n\nIn this section, we will briefly introduce two mainstream methods for eliciting the confidence from language models, namely the Logit-based Method and the Verbalization-based Method.",
                    "score": 0.4064586197702927,
                    "section_title": "Related Work",
                    "char_start_offset": 5449,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 98
                        },
                        {
                            "start": 101,
                            "end": 283
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87158203125
                }
            ],
            "relevance_judgement": 0.87158203125,
            "relevance_judgment_input_expanded": "# Title: Calibrating the Confidence of Large Language Models by Eliciting Fidelity\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Mozhi Zhang, Mianqiu Huang, Rundong Shi, Linsen Guo, Chong Peng, Peng Yan, Yaqian Zhou, Xipeng Qiu\n## Abstract\nLarge language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate. In this paper, we decompose the language model confidence into the Uncertainty about the question and the Fidelity to the answer generated by language models. Then, we propose a plug-and-play method, UF Calibration, to estimate the confidence of language models. Our method has shown good calibration performance by conducting experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on Truly Well-Calibrated Confidence for large language models. Our method could serve as a strong baseline, and we hope that this work will provide some insights into the model confidence calibration.\n## Related Work\nRecent work has focused on LLM calibration (Lin et al., 2022a;Kadavath et al., 2022;OpenAI, 2023). \n\nIn this section, we will briefly introduce two mainstream methods for eliciting the confidence from language models, namely the Logit-based Method and the Verbalization-based Method.",
            "reference_string": "[268876453 | Zhang et al. | 2024 | Citations: 13]"
        },
        {
            "title": "Confidence Estimation for Attention-Based Sequence-to-Sequence Models for Speech Recognition",
            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "year": 2020,
            "reference_count": 39,
            "citation_count": 49,
            "influential_citation_count": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2010.11428",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.11428, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1522347808",
                    "name": "Qiujia Li"
                },
                {
                    "authorId": "150080713",
                    "name": "David Qiu"
                },
                {
                    "authorId": "2153632494",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "143771569",
                    "name": "Bo Li"
                },
                {
                    "authorId": "2352010",
                    "name": "Yanzhang He"
                },
                {
                    "authorId": "1716393",
                    "name": "P. Woodland"
                },
                {
                    "authorId": "48749954",
                    "name": "Liangliang Cao"
                },
                {
                    "authorId": "2985957",
                    "name": "Trevor Strohman"
                }
            ],
            "abstract": "For various speech-related tasks, confidence scores from a speech recogniser are a useful measure to assess the quality of transcriptions. In traditional hidden Markov model-based automatic speech recognition (ASR) systems, confidence scores can be reliably obtained from word posteriors in decoding lattices. However, for an ASR system with an auto-regressive decoder, such as an attention-based sequence-to-sequence model, computing word posteriors is difficult. An obvious alternative is to use the decoder softmax probability as the model confidence. In this paper, we first examine how some commonly used regularisation methods influence the softmax-based confidence scores and study the overconfident behaviour of end-to-end models. Then we propose a lightweight and effective approach named confidence estimation module (CEM) on top of an existing end-to-end ASR model. Experiments on LibriSpeech show that CEM can mitigate the overconfidence problem and can produce more reliable confidence scores with and without shallow fusion of a language model. Further analysis shows that CEM generalises well to speech from a moderately mismatched domain and can potentially improve downstream tasks such as semi-supervised learning.",
            "corpus_id": 225041216,
            "sentences": [
                {
                    "corpus_id": "225041216",
                    "title": "Confidence Estimation for Attention-Based Sequence-to-Sequence Models for Speech Recognition",
                    "text": "For various speech-related tasks, confidence scores from a speech recogniser are a useful measure to assess the quality of transcriptions. In traditional hidden Markov model-based automatic speech recognition (ASR) systems, confidence scores can be reliably obtained from word posteriors in decoding lattices. However, for an ASR system with an auto-regressive decoder, such as an attention-based sequence-to-sequence model, computing word posteriors is difficult. An obvious alternative is to use the decoder softmax probability as the model confidence. In this paper, we first examine how some commonly used regularisation methods influence the softmax-based confidence scores and study the overconfident behaviour of end-to-end models. Then we propose a lightweight and effective approach named confidence estimation module (CEM) on top of an existing end-to-end ASR model. Experiments on LibriSpeech show that CEM can mitigate the overconfidence problem and can produce more reliable confidence scores with and without shallow fusion of a language model. Further analysis shows that CEM generalises well to speech from a moderately mismatched domain and can potentially improve downstream tasks such as semi-supervised learning.",
                    "score": 0.42227677096392596,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86669921875
                }
            ],
            "relevance_judgement": 0.86669921875,
            "relevance_judgment_input_expanded": "# Title: Confidence Estimation for Attention-Based Sequence-to-Sequence Models for Speech Recognition\n# Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing\n# Authors: Qiujia Li, David Qiu, Yu Zhang, Bo Li, Yanzhang He, P. Woodland, Liangliang Cao, Trevor Strohman\n## Abstract\nFor various speech-related tasks, confidence scores from a speech recogniser are a useful measure to assess the quality of transcriptions. In traditional hidden Markov model-based automatic speech recognition (ASR) systems, confidence scores can be reliably obtained from word posteriors in decoding lattices. However, for an ASR system with an auto-regressive decoder, such as an attention-based sequence-to-sequence model, computing word posteriors is difficult. An obvious alternative is to use the decoder softmax probability as the model confidence. In this paper, we first examine how some commonly used regularisation methods influence the softmax-based confidence scores and study the overconfident behaviour of end-to-end models. Then we propose a lightweight and effective approach named confidence estimation module (CEM) on top of an existing end-to-end ASR model. Experiments on LibriSpeech show that CEM can mitigate the overconfidence problem and can produce more reliable confidence scores with and without shallow fusion of a language model. Further analysis shows that CEM generalises well to speech from a moderately mismatched domain and can potentially improve downstream tasks such as semi-supervised learning.\n",
            "reference_string": "[225041216 | Li et al. | 2020 | Citations: 49]"
        },
        {
            "title": "Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 160,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.15850, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2346334761",
                    "name": "Xiaoou Liu"
                },
                {
                    "authorId": "2278853459",
                    "name": "Tiejin Chen"
                },
                {
                    "authorId": "1387492282",
                    "name": "Longchao Da"
                },
                {
                    "authorId": "150946029",
                    "name": "Chacha Chen"
                },
                {
                    "authorId": "2351350698",
                    "name": "Zhen Lin"
                },
                {
                    "authorId": "2277576540",
                    "name": "Hua Wei"
                }
            ],
            "abstract": "Large Language Models (LLMs) excel in text generation, reasoning, and decision-making, enabling their adoption in high-stakes domains such as healthcare, law, and transportation. However, their reliability is a major concern, as they often produce plausible but incorrect responses. Uncertainty quantification (UQ) enhances trustworthiness by estimating confidence in outputs, enabling risk mitigation and selective prediction. However, traditional UQ methods struggle with LLMs due to computational constraints and decoding inconsistencies. Moreover, LLMs introduce unique uncertainty sources, such as input ambiguity, reasoning path divergence, and decoding stochasticity, that extend beyond classical aleatoric and epistemic uncertainty. To address this, we introduce a new taxonomy that categorizes UQ methods based on computational efficiency and uncertainty dimensions (input, reasoning, parameter, and prediction uncertainty). We evaluate existing techniques, assess their real-world applicability, and identify open challenges, emphasizing the need for scalable, interpretable, and robust UQ approaches to enhance LLM reliability.",
            "corpus_id": 277150701,
            "sentences": [
                {
                    "corpus_id": "277150701",
                    "title": "Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey",
                    "text": "\u2022 Calibration refers to closing the gap between the confidence score and the expected correctness conditioned on confidence score. It has a long history preceding LLM or even modern machine learning [25,97,98], but bears slightly different meanings in NLP. In general, we could define a perfectly calibrated confidence measure to achieve the following: \n\nwhere the expectation is taken over the joint distribution of x and generation s. A lot of papers focus on evaluating the calibration quality of specific LMs and tasks [61,63,134]. Evaluation typically relies on variants of Expected Calibration Error (ECE) [63,71,124]. Oftentimes confidence scores from classification could be directly applied [55,120,162] in order to evaluate whether an LM is overor under-confident, especially for de facto classification tasks like sentiment analysis or multiple-choice QA. \n\nAs uncertainty and confidence are often intertwined, many approaches used in uncertainty quantification have their counterpart in confidence estimation. For example, for black-box methods, Lin et al. [77] computes a similarity matrix of sampled responses and derives confidence estimates for each generation via its degree or distance derived from the graph Laplacian, before using these scores to compute uncertainty. Zhang et al. [153] extends such black-box methods to longer generations. For logit-based methods, Malinin and Gales [86] normalize Eq. ( 1) with the length of s. Further improvements include replacing the logit-sum or mean with weighted sum, by attention values on the important tokens [78] or by importance inferred from natural language inference (NLI) models [29]. Such variants of sequence likelihood could then be fed for (entropy-style) uncertainty computation [62,78]. \n\nAnother popular approach is asking the LM itself whether a particular free-form generation is correct [59]. However, this formulation also poses a restriction on the confidence estimation method, as it is essentially a scalar logit. Thus, many extensions focus on applying calibration methods from classification to calibrate such self-evaluation.",
                    "score": 0.4465896049820459,
                    "section_title": "Uncertainty and Confidence in LLMs",
                    "char_start_offset": 8771,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 130
                        },
                        {
                            "start": 131,
                            "end": 256
                        },
                        {
                            "start": 257,
                            "end": 352
                        },
                        {
                            "start": 355,
                            "end": 436
                        },
                        {
                            "start": 437,
                            "end": 535
                        },
                        {
                            "start": 536,
                            "end": 624
                        },
                        {
                            "start": 625,
                            "end": 866
                        },
                        {
                            "start": 869,
                            "end": 1021
                        },
                        {
                            "start": 1022,
                            "end": 1287
                        },
                        {
                            "start": 1288,
                            "end": 1360
                        },
                        {
                            "start": 1361,
                            "end": 1449
                        },
                        {
                            "start": 1450,
                            "end": 1655
                        },
                        {
                            "start": 1656,
                            "end": 1763
                        },
                        {
                            "start": 1766,
                            "end": 1873
                        },
                        {
                            "start": 1874,
                            "end": 1998
                        },
                        {
                            "start": 1999,
                            "end": 2113
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 199,
                            "end": 203,
                            "matchedPaperCorpusId": "109884250"
                        },
                        {
                            "start": 203,
                            "end": 206,
                            "matchedPaperCorpusId": "127203550"
                        },
                        {
                            "start": 206,
                            "end": 209,
                            "matchedPaperCorpusId": "122150550"
                        },
                        {
                            "start": 523,
                            "end": 527,
                            "matchedPaperCorpusId": "252847587"
                        },
                        {
                            "start": 530,
                            "end": 534,
                            "matchedPaperCorpusId": "218487046"
                        },
                        {
                            "start": 619,
                            "end": 623,
                            "matchedPaperCorpusId": "258865733"
                        },
                        {
                            "start": 700,
                            "end": 704,
                            "matchedPaperCorpusId": "235078802"
                        },
                        {
                            "start": 704,
                            "end": 708,
                            "matchedPaperCorpusId": "253510101"
                        },
                        {
                            "start": 708,
                            "end": 712,
                            "matchedPaperCorpusId": "231979430"
                        },
                        {
                            "start": 1069,
                            "end": 1073,
                            "matchedPaperCorpusId": "258967487"
                        },
                        {
                            "start": 1301,
                            "end": 1306,
                            "matchedPaperCorpusId": "268793903"
                        },
                        {
                            "start": 1404,
                            "end": 1408,
                            "matchedPaperCorpusId": "231895728"
                        },
                        {
                            "start": 1650,
                            "end": 1654,
                            "matchedPaperCorpusId": "270095084"
                        },
                        {
                            "start": 1755,
                            "end": 1759,
                            "matchedPaperCorpusId": "257039062"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8603515625
                }
            ],
            "relevance_judgement": 0.8603515625,
            "relevance_judgment_input_expanded": "# Title: Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey\n# Venue: arXiv.org\n# Authors: Xiaoou Liu, Tiejin Chen, Longchao Da, Chacha Chen, Zhen Lin, Hua Wei\n## Abstract\nLarge Language Models (LLMs) excel in text generation, reasoning, and decision-making, enabling their adoption in high-stakes domains such as healthcare, law, and transportation. However, their reliability is a major concern, as they often produce plausible but incorrect responses. Uncertainty quantification (UQ) enhances trustworthiness by estimating confidence in outputs, enabling risk mitigation and selective prediction. However, traditional UQ methods struggle with LLMs due to computational constraints and decoding inconsistencies. Moreover, LLMs introduce unique uncertainty sources, such as input ambiguity, reasoning path divergence, and decoding stochasticity, that extend beyond classical aleatoric and epistemic uncertainty. To address this, we introduce a new taxonomy that categorizes UQ methods based on computational efficiency and uncertainty dimensions (input, reasoning, parameter, and prediction uncertainty). We evaluate existing techniques, assess their real-world applicability, and identify open challenges, emphasizing the need for scalable, interpretable, and robust UQ approaches to enhance LLM reliability.\n## Uncertainty and Confidence in LLMs\n\u2022 Calibration refers to closing the gap between the confidence score and the expected correctness conditioned on confidence score. It has a long history preceding LLM or even modern machine learning [25,97,98], but bears slightly different meanings in NLP. In general, we could define a perfectly calibrated confidence measure to achieve the following: \n\nwhere the expectation is taken over the joint distribution of x and generation s. A lot of papers focus on evaluating the calibration quality of specific LMs and tasks [61,63,134]. Evaluation typically relies on variants of Expected Calibration Error (ECE) [63,71,124]. Oftentimes confidence scores from classification could be directly applied [55,120,162] in order to evaluate whether an LM is overor under-confident, especially for de facto classification tasks like sentiment analysis or multiple-choice QA. \n\nAs uncertainty and confidence are often intertwined, many approaches used in uncertainty quantification have their counterpart in confidence estimation. For example, for black-box methods, Lin et al. [77] computes a similarity matrix of sampled responses and derives confidence estimates for each generation via its degree or distance derived from the graph Laplacian, before using these scores to compute uncertainty. Zhang et al. [153] extends such black-box methods to longer generations. For logit-based methods, Malinin and Gales [86] normalize Eq. ( 1) with the length of s. Further improvements include replacing the logit-sum or mean with weighted sum, by attention values on the important tokens [78] or by importance inferred from natural language inference (NLI) models [29]. Such variants of sequence likelihood could then be fed for (entropy-style) uncertainty computation [62,78]. \n\nAnother popular approach is asking the LM itself whether a particular free-form generation is correct [59]. However, this formulation also poses a restriction on the confidence estimation method, as it is essentially a scalar logit. Thus, many extensions focus on applying calibration methods from classification to calibrate such self-evaluation.",
            "reference_string": "[277150701 | Liu et al. | 2025 | Citations: 8]"
        },
        {
            "title": "Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 61,
            "citation_count": 22,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.19249",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.19249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2218509878",
                    "name": "Guande He"
                },
                {
                    "authorId": "2276707",
                    "name": "Jianfei Chen"
                },
                {
                    "authorId": "2155220672",
                    "name": "Jun Zhu"
                }
            ],
            "abstract": "Large pre-trained language models (PLMs) have demonstrated strong performance on natural language understanding (NLU) tasks through fine-tuning. However, fine-tuned models still suffer from overconfident predictions, especially in out-of-domain settings. In this paper, we tackle the problem of calibrating fine-tuned language models. We demonstrate that the PLMs are well-calibrated on the masked language modeling task with robust predictive confidence under domain shift, yet the fine-tuned models fail to retain such property due to catastrophic forgetting, which impacts the calibration on the downstream classification task. In light of these observations, we evaluate the calibration of several methods that preserve pre-trained features and show that preserving pre-trained features can improve the calibration of fine-tuned language models. Among these methods, our proposed method that encourages the fine-tuned model to learn generative representations with auxiliary language modeling objective achieves competitive accuracy and the lowest expected calibration error compared to several strong baselines under both in-domain and out-of-domain settings on three downstream NLU tasks.",
            "corpus_id": 258967945,
            "sentences": [
                {
                    "corpus_id": "258967945",
                    "title": "Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models",
                    "text": "Based on the observations, we hypothesize that preserving the pre-trained features helps calibrate the fine-tuned LMs. \n\nTo validate our hypothesis, we first evaluate the calibration of some previous methods that can preserve pre-trained features, including (1) Parameter-efficient tuning (Houlsby et al., 2019;Hu et al., 2021;Li & Liang, 2021), (2) Pre-trained weight decay, (3) Mixout (Lee et al., 2020). Although these methods were originally designed to improve the performance beyond uncertainty estimation, our experiment demonstrates that these methods outperform vanilla fine-tuning in terms of calibration, especially under out-of-domain settings. Based on our observation that the PLMs are well-calibrated on the MLM task yet the fine-tuned LMs that forget the pre-trained features struggle with overconfidence under domain shift, we propose a simple baseline that utilizes the MLM objective to maintain the consistency between the pre-trained and fine-tuned models. The proposed method achieves the lowest expected calibration error and competitive accuracy compared to existing calibration methods in both ID and OD settings on three NLU tasks, including natural language inference, paraphrase detection, and commonsense reasoning, showing that preserving the pre-trained features is an effective approach for improving the calibration of fine-tuned LMs.",
                    "score": 0.4085672232019453,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 3190,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 118
                        },
                        {
                            "start": 121,
                            "end": 406
                        },
                        {
                            "start": 407,
                            "end": 656
                        },
                        {
                            "start": 657,
                            "end": 976
                        },
                        {
                            "start": 977,
                            "end": 1366
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 289,
                            "end": 311,
                            "matchedPaperCorpusId": "59599816"
                        },
                        {
                            "start": 387,
                            "end": 405,
                            "matchedPaperCorpusId": "202750126"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85791015625
                }
            ],
            "relevance_judgement": 0.85791015625,
            "relevance_judgment_input_expanded": "# Title: Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models\n# Venue: International Conference on Learning Representations\n# Authors: Guande He, Jianfei Chen, Jun Zhu\n## Abstract\nLarge pre-trained language models (PLMs) have demonstrated strong performance on natural language understanding (NLU) tasks through fine-tuning. However, fine-tuned models still suffer from overconfident predictions, especially in out-of-domain settings. In this paper, we tackle the problem of calibrating fine-tuned language models. We demonstrate that the PLMs are well-calibrated on the masked language modeling task with robust predictive confidence under domain shift, yet the fine-tuned models fail to retain such property due to catastrophic forgetting, which impacts the calibration on the downstream classification task. In light of these observations, we evaluate the calibration of several methods that preserve pre-trained features and show that preserving pre-trained features can improve the calibration of fine-tuned language models. Among these methods, our proposed method that encourages the fine-tuned model to learn generative representations with auxiliary language modeling objective achieves competitive accuracy and the lowest expected calibration error compared to several strong baselines under both in-domain and out-of-domain settings on three downstream NLU tasks.\n## INTRODUCTION\nBased on the observations, we hypothesize that preserving the pre-trained features helps calibrate the fine-tuned LMs. \n\nTo validate our hypothesis, we first evaluate the calibration of some previous methods that can preserve pre-trained features, including (1) Parameter-efficient tuning (Houlsby et al., 2019;Hu et al., 2021;Li & Liang, 2021), (2) Pre-trained weight decay, (3) Mixout (Lee et al., 2020). Although these methods were originally designed to improve the performance beyond uncertainty estimation, our experiment demonstrates that these methods outperform vanilla fine-tuning in terms of calibration, especially under out-of-domain settings. Based on our observation that the PLMs are well-calibrated on the MLM task yet the fine-tuned LMs that forget the pre-trained features struggle with overconfidence under domain shift, we propose a simple baseline that utilizes the MLM objective to maintain the consistency between the pre-trained and fine-tuned models. The proposed method achieves the lowest expected calibration error and competitive accuracy compared to existing calibration methods in both ID and OD settings on three NLU tasks, including natural language inference, paraphrase detection, and commonsense reasoning, showing that preserving the pre-trained features is an effective approach for improving the calibration of fine-tuned LMs.",
            "reference_string": "[258967945 | He et al. | 2023 | Citations: 22]"
        },
        {
            "title": "ETHICIST: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 38,
            "citation_count": 35,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2307.04401",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.04401, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "101371510",
                    "name": "Zhexin Zhang"
                },
                {
                    "authorId": "2104586007",
                    "name": "Jiaxin Wen"
                },
                {
                    "authorId": "2196817617",
                    "name": "Minlie Huang"
                }
            ],
            "abstract": "Large pre-trained language models achieve impressive results across many tasks. However, recent works point out that pre-trained language models may memorize a considerable fraction of their training data, leading to the privacy risk of information leakage. In this paper, we propose a method named Ethicist for targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation, investigating how to recover the suffix in the training data when given a prefix. To elicit memorization in the attacked model, we tune soft prompt embeddings while keeping the model fixed. We further propose a smoothing loss that smooths the loss distribution of the suffix tokens to make it easier to sample the correct suffix. In order to select the most probable suffix from a collection of sampled suffixes and estimate the prediction confidence, we propose a calibrated confidence estimation method, which normalizes the confidence of the generated suffixes with a local estimation. We show that Ethicist significantly improves the extraction performance on a recently proposed public benchmark. We also investigate several factors influencing the data extraction performance, including decoding strategy, model scale, prefix length, and suffix length. Our code is availabel at https://github.com/thu-coai/Targeted-Data-Extraction.",
            "corpus_id": 259370520,
            "sentences": [
                {
                    "corpus_id": "259370520",
                    "title": "ETHICIST: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation",
                    "text": "(1) With prompt tuning and extra training data, we can better induce large-scale language models to generate their memorized training data and successfully achieves a 9.5% performance improvement on Recall and a 12.4% performance improvement on Recall Early stop .\n\n(2) The proposed smoothing loss can further enhance the ability to extract training data, boosting the Recall score from 60.8% to 62.3%. (3) The calibrated confidence provides a 6.3% improvement on Recall Early stop as expected, demonstrating the importance of calibrating confidence estimation for this task. (4) The smoothing loss is more effective in predicting exact suffixes while the calibrated confidence is more beneficial for identifying highly confident predictions, according to the significant drop in Recall without smoothing and the substantial decrease in Recall Early stop without calibration. (5) The calibrated confidence estimation is effective regardless of whether using prompt tuning. And it demonstrates greater advantages compared to the comparing (LM) baseline in recognizing predictions with higher confidence when using prompt tuning, indicated by increasing Recall Early stop (from 48.7 to 52.4).",
                    "score": 0.4356187767802499,
                    "section_title": "Main Results",
                    "char_start_offset": 19939,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85693359375
                }
            ],
            "relevance_judgement": 0.85693359375,
            "relevance_judgment_input_expanded": "# Title: ETHICIST: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Zhexin Zhang, Jiaxin Wen, Minlie Huang\n## Abstract\nLarge pre-trained language models achieve impressive results across many tasks. However, recent works point out that pre-trained language models may memorize a considerable fraction of their training data, leading to the privacy risk of information leakage. In this paper, we propose a method named Ethicist for targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation, investigating how to recover the suffix in the training data when given a prefix. To elicit memorization in the attacked model, we tune soft prompt embeddings while keeping the model fixed. We further propose a smoothing loss that smooths the loss distribution of the suffix tokens to make it easier to sample the correct suffix. In order to select the most probable suffix from a collection of sampled suffixes and estimate the prediction confidence, we propose a calibrated confidence estimation method, which normalizes the confidence of the generated suffixes with a local estimation. We show that Ethicist significantly improves the extraction performance on a recently proposed public benchmark. We also investigate several factors influencing the data extraction performance, including decoding strategy, model scale, prefix length, and suffix length. Our code is availabel at https://github.com/thu-coai/Targeted-Data-Extraction.\n## Main Results\n(1) With prompt tuning and extra training data, we can better induce large-scale language models to generate their memorized training data and successfully achieves a 9.5% performance improvement on Recall and a 12.4% performance improvement on Recall Early stop .\n\n(2) The proposed smoothing loss can further enhance the ability to extract training data, boosting the Recall score from 60.8% to 62.3%. (3) The calibrated confidence provides a 6.3% improvement on Recall Early stop as expected, demonstrating the importance of calibrating confidence estimation for this task. (4) The smoothing loss is more effective in predicting exact suffixes while the calibrated confidence is more beneficial for identifying highly confident predictions, according to the significant drop in Recall without smoothing and the substantial decrease in Recall Early stop without calibration. (5) The calibrated confidence estimation is effective regardless of whether using prompt tuning. And it demonstrates greater advantages compared to the comparing (LM) baseline in recognizing predictions with higher confidence when using prompt tuning, indicated by increasing Recall Early stop (from 48.7 to 52.4).",
            "reference_string": "[259370520 | Zhang et al. | 2023 | Citations: 35]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "270371795",
            "title": "When is Multicalibration Post-Processing Necessary?",
            "text": "Pfohl et al. (2022) measure subgroup calibration, but do not discuss it at length.Although the focus in Haghtalab et al. (2023) is mainly theoretical, the authors also run experiments for both their proposed multicalibration algorithms and additional algorithms present in the literature.However, they do not compare the performance of such algorithms against a baseline NN or tree-based method trained on an identical amount of data, and also do not investigate how much data should be used for training the base model vs. used for multicalibration post-processing.We believe such a comparison is important given that ERM may have latent multicalibration properties, and is by far the most used learning algorithm in practice.\n\nIn recent work, Detommaso et al. (2024) utilize multicalibration as a tool to improve the overall uncertainty and confidence calibration of language model but, to our knowledge, do not focus on or report fairness towards protected subgroups.Like us, they point out various issues with the standard multicalibration algorithm, which they address with early stopping and adaptive binning.We instead perform a large hyperparameter sweep which effectively implements an early stopping mechanism.We discuss this further in Appendix A.1.Nonetheless, our results for large models are complementary to those of Detommaso et al. (2024): both works demonstrate that (1) standard multicalibration can at times be difficult to get working in practice; and (2) ideas from the theoretical multicalibration literature can have impact at the scale of large models.We provide additional discussion of related works in model calibration and subgroup robustness in Appendix A.",
            "score": 0.6284042837161197,
            "section_title": "Related Works: Theory and Practice",
            "char_start_offset": 11593,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 82,
                    "end": 288
                },
                {
                    "start": 288,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 727
                },
                {
                    "start": 729,
                    "end": 970
                },
                {
                    "start": 970,
                    "end": 1115
                },
                {
                    "start": 1115,
                    "end": 1220
                },
                {
                    "start": 1220,
                    "end": 1260
                },
                {
                    "start": 1260,
                    "end": 1577
                },
                {
                    "start": 1577,
                    "end": 1686
                }
            ],
            "ref_mentions": [
                {
                    "start": 104,
                    "end": 127,
                    "matchedPaperCorpusId": "261959418"
                },
                {
                    "start": 745,
                    "end": 768,
                    "matchedPaperCorpusId": "269004786"
                },
                {
                    "start": 1332,
                    "end": 1355,
                    "matchedPaperCorpusId": "269004786"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63134765625
        },
        {
            "corpus_id": "273228151",
            "title": "QA-Calibration of Language Model Confidence Scores",
            "text": "Calibration for Language Models. Reinforcement learning from human feedback objective may prioritize adherence to user instructions in dialogue over producing well-calibrated predictions. (Kadavath et al., 2022). Lin et al. (2022) introduced the concept of verbalized confidence that prompts LMs to express confidence directly, focusing on fine-tuning, instead of zero-shot verbalized confidence. Mielke et al. (2022) uses an external calibrator for a white-box large language model. Other methods use consistency measures to improve LM calibration (Lyu et al., 2024). Our experimental setup closely relates to recent works in LM confidence elicitation (Tian et al., 2023;Xiong et al., 2024). These methods lack novel posthoc calibrators and do not offer the rigorous calibration guarantees that ours provide. Calibration has been shown to impact selective QA performance Kamath et al. ( 2020), but they focus on uncertainty quantification and assumes that the LM allows access to the model likelihood. \n\nGroup Notions of Calibration. Previous works highlight the limitations of average-case calibration. Group-wise calibration, which uses predefined groupings (Kleinberg et al., 2017;Pleiss et al., 2017), has been adapted for language models (LMs). Li et al. ( 2024) train a model that approximates the precision-threshold curve for a given group by using few-shot samples to predict the LM's empirical precision at various confidence thresholds. Ulmer et al. (2024) train an auxiliary model using accuracy per group as target to predict an LM's confidence based on textual input and output. Detommaso et al. (2024) achieves multicalibration -simultaneous calibration across various intersecting groupings of the data. Our work complements multicalibration, and our methods could extend to this by adapting Algorithm 3 in Detommaso et al. (2024). Luo et al. (2022) measure calibration over a set of similar predictions, quantified by a kernel function on feature space.",
            "score": 0.6104855259619499,
            "section_title": "RELATED WORK",
            "char_start_offset": 23662,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1002
                },
                {
                    "start": 1005,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 1971
                }
            ],
            "ref_mentions": [
                {
                    "start": 213,
                    "end": 230,
                    "matchedPaperCorpusId": "249191391"
                },
                {
                    "start": 397,
                    "end": 417,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 653,
                    "end": 672,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 672,
                    "end": 691,
                    "matchedPaperCorpusId": "259224389"
                },
                {
                    "start": 1161,
                    "end": 1185,
                    "matchedPaperCorpusId": "12845273"
                },
                {
                    "start": 1185,
                    "end": 1205,
                    "matchedPaperCorpusId": "75455"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8896484375
        },
        {
            "corpus_id": "269004786",
            "title": "Multicalibration for Confidence Scoring in LLMs",
            "text": "This paper proposes the use of\"multicalibration\"to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data. We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and\"self-annotation\"- querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods.",
            "score": 0.5635416005167772,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.970703125
        },
        {
            "corpus_id": "273821704",
            "title": "Graph-based Confidence Calibration for Large Language Models",
            "text": "However, manually designed features are limited in their ability to capture the full extent of self-consistency among LLM responses, leading to poor calibration performance. \n\nTo better calibrate the confidence estimation, some methods directly use correctness labels in their calibration procedures. Mielke et al. (2022) train a calibrator to predict the correctness of a response for a given question. With a similar idea, Ulmer et al. (2024) train a language model (e.g., DeBERTa) based on question-response pairs to predict the probability of responses' correctness. Based on SelfCheckGPT (Manakul et al., 2023) and JAFC (Tian et al., 2023), Chen et al. (2024) train supervised models to reduce grouping losses and improve the confidence estimation. The method by Liu et al. (2024) uses an LLM's latent representations to predict the correctness of responses. Detommaso et al. (2024) use the \"multicalibration\" technique to calibrate the probability of correctness. Fadeeva et al. (2023) offer a detailed comparative study of various confidence estimation methods, providing empirical evidence on their effectiveness across different tasks. However, these studies have not sufficiently exploited response consistency to predict the probabilities of the responses being correct.",
            "score": 0.5564202991937732,
            "section_title": "Related Work",
            "char_start_offset": 7867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 176,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1281
                }
            ],
            "ref_mentions": [
                {
                    "start": 301,
                    "end": 321,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 593,
                    "end": 615,
                    "matchedPaperCorpusId": "257557820"
                },
                {
                    "start": 625,
                    "end": 644,
                    "matchedPaperCorpusId": "258865733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.890625
        },
        {
            "corpus_id": "260379149",
            "title": "Calibration in Deep Learning: A Survey of the State-of-the-Art",
            "text": "In recent years, a variety of approaches have emerged for model calibration, spanning post-hoc adjustments, regularization techniques, uncertainty estimation, and hybrid methods. Several related surveys have delved into this area (Silva Filho, Song, Perello-Nieto, Santos-Rodriguez, Kull, & Flach, 2023), or those focusing on the highly relevant topic of uncertainty estimation (Silva Filho et al., 2023;Gawlikowski, Tassi, Ali, Lee, Humt, Feng, Kruspe, Triebel, Jung, Roscher, et al., 2021;Mena, Pujol, & Vitria, 2021), which briefly touch upon model calibration. However, there remains a gap in the literature regarding a comprehensive review of recently proposed calibration techniques. Setting itself apart from previous surveys, our research emphasizes several key distinctions: \n\n\u2022 This survey reviews the state-of-the-art calibration methods and focuses mostly on the ones proposed in recent years. Those methods such as kernel-based methods, differentiable calibration proxy, and meta-learning-based approaches. Those are rarely discussed in previous and existing surveys. \n\n\u2022 This survey tries to explain calibration principles of each method via the discussion of the conceptual relationships among over-parameterization, over-fitting, and over-confidence. We systematically categorize those methods into post-hoc, regularization (explicit, implicit and differentiable calibration proxy), uncertainty estimation, and hybrid methods that combines multiple calibration techniques. \n\n\u2022 This survey also delves into the methodologies for calibrating large pre-trained models, with a specific focus on large language models (LLMs). Notably, the calibration of LLMs for zero-shot inference has garnered escalating interest within AI communities. \n\nThe rest of this survey is structured as follows. \n\n\u2022 Section 2 provides an introduction to the concept of model calibration, elucidating the factors that contribute to miscalibration. We explore how various aspects such as model complexity, data distribution, training and measurement procedures can impact the calibration of machine learning models. \n\n\u2022 Section 3 presents an overview of mainstream calibration metrics commonly used to evaluate the calibration performance of models.",
            "score": 0.5533760087562459,
            "section_title": "Scope and Focus",
            "char_start_offset": 3594,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1080
                },
                {
                    "start": 1083,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1488
                },
                {
                    "start": 1491,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1749
                },
                {
                    "start": 1752,
                    "end": 1801
                },
                {
                    "start": 1804,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2103
                },
                {
                    "start": 2106,
                    "end": 2237
                }
            ],
            "ref_mentions": [
                {
                    "start": 283,
                    "end": 303,
                    "matchedPaperCorpusId": "256901249"
                },
                {
                    "start": 378,
                    "end": 404,
                    "matchedPaperCorpusId": "256901249"
                },
                {
                    "start": 491,
                    "end": 519,
                    "matchedPaperCorpusId": "241258024"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66796875
        },
        {
            "corpus_id": "235078802",
            "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
            "text": "In this section, we discuss each method in detail and analyze why they can improve calibration performance. \n\nObjective Function Matters. The original Uni-fiedQA model is fine-tuned based on MLE, which maximizes the probability of the gold answer given the question. Both softmax-based and margin-based fine-tuning, which explicitly compare and adjust the probability of candidate answers, can further improve ECE on multiplechoice datasets. We argue that the softmax-based and margin-based objective functions are better suited for questions with potential candidates. \n\nPost-processing Confidence is Effective Universally. Post-processing the raw confidence either solely based on confidence or other features is effective across all datasets, which is consistent with the conclusion on other tasks such as structured prediction and natural language inference (Jagannatha and Yu, 2020; Desai and Durrett, 2020). We demonstrate the histogram of confidence before and after applying temperature-based scaling or feature-based decision trees in Figure 2. LMs tend to be over-confident, with most predictions having either extremely high or low confidence. Both methods can successfully re-scale the confidence to reasonable ranges, thus improving the calibration performance. \n\nParaphrasing Answers and Input Augmentation can Improve Confidence Estimation. The improvement brought by using paraphrasing is significant on multiple-choice datasets, demonstrating that using diverse expressions can indeed improve confidence estimation. To better understand under what circumstances paraphrasing works, we group candidate answers into two categories: the first group includes candidate answers that become better calibrated using paraphrases; the second group includes candidate answers whose confidence remains the same using paraphrases. We say that a candidate becomes better calibrated if its confidence increases/decreases by 20% if it is a correct or incorrect answer respectively. We found that the average length of questions for better calibrated candidates ( 187) is much shorter than that of candidates without improvement (320), indicating that paraphrasing is useful mainly for short questions. We also compute the diversity of word usage in paraphrases using the number of unique words divided by the total length of paraphrases.",
            "score": 0.5523821564947221,
            "section_title": "Analysis of Individual Calibration Methods",
            "char_start_offset": 25002,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 110,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1274
                },
                {
                    "start": 1277,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2203
                },
                {
                    "start": 2204,
                    "end": 2339
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.951171875
        },
        {
            "corpus_id": "267617073",
            "title": "Calibrating Long-form Generations from Large Language Models",
            "text": "\u2022 A cost-effective model usage strategy, illustrating the practicality of long-form calibration in optimizing LLM deployment. \n\nMeasuring Calibration Calibration (Guo et al., 2017, Minderer et al., 2021)has been widely studied in language models, whose probabilities derived from logits are generally found to not be calibrated (Jiang et al., 2020, Kadavath et al., 2022, Chen et al., 2023d). Standard metrics to measure the calibration include Expected Calibration Error (ECE) for confidence-accuracy disparity (Naeini et al., 2015), Brier Score for mean squared prediction-outcome differences, and AUROC for assessing confidence-based correct answer identification (Boyd et al., 2013, Kuhn et al., 2023). Selective Accuracy@Coverage measures accuracy within the model's most confident predictions (Liang et al., 2023, Cole et al., 2023). However, these metrics, rooted in a binary notion of correctness, fall short for long-form tasks where correctness spans a range, suggesting a distribution-based approach is more apt. \n\nImproving Calibration Traditional calibration methods focus on post-processing logits (Guo et al., 2017), but with LLMs generating unbounded text, logits could fall short. Thus, extracting better confidence scores (i.e., confidence elicitation) has become crucial for improving calibration. These include: verbalization, which directly asks the model to output its confidence (Lin et al., 2022), consistency, which uses the uniformity of multiple responses to gauge confidence (Kadavath et al., 2022, Kuhn et al., 2023, Cole et al., 2023, Chen et al., 2023c, Tian et al., 2023a, Lin et al., 2023), and the hybrid of both (Xiong et al., 2023, Tian et al., 2023b, Chen and Mueller, 2023).",
            "score": 0.5504831140346489,
            "section_title": "Introduction",
            "char_start_offset": 5549,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 128,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1023
                },
                {
                    "start": 1026,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1712
                }
            ],
            "ref_mentions": [
                {
                    "start": 162,
                    "end": 179,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 370,
                    "end": 391,
                    "matchedPaperCorpusId": "253244504"
                },
                {
                    "start": 512,
                    "end": 533,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 667,
                    "end": 685,
                    "matchedPaperCorpusId": "37409855"
                },
                {
                    "start": 799,
                    "end": 818,
                    "matchedPaperCorpusId": "253553585"
                },
                {
                    "start": 1112,
                    "end": 1130,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89697265625
        },
        {
            "corpus_id": "274789494",
            "title": "A Survey of Calibration Process for Black-Box LLMs",
            "text": "Given these limitations and the high computational costs of traditional methods, Confidence Estimation and Calibration Methods have emerged as promising alternatives (Ni et al., 2024;Mielke et al., 2022), offering cost-effective solutions applicable to both white-box and black-box models while maintaining scalability to large-scale deployments. \n\nIn recent years, Confidence Estimation and Calibration have frequently been discussed together, as the estimation of confidence is often influenced by the uncertainty in the model or data, and calibration methods help the model recognize its own knowledge limitations (Jiang et al., 2021;Shrivastava et al., 2023). Calibration allows LLMs to adjust their confidence to more accurately reflect the quality of their outputs (Kuhn et al., 2023;Duan et al., 2023). For example, in the context of diagnosing rare diseases, LLMs may estimate a 95% confidence score to an incorrect response, while the accurate confidence, due to a lack of domain knowledge, should be closer to 40%. Calibration can identify this discrepancy and adjust the model's confidence to more accurately reflect the quality of the response, preventing overconfidence in generated responses (Ren et al., 2023;Geng et al., 2024;Tian et al., 2023). This process of Confidence Estimation, and Calibration to achieve well-calibrated confidence is referred to as the Calibration Process in this survey. \n\nA growing number of surveys have reviewed factors influencing LLMs Calibration Process. For instance, some studies have summarized how model parameters, training stages, or training data influence confidence and calibration (Zhu et al., 2023), while others investigate which confidence representations are more beneficial for calibration during the RLHF phase (Tian et al., 2023). Additionally, many studies have reviewed various confidence estimation and calibration methods on Language Models(LMs) and LLMs (Geng et al., 2024).",
            "score": 0.5419711294723611,
            "section_title": "Introduction",
            "char_start_offset": 1309,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 346
                },
                {
                    "start": 349,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1412
                },
                {
                    "start": 1415,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1944
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 203,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 617,
                    "end": 637,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 1206,
                    "end": 1224,
                    "matchedPaperCorpusId": "266335508"
                },
                {
                    "start": 1224,
                    "end": 1242,
                    "matchedPaperCorpusId": "265157516"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "260063200",
            "title": "Strength in Numbers: Estimating Confidence of Large Language Models by Prompt Agreement",
            "text": "Confidence estimation is the counterpart to uncertainty estimation, which quantifies a model's lack of confidence in its predictions. Previous work has shown that modeling uncertainty improves task performance on neural machine translation (Wang et al., 2019), document quality prediction (Shen et al., 2019), sentiment analysis, named entity recognition, and language modeling using convolutional and recurrent neural network models (?). \n\nWork on model confidence estimation for NLP has included a range of models-Naive Bayes and logistic regression (Nguyen and O'Connor, 2015), neural networks (Jagannatha and yu, 2020)-and tasks-structured prediction (Jagannatha and yu, 2020), natural language understanding (Desai and Durrett, 2020;Kamath et al., 2020;Kong et al., 2020), and neural machine translation systems (Kumar and Sarawagi, 2019). Kamath et al. (2020) found that QA models are overconfident in out-ofdomain tasks when asked to answer as many questions as possible while maintaining high accuracy. More recently, this work has turned to language models, and researchers have struggled to obtain sensible confidence measures. Jiang et al. (2021) found that language models such as T5, BART, and GPT-2 did not produce well-calibrated scores based on generation probabilities for QA tasks. \n\nA variety of methods have been proposed to obtain calibrated confidence measures from LLMs. Jiang et al. (2021) experiment with several calibration methods, including fine-tuning, post hoc probability modification, or adjustment of the predicted outputs or inputs. Kong et al. (2020) use a regularized fine-tuning method to obtain better calibration for both in-distribution and out-of-distribution data. Xiao et al. (2022) focus on the design choices for pre-trained language model-based prediction pipelines, suggesting that the calibration of the model depends on the choice of the fine-tuning loss function. Desai and Durrett (2020) demonstrated a more calibrated model trained with label smoothing.",
            "score": 0.5085625800371127,
            "section_title": "Confidence Estimation",
            "char_start_offset": 7202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 438
                },
                {
                    "start": 441,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1299
                },
                {
                    "start": 1302,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2005
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 259,
                    "matchedPaperCorpusId": "202539965"
                },
                {
                    "start": 289,
                    "end": 308,
                    "matchedPaperCorpusId": "208331285"
                },
                {
                    "start": 552,
                    "end": 579,
                    "matchedPaperCorpusId": "2879445"
                },
                {
                    "start": 655,
                    "end": 680,
                    "matchedPaperCorpusId": "215548393"
                },
                {
                    "start": 738,
                    "end": 758,
                    "matchedPaperCorpusId": "219721462"
                },
                {
                    "start": 845,
                    "end": 865,
                    "matchedPaperCorpusId": "219721462"
                },
                {
                    "start": 1138,
                    "end": 1157,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 1394,
                    "end": 1413,
                    "matchedPaperCorpusId": "235078802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81591796875
        },
        {
            "corpus_id": "266051956",
            "title": "A Study on the Calibration of In-context Learning",
            "text": "Calibration of language models. Calibration is a safety property to measure the faithfulness of machine learning models' uncertainty, especially for error-prone tasks using LMs. Previous works find that pre-training (Desai and Durrett, 2020) and explanation (Zhang et al., 2020;Gonz\u00e1lez et al., 2021) improves calibration. Models can be very poorly calibrated when we prompt LMs (Jiang et al., 2021), while calibration can also depend on model size (Kadavath et al., 2022). (Braverman et al., 2020) assesses the long-term dependencies in a language model's generations compared to those of the underlying language and finds that entropy drifts as models such as when GPT-2 generates text. The intricacy of explanations on complementary team performance poses additional challenges due to the overreliance on explanations of users regardless of their correctness (Bansal et al., 2021). (Mielke et al., 2022) gives a framework for linguistic calibration, a concept that emphasizes the alignment of a model's expressed confidence or doubt with the actual accuracy of its responses. The process involves annotating generations with <DK>, <LO>, <HI> for confidence levels, then training the confidence-controlled model by appending the control token <DK/LO/HI> at the start of the output, followed by training a calibrator to predict these confidence levels, and finally predicting confidence when generating new examples. (Tian et al., 2023) finds that asking LMs for their probabilities can be better than using conditional probabilities in a traditional way. LHTS (Shih et al., 2023) is a simple amortized inference trick for temperaturescaled sampling from LMs and diffusion models. To aggregate log probabilities across semantically equivalent outputs, Kuhn et al. (2023) utilize bidirectional entailment through a model to identify outputs that are semantically similar, thereby refining the uncertainty estimation process.",
            "score": 0.5026992527845364,
            "section_title": "Related Work",
            "char_start_offset": 3677,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1924
                }
            ],
            "ref_mentions": [
                {
                    "start": 258,
                    "end": 278,
                    "matchedPaperCorpusId": "210023849"
                },
                {
                    "start": 278,
                    "end": 300,
                    "matchedPaperCorpusId": "236478213"
                },
                {
                    "start": 379,
                    "end": 399,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 474,
                    "end": 498,
                    "matchedPaperCorpusId": "189762078"
                },
                {
                    "start": 862,
                    "end": 883,
                    "matchedPaperCorpusId": "220128138"
                },
                {
                    "start": 885,
                    "end": 906,
                    "matchedPaperCorpusId": "250073258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7314453125
        },
        {
            "corpus_id": "265351565",
            "title": "On the Calibration of Large Language Models and Alignment",
            "text": "As large language models attract increasing attention and find widespread application, concurrent challenges of reliability also arise at the same time. Confidence calibration, an effective analysis method for gauging the reliability of deep models, serves as a crucial tool for assessing and improving their reliability. However, such investigation has been comparatively underexplored. In this work, we conduct a systematic examination of the calibration of aligned language models throughout the entire construction process, including pretraining and alignment training. At each stage, we investigate how different training settings, such as parameter scales and training data, affect model calibration. To thoroughly assess model calibration, we evaluate models on three most concerned aspects: generation, factuality and understanding. Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration.",
            "score": 0.5023206008642382,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.875
        },
        {
            "corpus_id": "276235552",
            "title": "Confidence Elicitation: A New Attack Vector for Large Language Models",
            "text": "Multiple studies have explored calibration in language models. A common method, which has been thoroughly explored in previous work (Guo et al., 2017a;Jiang et al., 2021) involves using output probabilities as a proxy for confidence. This could be implemented by focusing on the first generated vector for a specific token, by adding a binary classification prediction head that utilizes the last generated token (Kadavath et al., 2022), focusing on the answer specific token or take the average of the probabilities across the whole sequence, these techniques have been classified as white-box confidence estimation. While these approaches could be effective, several challenges arise. Firstly output logits or probabilities may not be accessible, particularly with proprietary models. Secondly the likelihood of the next token primarily signifies lexical confidence and not epistemic uncertainty (Lin et al., 2022), and therefore, struggles to capture the semantic uncertainty in the entire text (Xiong et al., 2024). \n\nAs a result, previous work highlighted the need for models capable of directly expressing uncertainty in natural language in a black-box setting. Some research has explored enhancing calibration by empirically deriving confidence through repetitive model querying (Portillo Wightman et al., 2023). Alternatively, models can be prompted to express their confidence verbally, either through verbalized numerical confidence elicitation (Xiong et al., 2024) or verbal confidence elicitation (Lin et al., 2022). It has been found that some prompts can achieve reasonable uncertainty quantification, especially by querying the model twice, first for the prediction, and the second time for the uncertainty estimates (Tian et al., 2023) (Example of a prompt for confidence elicitation is in Table 6 in the Appendix).",
            "score": 0.49895042633461406,
            "section_title": "CONFIDENCE ELICITATION",
            "char_start_offset": 8691,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1831
                }
            ],
            "ref_mentions": [
                {
                    "start": 998,
                    "end": 1018,
                    "matchedPaperCorpusId": "259224389"
                },
                {
                    "start": 1286,
                    "end": 1318,
                    "matchedPaperCorpusId": "260063200"
                },
                {
                    "start": 1455,
                    "end": 1475,
                    "matchedPaperCorpusId": "259224389"
                },
                {
                    "start": 1732,
                    "end": 1751,
                    "matchedPaperCorpusId": "258865733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.775390625
        },
        {
            "corpus_id": "253244504",
            "title": "A Close Look into the Calibration of Pre-trained Language Models",
            "text": "Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs\u2019 calibration performance in training. We consider six factors as control variables, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. We observe a consistent change in calibration performance across six factors. We find that PLMs don\u2019t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not. We highlight that our finding somewhat contradicts two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining improves model calibration. Next, we study the effectiveness of existing calibration methods in mitigating the overconfidence issue. Besides unlearnable calibration methods (e.g., label smoothing), we adapt and extend two recently proposed learnable methods that directly collect data to train models to have reasonable confidence estimations. Experimental results show that learnable methods significantly reduce PLMs\u2019 confidence in wrong predictions.",
            "score": 0.4985230420499696,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94189453125
        },
        {
            "corpus_id": "259859030",
            "title": "Making Pre-trained Language Models both Task-solvers and Self-calibrators",
            "text": "Calibration methods. Typically, calibration methods rely on human intuitions or posterior adjustments to make the confidence estimations more accurate. Data augmentation (Hendrycks et al., 2020b;Wang et al., 2021) and model ensemble (Gal and Ghahramani, 2016;Lakshminarayanan et al., 2017) have been empirically proven to be successful in computer vision. However, they cannot bring the same benefits in NLP according to the empirical study in Chen et al. (2022b). So we don't consider them as baseline methods for comparison in this work, and two empirically effective methods are adopted. Temperature scaling (Platt et al., 1999;Guo et al., 2017) readjusts the output logits in a posterior way according to the calibration performance on a held-out set. Label smoothing (Szegedy et al., 2016) imposes a confidence regularization during the training process, discouraging models from being overconfident in their predictions.\n\nRecently, there is an emergent trend in NLP, motivating to directly collect data for training models to have reasonable confidence estimations. Kadavath et al. (2022) assume that the last hidden states of PLMs contain the uncertainty information, and directly apply a multi-layer perceptron on them to perform confidence estimations. Lin et al. (2022) also show that PLMs can be directly trained to give their confidence estimations by words. These two methods are proven to be successful in significantly reducing the overconfidence issue and are further extended to exploit the potential in this kind of methods (Chen et al., 2022b). Existing work demonstrates the feasibility and potential of this kind of method in ideal experimental settings that enough training data is given for the calibration task. We further consider the practical setting and propose an effective method in this work.\n\nApplications. The confidence scores have been widely utilized for various applications. A bunch of active learning methods relies on models' confidence to select the most informative samples to annotate (Zhang et al., 2022;Schr\u00f6der et al., 2022). Models' confidence can also be directly utilized for OOD and misclassification detection (Hendrycks and Gimp",
            "score": 0.49410417568216125,
            "section_title": "Related Work",
            "char_start_offset": 22642,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 195,
                    "matchedPaperCorpusId": "208637407"
                },
                {
                    "start": 195,
                    "end": 213,
                    "matchedPaperCorpusId": "239885364"
                },
                {
                    "start": 259,
                    "end": 289,
                    "matchedPaperCorpusId": "6294674"
                },
                {
                    "start": 611,
                    "end": 631,
                    "matchedPaperCorpusId": "56563878"
                },
                {
                    "start": 772,
                    "end": 794,
                    "matchedPaperCorpusId": "206593880"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9130859375
        },
        {
            "corpus_id": "266174407",
            "title": "On Diversified Preferences of Large Language Model Alignment",
            "text": "where y best = arg max 1\u2264s\u2264S{r(x,y s )} is the sampled response with the highest reward score. \n\nCalibration Error Calibration error is an effective method to estimate the confidence of a model's outputs (Guo et al., 2017). We divide the confidence interval [0, 1] with finite samples into M bins with equal length (1/M ). Then, we place model predictions into these bins according to their prediction confidence. Let B m be the set of indices of samples that fall into the internal ( m\u22121 M , m M ]. We calculate the corresponding accuracy and average confidence of each bin as follows: \n\nwhere \u0177i are the prediction results, and y i is the ground-truth of the i-th sample. I is the indicator function which produces 1 if \u0177i = y i otherwise 0. pi is the prediction confidence of the i-th sample. In the context of reward modeling, the prediction confidence pi = \u03c3(\u2022) in (1). For a set of N samples, we can compute the Expected Calibration Error as follows: \n\nWe set M = 10 for measuring calibration performance in this paper. \n\nNumerous studies have focused on improving the calibration performance of statistical machinelearning systems (DeGroot and Fienberg, 1983;Palmer et al., 2008;Yang and Thompson, 2010). Furthermore, the calibration error of neural networks provides additional information for users to determine whether to trust the model's predictions, especially for modern neural networks that are more challenging to interpret (Guo et al., 2017;Zhu et al., 2023). In the field of natural language processing, studies have revealed a positive relationship between calibration performance and the reduction of hallucination (Xiao and Wang, 2021;Tian et al., 2019), and the evaluation of pre-trained language models (Kadavath et al., 2022;Tian et al., 2023). The calibration error has demonstrated its ability to evaluate the performance of language models. In this paper, we first employ the calibration error to evaluate the RMs. Subsequently, we investigate the implicit connection between RMs and LLM alignment under diversified preferences.",
            "score": 0.49393187849871234,
            "section_title": "Background",
            "char_start_offset": 7003,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 97,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 586
                },
                {
                    "start": 589,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 956
                },
                {
                    "start": 959,
                    "end": 1025
                },
                {
                    "start": 1028,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 1941
                },
                {
                    "start": 1942,
                    "end": 2055
                }
            ],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 222,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1138,
                    "end": 1166,
                    "matchedPaperCorpusId": "109884250"
                },
                {
                    "start": 1166,
                    "end": 1186,
                    "matchedPaperCorpusId": "121625992"
                },
                {
                    "start": 1186,
                    "end": 1210,
                    "matchedPaperCorpusId": "24392407"
                },
                {
                    "start": 1440,
                    "end": 1458,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56103515625
        },
        {
            "corpus_id": "274281176",
            "title": "Text-to-SQL Calibration: No Need to Ask - Just Rescale Model Probabilities",
            "text": "Calibration of classification models is a classical ML topic (Niculescu-Mizil and Caruana, 2005;Guo et al., 2017a), with much work in pre-LLM NLP literature (Kumar and Sarawagi, 2019;Desai and Durrett, 2020). We focus on recent work on calibration of tasks on LLMs . \n\nCalibration of LLMs for short response generation Kadavath et al. (2022) study LLMs on a variety of tasks and propose to extract confidence by a self-probe using a follow up True/False question to the LLM on whether the generated response was correct. Probability of True in the follow up question is measured as confidence. Tian et al. (2023) further expand the set of prompts asking to verbalize confidence and show that a better strategy for calibration is getting the LLM to generate top-K responses with probabilities. Ren et al. (2023) also show that self-evaluation improves calibration. Zhou et al. (2023) study if language markers like: \"I believe\", \"I am sure..\"etc reflect confidence, and show that these language markers do not faithfully reflect uncertainty. Kuhn et al. (2023) propose to arXiv:2411.16742v1 [cs.DB] 23 Nov 2024 generate multiple answers with a confidence score assigned by the LLM for each, cluster them based on semantic similarity, and measure entropy over the total confidence across the clusters. Kapoor et al. (2024) show that the probabilities output by the self-probe methods are not well-calibrated, and proposes to correct the calibration by further finetuning. Xiong et al. (2024) also studies these techniques and additionally introduces PairRank that scores based on the ranking of responses across multiple Top-K sets. Xie et al. (2024) proposes to apply adaptive temperature scaling where the temperature is a linear function of the last layer vector. \n\nUncertainty for Semantic Parsing and SQL.",
            "score": 0.4930679900834088,
            "section_title": "Related Work",
            "char_start_offset": 1938,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 266
                },
                {
                    "start": 269,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1764
                },
                {
                    "start": 1767,
                    "end": 1808
                }
            ],
            "ref_mentions": [
                {
                    "start": 96,
                    "end": 114,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 183,
                    "end": 207,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 1041,
                    "end": 1059,
                    "matchedPaperCorpusId": "257039062"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.341796875
        },
        {
            "corpus_id": "275342783",
            "title": "Influences on LLM Calibration: A Study of Response Agreement, Loss Functions, and Prompt Styles",
            "text": "Calibration, the alignment between model confidence and prediction accuracy, is critical for the reliable deployment of large language models (LLMs). Existing works neglect to measure the generalization of their methods to other prompt styles and different sizes of LLMs. To address this, we define a controlled experimental setting covering 12 LLMs and four prompt styles. We additionally investigate if incorporating the response agreement of multiple LLMs and an appropriate loss function can improve calibration performance. Concretely, we build Calib-n, a novel framework that trains an auxiliary model for confidence estimation that aggregates responses from multiple LLMs to capture inter-model agreement. To optimize calibration, we integrate focal and AUC surrogate losses alongside binary cross-entropy. Experiments across four datasets demonstrate that both response agreement and focal loss improve calibration from baselines. We find that few-shot prompts are the most effective for auxiliary model-based methods, and auxiliary models demonstrate robust calibration performance across accuracy variations, outperforming LLMs' internal probabilities and verbalized confidences. These insights deepen the understanding of influence factors in LLM calibration, supporting their reliable deployment in diverse applications.",
            "score": 0.4917232245953574,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93994140625
        },
        {
            "corpus_id": "270392060",
            "title": "Large Language Models Must Be Taught to Know What They Don't Know",
            "text": "Zero-Shot Classifier \"Question.Answer.True/False: True\" P(\" True\") / (P(\" True\") + P(\" False\"))\n\nVerbalized \"Question.Answer.Confidence: 90%\" float(\" 90%\")\n\nIn the first approach, the language model's logits are used to create a binary classifier by scoring two possible strings denoting true and false.Similarly, in Kadavath et al. [25], the classifier takes in a slightly modified prompt, \"Is the answer correct?(a) Yes (b) No \" and confidence is then computed P(\"(a)\") / (P(\"(a)\") + P(\"(b)\")).In the second approach (also used in [51,59]), uncertainty estimates are sampled as text and then converted into numbers.We provide the extended details in Appendix B.2.\n\nThe prospects of calibration by learning to model human language.If we view language modeling as behavior cloning [46] on human writing, the optimal outcome is a language model that recapitulates the full distribution of human writers present in the training data.Unfortunately, most humans exhibit poor calibration on tasks they are unfamiliar with [28,29,32], and not all pre-training data is generated by experts.Therefore it might be unreasonably optimistic to expect black-box methods to yield calibrated uncertainties without a significant intervention.Alignment procedures (e.g.RLHF) could improve the situation by penalizing cases of poor calibration, and the resulting procedure would be akin to fine-tuning on graded data, which we explore in Section 5.\n\nExperiments with open-source models.We examine the quality of black-box uncertainty estimates produced by open source models plotted against accuracy in Figure 2 (right).We use LLaMA-2 [52,53], Mistral [24], and LLaMA-3 models, and we evaluate on open-ended MMLU to highlight how the methods might perform in a \"chat-bot\" setting.Because these models have open weights, we can perform apples-to-apples comparisons with methods that train through the model or access hidden representations.",
            "score": 0.49111529303230417,
            "section_title": "Name Format Confidence",
            "char_start_offset": 13798,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 31,
                    "end": 38
                },
                {
                    "start": 38,
                    "end": 95
                },
                {
                    "start": 97,
                    "end": 118
                },
                {
                    "start": 118,
                    "end": 125
                },
                {
                    "start": 125,
                    "end": 155
                },
                {
                    "start": 157,
                    "end": 303
                },
                {
                    "start": 303,
                    "end": 414
                },
                {
                    "start": 414,
                    "end": 496
                },
                {
                    "start": 496,
                    "end": 617
                },
                {
                    "start": 617,
                    "end": 665
                },
                {
                    "start": 667,
                    "end": 732
                },
                {
                    "start": 732,
                    "end": 931
                },
                {
                    "start": 931,
                    "end": 1083
                },
                {
                    "start": 1083,
                    "end": 1226
                },
                {
                    "start": 1226,
                    "end": 1252
                },
                {
                    "start": 1252,
                    "end": 1430
                },
                {
                    "start": 1432,
                    "end": 1468
                },
                {
                    "start": 1468,
                    "end": 1602
                },
                {
                    "start": 1602,
                    "end": 1762
                },
                {
                    "start": 1762,
                    "end": 1921
                }
            ],
            "ref_mentions": [
                {
                    "start": 781,
                    "end": 785,
                    "matchedPaperCorpusId": "291920"
                },
                {
                    "start": 1017,
                    "end": 1021,
                    "matchedPaperCorpusId": "2109278"
                },
                {
                    "start": 1024,
                    "end": 1027,
                    "matchedPaperCorpusId": "60832216"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.513671875
        },
        {
            "corpus_id": "259095925",
            "title": "Uncertainty in Natural Language Processing: Sources, Quantification, and Applications",
            "text": "Calibration methods aim to correct the reliability of the uncertainty estimates provided by a model. The basic idea is to measure the accuracy of the predicted probabilities against the true probabilities. As shown in Fig. 5, if the predicted probabilities are well calibrated, then they will accurately reflect probabilities, and the model will be considered reliable. It is worth noting that the probability distribution can be obtained by a single forward pass of the network, also including some post-processing methods. We introduce below two commonly used methods of expressing uncertainty, as well as calibration methods in NLP research. \n\nSoftmax Response. Confidence method is a widely used uncertainty estimation method. It is based on the idea that the predicted category with the highest probability is the most likely category, and the uncertainty can be represented by Softmax Response (SR). There are also methods based on the difference (SD) between the top two values of the Softmax output, with a slight difference indicating uncertainty and a significant difference indicating confidence. Given an input x (i) , the uncertainty describe as: \n\n(2) \n\nEntropy-based. Prediction entropy, which can be understood as the total uncertainty of the output distribution. When the certainty of the output distribution is higher, the entropy value is lower. Maximum entropy is reached when all outcomes have equal probability. In other words, entropy is highest value when the model is unable to distinguish between different outcomes. The predictive entropy at point x (i) is equal to the conditional entropy of the output random variable Y : \n\nIn NLP research, the entropy value can represent a variety of uncertain information, such as the N-grams word entropy value as a heuristic feature to select domain adaptation data [65], the entropy value at the sentence level can represent semantic uncertainty [4], The composition of information entropy in NLP is diverse and complex, and these issues need to be considered when using entropy values. \n\nRecently, the problem of overconfidence can arise in DNNs [10], often requiring calibration to obtain reliable confidence. A simple but effective calibration method is Temperature scaling (TS), which was originally introduced into the machine learning community as a tool for knowledge distillation [66].",
            "score": 0.49086194418630463,
            "section_title": "Calibration Confidence-based Methods",
            "char_start_offset": 29608,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 644
                },
                {
                    "start": 647,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1159
                },
                {
                    "start": 1162,
                    "end": 1165
                },
                {
                    "start": 1168,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1650
                },
                {
                    "start": 1653,
                    "end": 2054
                },
                {
                    "start": 2057,
                    "end": 2179
                },
                {
                    "start": 2180,
                    "end": 2361
                }
            ],
            "ref_mentions": [
                {
                    "start": 1833,
                    "end": 1837,
                    "matchedPaperCorpusId": "7403346"
                },
                {
                    "start": 2115,
                    "end": 2119,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72509765625
        },
        {
            "corpus_id": "268358242",
            "title": "Calibrating Large Language Models Using Their Generations Only",
            "text": "We presented APRICOT , a general method to obtain confidence scores from any language model on the input and text output alone.We showed that it is possible to compute calibration targets through the clustering of question embeddings.Through the subsequent finetuning of a smaller language model, we then outperform other methods to distinguish incorrect from correct answers with competitive calibration scores, on different models and datasets.While we only presented a first, more fundamental version this approach in this work, it lends itself naturally to a whole body of research that aims to improve the calibration of pretrained language models (Desai and Durrett, 2020;Jiang et al., 2021;Chen et al., 2023).Lastly, future studies might also investigate the uncertainty of the auxiliary model itself and use techniques such as conformal prediction (Vovk et al., 2005;Papadopoulos et al., 2002;Angelopoulos and Bates, 2021) to produce estimates of LLM confidence intervals.",
            "score": 0.4885837363184927,
            "section_title": "Conclusion",
            "char_start_offset": 26219,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 127,
                    "end": 234
                },
                {
                    "start": 234,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 716
                },
                {
                    "start": 716,
                    "end": 980
                }
            ],
            "ref_mentions": [
                {
                    "start": 653,
                    "end": 678,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 678,
                    "end": 697,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 875,
                    "end": 901,
                    "matchedPaperCorpusId": "42084298"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "235078802",
            "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
            "text": "This is especially true if these models are deployed to safety-critical domains such as healthcare and finance, where mistaken answers can have serious consequences. 1 n this paper, we ask the question \"how can we know when language models know, with confidence, the answer to a particular knowledge-based query?\" Specifically, we examine this from the point of view of calibration, whether the model's probability estimates are well-aligned with the actual probability of the answer being correct. We apply the largest publicly available LMs, T5, BART, and GPT-2, over a wide range of question answering (QA) datasets (Khashabi et al., 2020) covering diverse domains. We first observe that despite the models' high performance (e.g. T5 eclipses other alternatives such as GPT-3 on some datasets), the models tend to not be well calibrated; their probability estimates over candidates have far-from-perfect correspondence with the actual probability that the answer they provide is correct. Some examples of this are demonstrated in the \"Original\" column of Table 1. \n\nTo alleviate this problem, we propose methods to make LMs' confidence scores correlate better with the likelihood of model prediction being correct. We examined both fine-tuning methods that modify LMs' parameters and post-hoc methods that keep LMs fixed and only manipulate the confidence values or inputs. Specifically, we fine-tune the LM using softmax-or marginbased objective functions based on multiple candidate answers. For post-hoc calibration, we examined temperature-based scaling and feature-based decision trees that take prediction probability and input-related features as input and produce calibrated confidence (Jagannatha and Yu, 2020;Desai and Durrett, 2020;Kamath et al., 2020). We also study the sensitivity of LMs' confidence estimation with respect to language variation by paraphrasing candidate answers and augmenting questions using retrieved context. \n\nExperimental results demonstrate that both finetuning and post-hoc methods can improve calibration performance without sacrificing accuracy. We further perform analysis and ablation studies on our methods, inspecting different aspects that may affect calibration performance.",
            "score": 0.4851874149954394,
            "section_title": "Introduction",
            "char_start_offset": 1723,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1066
                },
                {
                    "start": 1069,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1946
                },
                {
                    "start": 1949,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2224
                }
            ],
            "ref_mentions": [
                {
                    "start": 1697,
                    "end": 1722,
                    "matchedPaperCorpusId": "215548393"
                },
                {
                    "start": 1746,
                    "end": 1766,
                    "matchedPaperCorpusId": "219721462"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95947265625
        },
        {
            "corpus_id": "273963769",
            "title": "Epistemic Integrity in Large Language Models",
            "text": "As research on estimating the internal confidence of LLMs has increased, scholars have started to focus on model calibration-i.e., the alignment between predicted probabilities and actual correctness. Desai & Durrett (2020) find that pre-trained transformers such as BERT often exhibit poor calibration out-of-the-box, where their confidence estimates fail to correspond to actual correctness. Jiang et al. (2021) also explore methods to improve model calibration, such as temperature scaling, which adjusts predicted probabilities to better match actual outcomes. However, most calibration methods focus on probabilistic outputs (internal confidence) without addressing how certainty is expressed through language generation. The same miscalibration is found by Si et al. (2022), who demonstrate that models like GPT-3 frequently produce overconfident responses even when incorrect. \n\nHowever, we note that despite these known limits, internal confidence scores remain largely inaccessible to users today. Without these scores, it becomes essential for models to effectively communicate uncertainty through external means-such as linguistic cues-to ensure users correctly interpret the model's output.",
            "score": 0.48229517584282927,
            "section_title": "Challenges in Confidence Alignment",
            "char_start_offset": 11412,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 883
                },
                {
                    "start": 886,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1202
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 413,
                    "matchedPaperCorpusId": "235078802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85546875
        },
        {
            "corpus_id": "265212730",
            "title": "On the Calibration of Multilingual Question Answering LLMs",
            "text": "Confidence calibration is one such reliability metric that measures whether the model's prediction probability estimates are aligned with the actual probability of the answer being correct. Confidence calibration has been studied in Computer Vision (Guo et al., 2017;Minderer et al., 2021) and Natural Language Processing (Desai and Durrett, 2020;Dan and Roth, 2021). However, most of the prior works are limited to classification settings, which is inapplicable to the generality of the QA task. Recently, Zhang et al. (2021); Jiang et al. (2021) have shown that state-of-the-art English QA models are surprisingly poorly calibrated. However, there remains a gap in understanding of the calibration properties of multilingual QA models. In this paper, we address this gap by a comprehensive study on the Calibration of Multilingual Question Answering Large Language Models. The main research questions we investigate in this paper are: \n\n1) How well are MLLMs calibrated in the crosslingual transfer scenario? \n\n2) How can we improve MLLMs' confidence calibration on multilingual QA datasets? The contributions of our work are as follows: \n\n\u2022 We provide the first comprehensive benchmarking of confidence calibration of multilingual QA models (architectures including extractive models: mBERT, XLM-R and generative models: mT5, mBART, and LLaMa2) over both low-and high-resource languages, in-distribution and out-ofdistribution settings. \n\n\u2022 We observe that the calibration performance on English is not transferable to other languages, across various datasets and architectures. Distance between the target languages and English, and the distribution of different languages at the pretraining stage, are all highly correlated with calibration performance, across the various model types. \n\n\u2022 An investigation of various calibration strategies including post-hoc methods and regularization methods, aimed at enhancing cross-lingual calibration. Temperature scaling (optimized over a cross-lingual validation dataset) shows the most significant improvement even if the target language is absent in the validation data.",
            "score": 0.4819058315008946,
            "section_title": "Introduction",
            "char_start_offset": 1894,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 1010
                },
                {
                    "start": 1013,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1139
                },
                {
                    "start": 1142,
                    "end": 1439
                },
                {
                    "start": 1442,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1790
                },
                {
                    "start": 1793,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2119
                }
            ],
            "ref_mentions": [
                {
                    "start": 249,
                    "end": 267,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 267,
                    "end": 289,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 322,
                    "end": 347,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 347,
                    "end": 366,
                    "matchedPaperCorpusId": "244119588"
                },
                {
                    "start": 528,
                    "end": 547,
                    "matchedPaperCorpusId": "235078802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83837890625
        },
        {
            "corpus_id": "253244504",
            "title": "A Close Look into the Calibration of Pre-trained Language Models",
            "text": "Pre-trained language models (PLMs) are successful in many downstream tasks regarding performance (Wang et al., 2019). But for a reliable deployment in practice, the calibration performance should also be carefully examined (Vaicenavicius et al., 2019). Well-calibrated models assign appropriate confidence scores that truly reflect the outcome probability of their predictions. However, the confidence scores of existing deep neural networks cannot serve as reliable estimates of their uncertainty (Guo et al., 2017), and a deep understanding of PLMs calibration is lacking.\n\nIn this paper, we give a systematical analysis of PLMs calibration. We consider two questions about PLMs calibration: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? We first introduce the metrics we adopt for calibration performance evaluation. The most widely used calibration metric ECE (Naeini et al., 2015) is considered. It measures the difference between confidence and accuracy by portioning samples into various confidence zones. To give a more comprehensive and practical calibration evaluation, we provide an application-driven perspective, describing two undesirable situations in practice: (1) For the first question, we consider the influence of six factors that have influence on PLMs' calibration performance, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. Some of them are overlooked in previous empirical studies (Snoek et al., 2019;Nixon et al., 2019;Minderer et al., 2021). We motivate to conduct fine-grained control experiments to study the dynamic change in PLMs' calibration performance in training through manipulating controlling variables. We empirically observe a consistent change in calibration performance across six factors. All six factors influence PLMs' fitness on the training distribution. This results in two states of PLMs considering calibration performance, namely under-fitted and over-fitted states (see Fig. 1). In the under-fitted state, PLMs' performance and confidence increase at different speeds when more fitted on the training distribution. In the over-fitting state, PLMs' confidence continues to",
            "score": 0.480202632707609,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 116,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 223,
                    "end": 251,
                    "matchedPaperCorpusId": "67749814"
                },
                {
                    "start": 498,
                    "end": 516,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1566,
                    "end": 1586,
                    "matchedPaperCorpusId": "174803437"
                },
                {
                    "start": 1586,
                    "end": 1605,
                    "matchedPaperCorpusId": "102486060"
                },
                {
                    "start": 1605,
                    "end": 1627,
                    "matchedPaperCorpusId": "235435823"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92822265625
        },
        {
            "corpus_id": "272423997",
            "title": "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration",
            "text": "In this section, we describe the methods used to elicit confidence from large language models (LLMs) as well as our recalibration methods. Calibration in our settings refers to the alignment between the confidence estimates and the true likelihood of outcomes (Yuksekgonul et al., 2023;Gneiting and Raftery, 2007). Our experiments are based on the framework described by Xiong et al. (2024), which divides the approaches into three main components: prompting, sampling, and aggregation, and uses it as baselines. In their framework, they leverage common prompting strategies such as vanilla prompting and Chain-of-Thoughts while also leveraging the stochasticity of LLMs. In contrast, we propose an approach, Atypical Presentation Recalibration, that retrieves atypicality scores and use them as a recalibration method in order to have more accurate confidence estimates. Our framework is mainly divided into two parts: Atypicality Prompting and Atypicality Recalibration. We explain how each of the three components are applied to our tasks and how we integrate atypicality to develop hybrid methods that combine these elements.",
            "score": 0.4802011050315349,
            "section_title": "Method",
            "char_start_offset": 8140,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1129
                }
            ],
            "ref_mentions": [
                {
                    "start": 286,
                    "end": 313,
                    "matchedPaperCorpusId": "1878582"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7470703125
        },
        {
            "corpus_id": "269157057",
            "title": "Language Model Cascades: Token-level uncertainty and beyond",
            "text": "Another key challenge in measuring the uncertainty for natural language outputs is that there are a lot of semantic equivalent sentences and hence, the probability can be divided among multiple outputs which mean the exact same thing.Kuhn et al. (2023a) proposes to mitigate this problem by sampling multiple outputs and then clustering semantically equivalent outputs together and combining their probability together.It would be interesting to understand how well this method can work for our setting.\n\nGenerative uncertainty.The above has largely focussed on generalizing the standard maximum predictive probability (Equation 2) from classification to the LM setting.While this by itself leads to a rich array of possible confidence measures, LMs intriguingly offer a wholly new possible means of assessing confidence: one may directly probe the model to obtain how confident it is on the proposed answer (Kadavath et al., 2022).Kadavath et al. (2022) discuss various ways of the input prompt format for this confidence probe.They also discuss the training of an additional head of the model to predict the model confidence but again, it is not clear how this compares with the standard probability output by the model without any additional finetuning.However, (Shrivastava et al., 2023) found that the confidence measures generated linguistically give worse estimates of uncertainty compared to the classical softmax-based measures even when these softmax-based probabilities come from a different and weaker model.Moreover, they observed that two sources of uncertainty are complementary and it can be beneficial to combine them.\n\nOther work.Zhao et al. (2023) proposed sequence-level calibration as a means to improve the generative ability of LMs; such calibration could also be useful in improving methods such as Chow-Sum.Kuhn et al. (2023b) proposed to ask the model to detect ambiguous questions which the model is likely to get wrong and answer clarifying questions if the question is indeed ambiguous.Hendy et al. (2023) proposed to use an exogeneous quality estimation model to decide how to route between two models.\u0160akota et al. (2023) similarly proposed to train a meta-model to pick an appropriate model from a family.Fadeeva et al. (2023) did a comprehensive experimental analysis of various uncertainty methods.",
            "score": 0.4790867910747091,
            "section_title": "Uncertainty Quantification for LMs",
            "char_start_offset": 31087,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 234,
                    "end": 419
                },
                {
                    "start": 419,
                    "end": 503
                },
                {
                    "start": 505,
                    "end": 528
                },
                {
                    "start": 528,
                    "end": 670
                },
                {
                    "start": 670,
                    "end": 932
                },
                {
                    "start": 932,
                    "end": 1029
                },
                {
                    "start": 1029,
                    "end": 1256
                },
                {
                    "start": 1256,
                    "end": 1520
                },
                {
                    "start": 1520,
                    "end": 1635
                },
                {
                    "start": 1637,
                    "end": 1648
                },
                {
                    "start": 1648,
                    "end": 1832
                },
                {
                    "start": 1832,
                    "end": 2015
                },
                {
                    "start": 2015,
                    "end": 2132
                },
                {
                    "start": 2132,
                    "end": 2237
                },
                {
                    "start": 2237,
                    "end": 2332
                }
            ],
            "ref_mentions": [
                {
                    "start": 234,
                    "end": 253,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 1648,
                    "end": 1666,
                    "matchedPaperCorpusId": "252683988"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.400146484375
        },
        {
            "corpus_id": "265212730",
            "title": "On the Calibration of Multilingual Question Answering LLMs",
            "text": "The calibration properties of a model can be evaluated directly out-of-box based on the probabilities it assigns to the predicted answers. Further, one can adopt strategies to calibrate a model, which can be broadly categorized into: \n\n\u2022 Post-hoc calibration methods that do not require any additional training, for example, Temperature Scaling (TS) (Guo et al., 2017). \n\n\u2022 Specialized fine-tuning, such as Label Smoothing and Few Shot Learning, which regularizes training, or leverages augmented data respectively. Temperature Scaling (TS) is a technique to improve the calibration of a machine learning model (Guo et al., 2017). When applying the softmax function to output the probability distribution of the logits, TS utilizes a single parameter \u03c4 to scale the logits: sof tmax(z i ) = exp(z i /\u03c4 \n\n. We perform TS in two ways specialized to the QA model type: \n\n\u2022 Extractive Models: we compute the TS factors on the start logits and the end logits separately (\u03c4 start and \u03c4 end ), obtained by optimizing the Negative Log-Likelihood (NLL) loss on the validation set, extending the standard classification setting in (Guo et al., 2017).The softmax score of the answer is computed as follows: \n\n\u2022 Generative Models: For each of the K candidate answers, we have an associated normalized probability P ( \u0176 | X). We use the log probabilities of these as logits z = log P ( \u0176 | X), in the softmax function Jiang et al. (2021). We then similarly optimize for a temperature T , with respect to the NLL loss, on the validation set. \n\nFew-shot Learning (FS) Incorporating a small number of examples from the target language during fine-tuning, mixed in with the English examples improves the calibration performance in classification tasks (Ahuja et al., 2022). We investigate whether this strategy helps calibrate multilingual QA models by randomly selecting 1000 samples from 5 different languages to fine-tune LLMs, in addition to the almost 100 times larger English data.",
            "score": 0.47813727310100407,
            "section_title": "Techniques to improve the Confidence Calibration of a model",
            "char_start_offset": 9875,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 233
                },
                {
                    "start": 236,
                    "end": 369
                },
                {
                    "start": 372,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 801
                },
                {
                    "start": 804,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 1195
                },
                {
                    "start": 1198,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1527
                },
                {
                    "start": 1530,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1970
                }
            ],
            "ref_mentions": [
                {
                    "start": 350,
                    "end": 368,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 611,
                    "end": 629,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1121,
                    "end": 1139,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1405,
                    "end": 1424,
                    "matchedPaperCorpusId": "235078802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83203125
        },
        {
            "corpus_id": "267750237",
            "title": "Multi-Perspective Consistency Enhances Confidence Estimation in Large Language Models",
            "text": "Large language models, such as GPT-4 (OpenAI et al., 2023), have achieved outstanding performance in multiple downstream NLP tasks (Sanh et al., 2021;Chung et al., 2022;Yuan et al., 2023;Luo et al., 2023;Dong et al., 2023). However, as models are increasingly used in practical applications, it is important to accurately assess their confidence (Guo et al., 2017;Tomani and Buettner, 2021). Confidence estimation is to evaluate the uncertainty of the model prediction, which is critical for ensuring the clarity and trustworthiness of human-machine interaction (Kuleshov et al., 2018;Xiao and Wang, 2021;Kuleshov and Deshpande, 2022;Song et al., 2023). 1: Confidence score distribution of GPT-4's incorrect samples in TruthfulQA. The horizontal axis represents the confidence scores predicted under this method, and the vertical axis represents the probability density. Theoretically, it is preferable for the entire distribution to shift left as far as possible. \n\nThe standard approach of estimating confidence is to use the softmax probabilities of these models. However, due to the unavailability of the logits, as the most powerful LLM currently available is closed-source, researchers employ two alternative methods for confidence estimation. one is Verbalized-based method (Verb) (Kadavath et al., 2022;Lin et al., 2022a;Tian et al., 2023). They prompt LLMs to provide a confidence probability verbally and optimize the prompt template by combining techniques like CoT (Xiong et al., 2023) and TOT (Yao et al., 2023). The other one is Self-Consistency Confidence (SC) (Wang et al., 2023;Xiong et al., 2023), which calculates the probability of the answer appearing as the confidence. The essence is to measure the correctness of the answer by using the consistency between answers.",
            "score": 0.47806950147017613,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 964
                },
                {
                    "start": 967,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1789
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63720703125
        },
        {
            "corpus_id": "267522811",
            "title": "Reconfidencing LLMs from the Grouping Loss Perspective",
            "text": "In this section, we present a simple yet effective solution to reconfidence LLMs. The core idea is to calibrate each sub-group separately. \n\nStandard Calibration Following standard calibration procedures, we train a regressor, commonly known as a calibrator, to conduct the calibration of a model (Niculescu-Mizil and Caruana, 2005) This calibrator works by mapping the model's output to a refined probability within the interval [0, 1], with the aim of aligning closely with the true probability. Concretely, we train an isotonic regressor using our constructed training and validation sets for calibration purposes (Zadrozny and Elkan, 2002). Subsequently, we apply this trained regressor to calibrate the confidence scores on the test set. \n\nReconfidencing The standard calibration approaches are marginal: they control average error on confidence and overlook the nuances of subgroups, where confidence errors can be especially marked. Inspired by this, we propose a more refined method to calibrate LLMs from the sub-group perspective. Adapting Perez-Lebel et al. ( 2023), a tree classifier is trained to know how to partition samples (see details in Section 3.4). We employ a loss function derived from the squared loss for the Brier score on labels (Y ) to optimize the predicted confidence scores. This decision tree algorithm partitions the data into sub-groups that minimize the specified loss. The tree's input consists of embeddings from the top layer of a LLM for a given query, which can effectively cluster samples exhibiting similar levels of over-confidence or under-confidence. This, in contrast to user-defined sub-groups, does not need background knowledge and thus applies to queries that are not matched to the knowledge base. Following this step, a distinct isotonic regressor is trained for each identified subgroup. The final step is to apply this refined method to reconfidence the test set The reconfidencing can effectively reduce the grouping loss thus yielding improved calibration results. \n\nTo validate our proposed solution, we conduct a comparative analysis of calibration performance between the standard calibration and our reconfidencing approach.",
            "score": 0.4776707444279772,
            "section_title": "Reconfidencing LLMs",
            "char_start_offset": 22516,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 138
                },
                {
                    "start": 141,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 742
                },
                {
                    "start": 745,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 2020
                },
                {
                    "start": 2023,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 617,
                    "end": 643,
                    "matchedPaperCorpusId": "3349576"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81201171875
        },
        {
            "corpus_id": "267617073",
            "title": "Calibrating Long-form Generations from Large Language Models",
            "text": "To enhance Large Language Models' (LLMs) reliability, calibration is essential -- the model's assessed confidence scores should align with the actual likelihood of its responses being correct. However, current confidence elicitation methods and calibration metrics typically rely on a binary true/false assessment of response correctness. This approach does not apply to long-form generation, where an answer can be partially correct. Addressing this gap, we introduce a unified calibration framework, in which both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores. Within this framework, we develop three metrics to precisely evaluate LLM calibration and further propose two confidence elicitation methods based on self-consistency and self-evaluation. Our experiments, which include long-form QA and summarization tasks, demonstrate that larger models don't necessarily guarantee better calibration, that calibration performance is found to be metric-dependent, and that self-consistency methods excel in factoid datasets. We also find that calibration can be enhanced through techniques such as fine-tuning, integrating relevant source documents, scaling the temperature, and combining self-consistency with self-evaluation. Lastly, we showcase a practical application of our system: selecting and cascading open-source models and ChatGPT to optimize correctness given a limited API budget. This research not only challenges existing notions of LLM calibration but also offers practical methodologies for improving trustworthiness in long-form generation.",
            "score": 0.4768309723492106,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9609375
        },
        {
            "corpus_id": "273345593",
            "title": "On Calibration of LLM-based Guard Models for Reliable Content Moderation",
            "text": "Confidence calibration is a critical aspect in developing reliable and trustworthy language models (Nguyen & O'Connor, 2015;Guo et al., 2017;Minderer et al., 2021). In the context of LLMs, prior research has explored calibration in question-answering tasks (Jiang et al., 2021) and has empirically examined calibration during the pre-training and alignment stages (Chen et al., 2022;Zhu et al., 2023). Studies such as Lin et al. (2022); Mielke et al. (2022); Xiong et al. (2023) have investigated uncertainty estimation through verbalized confidence, and Kadavath et al. (2022) demonstrated improved calibration of larger models when handling multiple choice and true/false questions given appropriate formats. Another line of research addresses the calibration of biases stemming from in-context samples, instruction templates, sample ordering, and label distribution (Zhao et al., 2021;Zhou et al., 2023b;Liu & Wang, 2023;Fei et al., 2023;Abbas et al., 2024). These bias calibration techniques indirectly influence the prediction confidence by altering the linear decision boundary (Zhou et al., 2023a), yet they are not designed for explicit confidence calibration. In contrast, our work specifically addresses the challenge of confidence calibration in instruction-tuned guard models for content moderation tasks.",
            "score": 0.47506081224081176,
            "section_title": "Calibration of LLMs.",
            "char_start_offset": 7004,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1317
                }
            ],
            "ref_mentions": [
                {
                    "start": 124,
                    "end": 141,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 141,
                    "end": 163,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 257,
                    "end": 277,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 437,
                    "end": 457,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 869,
                    "end": 888,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 941,
                    "end": 960,
                    "matchedPaperCorpusId": "267094982"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72900390625
        },
        {
            "corpus_id": "273403321",
            "title": "Atomic Calibration of LLMs in Long-Form Generations",
            "text": "Overall, the tested LLMs are not well-calibrated at the atomic fact level. (Guo et al., 2017) and (Zhu et al., 2023). However, even with the most robust method, GEN-BINARY, the ECE scores remain around 10%, indicating a significant calibration gap. Among the models, Qwen2-7B-Instruct demonstrates slightly better calibration compared to the other two. \n\nAtomic calibration can enhance macro calibration. Table 2 shows the main results of responselevel calibration. For the five atomic-level methods, we calculate the average confidence of the facts in a response to obtain the response-level confidence. \n\nThe results indicate that atomic calibration leads to better overall results compared to the baseline methods, highlighting the helpfulness of more finegrained calibration analysis. \n\nConfidence fusion can further improve both atomic and macro calibration. Table 4 presents the results of various confidence fusion strategies at the atomic level (with response-level results in Appendix D). The best results are consistently achieved by fusion methods. Among these strategies, the weighted average (WAvg) is the most effective. The benefits of the fusion strategies are further discussed in \u00a76.1. Interestingly, we find that combining methods within the same confidence type (such as DIS-RATING with DIS-CONTEXT) does not lead to improved calibration (in Appendix D). \n\nA case study showing the effectiveness of confidence fusion is in Figure 4. Larger model size does not necessarily result in better calibration. Table 3 compares the calibration levels of models with different sizes. Our two key findings are: (1) With generative methods, there is little difference in calibration between larger and smaller models; \n\n(2) With discriminative methods, larger models generally provide better calibration. We hypothesize that this is because discriminative methods require models to self-assess the confidence of their own outputs, and larger models typically possess stronger discriminative abilities (Saunders et al., 2022).",
            "score": 0.47421173042716214,
            "section_title": "Results",
            "char_start_offset": 14739,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 352
                },
                {
                    "start": 355,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 788
                },
                {
                    "start": 791,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1374
                },
                {
                    "start": 1377,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1725
                },
                {
                    "start": 1728,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 2033
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 116,
                    "matchedPaperCorpusId": "265351565"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6494140625
        },
        {
            "corpus_id": "259859030",
            "title": "Making Pre-trained Language Models both Task-solvers and Self-calibrators",
            "text": "We have witnessed the great success of pre-trained language models (PLMs) over the past few years in various tasks (Wang et al., 2019a,b). Nowadays, real-world natural language processing (NLP) systems are mostly built upon PLMs to effectively  utilize their strong capacities (Bommasani et al., 2021;Han et al., 2021).\n\nBeyond the performance evaluation, an essential requirement in high-stake applications for PLMs is to assign reasonable confidence to their predictions. This can enable the decision-makers to better handle the low-confident predictions, e.g., directly abstain to give predictions or transfer the input to human experts. The original predictive probability of PLMs can be effectively utilized for ranking predictions. This simple strategy can reasonably give relatively higher confidence to correct predictions compared to the wrong ones (Hendrycks et al., 2020a).\n\nHowever, relying on the vanilla confidence scores cannot well distinguish between correct and wrong predictions. PLMs consistently assign high confidence in their predictions, no matter correct or not (Chen et al., 2022b). This results in a large number of wrong predictions distributed in the high-confident zone (see Figure 1). The direct undesirable consequence is the false acceptance of wrong but high-confident predictions. Besides, previous work avoids the issue of selecting a concrete confidence threshold by using hyperparameter-free metrics (e.g., AUROC) in relevant tasks (e.g., selective classification). But in practice, the small gap between confidence in correct and wrong predictions may cause large performance variance due to the manually chosen threshold.\n\nExisting work shows that an extra calibration task can be taken as a remedy (Chen et al., 2022b;Lin et al., 2022). The calibration task uses extra samples to train models to have reasonable confidence estimations. However, previous work considers ideal situations to demonstrate the feasibility, assuming access to a large number of unused labeled samples, typically from the validation set. In practice, the samples in the validation dataset may be too small to guarantee good calibration performance. Besides, relying on the validation samples for the calibration task training causes data leakage, which may result in unreliable performance estimation when adopting the validation dataset to choose",
            "score": 0.47405486168213334,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75390625
        },
        {
            "corpus_id": "256846523",
            "title": "Bag of Tricks for In-Distribution Calibration of Pretrained Transformers",
            "text": "While pre-trained language models (PLMs) have become a de-facto standard promoting the accuracy of text classification tasks, recent studies find that PLMs often predict over-confidently.Although calibration methods have been proposed, such as ensemble learning and data augmentation, most of the methods have been verified in computer vision benchmarks rather than in PLM-based text classification tasks. In this paper, we present an empirical study on confidence calibration for PLMs, addressing three categories, including confidence penalty losses, data augmentations, and ensemble methods. We find that the ensemble model overfitted to the training set shows sub-par calibration performance and also observe that PLMs trained with confidence penalty loss have a trade-off between calibration and accuracy. Building on these observations, we propose the Calibrated PLM (CALL), a combination of calibration techniques. The CALL complements shortcomings that may occur when utilizing a calibration method individually and boosts both classification and calibration accuracy. Design choices in CALL\u2019s training procedures are extensively studied, and we provide a detailed analysis of how calibration techniques affect the calibration performance of PLMs.",
            "score": 0.47390604669222347,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96484375
        },
        {
            "corpus_id": "278129791",
            "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review",
            "text": "We follow the setting for baselines as in [11], with some additional baselines explained as follows: normalized and average sequence likelihood with few-shot learning, few-shot learning and CoT tokens, and recent method of verbalized qualitative uncertainty [10], [21], [83], [200] method to obtain confidence of model. We ask verbally to provide confidence from \"very high\" to \"very low\" and then map this scale to numerical values using the scheme we describe in Appendix A-B. In addition, we apply Platt Scaling [97] to average sequence likelihoods and TS [28] to logit scores of generated sequences for obtaining scaled sequence likelihoods. \n\nAnalyses. In this section, we will discuss the suitability of methods and metrics for LLMs based on our experimental evaluation. In general, we notice that calibration methods can make models provide a more accurate probability of their predictions, hence making them more reliable, as noted in Table V. We bold the best results per dataset and model and underline those that are statistically significant compared to all other results after multiple experimental runs. \n\nFor TriviaQA dataset, we provide reliability diagrams for Qwen2.5 in Figure 4, Llama3.1 in Figure 2, and Mistral v0.3 in Figure 3 . We provide the remaining diagrams in Appendix B. We observe that the Qwen2.5 model exhibits low calibration and accuracy. As this model is a multi-lingual model and domain knowledge is also different, it can affect the accuracy of generation and the language mixing problem [4]. In this context, the calibration of LLMs across different domains, such as in scenarios involving dataset shift [21], remains an interesting and open problem for further investigation. Parametric calibration methods make bin-wise confidences align with bin-wise accuracy, hence making the model more calibrated. TS can be implemented in two ways: (1) scaling the model confidences or log probabilities of candidate sequences [11], [199], [200] or (2) scaling the logits before the softmax layer, which operates on the vocabulary size.",
            "score": 0.4717109039029965,
            "section_title": "V. RQ3: EVALUATION",
            "char_start_offset": 43238,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1117
                },
                {
                    "start": 1120,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 2065
                }
            ],
            "ref_mentions": [
                {
                    "start": 515,
                    "end": 519,
                    "matchedPaperCorpusId": "56563878"
                },
                {
                    "start": 559,
                    "end": 563,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72119140625
        },
        {
            "corpus_id": "238419596",
            "title": "Improving Confidence Estimation on Out-of-Domain Data for End-to-End Speech Recognition",
            "text": "The simplest baseline for confidence estimation is to directly use the softmax probability from the decoder output as the confidence scores. As discussed in [21], the softmax-based confidence scores can be severely impacted by the regularisation techniques used during training. As shown in Tables 1 and 2, by having a dedicated confidence estimation module at either the word-level [21] or utterancelevel [24], the in-domain performance on LibriSpeech test-clean/other is greatly improved. Especially at the utterance level, the AUC increases from around 70% to around 90% while the EER is approximately halved. On the two OOD sets, the benefit of using CEM or R-EBM is also noticeable. However, the relative improvement of confidence estimation performance by using a confidence module diminishes as the data becomes increasingly dissimilar to the in-domain data. For example, in Table 2, the AUC on TED increased from 33.57% to 71.24% whereas the AUC on SWBD only increased from 34.91% to 52.31%. The EER on TED is reduced by more than half whereas the EER on SWBD is only reduced by 23% relatively. This is expected as the confidence module is trained only on in-domain data.",
            "score": 0.47158153559835514,
            "section_title": "Baselines",
            "char_start_offset": 11947,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1179
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 161,
                    "matchedPaperCorpusId": "225041216"
                },
                {
                    "start": 383,
                    "end": 387,
                    "matchedPaperCorpusId": "225041216"
                },
                {
                    "start": 406,
                    "end": 410,
                    "matchedPaperCorpusId": "232380109"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65576171875
        },
        {
            "corpus_id": "261065011",
            "title": "Uncertainty Estimation of Transformers' Predictions via Topological Analysis of the Attention Matrices",
            "text": "While this approach produces reliable confidence scores, the computational overhead for the training and inference becomes a major concern (Izmailov et al., 2018). \n\nMore efficient approaches reduce the computational overhead by training much smaller auxiliary models to approximate confidence (Kendall and Gal, 2017;Kail et al., 2022) or using Mahalanobis distance between samples in the latent space as a proxy for confidence (Lee et al., 2018). Such techniques achieve superior performance for computer vision tasks, however, their potential for natural language processing tasks is not fully explored. \n\nAs a result, a research gap still exists in the field of uncertainty estimation for language models. Although recent studies (Shelmanov et al., 2021;Vazhentsev et al., 2022) propose modifications that make MC Dropout and the Mahalanobis Estimator optimal in terms of the trade-off between accuracy and efficiency, their improvements over the naive Softmax Response method are not always significant. Morever, these wThis leaves room for further enhancement, which we achieve by exploiting the built-in capability of Transformers to capture the relations between tokens.",
            "score": 0.47157831229326425,
            "section_title": "Uncertainty Estimation",
            "char_start_offset": 6164,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 166,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 605
                },
                {
                    "start": 608,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1177
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 162,
                    "matchedPaperCorpusId": "3833416"
                },
                {
                    "start": 428,
                    "end": 446,
                    "matchedPaperCorpusId": "49667948"
                },
                {
                    "start": 733,
                    "end": 757,
                    "matchedPaperCorpusId": "231993280"
                },
                {
                    "start": 757,
                    "end": 781,
                    "matchedPaperCorpusId": "248780161"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66015625
        },
        {
            "corpus_id": "271878082",
            "title": "Program-Aided Reasoners (better) Know What They Know",
            "text": "Calibration has been extensively studied in structured prediction problems, such as named entity recognition and part of speech tagging (Jagannatha and Yu, 2020), as well as in natural language understanding tasks, like question answering and text classification (Kamath et al., 2020;Kong et al., 2020;Desai and Durrett, 2020). More recently, studies have directed their attention to calibrating language models when used as generators (Jiang et al., 2021;Zhao et al., 2021). Additionally, the study by Kadavath et al. (2022) explored the likelihood of a model knowing the answer before proposing a response. However, all of these approaches typically rely on access to the model's logits. \n\nIn contrast, the work by (Tian et al., 2023) investigates verbalized probability estimates to assess the calibration of large language models without needing access to logits. This involves the practice of querying the model about its confidence in the answers it generates. Furthermore, (Xiong et al., 2023b) introduced self-consistency-based methods for calibration, demonstrating their superior performance compared to verbalized methods. In our research, we adopt self-consistency as the method of choice for measuring calibration. Across datasets, the accuracy and calibration improve upon lower the temperature up to a certain extent. This is in line with having lower generation similarity and lower answer entropy. The optimal temperatures seen are 0.5 and 0.7 across datasets. For other datasets, refer Appendix, Figure 6. calibration of OpenAI models and LLaMA models. Our investigation spans 5 reasoning datasets, employing self-consistency as the methodology for eliciting calibration. We analyze four different metrics -calibration (ECE) , accuracy (ACC) , average similarity in generations (SIM) , and answer entropy (ENT) . We summarize our findings as follows: \n\n\u2022 RQ 1: Does one prompting style result in significantly better calibration than the other? Empirical results show that PAL generally has higher calibration and accuracy for 82.5% of the cases across OpenAI and LLaMA models for a varied range of temperatures.",
            "score": 0.4711145598296074,
            "section_title": "Calibration in Language Models",
            "char_start_offset": 18709,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 689
                },
                {
                    "start": 692,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1868
                },
                {
                    "start": 1871,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2130
                }
            ],
            "ref_mentions": [
                {
                    "start": 136,
                    "end": 161,
                    "matchedPaperCorpusId": "215548393"
                },
                {
                    "start": 436,
                    "end": 456,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 456,
                    "end": 474,
                    "matchedPaperCorpusId": "231979430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65673828125
        },
        {
            "corpus_id": "273403321",
            "title": "Atomic Calibration of LLMs in Long-Form Generations",
            "text": "While large language models (LLMs), such as Llama (Touvron et al., 2023), Mistral (Jiang et al., 2023), and GPT models (OpenAI, 2022), excel in various tasks, they still struggle with faithfulness and reliability issues. LLMs often suffer from hallucinations, generating factually inaccurate content and misleading responses (Zhang et al., 2023b;Huang et al., 2023), which limits their application in high-risk real-world scenarios (Hu et al., 2023). To address this, confidence calibration aims to estimate the underlying uncertainty of model predic-Pembroke College is one of the 31 colleges of the \u2026 \u2026 Its members are termed \"Valencians\". tions and reflect the true likelihood of correctness (Guo et al., 2017). A calibrated model is crucial for real-world applications, as it allows us to determine the extent to which we can trust models' predictions (Zhu et al., 2023;Mahaut et al., 2024). \n\nMost existing work on LLM calibration focuses on short-form QA tasks (Jiang et al., 2021;Tian et al., 2023;Zhu et al., 2023;Ulmer et al., 2024). These studies often use datasets like TriviaQA (Joshi et al., 2017) and Natural Questions (Joshi et al., 2017), where answers typically have fewer than 10 words. However, in real-world applications, responses to user queries are often much longer (Zhang et al., 2024), sometimes extending to hundreds or even thousands of words. In such cases, the quality of LLM responses is not simply binary (correct or incorrect), as answers may in-clude both accurate and inaccurate statements. Previous work on long-form calibration has primarily focused on response-level calibration (Zhang et al., 2024;Huang et al., 2024) (which we term macro calibration). This approach provides an overall confidence estimation for the entire response (as shown in the upper part of Figure 1).",
            "score": 0.47035401918191005,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 895
                },
                {
                    "start": 898,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1813
                }
            ],
            "ref_mentions": [
                {
                    "start": 856,
                    "end": 874,
                    "matchedPaperCorpusId": "265351565"
                },
                {
                    "start": 874,
                    "end": 894,
                    "matchedPaperCorpusId": "270619760"
                },
                {
                    "start": 967,
                    "end": 987,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 987,
                    "end": 1005,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 1005,
                    "end": 1022,
                    "matchedPaperCorpusId": "265351565"
                },
                {
                    "start": 1022,
                    "end": 1041,
                    "matchedPaperCorpusId": "268358242"
                },
                {
                    "start": 1090,
                    "end": 1110,
                    "matchedPaperCorpusId": "26501419"
                },
                {
                    "start": 1133,
                    "end": 1153,
                    "matchedPaperCorpusId": "26501419"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.425537109375
        },
        {
            "corpus_id": "278129791",
            "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review",
            "text": "Calibration, initially developed for forecasting [92]- [94], is a technique to assess and improve the quality of uncertainty estimates by ensuring that models provide more transparent and reliable predictive probabilities [25]. Since modern deep learning models frequently rely on softmax output probabilities to estimate uncertainty, improper calibration can lead to misleading confidence scores, which is particularly problematic in high-stakes decision-making tasks. To achieve proper calibration, a model's predicted confidence level (e.g., the softmax score \u03b1) should align with its actual accuracy [41]. In other words, among all instances where the model assigns a confidence of \u03b1, the proportion of correct predictions should approximate \u03b1. This alignment can be evaluated through techniques such as reliability diagrams, expected calibration error (ECE), and temperature scaling (TS) [28]. A classification model can be considered calibrated if: \n\nwhere f (X) function that maps the feature space X and label space Y , while pf represents the predicted confidence \u03b1 of f . To assess calibration, we can partition this confidence into intervals and construct reliability diagrams [14], which visualize the relationship between predicted confidence and actual accuracy within each interval. These diagrams help identify whether the model is under-confident, over-confident, or wellcalibrated, providing insights into model reliability based on bin-wise accuracy and confidence. While achieving perfect accuracy in real-world scenarios is often infeasible [29], perfect calibration of uncertainty remains an attainable goal [95], [96]. \n\nPost-hoc or post-training methods, such as TS [28], Platt scaling [97], histogram binning [98], and training with label smoothing [99], are classical methods of calibration. Mixup training with learning the complexity of samples was proposed to calibrate the pre-trained language models [100]. We note that open-box methods of calibration can be complex and difficult to apply to LLMs, but they can be more informative of inner states than post-hoc methods [101].",
            "score": 0.47003931892317397,
            "section_title": "A. Model Calibration",
            "char_start_offset": 18857,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 954
                },
                {
                    "start": 957,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1641
                },
                {
                    "start": 1644,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2107
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 53,
                    "matchedPaperCorpusId": "127203550"
                },
                {
                    "start": 55,
                    "end": 59,
                    "matchedPaperCorpusId": "109884250"
                },
                {
                    "start": 604,
                    "end": 608,
                    "matchedPaperCorpusId": "202718866"
                },
                {
                    "start": 893,
                    "end": 897,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1188,
                    "end": 1192,
                    "matchedPaperCorpusId": "207158152"
                },
                {
                    "start": 1690,
                    "end": 1694,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1710,
                    "end": 1714,
                    "matchedPaperCorpusId": "56563878"
                },
                {
                    "start": 1734,
                    "end": 1738,
                    "matchedPaperCorpusId": "9594071"
                },
                {
                    "start": 1774,
                    "end": 1778,
                    "matchedPaperCorpusId": "174802983"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.953125
        },
        {
            "corpus_id": "274789494",
            "title": "A Survey of Calibration Process for Black-Box LLMs",
            "text": "Calibration aims to align confidence scores with correctness. Commonly used calibration methods include temperature scaling (Guo et al., 2017), label smoothing (Szegedy et al., 2016), and logistic calibration (Kull et al., 2017). However, methods like label smoothing and logistic calibration are unsuitable for black-box LLMs, as they require internal model access or fine-tuning, whereas temperature scaling, which operates on the logits of model outputs, is better suited for gray-box models. In black-box LLMs, common calibration methods include post-processing or transforming the model into a gray-box. Due to the near-well-calibrated nature of Confidence Estimation in black-box LLMs, calibration pressure is reduced. This section discusses the subsequent steps required to achieve well-calibration following Confidence Estimation. \n\nIn post-processing methods, popular techniques include Histogram Binning (Zadrozny and Elkan, 2001) and Isotonic Regression (Jiang et al., 2011). Histogram Binning (Zadrozny and Elkan, 2001) adjusts confidence values within different intervals by assessing the correctness of predictions in each interval, providing a simple but potentially limited calibration when confidence distributions are uneven. Isotonic Regression (Jiang et al., 2011), on the other hand, seeks an optimal monotonically increasing function f (c) to map the model's confidence c 1 , c 2 , . . . , c n to correctness. f (c) need to satisfy the condition in Formula 2, ensuring that the confidence ranking aligns with the correctness. \n\nThough flexible, Isotonic Regression is less effective in classification tasks as it doesn't directly modify the decision boundary. \n\nTo address more complex scenarios, some methods introduce additional constraints. For instance, H\u00e9bert-Johnson et al. ( 2018) introduced a multicalibration method, ensuring not only overall alignment but also calibration across multiple subgroups S j through iterative alignment. The overall goal can be expressed as Formula 3. \n\n(3) \n\nWhile this method promotes fairness, it is computationally expensive and depends heavily on subgrouping strategies, usually relying on known features.",
            "score": 0.468478230837678,
            "section_title": "Calibration Methods",
            "char_start_offset": 18690,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 838
                },
                {
                    "start": 841,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1547
                },
                {
                    "start": 1550,
                    "end": 1681
                },
                {
                    "start": 1684,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2011
                },
                {
                    "start": 2014,
                    "end": 2017
                },
                {
                    "start": 2020,
                    "end": 2170
                }
            ],
            "ref_mentions": [
                {
                    "start": 124,
                    "end": 142,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 160,
                    "end": 182,
                    "matchedPaperCorpusId": "206593880"
                },
                {
                    "start": 209,
                    "end": 228,
                    "matchedPaperCorpusId": "217264968"
                },
                {
                    "start": 914,
                    "end": 940,
                    "matchedPaperCorpusId": "3094990"
                },
                {
                    "start": 965,
                    "end": 985,
                    "matchedPaperCorpusId": "14711865"
                },
                {
                    "start": 1005,
                    "end": 1031,
                    "matchedPaperCorpusId": "3094990"
                },
                {
                    "start": 1264,
                    "end": 1284,
                    "matchedPaperCorpusId": "14711865"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93408203125
        },
        {
            "corpus_id": "215548393",
            "title": "Calibrating Structured Output Predictors for Natural Language Processing",
            "text": "We address the problem of calibrating prediction confidence for output entities of interest in natural language processing (NLP) applications. It is important that NLP applications such as named entity recognition and question answering produce calibrated confidence scores for their predictions, especially if the applications are to be deployed in a safety-critical domain such as healthcare. However the output space of such structured prediction models are often too large to directly adapt binary or multi-class calibration methods. In this study, we propose a general calibration scheme for output entities of interest in neural network based structured prediction models. Our proposed method can be used with any binary class calibration scheme and a neural network model. Additionally, we show that our calibration method can also be used as an uncertainty-aware, entity-specific decoding step to improve the performance of the underlying model at no additional training cost or data requirements. We show that our method outperforms current calibration techniques for Named Entity Recognition, Part-of-speech tagging and Question Answering systems. We also observe an improvement in model performance from our decoding step across several tasks and benchmark datasets. Our method improves the calibration and model performance on out-of-domain test scenarios as well.",
            "score": 0.46643136083894776,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9140625
        },
        {
            "corpus_id": "273345661",
            "title": "Taming Overconfidence in LLMs: Reward Calibration in RLHF",
            "text": "This paper addresses the issue of overconfidence in RLHF-LLMs by identifying a systematic bias in reward models that favors high-confidence responses, regardless of their actual quality. A RELATED WORKS LLM Calibration. Model Calibration aims to align a model's confidence with its accuracy. It has been observed that modern neural networks, including Large Language Models (LLMs), often exhibit overconfidence, suggesting poor calibration (Tian et al., 2023;Chen et al., 2024a;Xiong et al., 2023;Achiam et al., 2023). Previous studies have explored methods like scaling-based (Deng et al., 2023;Guo et al., 2017;Zhang et al., 2020) approaches and nonparametric methods such as binning (Zadrozny & Elkan, 2001). Among these, temperature scaling (Guo et al., 2017;Zhang et al., 2020) has been proven to be effective when combined with large pre-trained LLMs (Kadavath et al., 2022;Xiao et al., 2022;Kuhn et al., 2023). However, previous evaluations focus on probabilities derived from model logits (Hendrycks et al., 2020;Mukhoti et al., 2020;Guo et al., 2017;Minderer et al., 2021), which can sometimes be inaccessible in proprietary models and unclear to human users. \n\nRecently, verbalized confidence has been introduced (Lin et al., 2022), prompting models to directly output confidence scores alongside responses. While most studies focus on calibrating pre-trained LLMs through supervised fine-tuning (Lin et al., 2022;Han et al., 2024), which typically involves sampling responses and calculating average accuracy as the estimation for ground truth confidence scores, other studies have examined verbalized confidence in instruction fine-tuned and RLHF-trained LLMs, and propose calibration through prompting strategies (Xiong et al., 2023;Tian et al., 2023).",
            "score": 0.4662428945674383,
            "section_title": "CONCLUSION",
            "char_start_offset": 24478,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1168
                },
                {
                    "start": 1171,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1765
                }
            ],
            "ref_mentions": [
                {
                    "start": 596,
                    "end": 613,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 613,
                    "end": 632,
                    "matchedPaperCorpusId": "212725167"
                },
                {
                    "start": 686,
                    "end": 710,
                    "matchedPaperCorpusId": "9594071"
                },
                {
                    "start": 745,
                    "end": 763,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 763,
                    "end": 782,
                    "matchedPaperCorpusId": "212725167"
                },
                {
                    "start": 1021,
                    "end": 1042,
                    "matchedPaperCorpusId": "211252346"
                },
                {
                    "start": 1042,
                    "end": 1059,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1059,
                    "end": 1081,
                    "matchedPaperCorpusId": "235435823"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81640625
        },
        {
            "corpus_id": "278165590",
            "title": "Bi-directional Model Cascading with Proxy Confidence",
            "text": "A model's calibration is its ability to express some level of confidence associated with an output that accurately reflects the likelihood that it is correct [13]. LLM calibration generally has great value to researchers and industry applications, and is the subject of significant research efforts [12]. The acceptance of LLMs in real-world application depends on their trustworthiness and reliability [4], while in cascading context, calibration has direct and measurable benefits on performance. The success of any model cascade depends on the quality of its deferral process, and therefore the calibration of its component models or accuracy of its confidence metric . \n\nThere are many approaches to approximating LLM confidence. A large subset of these methods involves deriving the confidence from the log-probabilities of the model outputs. In the case of LLMs, these log-probabilities, or \"logits\", reflect a distribution over tokens and the confidence can be estimated using some variation of entropy or maximum probability at the individual token or sequence level. Some techniques that have shown recent promise involve examining consistency across multiple stochastic or permuted generations [18], but the additional computation required makes these approaches poorly suited to efficiency optimizations. Other researchers have asked generative models to explicitly report their confidence in their output with mixed results [22,32], but benchmarks in some domains have found self reported confidence to be very poor, especially among smaller models [29]. \n\nDifferent layers in a language model capture different types of information and fulfill different roles in inference [41]. \n\nOne method of identifying and promoting fact-based and truthful outputs from LLMs is to consider the likelihood of different tokens at different layers within the model, emphasizing the effect of layers most likely to inject knowledge [8]. The internal layers of a pretrained model inform output rather than generate output directly and may hold useful information without the same biases as the output layer [19]. Other, similar techniques applying an output head over internal layers have been used in early-layer exit strategies [40], improve interpretability [52], and to better understand how the internal layers and mechanisms function in LLMs [15,42].",
            "score": 0.46518608270656836,
            "section_title": "LLM Calibration",
            "char_start_offset": 8103,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 672
                },
                {
                    "start": 675,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1566
                },
                {
                    "start": 1569,
                    "end": 1691
                },
                {
                    "start": 1694,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2352
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 162,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 299,
                    "end": 303,
                    "matchedPaperCorpusId": "265157516"
                },
                {
                    "start": 403,
                    "end": 406,
                    "matchedPaperCorpusId": "226965491"
                },
                {
                    "start": 1204,
                    "end": 1208,
                    "matchedPaperCorpusId": "276349944"
                },
                {
                    "start": 1686,
                    "end": 1690,
                    "matchedPaperCorpusId": "155092004"
                },
                {
                    "start": 1929,
                    "end": 1932,
                    "matchedPaperCorpusId": "261582463"
                },
                {
                    "start": 2103,
                    "end": 2107,
                    "matchedPaperCorpusId": "246652372"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7861328125
        },
        {
            "corpus_id": "267938751",
            "title": "C3: Confidence Calibration Model Cascade for Inference-Efficient Cross-Lingual Natural Language Understanding",
            "text": "In this paper, we propose a simple, yet effective method, Confidence Calibration Cascade (C 3 ) that enhances cross-lingual inference accuracy through more reliable confidence scores. We introduce calibration methods for both Language Model cascade and Large Language Model cascade. We conduct extensive experiment on cross-lingual benchmarks. By comparing with state-of-the-art methods, the results demonstrates the effectiveness of C 3 . Furthermore, C 3 also demonstrates strong calibration results compared to vanilla cascade methods.",
            "score": 0.4642747588997227,
            "section_title": "Conclusion",
            "char_start_offset": 23824,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 538
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93359375
        },
        {
            "corpus_id": "250959353",
            "title": "Failure Detection in Deep Neural Networks for Medical Imaging",
            "text": "Many different approaches have been proposed in the literature. Generally, the output of the softmax function, where available, is used as the model confidence (31). However, the softmax output is not a calibrated measure of model confidence (15). The raw softmax probabilities overestimate model confidence for the correct as well as incorrect predictions (15). We categorize the existing non-Bayesian methods into three main areas; (1) calibration techniques, (2) ad-hoc methods, and (3) ensemble techniques, as explained in the following paragraphs. The calibration techniques help models adjust the softmax output values such that these values represent the model confidence. Among calibration techniques, temperature scaling uses a scalar parameter to re-scale logit scores (15). Temperature scaling with small perturbation in input data improves the detection of out-of-distribution samples (32). Label smoothing is another calibration technique that diffuses the one-hot encoded The terms ver-1 and ver-2 correspond to different hyperparameter settings.\n\nlabels using a small probability value for incorrect classes. There are various methods for label smoothing, including a data augmentation strategy called mixup (33,34). Mixup employs vicinal risk minimization, which uses additional data samples and labels from the original dataset. Moon et al. (35) proposed training DNNs with a different loss function, named correctness ranking loss. The correctness ranking loss regularizes the class probabilities such that these values are ordinal rankings based on the true model confidence (35). The Deep Gamblers technique is based on the portfolio theory and uses a loss function designed for selective classification problems using the doubling rate of gambling (36). It transforms the m-class classification problem into m+1 classes, with the additional class representing whether the model abstains from making a decision due to low confidence. Calibration techniques are computationally efficient. However, the model outputs decisions and represents the associated confidence in those decisions using the same softmax output values. Calibration techniques are generally not accurate and (except for correctness ranking loss) do not differentiate between the confidence values of correct and incorrect decisions. Ad-hoc methods generally extend the model architecture to predict",
            "score": 0.4639205762147362,
            "section_title": "Non-Bayesian Methods",
            "char_start_offset": 31275,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 242,
                    "end": 246,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 357,
                    "end": 361,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 779,
                    "end": 783,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1223,
                    "end": 1227,
                    "matchedPaperCorpusId": "166228660"
                },
                {
                    "start": 1358,
                    "end": 1362,
                    "matchedPaperCorpusId": "220347323"
                },
                {
                    "start": 1594,
                    "end": 1598,
                    "matchedPaperCorpusId": "220347323"
                },
                {
                    "start": 1769,
                    "end": 1773,
                    "matchedPaperCorpusId": "195767452"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3271484375
        },
        {
            "corpus_id": "247778884",
            "title": "Towards Improving Selective Prediction Ability of NLP Systems",
            "text": "Kamath et al. (2020) study a calibration-based selective prediction technique for Question Answering datasets where they annotate a held-out dataset such that correctly predicted instances are assigned class label '1' and incorrect ones are assigned label '0'. Then, a calibrator is trained using this annotated binary classification dataset using features such as input length and probabilities of top 5 predictions. The softmax probability assigned to class '1' by this calibrator is used as the confidence estimator for selective prediction. We refer to this approach as Calib C. We also train a transformer-based model for calibration (Calib T) that leverages the entire input text for this classifi-cation task instead of the syntactic features (Garg and Moschitti, 2021).\n\nOur proposed calibration method differs from these approaches as we quantify the correctness on a continuous scale (instead of categorical labels '1' and '0') using prediction confidence and difficulty of the instances and use explicitly provided general syntactic features described in Section 3.3 for training. Our annotation procedure provides more flexibility for the calibrator to look for finegrained features distinguishing various annotation scores. We note that our simplest annotation strategy (AS 1 ) that does not incorporate difficulty score is similar to Calib R method described in Varshney et al. (2022) but our calibration method uses more general syntactic features.\n\nNote that for fair estimation of abilities of the proposed method, we compare it with other calibration-based techniques only. Other techniques such as Monte-Carlo dropout (Gal and Ghahramani, 2016) and Error Regularization (Xin et al., 2021) are complementary and can further improve our performance.",
            "score": 0.461497788567023,
            "section_title": "Calibration Baselines",
            "char_start_offset": 8334,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 750,
                    "end": 776,
                    "matchedPaperCorpusId": "237513496"
                },
                {
                    "start": 1637,
                    "end": 1663,
                    "matchedPaperCorpusId": "160705"
                },
                {
                    "start": 1689,
                    "end": 1707,
                    "matchedPaperCorpusId": "235719735"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84130859375
        },
        {
            "corpus_id": "277113491",
            "title": "Evaluating ASR Confidence Scores for Automated Error Detection in User-Assisted Correction Interfaces",
            "text": "Confidence measures are used for various ASR downstream tasks, particularly ML methods like unsupervised [69], semi-supervised [7,24,42], and active learning [23,35,68]. They are also used for speaker adaptation [12,60], keyword spotting [2,29,53], and dialogue systems [20,59]. In HMM-based ASR, confidence is often predicted using binary classifiers on decoding features such as word posterior probabilities [15], word trellis stability [48], normalised likelihoods, and language model score [65]. These features are classified using simple linear mappings, such as decision trees [15] and Gaussian mixture classifiers [10], or sequence models such as conditional random fields [51], neural networks [49,64], and recurrent neural networks [27,28,36]. ASR systems have evolved from HMM and hybrid designs to E2E models, which rely on an encoder-decoder architecture [8,14,30]. A common confidence measure for these models, the softmax probability [42], is often unreliable due to the overconfident behaviour of E2E models [19,22,37]. To improve the reliability of confidence scores, a model can be calibrated using temperature scaling [19]. However, mitigating high confidence predictions for Out-of-Domain (OOD) data remains challenging and may be an inherent limitation of neural network architectures [21]. While approaches such as dropout, ensembles, or Bayesian methods can improve uncertainty estimation for some tasks, the accuracy also degrades under dataset shifts [33,41]. Alternatively, a separate neural network can estimate confidence [37,66], using features from the encoder, decoder, and attention blocks [32,56]. It has also been explored to improve confidence scores around word deletions [45,52]. \n\nE2E models have become the industry standard due to their improved accuracy, but they also exhibit overconfidence. Several techniques can improve their confidence estimation, but none has proven superior. Research suggests that confidence measures become less accurate as ASR accuracy decreases.",
            "score": 0.4612058985410389,
            "section_title": "Confidence Measures in Machine Learning",
            "char_start_offset": 3658,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1715
                },
                {
                    "start": 1718,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2013
                }
            ],
            "ref_mentions": [
                {
                    "start": 105,
                    "end": 109,
                    "matchedPaperCorpusId": "12231265"
                },
                {
                    "start": 127,
                    "end": 130,
                    "matchedPaperCorpusId": "13601225"
                },
                {
                    "start": 130,
                    "end": 133,
                    "matchedPaperCorpusId": "2097513"
                },
                {
                    "start": 133,
                    "end": 136,
                    "matchedPaperCorpusId": "218684988"
                },
                {
                    "start": 158,
                    "end": 162,
                    "matchedPaperCorpusId": "12369624"
                },
                {
                    "start": 162,
                    "end": 165,
                    "matchedPaperCorpusId": "9875181"
                },
                {
                    "start": 165,
                    "end": 168,
                    "matchedPaperCorpusId": "11364150"
                },
                {
                    "start": 212,
                    "end": 216,
                    "matchedPaperCorpusId": "13843598"
                },
                {
                    "start": 238,
                    "end": 241,
                    "matchedPaperCorpusId": "14458969"
                },
                {
                    "start": 241,
                    "end": 244,
                    "matchedPaperCorpusId": "1782763"
                },
                {
                    "start": 244,
                    "end": 247,
                    "matchedPaperCorpusId": "15825609"
                },
                {
                    "start": 270,
                    "end": 274,
                    "matchedPaperCorpusId": "3127409"
                },
                {
                    "start": 274,
                    "end": 277,
                    "matchedPaperCorpusId": "6204581"
                },
                {
                    "start": 410,
                    "end": 414,
                    "matchedPaperCorpusId": "11673607"
                },
                {
                    "start": 439,
                    "end": 443,
                    "matchedPaperCorpusId": "43341512"
                },
                {
                    "start": 494,
                    "end": 498,
                    "matchedPaperCorpusId": "14077199"
                },
                {
                    "start": 583,
                    "end": 587,
                    "matchedPaperCorpusId": "11673607"
                },
                {
                    "start": 621,
                    "end": 625,
                    "matchedPaperCorpusId": "61407700"
                },
                {
                    "start": 680,
                    "end": 684,
                    "matchedPaperCorpusId": "28442245"
                },
                {
                    "start": 702,
                    "end": 706,
                    "matchedPaperCorpusId": "6012818"
                },
                {
                    "start": 706,
                    "end": 709,
                    "matchedPaperCorpusId": "14011613"
                },
                {
                    "start": 741,
                    "end": 745,
                    "matchedPaperCorpusId": "14484284"
                },
                {
                    "start": 745,
                    "end": 748,
                    "matchedPaperCorpusId": "204976389"
                },
                {
                    "start": 748,
                    "end": 751,
                    "matchedPaperCorpusId": "53110756"
                },
                {
                    "start": 867,
                    "end": 870,
                    "matchedPaperCorpusId": "18165915"
                },
                {
                    "start": 870,
                    "end": 873,
                    "matchedPaperCorpusId": "52287921"
                },
                {
                    "start": 948,
                    "end": 952,
                    "matchedPaperCorpusId": "218684988"
                },
                {
                    "start": 1023,
                    "end": 1027,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1030,
                    "end": 1033,
                    "matchedPaperCorpusId": "225041216"
                },
                {
                    "start": 1136,
                    "end": 1140,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1305,
                    "end": 1309,
                    "matchedPaperCorpusId": "55700923"
                },
                {
                    "start": 1475,
                    "end": 1479,
                    "matchedPaperCorpusId": "6294674"
                },
                {
                    "start": 1479,
                    "end": 1482,
                    "matchedPaperCorpusId": "174803437"
                },
                {
                    "start": 1549,
                    "end": 1553,
                    "matchedPaperCorpusId": "225041216"
                },
                {
                    "start": 1553,
                    "end": 1556,
                    "matchedPaperCorpusId": "226200399"
                },
                {
                    "start": 1621,
                    "end": 1625,
                    "matchedPaperCorpusId": "226201886"
                },
                {
                    "start": 1625,
                    "end": 1628,
                    "matchedPaperCorpusId": "271270569"
                },
                {
                    "start": 1707,
                    "end": 1711,
                    "matchedPaperCorpusId": "233407794"
                },
                {
                    "start": 1711,
                    "end": 1714,
                    "matchedPaperCorpusId": "11390852"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63623046875
        },
        {
            "corpus_id": "268876453",
            "title": "Calibrating the Confidence of Large Language Models by Eliciting Fidelity",
            "text": "Dataset. We have conducted experiments on four MCQA datasets to verify the effectiveness of our proposed confidence estimation method. ARC (Clark et al., 2018) is a dataset of 7,787 grade-school-level questions. We use the test split of the ARC-Challenge with 1,172 questions for our experiments. MMLU (Hendrycks et al., 2021) is a dataset designed to measure knowledge acquired during pretraining and covers 57 subjects. TruthfulQA (Lin et al., 2022b) is a dataset that contains 817 questions designed to evaluate language models' preference to mimic some human falsehoods. All the experiments are conducted under a 0-shot setting. \n\nMetrics. We utilize multiple metrics to evaluate. We bin the predictions from the model by their confidence and report the ECE (expected calibration error). We also report the Brier Score of different methods in Table 11. In this paper, we also defines two novel metrics to evaluate the calibration. The first one is IPR (Inverse Pair Ratio), which is used to measure the monotonicity of the reliability diagram. If the reliability diagram is monotonic, it indicates that the average accuracy of answers with low confidence is lower than the average accuracy of answers with high confidence. \n\nwhere IP is the inverse pair number in the reliable diagram, and K is the bin number with a density larger than 0. We found that as the number of model parameters increases, the accuracy of the model improves across various datasets. However, language models still tend to consistently express uncertainty within certain fixed ranges, and ECE cannot clearly reflect this phenomenon. Therefore, we suggest using the CE (Confidence Evenness) to evaluate the uniformity of the density of each bar in the reliability diagram. \n\nIn this paper, we adopt 10 equal-size bins to calculate ECE 10 , IPR 10 and CE 10 . We also report the accuracy on these benchmarks to measure whether calibration reduces the accuracy. \n\nBaselines. We compared our approach with different baselines for eliciting the confidence of language model. First, we reproduced the Verb and Ling method proposed by Tian et al. (2023).",
            "score": 0.4607742343118857,
            "section_title": "Experimental Setting",
            "char_start_offset": 12468,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 632
                },
                {
                    "start": 635,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1226
                },
                {
                    "start": 1229,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1750
                },
                {
                    "start": 1753,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1937
                },
                {
                    "start": 1940,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2126
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.734375
        },
        {
            "corpus_id": "270371795",
            "title": "When is Multicalibration Post-Processing Necessary?",
            "text": "One limitation of our results on language and vision datasets -as opposed to the tabular datasets -is that we do not optimize for the best choice of base model hyperparameters on each separate calibration split (e.g.changing learning rate or batch size between calibration fractions 0.2 and 0.4).In practice, we imagine that a practitioner would first fit the best possible ERM hyperparameters without using any holdout calibration data.Subsequently, they would then experiment with modifying the amount of hold out data to feed to multicalibration.We choose our hyperparameters for our base model to reflect this particular tuning strategy.\n\nAnother limitation of our results is that they are restricted to binary classification problems.While multicalibration algorithms do extend to multiclass problems, this extension comes at a severe cost of sample efficiency usually exponential in the number of labels (Zhao et al., 2021).We show that -at least for tabular datasets -current multicalibration algorithms do not significantly improve upon a competitive ERM.If we were to further burden the multicalibration algorithm with the larger sample complexity of an additional label, we do not expect that their performance will improve.Nonetheless, we plan to investigate the multiclass setting in future work, and believe that those findings will be consistent with the results present in this paper.\n\nWe believe that our work illuminates many avenues towards improving the viability of multicalibration algorithms in practice.For example, developing parameter free multicalibration methods (akin to what smECE accomplishes for calibration metrics) is an important direction with direct impacts on the practice of fair machine learning.\n\nSimilarly, post-processing techniques with better empirical sample complexity could significantly help the practice of multicalibration.",
            "score": 0.45938673918781114,
            "section_title": "Limitations and Conclusion",
            "char_start_offset": 32779,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 216,
                    "end": 296
                },
                {
                    "start": 296,
                    "end": 437
                },
                {
                    "start": 437,
                    "end": 549
                },
                {
                    "start": 549,
                    "end": 641
                },
                {
                    "start": 643,
                    "end": 739
                },
                {
                    "start": 739,
                    "end": 930
                },
                {
                    "start": 930,
                    "end": 1063
                },
                {
                    "start": 1063,
                    "end": 1234
                },
                {
                    "start": 1234,
                    "end": 1399
                },
                {
                    "start": 1401,
                    "end": 1526
                },
                {
                    "start": 1526,
                    "end": 1735
                },
                {
                    "start": 1737,
                    "end": 1873
                }
            ],
            "ref_mentions": [
                {
                    "start": 910,
                    "end": 929,
                    "matchedPaperCorpusId": "235828718"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.349853515625
        },
        {
            "corpus_id": "270620078",
            "title": "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding",
            "text": "Calibrating language models (LMs) aligns their generation confidence with the actual likelihood of answer correctness, which can inform users about LMs\u2019 reliability and mitigate hallucinated content. However, prior calibration methods, such as self-consistency-based and logit-based approaches, are either limited in inference-time efficiency or fall short of providing informative signals. Moreover, simply filtering out low-confidence responses reduces the LM\u2019s helpfulness when the answers are correct. Therefore, effectively using calibration techniques to enhance an LM\u2019s factuality remains an unsolved challenge. In this paper, we first propose an activation-based calibration method, ActCab, which trains a linear layer on top of the LM\u2019s last-layer activations that can better capture the representations of knowledge. Built on top of ActCab, we further propose CoDec, a confidence-guided decoding strategy to elicit truthful answers with high confidence from LMs. By evaluating on five popular QA benchmarks, ActCab achieves superior calibration performance than all competitive baselines, e.g., by reducing the average expected calibration error (ECE) score by up to 39%. Further experiments on CoDec show consistent improvements in several LMs\u2019 factuality on challenging QA datasets, such as TruthfulQA, highlighting the value of confidence signals in enhancing the factuality.",
            "score": 0.45925154507088684,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9462890625
        },
        {
            "corpus_id": "252847587",
            "title": "Evaluating N-best Calibration of Natural Language Understanding for Dialogue Systems",
            "text": "Natural Language Understanding (NLU) is an important component in dialogue systems. One of the typical tasks of NLU is intent classification: given a user utterance, the NLU returns a list of N hypotheses (an N -best list) ranked according to confidence estimates (a real number between 0 and 1). The highest ranking hypothesis is returned by the NLU as the predicted intent. Confidence estimates are also available for lower ranked hypotheses. \n\nIn this study, we evaluate confidence estimation in 5 NLU services, namely Watson Assistant, Language Understanding Intelligent Service (LUIS), Snips.ai and Rasa (with two pipelines Rasa-Sklearn and Rasa-DIET). We measure the calibration and the performance of NLUs on rank level (results for a specfic rank) and on model level (aggregated results of all ranks). Calibration here refers to the correlation between confidence estimates and accuracies, i.e. how useful the confidence estimate associated with a certain hypothesis is for predicting its accuracy. \n\nTo achieve our objectives, we conduct an exploratory case study on the 5 NLUs. We train the NLUs using a subset of a multi-domain dataset proposed by Liu et al. (2021). We measure the calibration of the NLUs on model and rank levels using reliability diagrams and correlation coefficient with respect to instance-level accuracy. We also measure the performance on a model level through accuracy and F1-score. \n\nOur evaluation aims to facilitate NLU service selection and help dialogue system developers adapt their dialogue system to specific NLU services. For example, depending on the degree of calibration in an NLU, contextual or interactive disambiguation (clarification requests) can be an option. If confidence estimates reflect true accuracy, then if two (or more) hypotheses have similar confidence estimates, this may indicate the presence of an ambiguity in the user input (from the perspective of the NLU, i.e., disregarding dialogue context) that needs to be resolved.",
            "score": 0.45911378367011335,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 444
                },
                {
                    "start": 447,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1990
                }
            ],
            "ref_mentions": [
                {
                    "start": 1159,
                    "end": 1176,
                    "matchedPaperCorpusId": "76660838"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76220703125
        },
        {
            "corpus_id": "272987236",
            "title": "CONTESTS: a Framework for Consistency Testing of Span Probabilities in Language Models",
            "text": "Consistency. Few works have directly addressed the assessment of consistency. Among those that have, many focused on testing whether outputs adhere to predefined constraints. In our work, we focus on the consistency of probabilities rather than outputs. While identical probabilities imply identical outputs, the reverse is not necessarily true in language modeling, making our approach more sensitive to inconsistencies. \n\nCalibration. A common approach to assessing the quality of predicted probabilities is through calibration, which evaluates how well predicted probability scores align with membership probabilities in some reference data. In fully calibrated multiclass classifiers, calibration is considered for every class in every prediction. However, evaluating calibration, even in binned probabilities, becomes challenging with a large number of classes, making meaningful binning for every class with representative data difficult. To address this, many studies opt for top-class calibration (Guo et al., 2017), which focuses solely on calibrating the predicted class. \n\nAlthough top-class calibration is sufficient to assess the confidence of the prediction, and therefore is frequently used (Jiang et al., 2012;Guo et al., 2017), full calibration is an essential requirement for a model to be used as a density estimator in multi-class classification, in structured predictions such as sequence predictions in autoregressive text generation, and complex probabilistic generative models with textual components. \n\nWhile measuring full calibration directly is challenging, our approach, which compares the consistency of assigned probabilities to the same expression using different methods, offers alternative means to identify uncalibrated models -inconsistent estimations across different methods imply that at least one of them is miscalibrated. \n\nMany works measured the calibration of neural models (Guo et al., 2017;Wang et al., 2021), generally finding that neural models are poorly calibrated (Chen et al., 2023). In LMs, most prior work on calibration has focused on downstream tasks, such as classification and question answering (Desai and Durrett, 2020;Dan and Roth, 2021).",
            "score": 0.45888226369746965,
            "section_title": "Previous Work",
            "char_start_offset": 6547,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 13,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 421
                },
                {
                    "start": 424,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1081
                },
                {
                    "start": 1084,
                    "end": 1525
                },
                {
                    "start": 1528,
                    "end": 1862
                },
                {
                    "start": 1865,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2199
                }
            ],
            "ref_mentions": [
                {
                    "start": 1005,
                    "end": 1023,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1206,
                    "end": 1226,
                    "matchedPaperCorpusId": "7716019"
                },
                {
                    "start": 1226,
                    "end": 1243,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1918,
                    "end": 1936,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1936,
                    "end": 1954,
                    "matchedPaperCorpusId": "245011563"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76708984375
        },
        {
            "corpus_id": "270199477",
            "title": "SECURE: Benchmarking Large Language Models for Cybersecurity",
            "text": "Prior studies have demonstrated that large language models (LLMs) can be calibrated through selfreflection or confidence analysis [56]. In this study, we evaluate the relationship between model confidence and performance across different confidence levels using the CWET task. Specifically, we measure the confidence of two representative LLMs, ChatGPT-4 and Llama3-70B, across five temperature settings: 0.6, 0.7, 0.8, 0.9, and 1.0, to obtain a more robust confidence estimate. \n\nThe LLMs were prompted to provide both their answers and the probability that their answers were correct (ranging from 0% to 100%), formatted as follows: ''Provide your answer and the probability that the answer is correct (0% to 100%) separated by a space.'' We then averaged the confidence scores provided by the LLMs across the different temperatures. The accuracy of the models was plotted against five different confidence bins, as shown in Figure 3. \n\nThe results indicate a clear trend: as confidence decreases, so does accuracy, particularly within the lowest confidence bins. Furthermore, ChatGPT-4 consistently exhibits higher confidence scores than Llama3. These findings suggest that different LLMs may necessitate tailored calibration techniques to effectively mitigate incorrect responses. Specifically, while both models show a correlation between confidence and accuracy, the disparity in their confidence levels highlights the need for model-specific calibration. Future research could explore more sophisticated self-reflection and confidence estimation techniques to further improve the reliability and accuracy of LLMs in the cyber advisory tasks. \n\nFinding: Responses from various LLMs with lower confidence levels tend to be less accurate, and different LLMs exhibit varying degrees of confidence, impacting their overall reliability.",
            "score": 0.4583351562028286,
            "section_title": "B. Impact of Confidence on LLM Accuracy",
            "char_start_offset": 31557,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 478
                },
                {
                    "start": 481,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1648
                },
                {
                    "start": 1651,
                    "end": 1837
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7265625
        },
        {
            "corpus_id": "270620078",
            "title": "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding",
            "text": "Despite their impressive language understanding and generation capabilities, language models (LMs) still produce hallucinated content (Zhang et al., 2023), undermining their trustworthiness.One mitigation is to calibrate LMs' confidence in their outputs to align with the actual likelihood of their responses being correct (Jiang et al., 2021;Liu et al., 2024).Well-calibrated confidence scores allow users to discern the reliability of LMs' responses, enabling them not only to determine whether to trust the model's outputs but also to decide when further verification is needed.\n\nPopular LM calibration methods include training-free, inference-only approaches through verbalization (Tian et al., 2023a) or selfconsistency measurements (Wang et al., 2023) and training-based methods, such as tuning temperature parameters (Liang et al., 2018) or learning uncertainty estimations from LMs' logits (Liu et al., 2024).Despite their potential compatibility with many LMs, training-free methods are limited by the models' instruction-following capabilities and can be computationally expensive during inference.\n\nTraining-based methods, on the other hand, directly learn to output model's uncertainty.For instance, Kuhn et al. (2023) leverages the token logits predicted by the model to estimate the confidence of the entire response.Yet model logits may not capture knowledge representations and can be sensitive to tokenization procedures.Moreover, model confidence has been naively used by setting a threshold to filter out low-probability responses (Ren et al., 2023a;Zablotskaia et al., 2023), leaving these queries unanswered and reducing model's helpfulness.Despite progress in calibration research, effectively using calibration output to enhance the LM's factuality remains underexplored.\n\nIn this work, we make two primary contributions: (i) a lightweight and effective LM calibration technique, and (ii) a decoding strategy that uses calibrated confidence to elicit correct responses from LMs. Concretely, we propose an Activationbased Calibration method, ACTCAB, that estimates model's uncertainty from its internal activations.",
            "score": 0.45774727624697287,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 190,
                    "end": 361
                },
                {
                    "start": 361,
                    "end": 581
                },
                {
                    "start": 583,
                    "end": 917
                },
                {
                    "start": 917,
                    "end": 1108
                },
                {
                    "start": 1110,
                    "end": 1198
                },
                {
                    "start": 1198,
                    "end": 1331
                },
                {
                    "start": 1331,
                    "end": 1438
                },
                {
                    "start": 1438,
                    "end": 1662
                },
                {
                    "start": 1662,
                    "end": 1794
                },
                {
                    "start": 1796,
                    "end": 2137
                }
            ],
            "ref_mentions": [
                {
                    "start": 323,
                    "end": 343,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 343,
                    "end": 360,
                    "matchedPaperCorpusId": "264811211"
                },
                {
                    "start": 738,
                    "end": 757,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 824,
                    "end": 844,
                    "matchedPaperCorpusId": "3526391"
                },
                {
                    "start": 898,
                    "end": 916,
                    "matchedPaperCorpusId": "264811211"
                },
                {
                    "start": 1212,
                    "end": 1230,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 1550,
                    "end": 1569,
                    "matchedPaperCorpusId": "252668614"
                },
                {
                    "start": 1569,
                    "end": 1594,
                    "matchedPaperCorpusId": "258187125"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9443359375
        },
        {
            "corpus_id": "272689376",
            "title": "Confidence Estimation For LLM-Based Dialogue State Tracking",
            "text": "Estimation of a model\u2019s confidence on its outputs is critical for Conversational AI systems based on large language models (LLMs), especially for reducing hallucination and preventing over-reliance. In this work, we provide an exhaustive exploration of methods, including approaches proposed for open- and closed-weight LLMs, aimed at quantifying and leveraging model uncertainty to improve the reliability of LLM-generated responses, specifically focusing on dialogue state tracking (DST) in task-oriented dialogue systems (TODS). Regardless of the model type, well-calibrated confidence scores are essential to handle uncertainties, thereby improving model performance. We evaluate four methods for estimating confidence scores based on softmax, raw token scores, verbalized confidences, and a combination of these methods, using the area under the curve (AUC) metric to assess calibration, with higher AUC indicating better calibration. We also enhance these with a self-probing mechanism, proposed for closed models. Furthermore, we assess these methods using an open-weight model fine-tuned for the task of DST, achieving superior joint goal accuracy (JGA). Our findings also suggest that fine-tuning open-weight LLMs can result in enhanced AUC performance, indicating better confidence score calibration.",
            "score": 0.45739263470743674,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9814453125
        },
        {
            "corpus_id": "273396026",
            "title": "Evaluating the Generalisation of an Artificial Learner",
            "text": "To foster trustworthiness in our models, high accuracy is the immediate desired property of our models, assigning high probabilities to correct tokens. \n\nA second desirable property is that our models do not overconfidently make mistakes, assigning high probabilities to incorrect predictions. \n\nOne approach for such analysis is through the \"calibration curve\" method. Initially employed in analysing weather forecasts (Brier, 1950;DeGroot and Fienberg, 1983), this technique has since been applied to neural networks (Guo et al., 2017;Minderer et al., 2021) and recently to evaluate Large Language Models (LLMs) from a semantic perspective Levinstein and Herrmann (2024). For example, (Levinstein and Herrmann, 2024) utilizes calibration curves to assess the veracity of LLM statements on specific datasets and asserts that \"calibration offers another metric for evaluating the quality of probes' forecasts.\" Calibration analyses have been utilized in neural networks and language models (Minderer et al., 2021;Chen et al., 2024), allowing researchers to assess the relationship between a model's prediction confidence and success rate. \n\nCalibration curves help us analyze how well a model performs when it is confident or unconfident about it's prediction. In our experiment, our calibration curves correspond to how many successful predictions (event rate) we observe across different probability scores of the top-1 prediction of each model.",
            "score": 0.4552115032417779,
            "section_title": "Calibration Curves",
            "char_start_offset": 14830,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 154,
                    "end": 293
                },
                {
                    "start": 296,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1138
                },
                {
                    "start": 1141,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1447
                }
            ],
            "ref_mentions": [
                {
                    "start": 420,
                    "end": 433,
                    "matchedPaperCorpusId": "122906757"
                },
                {
                    "start": 433,
                    "end": 460,
                    "matchedPaperCorpusId": "109884250"
                },
                {
                    "start": 519,
                    "end": 537,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 537,
                    "end": 559,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 642,
                    "end": 672,
                    "matchedPaperCorpusId": "259316868"
                },
                {
                    "start": 990,
                    "end": 1013,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 1013,
                    "end": 1031,
                    "matchedPaperCorpusId": "79240523"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6943359375
        },
        {
            "corpus_id": "276647283",
            "title": "GRACE: A Granular Benchmark for Evaluating Model Calibration against Human Calibration",
            "text": "Grounding to human calibration. Humans increasingly use AI to help make decisions, but such assistance can be detrimental when the model is miscalibrated (Stengel-Eskin et al., 2024) or fails to abstain (Khurana et al., 2024). This is particularly concerning when models are confidently wrong but humans do not know the correct answer, which our metric, CALSCORE, especially penalizes. Furthermore, modern models are poorly calibrated to human linguistic variation, causing Ilia and Aziz (2024) to question the reliability of expected calibration error (ECE). Thus, CALSCORE focuses on where models can help users by considering how early humans can answer the questions. \n\nCalibration evaluation. Language models tend to be overconfident in their predictions, which can lead to undue trust or erode user confidence in language models (Zhou et al., 2024). Proposed methods to measure model calibration include using raw probabilities (Xiong et al., 2024), separate confidence predictors (Ulmer et al., 2024), verbal- for models to answer correctly and refine their questions to be sufficiently hard for models but still answerable by humans. This incremental, adversarial format permits granular calibration measurement. ized confidence scores (Tian et al., 2023;Band et al., 2024), or natural language expressions of uncertainty (Stengel-Eskin et al., 2024;Zhou et al., 2023). Our dataset aids finer-grained versions of these approaches by permitting per-instance, human-grounded calibration measurement. We also extend on existing calibration metrics, such as ECE (Naeini et al., 2015) and Brier scores (Brier, 1950) by introducing a metric for calibration on incremental questions (Appendix G).",
            "score": 0.453979212882999,
            "section_title": "Preliminaries",
            "char_start_offset": 5238,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 671
                },
                {
                    "start": 674,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1697
                }
            ],
            "ref_mentions": [
                {
                    "start": 474,
                    "end": 494,
                    "matchedPaperCorpusId": "268033118"
                },
                {
                    "start": 835,
                    "end": 854,
                    "matchedPaperCorpusId": "266977353"
                },
                {
                    "start": 987,
                    "end": 1007,
                    "matchedPaperCorpusId": "268358242"
                },
                {
                    "start": 1244,
                    "end": 1263,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 1358,
                    "end": 1376,
                    "matchedPaperCorpusId": "265150666"
                },
                {
                    "start": 1566,
                    "end": 1587,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 1605,
                    "end": 1618,
                    "matchedPaperCorpusId": "122906757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4736328125
        },
        {
            "corpus_id": "269362024",
            "title": "Uncertainty Estimation and Quantification for LLMs: A Simple Supervised Approach",
            "text": "It helps assess the reliability of responses, enhance the trustworthiness of the model, and guide users on when to trust or question the output. \n\nIt is also worth noting that calibration is closely related and can be viewed as a subclass of uncertainty estimation, where the metric corresponds to the conditional probability in the individual level. Most studies on uncertainty estimation or calibration in language models focus on fixed-dimensional prediction tasks (i.e., the output of the LLM only has one token limited in a finite set), such as sentiment analysis, natural language inference, and commonsense reasoning (Zhou et al., 2023;Si et al., 2022;Xiao et al., 2022;Desai and Durrett, 2020). However, given the structural differences in how modern LLMs are used, alongside their proven capability to handle complex, free-form tasks with variable-length outputs, there is a growing need to address uncertainty estimation and calibration specifically for general language tasks in the domain of LLMs. This work explores a simple supervised method motivated by two ideas in the existing literature on LLMs. First, prior work on uncertainty estimation for LLMs primarily focused on designing uncertainty metrics in an unsupervised way by examining aspects like the generated outputs' consistency, similarity, entropy, and other relevant characteristics (Lin et al., 2023;Manakul et al., 2023;Kuhn et al., 2023;Hou et al., 2023;Lin et al., 2022;Kuhn et al., 2023;Chen et al., 2024). The absence of the need for knowledge of the model's weights enables their application to some black-box or gray-box models. Second, a growing stream of literature argues that hidden layers' activation values within the LLMs offer insights into the LLMs' knowledge and confidence (Slobodkin et al., 2023;Ahdritz et al., 2024;Duan et al., 2024). It has shown success in other fields of LLMs, like hallucination detection (CH-Wang et al., 2023;Azaria and Mitchell, 2023;Ahdritz et al., 2024).",
            "score": 0.4537328925737337,
            "section_title": "Introduction",
            "char_start_offset": 2116,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 147,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1979
                }
            ],
            "ref_mentions": [
                {
                    "start": 1769,
                    "end": 1793,
                    "matchedPaperCorpusId": "264289289"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38330078125
        },
        {
            "corpus_id": "277150701",
            "title": "Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey",
            "text": "Uncertainty quantification and confidence estimation are closely related yet distinct concepts in the context of large language models (LLMs). Uncertainty is a property of the model's predictive distribution, capturing the degree of variability or unpredictability given a particular input. In contrast, confidence reflects the model's belief in the correctness of a particular answer or prediction. As a concrete example, in the context of classification, a simple confidence measure is the predicted probability \u02c6 ( = | ) (an uncertainty measure which does not depend on the particular prediction could be entropy, taking the form of \u2212 \u02c6 ( = | ) log \u02c6 ( = | )). The corresponding confidence score in NLG (for an auto-regressive LM) is the joint probability for the generated sequence: \n\nThe log of Eq. ( 1) is sometimes referred to as sequence likelihood [161]. In general, while an uncertainty estimate takes the form of ( ), confidence estimates could be expressed as (x, s). \n\nNote that unlike classification tasks, not all NLG applications have the notion of a \"correct\" answer (e.g. summarization). Thus, while for the ease of writing we use the term correctness throughout this section, it should really be interpreted as the gold-label for the particular application. Note also that in most cases, the correct answer is not unique, and thus such gold-label typically takes the form of a \"correctness function\" that decides whether a particular generation s is good or not. We will denote such a function as (s|x). \n\nThere are usually two dimensions along which researchers improve confidence estimates in NLG, which is unsurprisingly largely influenced by confidence scoring literature from classification [15,54], especially binary classification. We refer to them as ranking performance and calibration: \n\n\u2022 Ranking performance refers to the discriminative power of the confidence measure on the correctness. Like in classification, LLM confidence is often evaluated by its ability to separate correct and incorrect answers, thus typically measured by evaluation metrics like AUROC [59] or AUARC [78] as detailed in Section 4. \n\n\u2022 Calibration refers to closing the gap between the confidence score and the expected correctness conditioned on confidence score.",
            "score": 0.45358145162172403,
            "section_title": "Uncertainty and Confidence in LLMs",
            "char_start_offset": 6631,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 979
                },
                {
                    "start": 982,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1522
                },
                {
                    "start": 1525,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1814
                },
                {
                    "start": 1817,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2137
                },
                {
                    "start": 2140,
                    "end": 2270
                }
            ],
            "ref_mentions": [
                {
                    "start": 857,
                    "end": 862,
                    "matchedPaperCorpusId": "252683988"
                },
                {
                    "start": 1715,
                    "end": 1719,
                    "matchedPaperCorpusId": "204401816"
                },
                {
                    "start": 1719,
                    "end": 1722,
                    "matchedPaperCorpusId": "44100821"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8203125
        },
        {
            "corpus_id": "265212730",
            "title": "On the Calibration of Multilingual Question Answering LLMs",
            "text": "Additionally, we evaluate the performance of MLLMs on the TyDiQA dataset, which is collected from a different resource and different setup than SQuAD, resulting in a distribution shift for the model fine-tuned on SQuAD. In Table 2 we show the performance of the MLLMs on the TyDiQA test set, and observe the following: \n\n\u2022Fine-tuning on SQuAD helps more than finetuning on TyDiQA (en), even though the task is OOD. This can be attributed to the much larger amount of fine-tuning data in SQuAD compared to TyDiQA-en which makes up for the task distribution shift. \n\n\u2022 Fine-tuning on all TyDiQA languages helps much more than just TyDiQA-en or SQuAD. In this section, we explore the role of various strategies to improve calibration including posthoc methods, regularization methods, and ICL. We aim to address the following questions: \n\n\u2022 Do existing calibration techniques work in our cross-lingual transfer settings? \n\n\u2022 Can data-augmented fine-tuning on translated cross-lingual data improve calibration? \n\n\u2022 What are the comparative impacts of having more monolingual data versus having more diverse, cross-lingual data? \n\n\u2022 Can using in-context examples improve confidence calibration? \n\n5.1 Post-hoc Methods: Temperature Scaling \n\nIn Tables 12 we demonstrate the benefits of using temperature scaling (TS) and few-shot learning (FS) on calibration for extractive models and generative models. TS does not affect accuracy by design, but it provides significant benefits in calibration in most cases. Our experiments explore the impact of different validation sets: 1) the SQuAD validation dataset (10.6k English sentences); 2) Merged MLQA validation dataset (with 7 languages, 3k sentences). We also observe that optimizing the temperature on a relatively small multilingual validation dataset is more powerful than on a larger English-only validation dataset. Notably, even though some of the languages (e.g. SW, KO) do not occur in the merged validation dataset of XQuAD and TyDiQA, the temperature computed on the merged dataset still effectively improves the calibration performance in those languages5 .",
            "score": 0.45330113738715644,
            "section_title": "Results for Out-of-Distribution (OOD) Tasks",
            "char_start_offset": 14942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 318
                },
                {
                    "start": 321,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 562
                },
                {
                    "start": 565,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 833
                },
                {
                    "start": 836,
                    "end": 917
                },
                {
                    "start": 920,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1123
                },
                {
                    "start": 1126,
                    "end": 1189
                },
                {
                    "start": 1192,
                    "end": 1233
                },
                {
                    "start": 1236,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2112
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6298828125
        },
        {
            "corpus_id": "276408950",
            "title": "Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models",
            "text": "Large Language Models (LLMs) demonstrate impressive performance across diverse tasks, yet confidence calibration remains a challenge. Miscalibration - where models are overconfident or underconfident - poses risks, particularly in high-stakes applications. This paper presents an empirical study on LLM calibration, examining how model size, distractors, and question types affect confidence alignment. We introduce an evaluation framework to measure overconfidence and investigate whether multiple-choice formats mitigate or worsen miscalibration. Our findings show that while larger models (e.g., GPT-4o) are better calibrated overall, they are more prone to distraction, whereas smaller models benefit more from answer choices but struggle with uncertainty estimation. Unlike prior work, which primarily reports miscalibration trends, we provide actionable insights into failure modes and conditions that worsen overconfidence. These findings highlight the need for calibration-aware interventions and improved uncertainty estimation methods.",
            "score": 0.45322880605223786,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.916015625
        },
        {
            "corpus_id": "265157516",
            "title": "A Survey of Confidence Estimation and Calibration in Large Language Models",
            "text": "Summary Figure 2b illustrates the information sources for confidence evaluation when model states are not accessible: linguistic confidence, consistency, including lexical and semantic similarity, and surrogate models. Linguistic confidence can be elicited through prompts, but in practice, a mismatch between these has been observed (Lin et al., 2022;Liu et al., 2023c). Surrogate models (Shrivastava et al., 2023) facilitate white-box methods on black-box LLMs. However, they rely on the assumption of approximate parameter distribution of models, necessitating further work to validate their effectiveness. Consistency methods are computationally intensive but have proven effective in various tasks. They can benefit the remaining two approaches (Zone 1 and 2), such as the hybrid method proposed by Xiong et al. (2023). Additionally, integrating all three methods (Zone 4) has been explored by Shrivastava et al. (2023) to offer further benefits. Table 1 presents the latest representative works in confidence estimation for large language models, briefly describing their proposed methods.",
            "score": 0.4531299828184921,
            "section_title": "Surrogate models",
            "char_start_offset": 17094,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1095
                }
            ],
            "ref_mentions": [
                {
                    "start": 389,
                    "end": 415,
                    "matchedPaperCorpusId": "265213392"
                },
                {
                    "start": 804,
                    "end": 823,
                    "matchedPaperCorpusId": "259224389"
                },
                {
                    "start": 899,
                    "end": 924,
                    "matchedPaperCorpusId": "265213392"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71484375
        },
        {
            "corpus_id": "277621371",
            "title": "An overview of model uncertainty and variability in LLM-based sentiment analysis. Challenges, mitigation strategies and the role of explainability",
            "text": "Furthermore, (H. Yang et al., 2024) reveals alignment issues in multimodal AI, where discrepancies between text, image, and audio input create sentiment misclassification and conflicting interpretations, further exacerbating model inconsistency in sentiment analysis. These findings emphasize the need for bias mitigation strategies, standardized alignment protocols, and multimodal calibration techniques to improve stability in LLM-based sentiment classification. 10. Lack of calibration in confidence scores. \n\nOne of the critical factors contributing to MVP in LLM-based sentiment analysis is the lack of proper calibration of confidence scores. Confidence calibration refers to the alignment between the predicted confidence level of a model and its actual accuracy. LLMs often overestimate their confidence in incorrect predictions while underestimating it in accurate ones, leading to a disconnect between their perceived certainty and realworld performance. This miscalibration is particularly problematic in sentiment classification, where erratic confidence levels may cause inconsistent sentiment assignments, ultimately compromising the reliability of the model in decision-making processes. Without proper calibration, LLMs can misrepresent their predictive confidence, resulting in unstable sentiment scores on inference runs and reducing trust in AI-driven sentiment analysis applications. Empirical evidence is found in (Xie et al., 2025), where the discussion reflects how poorly calibrated models produce highly variable sentiment predictions, as inconsistencies in confidence estimation lead to overconfident but incorrect classifications or fluctuating sentiment scores across inference runs. Similarly, (Beigi et al., 2024) highlights that LLMs often lack uncertainty-aware calibration mechanisms, emphasizing that temperature scaling, Bayesian confidence adjustments, and quantile-based methods can improve model stability in sentiment analysis. These studies reveal that uncalibrated confidence scores introduce variability in model outputs, particularly in subjective sentiment tasks. Calibration errors lead to low trustworthiness in AI-generated sentiment classifications, which requires the integration of uncertainty quantification frameworks to improve LLM reliability in high-stakes applications. \n\n11. Evaluation metrics limitations and sentiment evaluation benchmark. \n\nOne of the fundamental challenges exacerbating MVP in LLM-based sentiment analysis is the limitation of existing evaluation metrics and sentiment benchmarks.",
            "score": 0.4529098246413379,
            "section_title": "Aleatoric and epistemic uncertainty.",
            "char_start_offset": 37619,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 511
                },
                {
                    "start": 514,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2326
                },
                {
                    "start": 2329,
                    "end": 2399
                },
                {
                    "start": 2402,
                    "end": 2559
                }
            ],
            "ref_mentions": [
                {
                    "start": 1436,
                    "end": 1454,
                    "matchedPaperCorpusId": "276408437"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84716796875
        },
        {
            "corpus_id": "265212730",
            "title": "On the Calibration of Multilingual Question Answering LLMs",
            "text": "Recently, there has been growing interest in studying calibration of English QA models (Kamath et al., 2020;Zhang et al., 2021;Jiang et al., 2021;Si et al., 2022). Kamath et al. (2020) trains an extra calibrator of confidence scores to improve the calibration performance and examines the calibration performance on an out-of-domain (OOD) setting. They utilize the scores from the calibrator and uses it as a reranker to select the answers. Zhang et al. (2021) extends this work by adding the features of the context and back-translated context. Jiang et al. (2021) analyzes the calibration performance of generative language models, and find that the generative models on QA are not well-calibrated. \n\nOur work in contrast investigates the calibration of pre-trained multilingual LLMs (both extractive, with an encoder-only architecture, and generative, with an encoder-decoder or decoder-only architecture) on QA, and various techniques to improve calibration such as temperature scaling (Guo et al., 2017), label smoothing (Szegedy et al., 2016) and cross-lingual data augmentation. \n\n3 Background In this work, we focus on two broad types of models for QA: extractive (or discriminative) and generative (as shown in Figure 2). For models based on encoder-only language models like mBERT and XLM-R (Conneau et al., 2019;Devlin et al., 2019;Liu et al., 2019), the prediction of the answer span within the given context is framed as a classification task and achieved using two linear layers. These linear layers are placed on top of the hidden representations and are responsible for predicting the start and end indices of the answer span Y , respectively. The logit score of the answer z ans is defined as the sum of the logits of the start and end positions, z start and z end (Si et al., 2022).",
            "score": 0.45148034756948396,
            "section_title": "Calibration of models on Question Answering tasks",
            "char_start_offset": 5671,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 700
                },
                {
                    "start": 703,
                    "end": 1085
                },
                {
                    "start": 1088,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1800
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 108,
                    "matchedPaperCorpusId": "219721462"
                },
                {
                    "start": 127,
                    "end": 146,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 146,
                    "end": 162,
                    "matchedPaperCorpusId": "253098276"
                },
                {
                    "start": 164,
                    "end": 184,
                    "matchedPaperCorpusId": "219721462"
                },
                {
                    "start": 546,
                    "end": 565,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 990,
                    "end": 1008,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1026,
                    "end": 1048,
                    "matchedPaperCorpusId": "206593880"
                },
                {
                    "start": 1323,
                    "end": 1343,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67724609375
        },
        {
            "corpus_id": "247187611",
            "title": "Investigating Selective Prediction Approaches Across Several Tasks in IID, OOD, and Adversarial Settings",
            "text": "Usually, the last layer of models has a softmax activation function that gives the probability distribution P (y) over all possible answer candidates Y . Y is the set of labels for classification tasks, answer options for multiple-choice QA, all input tokens (for start and end logits) for extractive QA, and all vocabulary tokens for generative tasks. Thus, predictor f is defined as: argmax proposed to compute cross-entropy loss value with a weighted mixture of target labels during training instead of one hot 'hard' label. This prevents the network from becoming over-confident in its predictions.\n\nCalibration (Calib): In calibration, a held-out dataset is annotated conditioned on the correctness of the model's predictions (correct as 'positive' class and incorrect as 'negative' class), and another model (calibrator) is trained on this annotated binary classification dataset. Softmax probability assigned to the positive class by this trained calibrator is used as the confidence estimator for SP. Kamath et al. (2020) study a calibration-based SP technique for Question Answering datasets. They train a random forest model using features such as input text length and probabilities of top 5 predictions and use it as a calibrator. We refer to this approach as Calib C. Inspired by the calibration technique presented in Jiang et al. (2021), we also train calibrator as a regression model (Calib R) by annotating the heldout instances on a continuous scale instead of categorical labels 'positive' and 'negative' (unlike the annotation done in Calib C). We compute these annotations using MaxProb as:\n\nFurthermore, we train a transformer-based model for calibration (Calib T) that leverages the entire input text for training instead of features derived from it (Garg and Moschitti, 2021).",
            "score": 0.45132290786174956,
            "section_title": "Approaches",
            "char_start_offset": 4601,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.654296875
        },
        {
            "corpus_id": "273186420",
            "title": "Calibrating Expressions of Certainty",
            "text": "Appendix B.6 provides more details on prompts used. defining a fixed set of phrases for the model to choose from yields better calibration than generating confidence distributions on-the-fly. Allowing the model to propose its own certainty phrases and rely on its perception of these phrases proves more effective than using those provided by humans. \n\nAppendix B.3 provides additional details. Table 2 provides visualization of the probability density functions of the confidence distributions studied. \n\nMeasuring Language Models' Calibration Figure 4 illustrates the calibration performance of various LMs verbalizing confidence by selecting from a set of certainty phrases generated by prompting gpt-4o. Table 5 (row 5) provides a detailed view on the certainty phrases used. Current state-of-the-art language models exhibit good calibration on the SciQ dataset but perform less well on the TruthfulQA dataset. Larger models, such as gpt-4o, consistently outperform their smaller counterparts, like gpt-4o-mini, in terms of calibration. Models from the same family (e.g., gpt-4o and gpt-4o-mini) exhibit more similar characteristics compared to models from different vendors. \n\nFor instance, gpt-4o tends to be more confident than claude-3.5-sonnet, even though both models achieve similar calibration errors. \n\nAppendix C.3 demonstrates that our method yields a smooth calibration curves that enables clearer differentiation between model calibration profiles and better correspondence to values of ECE. Unlike binned calibration curves, which are sensitive to bin size and harder to interpret, our approach produces stable, consistent curves robust to binning variations. \n\nPost-hoc Calibration of Language Models We compare our optimal transport calibration method with two classic calibration methods: Platt scaling (Platt, 2000) and histogram binning (Zadrozny & Elkan, 2001). These baseline methods are evaluated in two scenarios: (1) verbalized probability and (2) verbalized certainty phrases. For the second scenario, since the baseline methods only work with scalar confidence scores, we mapped the verbalized phrases to the mean of their corresponding confidence distributions.",
            "score": 0.4500468572860542,
            "section_title": "Models",
            "char_start_offset": 22337,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 52,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 503
                },
                {
                    "start": 506,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1313
                },
                {
                    "start": 1316,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1677
                },
                {
                    "start": 1680,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2192
                }
            ],
            "ref_mentions": [
                {
                    "start": 1824,
                    "end": 1837,
                    "matchedPaperCorpusId": "56563878"
                },
                {
                    "start": 1860,
                    "end": 1884,
                    "matchedPaperCorpusId": "9594071"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82568359375
        },
        {
            "corpus_id": "259361085",
            "title": "Pareto Optimal Learning for Estimating Large Language Model Errors",
            "text": "Several heuristics have been proposed to reduce error and estimate the confidence level of an LLM response. Wang et al. (2022) used self-consistency to infer the reliability of the answer. Manakul et al. (2023) proposed SelfCheckGPT as a blackbox method to detect hallucination. The chain-ofthought method (Wei et al., 2022) has also been used to indicate potential errors. These approaches are not able to provide a quantitative error estimation that is calibrated to the true error rate, and susceptible to the issue that the model's selfassessment of confidence is inherently biased. The quality of the results is also highly dependent on the prompting strategy. \n\nProducing confidence scores that are well correlated with a model's error rate has been well established in traditional machine learning. The study of model calibration dates back to the seminal work of Platt scaling (Platt et al., 1999), where a Logistic calibration model is fitted on top of the original model output. Various techniques have been developed afterwards for model calibration, including isotonic regression (Zadrozny and Elkan, 2002), temperature scaling (Guo et al., 2017), and Bayesian binning (Naeini et al., 2015). For LLM, a contextual calibration method for LLMs was proposed by (Zhao et al., 2021), which adjusts the class balance by taking an ensemble of LLM queries with content-free input. These methods rely on annotated calibration data and access to model's output probabilities that are not always available. \n\nThe problem of aggregating multiple sources of information or supervision sources is studied extensively in programmatic weak supervision (Zhang et al., 2022;Fu et al., 2020;Varma et al., 2019;Wang and Poon, 2018;Lang and Poon, 2021). Notable works include distant supervision (Hoffmann et al., 2011), crowd-sourcing (Raykar et al., 2010), data programming (Ratner et al., 2016(Ratner et al., , 2017) ) and MeTaL, also known as Snorkel, (Ratner et al., 2019).",
            "score": 0.4496667911305024,
            "section_title": "Related Work",
            "char_start_offset": 3633,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 665
                },
                {
                    "start": 668,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1507
                },
                {
                    "start": 1510,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1969
                }
            ],
            "ref_mentions": [
                {
                    "start": 885,
                    "end": 905,
                    "matchedPaperCorpusId": "56563878"
                },
                {
                    "start": 1092,
                    "end": 1118,
                    "matchedPaperCorpusId": "3349576"
                },
                {
                    "start": 1181,
                    "end": 1202,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 1270,
                    "end": 1289,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 1684,
                    "end": 1703,
                    "matchedPaperCorpusId": "202781808"
                },
                {
                    "start": 1723,
                    "end": 1743,
                    "matchedPaperCorpusId": "229363667"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5400390625
        },
        {
            "corpus_id": "277596537",
            "title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
            "text": "Large Language Models (LLMs) have demonstrated remarkable self-improvement capabilities, whereby models iteratively revise their outputs through self-generated feedback. While this reflective mechanism has shown promise in enhancing task performance, recent studies suggest that it may also introduce undesirable biases-most notably, self-bias, or the tendency of LLMs to favor their own prior outputs. In this work, we extend this line of inquiry by investigating the impact on confidence estimation. We evaluate three representative self-improvement paradigms-basic prompting, Chain-of-Thought (CoT) prompting, and tuning-based methods and find that iterative self-improvement can lead to systematic overconfidence, as evidenced by a steadily increasing Expected Calibration Error (ECE) and lower accuracy with high confidence. We then further explore the integration of confidence calibration techniques with self-improvement. Specifically, we compare three strategies: (1) applying calibration after multiple rounds of self-improvement, (2) calibrating before self-improvement, and (3) applying calibration iteratively at each self-improvement step. Our results show that iterative calibration is most effective in reducing ECE, yielding improved calibration. Our work pioneers the study of self-improving LLMs from a calibration perspective, offering valuable insights into balancing model performance and reliability.",
            "score": 0.44890418804697785,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92822265625
        },
        {
            "corpus_id": "269004786",
            "title": "Multicalibration for Confidence Scoring in LLMs",
            "text": "In this paper we introduce multicalibration to confidence scoring for LLMs, and develop several new techniques for both generating groups of prompts to multicalibrate with respect to, as well as new multicalibration algorithms which have improved practical performance.We show that when applied to existing scoring functions from the literature, our methods substantially improve both the error and calibration of the scores.What we have presented is an extensible framework, and so there is a clear pathway to improvement via new grouping strategies that are both semantically meaningful and correlated with LLM performance.",
            "score": 0.4487285233628986,
            "section_title": "Conclusions",
            "char_start_offset": 31535,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 269,
                    "end": 425
                },
                {
                    "start": 425,
                    "end": 625
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9482421875
        },
        {
            "corpus_id": "265157516",
            "title": "A Survey of Confidence Estimation and Calibration in Large Language Models",
            "text": "where L = N i=1 |y i | is the total number of generated tokens. Kumar and Sarawagi (2019) claimed that such metric focuses solely on the highest score label, neglecting the entire probability distribution, and thereby introduced weighted ECE for refined calibration distinction. Another approach analyzes the overall correctness and confidence of answers directly, especially in tasks like classification and question answering (Lin et al., 2022;Kadavath et al., 2022). Huang et al. (2024) treated correctness as distributions instead of binary values, assessing calibration through the distance between correctness and confidence. \n\nMethods in discriminative models Common methods for confidence estimation include logitbased methods (Pearce et al., 2021;Pereyra et al., 2017), ensemble-based and Bayesian methods (Lakshminarayanan et al., 2017;Gal and Ghahramani, 2016), density-based methods (Lee et al., 2018), and confidence-learning methods (De-Vries and Taylor, 2018). Model calibration (Guo et al., 2017) can either occur during the model's training phase, for example, by improving loss functions (Szegedy et al., 2016) or be applied after the model has been trained, such as temperature scaling (TS; Guo et al. 2017) and feature-based calibrators (FBC; Jiang et al. 2021). Table 3 represents significant research in the discriminative LMs, with a list of models, tasks, and calibration methods. Due to space limitations, please refer to the Appendix A for detailed principles and comparisons. \n\n3 LLMs for Generation Tasks",
            "score": 0.44860541393010644,
            "section_title": "Metrics and Methods",
            "char_start_offset": 7019,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1502
                },
                {
                    "start": 1505,
                    "end": 1532
                }
            ],
            "ref_mentions": [
                {
                    "start": 815,
                    "end": 846,
                    "matchedPaperCorpusId": "6294674"
                },
                {
                    "start": 846,
                    "end": 871,
                    "matchedPaperCorpusId": "160705"
                },
                {
                    "start": 895,
                    "end": 913,
                    "matchedPaperCorpusId": "49667948"
                },
                {
                    "start": 994,
                    "end": 1012,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1106,
                    "end": 1128,
                    "matchedPaperCorpusId": "206593880"
                },
                {
                    "start": 1210,
                    "end": 1225,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1263,
                    "end": 1281,
                    "matchedPaperCorpusId": "235078802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7568359375
        },
        {
            "corpus_id": "232233576",
            "title": "Learning a Word-Level Language Model with Sentence-Level Noise Contrastive Estimation for Contextual Sentence Probability Estimation",
            "text": "In terms of language modeling, the calibration gives more confidence where the next word is more obvious. We assume that the relaxed softmax provides more accurate probability estimation for a sentence than it does for a word since the amount of adjustment is accumulated during the joint probability computation.",
            "score": 0.4483924175418692,
            "section_title": "III. WORD-LEVEL RNN LM FOR SENTENCE PROBABILITY ESTIMATION",
            "char_start_offset": 11338,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 313
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63037109375
        },
        {
            "corpus_id": "264811211",
            "title": "LitCab: Lightweight Language Model Calibration over Short- and Long-form Responses",
            "text": "An essential step in evaluating the calibration for LMs involves estimating the confidence from the model. Recent research primarily concentrates on methods classified into three types: logit-based, consistency-based, and verbalization-based. Logit-based estimation (Guo et al., 2017;Cheng et al., 2023) measures the model confidence based on the model predicted logits. Consistency-based estimation (Wang et al., 2023;Kuhn et al., 2023) relies on the intuition that LMs will consistently generate similar outputs when they are confident in responding to a query. A major challenge of consistency-based methods is deciding on the semantic equivalence of multiple long outputs, which is typically accomplished by utilizing a natural language inference model (Kuhn et al., 2023), BERTScore (Zhang et al., 2020b), or QA-based metrics (Fabbri et al., 2022). However, these methods are limited to sentence-length generations, and it is still unclear how to decide on semantic equivalence over long-form outputs. More recent studies (Tian et al., 2023;Xiong et al., 2023) investigate directly prompting instruction-tuned LMs to verbalize their confidence. While consistency-based and verbalization-based methods have demonstrated their effectiveness in recent LMs, their utilization is largely restricted due to their high computational costs during inference and the requirement for LM's instruction-following capability. In contrast, the LITCAB works only adds minimal computational cost and is suitable for use with any LM whose last-layer hidden states are accessible.",
            "score": 0.4482699124258201,
            "section_title": "CONFIDENCE ESTIMATION FOR LANGUAGE MODELS (LMS)",
            "char_start_offset": 8058,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1566
                }
            ],
            "ref_mentions": [
                {
                    "start": 266,
                    "end": 284,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 284,
                    "end": 303,
                    "matchedPaperCorpusId": "252917981"
                },
                {
                    "start": 400,
                    "end": 419,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 419,
                    "end": 437,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 757,
                    "end": 776,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 831,
                    "end": 852,
                    "matchedPaperCorpusId": "245218667"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79248046875
        },
        {
            "corpus_id": "270371795",
            "title": "When is Multicalibration Post-Processing Necessary?",
            "text": "Calibration is a well-studied property of predictors which guarantees meaningful uncertainty estimates. Multicalibration is a related notion -- originating in algorithmic fairness -- which requires predictors to be simultaneously calibrated over a potentially complex and overlapping collection of protected subpopulations (such as groups defined by ethnicity, race, or income). We conduct the first comprehensive study evaluating the usefulness of multicalibration post-processing across a broad set of tabular, image, and language datasets for models spanning from simple decision trees to 90 million parameter fine-tuned LLMs. Our findings can be summarized as follows: (1) models which are calibrated out of the box tend to be relatively multicalibrated without any additional post-processing; (2) multicalibration post-processing can help inherently uncalibrated models and large vision and language models; and (3) traditional calibration measures may sometimes provide multicalibration implicitly. More generally, we also distill many independent observations which may be useful for practical and effective applications of multicalibration post-processing in real-world contexts. We also release a python package implementing multicalibration algorithms, available via `pip install multicalibration'.",
            "score": 0.44804024366042255,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.771484375
        },
        {
            "corpus_id": "277150701",
            "title": "Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey",
            "text": "\u2022 Calibration refers to closing the gap between the confidence score and the expected correctness conditioned on confidence score. It has a long history preceding LLM or even modern machine learning [25,97,98], but bears slightly different meanings in NLP. In general, we could define a perfectly calibrated confidence measure to achieve the following: \n\nwhere the expectation is taken over the joint distribution of x and generation s. A lot of papers focus on evaluating the calibration quality of specific LMs and tasks [61,63,134]. Evaluation typically relies on variants of Expected Calibration Error (ECE) [63,71,124]. Oftentimes confidence scores from classification could be directly applied [55,120,162] in order to evaluate whether an LM is overor under-confident, especially for de facto classification tasks like sentiment analysis or multiple-choice QA. \n\nAs uncertainty and confidence are often intertwined, many approaches used in uncertainty quantification have their counterpart in confidence estimation. For example, for black-box methods, Lin et al. [77] computes a similarity matrix of sampled responses and derives confidence estimates for each generation via its degree or distance derived from the graph Laplacian, before using these scores to compute uncertainty. Zhang et al. [153] extends such black-box methods to longer generations. For logit-based methods, Malinin and Gales [86] normalize Eq. ( 1) with the length of s. Further improvements include replacing the logit-sum or mean with weighted sum, by attention values on the important tokens [78] or by importance inferred from natural language inference (NLI) models [29]. Such variants of sequence likelihood could then be fed for (entropy-style) uncertainty computation [62,78]. \n\nAnother popular approach is asking the LM itself whether a particular free-form generation is correct [59]. However, this formulation also poses a restriction on the confidence estimation method, as it is essentially a scalar logit. Thus, many extensions focus on applying calibration methods from classification to calibrate such self-evaluation.",
            "score": 0.4465896049820459,
            "section_title": "Uncertainty and Confidence in LLMs",
            "char_start_offset": 8771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 352
                },
                {
                    "start": 355,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 866
                },
                {
                    "start": 869,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1763
                },
                {
                    "start": 1766,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2113
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 203,
                    "matchedPaperCorpusId": "109884250"
                },
                {
                    "start": 203,
                    "end": 206,
                    "matchedPaperCorpusId": "127203550"
                },
                {
                    "start": 206,
                    "end": 209,
                    "matchedPaperCorpusId": "122150550"
                },
                {
                    "start": 523,
                    "end": 527,
                    "matchedPaperCorpusId": "252847587"
                },
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "218487046"
                },
                {
                    "start": 619,
                    "end": 623,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 700,
                    "end": 704,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 704,
                    "end": 708,
                    "matchedPaperCorpusId": "253510101"
                },
                {
                    "start": 708,
                    "end": 712,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 1069,
                    "end": 1073,
                    "matchedPaperCorpusId": "258967487"
                },
                {
                    "start": 1301,
                    "end": 1306,
                    "matchedPaperCorpusId": "268793903"
                },
                {
                    "start": 1404,
                    "end": 1408,
                    "matchedPaperCorpusId": "231895728"
                },
                {
                    "start": 1650,
                    "end": 1654,
                    "matchedPaperCorpusId": "270095084"
                },
                {
                    "start": 1755,
                    "end": 1759,
                    "matchedPaperCorpusId": "257039062"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8603515625
        },
        {
            "corpus_id": "259095827",
            "title": "Proximity-Informed Calibration for Deep Neural Networks",
            "text": "We compare the performance of our proposed approach with baseline calibration algorithms, namely BeiT, MLP Mixer, ResNet50 and ViT. The detailed results are presented in Table 4, showcasing the effectiveness of our methods in mitigating proximity bias and enhancing calibration performance compared to existing calibration techniques. Additionally, even when applied to the most successful base calibration algorithm, our method achieves a notable reduction in calibration errors. This consistent improvement is particularly remarkable, especially in scenarios where the original calibration methods fail to enhance or even worsen performance. Third, we present the outcomes of our approach on the Natural Language Understanding task, specifically on the MultiNLI Match dataset, as displayed in Table 5. The results demonstrate that our method can improve confidence calibration performance in balanced datasets and achieve comparable performance in addressing proximity bias. To evaluate our method in large-scale long-tail datasets, we conduct experiments on long-tail datasets ImageNet-LT, and iNaturalist 2021. Table 6 shows our method's performance on ImageNet-LT, showing that our algorithm improves upon the original calibration algorithms in most cases under all four evaluation metrics, particularly on ECE, ACE, and PIECE. This suggests that our algorithm",
            "score": 0.44613896261609776,
            "section_title": "E Additional Experimental Results",
            "char_start_offset": 45262,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1365
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64892578125
        },
        {
            "corpus_id": "258426367",
            "title": "Calibration Error Estimation Using Fuzzy Binning",
            "text": "Neural network-based decision-making systems have evolved rapidly in the recent decade. Within the domain of natural language processing, deep learning has shaped the current evolution in language modeling. These neural network-based language models are trained on large text corpora and can be fine-tuned across a wide range of NLP tasks and further improved using synthetic semantic enhancement schemes [1], yielding state-of-the-art performance [2][3][4][5]. Ideally, a neural model should output reliable and confident prediction probabilities. But recent works have shown that neural networks are unreliable and output highly overconfident predictions, resulting in over-estimation of the model's confidence in decisions [6][7][8]. This leads to model miscalibration, i.e. a lack of alignment between a model's decision probabilities and its actual likelihood of correctness. This lack of calibration can severely impact the trustworthiness of a model's decisions. \n\nA widely adopted measure of the degree of miscalibration is Expected Calibration Error (ECE) [9], used to measure neural network reliability [10][11][12]. The highly overconfident output prediction probabilities of neural networks result in a left-skewed probability distribution [13]. Since ECE utilizes a fixedwidth crisp binning scheme, this skew results in higher probability bins largely contributing to the calibration error estimation, while lower probability bins are ignored [13][14][15]. To overcome these limitations, prior works have proposed alternative binning strategies such as equal-frequency binning [14], adaptive binning [15], replacing binning with smoothed kernel density estimation [16], and more. Most calibration error estimation techniques rely on crisp binning, which discards edge probabilities (probabilities that typically lie on the bin edge) that could have contributed to a more accurate calibration error estimation. Although some works have utilized fuzzification of prediction probabilities for downstream NLP tasks [17], the calibration impacts of such fuzzification are yet to be studied.",
            "score": 0.44583035887951505,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 969
                },
                {
                    "start": 972,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2098
                }
            ],
            "ref_mentions": [
                {
                    "start": 405,
                    "end": 408,
                    "matchedPaperCorpusId": "233347145"
                },
                {
                    "start": 454,
                    "end": 457,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 457,
                    "end": 460,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 726,
                    "end": 729,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 732,
                    "end": 735,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 1065,
                    "end": 1068,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 1113,
                    "end": 1117,
                    "matchedPaperCorpusId": "174803437"
                },
                {
                    "start": 1117,
                    "end": 1121,
                    "matchedPaperCorpusId": "211555673"
                },
                {
                    "start": 1121,
                    "end": 1125,
                    "matchedPaperCorpusId": "220546269"
                },
                {
                    "start": 1460,
                    "end": 1464,
                    "matchedPaperCorpusId": "229212600"
                },
                {
                    "start": 1464,
                    "end": 1468,
                    "matchedPaperCorpusId": "262648748"
                },
                {
                    "start": 1590,
                    "end": 1594,
                    "matchedPaperCorpusId": "229212600"
                },
                {
                    "start": 1613,
                    "end": 1617,
                    "matchedPaperCorpusId": "262648748"
                },
                {
                    "start": 1677,
                    "end": 1681,
                    "matchedPaperCorpusId": "212725167"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5869140625
        },
        {
            "corpus_id": "268889516",
            "title": "Uncertainty in Language Models: Assessment through Rank-Calibration",
            "text": "Language Models (LMs) have shown promising performance in natural language generation. However, as LMs often generate incorrect or hallucinated responses, it is crucial to correctly quantify their uncertainty in responding to given inputs. In addition to verbalized confidence elicited via prompting, many uncertainty measures (e.g., semantic entropy and affinity-graph-based measures) have been proposed. However, these measures can differ greatly, and it is unclear how to compare them, partly because they take values over different ranges (e.g., [0,\\infty) or [0,1]). In this work, we address this issue by developing a novel and practical framework, termed *Rank-Calibration*, to assess uncertainty and confidence measures for LMs. Our key tenet is that higher uncertainty (or lower confidence) should imply lower generation quality, on average. Rank-calibration quantifies deviations from this ideal relationship in a principled manner, without requiring ad hoc binary thresholding of the correctness score (e.g., ROUGE or METEOR). The broad applicability and the granular interpretability of our methods are demonstrated empirically.",
            "score": 0.445338286140859,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69921875
        },
        {
            "corpus_id": "271571420",
            "title": "Cost-Effective Hallucination Detection for LLMs",
            "text": "Initial hallucination scores may not be properly calibrated, which can lead to poor downstream decisions. Seminal work by Guo et al. [16] has demonstrated that neural models tend to be miscalibrated, particularly in the form of models being overconfident. Later work, focusing on language models, has confirmed this across a wide range of tasks, models and datasets [15]. Notably, there is also research which demonstrates that given particular prompts and in-distribution tasks, LLMs can be well-calibrated [10]. However, this has been shown to be brittle and dependent on context [17]. \n\nFormally, our score outputs probabilities p \u03c8 (\u0177 = 0|x, z) that z contains a hallucination, parameterized by \u03c8. The score is calibrated if, for any probability level p \u2208 [0, 1], the average observed frequency of hallucinations matches the predicted probability: \n\nA naive approach for obtaining scores would be to compute the model's probabilities marginally, ignoring the context/question x and generated response z and directly estimating p(y = 1). However, this does not allow for conditioning on individual inputs to obtain calibrated probabilities p(y = 1 | x, z) for specific x, and z, which is, however, impossible to guarantee [18]. Common calibration methods include temperature scaling (Platt scaling) of logit outputs [16,19], isotonic regression [20] or histogram binning [21], which operate marginally, and can thereby not account for different confidence levels for different inputs (e.g., with an LLM being more confident on certain domains than others). \n\nAn alternative is to partition the inputs into G groups and compute calibration separately for each group g \u2208 G. However, this assumes the groups are disjoint and does not handle inputs belonging to multiple groups, which is often necessary [18]. More advanced calibration methods, such as multicalibration, which we use in this work, allow defining G potentially overlapping groups [18]. Prior work has shown the effectiveness of modern calibration techniques in scoring LLM confidence, albeit in a white-box setting [8].",
            "score": 0.444625229032423,
            "section_title": "Calibration",
            "char_start_offset": 14360,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 587
                },
                {
                    "start": 590,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 851
                },
                {
                    "start": 854,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2084
                }
            ],
            "ref_mentions": [
                {
                    "start": 133,
                    "end": 137,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1319,
                    "end": 1323,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1323,
                    "end": 1326,
                    "matchedPaperCorpusId": "56563878"
                },
                {
                    "start": 1348,
                    "end": 1352,
                    "matchedPaperCorpusId": "3349576"
                },
                {
                    "start": 1374,
                    "end": 1378,
                    "matchedPaperCorpusId": "9594071"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79541015625
        },
        {
            "corpus_id": "225041216",
            "title": "Confidence Estimation for Attention-Based Sequence-to-Sequence Models for Speech Recognition",
            "text": "For end-to-end ASR, shallow fusion of a language model (LM) [34] is commonly used to improve ASR performance during decoding. The effect of shallow fusion on confidence estimation is investigated. The language model used is a three-layer LSTM network with width 4096 trained on the LibriSpeech LM corpus, which shares the same word-piece vocabulary as the attention-based sequence-to-sequence model. To take LM information for confidence estimation, the input to the CEM is extended by the LM probability for the current token. Other setups are the same as Sec. 4 3: ASR and word-level confidence performance for models with and without RNNLM shallow fusion (with PWLM). \n\nTable 3 shows the word-level confidence scores after PWLM for both softmax probabilities and CEM. Although WERs on test-clean and test-other decreased by 8\u223c9% relatively, there is no clear improvement on AUC and even substantial degradation for NCE. By comparing the first and second blocks of Table 3, the CEM improves the quality of confidence estimation more significantly when an additional LM is used. The contrast of P-R curves between softmax and CEM with LM shallow fusion is similar to Fig. 3.",
            "score": 0.4445740449136161,
            "section_title": "Effect of Language Model Fusion on Confidence Scores",
            "char_start_offset": 14310,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 670
                },
                {
                    "start": 673,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1175
                }
            ],
            "ref_mentions": [
                {
                    "start": 562,
                    "end": 563,
                    "matchedPaperCorpusId": "13601225"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45751953125
        },
        {
            "corpus_id": "276767564",
            "title": "An Efficient Plugin Method for Metric Optimization of Black-Box Models",
            "text": "Recently, Wu et al. (2024) demonstrated that a stronger version of calibration from the algorithmic fairness literature, multicalibration (H\u00e9bert-Johnson et al., 2018), has deep connections to robustness under distribution shift, and proposed a post-processing algorithm which adapts a predictor under both co-variate and label shift for regression tasks. \n\nIt is worth mentioning that language models have their own set of domain adaptation techniques, such as fine-tuning from supervised (Han et al., 2024) or human feedback (Tian et al., 2023), prompt tuning/engineering (Liu et al., 2023), in-context learning (Dong et al., 2022), etc. Our method is agnostic to the choice of underlying base model; nonetheless, we include fine-tuning as a suitable baseline where applicable.",
            "score": 0.44305397434516564,
            "section_title": "Introduction",
            "char_start_offset": 3470,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 355
                },
                {
                    "start": 358,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 779
                }
            ],
            "ref_mentions": [
                {
                    "start": 138,
                    "end": 167,
                    "matchedPaperCorpusId": "51880858"
                },
                {
                    "start": 527,
                    "end": 546,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 574,
                    "end": 592,
                    "matchedPaperCorpusId": "236493269"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6015625
        },
        {
            "corpus_id": "270737848",
            "title": "On Calibration of Speech Classification Models: Insights from Energy-Based Model Investigations",
            "text": "Despite the impressive performance of deep learning models in speech classification [1], issues such as overconfidence, calibration errors, and uncertainty estimation may hinder their reliability and generalization in real-world scenarios [2,3,4].Confidence calibration in these models poses a significant challenge [2,5,6].For example, in speech emotion recognition (SER) systems, the inherent uncertainty in modeling emotions affects the trustworthiness of the model's predictions [6].Overconfidence and underconfidence can indicate suboptimal calibration, leading to false positives or missed opportunities [3,4,7].The current state of research in speech classification models often overlooks the issue of confidence calibration, resulting in a lack of reliable methods and leading to uncertainty in predictions.The current state of confidence calibration in speech classification underscores a lack of reliable methods, fostering uncertainty and mistrust in model predictions.\n\nUndoubtedly, ensuring a well-calibrated confidence measure in a classification model is crucial for accurate predictions.Therefore, developing methodologies to adjust the predictions of a speech classification model is essential, balancing calibration with performance.Existing techniques, like Temperature Scaling and Vector Scaling, typically rescale the posterior distributions of classifier predictions [3,8,9].However, these methods need post-processing adjustments and require a persistent development set with enough samples.Alternatively, adjusting calibration during the model training process, such as using confidence regularization, offers another approach [10,11].\n\nRecently, the effectiveness of EBMs in achieving enhanced model calibration has been demonstrated [12], wherein the joint training process incorporates both discriminative and generative models.The EBMs characterize the relationship between density of input data and model energy, enabling predictions based on energy minimization.While this flexibility is advantageous, the training process of energy models involves intricate adjustments, making it a challenging endeavor [8,13].Following [12]'s work, [14,15] has intricately improved the training process of EBMs, substantially boosting both training efficiency and ultimate performance.Despite their effectiveness in computer vision, EBMs' potential for calibrating speech classification models remains untapped.",
            "score": 0.44274057117533866,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 247,
                    "end": 324
                },
                {
                    "start": 324,
                    "end": 487
                },
                {
                    "start": 487,
                    "end": 618
                },
                {
                    "start": 618,
                    "end": 815
                },
                {
                    "start": 815,
                    "end": 980
                },
                {
                    "start": 982,
                    "end": 1103
                },
                {
                    "start": 1103,
                    "end": 1251
                },
                {
                    "start": 1251,
                    "end": 1397
                },
                {
                    "start": 1397,
                    "end": 1514
                },
                {
                    "start": 1514,
                    "end": 1659
                },
                {
                    "start": 1661,
                    "end": 1855
                },
                {
                    "start": 1855,
                    "end": 1992
                },
                {
                    "start": 1992,
                    "end": 2142
                },
                {
                    "start": 2142,
                    "end": 2301
                },
                {
                    "start": 2301,
                    "end": 2427
                }
            ],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 87,
                    "matchedPaperCorpusId": "235524858"
                },
                {
                    "start": 239,
                    "end": 242,
                    "matchedPaperCorpusId": "11506631"
                },
                {
                    "start": 242,
                    "end": 244,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 244,
                    "end": 246,
                    "matchedPaperCorpusId": "259275104"
                },
                {
                    "start": 316,
                    "end": 319,
                    "matchedPaperCorpusId": "11506631"
                },
                {
                    "start": 319,
                    "end": 321,
                    "matchedPaperCorpusId": "239011778"
                },
                {
                    "start": 321,
                    "end": 323,
                    "matchedPaperCorpusId": "259923520"
                },
                {
                    "start": 483,
                    "end": 486,
                    "matchedPaperCorpusId": "259923520"
                },
                {
                    "start": 610,
                    "end": 613,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 613,
                    "end": 615,
                    "matchedPaperCorpusId": "259275104"
                },
                {
                    "start": 1389,
                    "end": 1392,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1392,
                    "end": 1394,
                    "matchedPaperCorpusId": "238215193"
                },
                {
                    "start": 1394,
                    "end": 1396,
                    "matchedPaperCorpusId": "261092306"
                },
                {
                    "start": 1655,
                    "end": 1658,
                    "matchedPaperCorpusId": "216867328"
                },
                {
                    "start": 1759,
                    "end": 1763,
                    "matchedPaperCorpusId": "208857409"
                },
                {
                    "start": 2135,
                    "end": 2138,
                    "matchedPaperCorpusId": "238215193"
                },
                {
                    "start": 2152,
                    "end": 2156,
                    "matchedPaperCorpusId": "208857409"
                },
                {
                    "start": 2165,
                    "end": 2169,
                    "matchedPaperCorpusId": "237571657"
                },
                {
                    "start": 2169,
                    "end": 2172,
                    "matchedPaperCorpusId": "252355146"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82080078125
        },
        {
            "corpus_id": "265213392",
            "title": "Llamas Know What GPTs Don't Show: Surrogate Models for Confidence Estimation",
            "text": "Confidence Estimation for LLMs. Confidence estimation for LLMs has been studied in several related works. Kadavath et al. (2022) show that Claude's model probabilities are well-calibrated on multiple/choice and True/False questions. Zhou et al. (2023) study the effect of introducing expressions of uncertainty into prompts, on model accuracy. Our work differs from these since we focus on confidence elicitation for models which don't provide log probabilities. Concurrent work (Xiong et al., 2023) studies calibration and selective classification of linguistic confidence scores generated by LLMs. While this work also elicits prompted confidences, they focus on self-consistency (SC) based methods which are expensive because they require prompting GPT-4 several times. Our proposed Surrogate and Mixture of models methods are less expensive, since model probabilities from smaller models (Llama 2) are used to improve the confidence estimates of larger models (GPT-4). We also show performance improvements over their best method. (Lin et al., 2022) examine fine-tuning language models to improve confidence estimation, which we do not have access to. Figure 5: Embeddings of Incorrect Questions for GPT-4 and Surrogate Models Plots of the embeddings of questions GPT-4 and two surrogate models (Llama 2 70B and Llama 2 13B) answer incorrectly on two representative datasets -TruthfulQA and College Chemistry. Questions only GPT-4 answers incorrectly are in blue, questions GPT-4 and the surrogate answer incorrectly are in black, and questions only the surrogate answers incorrectly are in green. There are more questions that both GPT-4 and Llama 2 70B answer incorrectly and more semantic similarity in their incorrect questions. This indicates that Llama 2 70B and GPT-4 struggle with semantically related concepts and that the 70B model may more closely estimate GPT-4's uncertainty than the 13B model. \n\nSelective Classification and OOD Detection.",
            "score": 0.44120241352086936,
            "section_title": "RELATED WORK",
            "char_start_offset": 23814,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1911
                },
                {
                    "start": 1914,
                    "end": 1957
                }
            ],
            "ref_mentions": [
                {
                    "start": 1035,
                    "end": 1053,
                    "matchedPaperCorpusId": "249191391"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37451171875
        },
        {
            "corpus_id": "265157516",
            "title": "A Survey of Confidence Estimation and Calibration in Large Language Models",
            "text": "Black-box methods assume that all parameters during inference are unknown, allowing access only to the generations. \n\nLinguistic confidence (verbalized method) refers to prompting language models to express uncertainty in human language. This involves discerning different levels of uncertainty from the model's responses, such as \"I don't know,\" \"most probably,\" or \"Obviously\" (Mielke et al., 2022) or prompting the model to output various verbalized words (e.g., \"lowest\", \"low\", \"medium\", \"high\", \"highest\") or numbers (e.g., \"85%\"). Xiong et al. (2023) demonstrated that prompting strategies like CoT (Wei et al., 2022), top-k (Tian et al., 2023), and their proposed multi-step method can improve the calibration of linguistic confidence. \n\nConsistency-based estimation assumes that a model's lack of confidence correlates with various responses, often leading to hallucinatory outputs. SelfCheckGPT (Manakul et al., 2023b) proposed a simple sampling-based approach that uses consistency among generations to find potential hallucinations. Five variants are utilized to measure the consistency: BERTScore (Zhang et al., 2020b), question-answering, n-gram, natural language inference (NLI) model (He et al., 2023), and LLM prompting. Lin et al. (2023) proposed to calculate the similarity matrix between generations and then estimate the uncertainty based on the analysis of the similarity matrix, such as the sum of the eigenvalues of the graph Laplacian, the degree matrix, and the eccentricity. Shrivastava et al. (2023) introduced white-box models as surrogate models, like LLaMA-2 (Touvron et al., 2023b) and then employed logit-based methods to estimate the confidence of the target model when prompted with the same task. They also showed that integrating such confidence with linguistic confidence from blackbox LLMs can provide better confidence estimates across various tasks.",
            "score": 0.44068397218934335,
            "section_title": "Black-box Methods",
            "char_start_offset": 15183,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 118,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 743
                },
                {
                    "start": 746,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1890
                }
            ],
            "ref_mentions": [
                {
                    "start": 379,
                    "end": 400,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 538,
                    "end": 557,
                    "matchedPaperCorpusId": "259224389"
                },
                {
                    "start": 606,
                    "end": 624,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 632,
                    "end": 651,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 1110,
                    "end": 1131,
                    "matchedPaperCorpusId": "127986044"
                },
                {
                    "start": 1200,
                    "end": 1217,
                    "matchedPaperCorpusId": "244346093"
                },
                {
                    "start": 1502,
                    "end": 1527,
                    "matchedPaperCorpusId": "265213392"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65966796875
        },
        {
            "corpus_id": "272423997",
            "title": "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration",
            "text": "Black-box large language models (LLMs) are increasingly deployed in various environments, making it essential for these models to effectively convey their confidence and uncertainty, especially in high-stakes settings. However, these models often exhibit overconfidence, leading to potential risks and misjudgments. Existing techniques for eliciting and calibrating LLM confidence have primarily focused on general reasoning datasets, yielding only modest improvements. Accurate calibration is crucial for informed decision-making and preventing adverse outcomes but remains challenging due to the complexity and variability of tasks these models perform. In this work, we investigate the miscalibration behavior of black-box LLMs within the healthcare setting. We propose a novel method, \\textit{Atypical Presentations Recalibration}, which leverages atypical presentations to adjust the model's confidence estimates. Our approach significantly improves calibration, reducing calibration errors by approximately 60\\% on three medical question answering datasets and outperforming existing methods such as vanilla verbalized confidence, CoT verbalized confidence and others. Additionally, we provide an in-depth analysis of the role of atypicality within the recalibration framework.",
            "score": 0.4403055964948557,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89599609375
        },
        {
            "corpus_id": "273345661",
            "title": "Taming Overconfidence in LLMs: Reward Calibration in RLHF",
            "text": "As Large Language Models (LLMs) significantly expand their functionality across a wide range of applications from complex problem solving (Wei et al., 2022;Song et al., 2023a) to science discovery (Imani et al., 2023;OpenAI, 2023), the importance of their reliability becomes increasingly critical. A key aspect of this reliability is language model calibration -the alignment between model confidence and its actual performance. LLM confidence can be assessed using two primary methods: logit-based approaches, derived from output token probability distributions, and verbalized expressions, where the model explicitly states its confidence level. In this paper, we focus on verbalized confidence, where we prompt LLMs to express a confidence score for their responses (Figure 1, Top). \n\nReinforcement Learning from Human Feedback (RLHF) has become a widely adopted technique to improve the performance and alignment of LLMs. The improvement is achieved through two primary components: reward modeling, which learns to predict human preferences from ranking datasets, and policy optimization, guided by reward models and typically implemented with Proximal Policy Optimization (PPO) (Schulman et al., 2017). However, recent studies (Kadavath et al., 2022;OpenAI, 2023) show that RLHF-trained LLMs tend to exhibit overconfidence, potentially due to sharpened output distributions. Previous research has explored various approaches to addressing LLM overconfidence. Scaling-based approaches (Guo et al., 2017;Zhang et al., 2020) adjust model logits using decoding temperature, while verbalized confidence is enhanced through prompting The vanilla model shows bias towards high confidence though the answer is incorrect. Our calibrated reward model can correctly assign a higher reward to low-confidence one for the incorrect answer. \n\nstrategies (Tian et al., 2023) and supervised fine-tuning (Lin et al., 2022) with ground truth accuracy. Recently, RLHF-based calibration methods (Xu et al., 2024;Tao et al., 2024) have been proposed. \n\nOur study investigates the underlying causes of overconfidence introduced by RLHF.",
            "score": 0.4396292170243078,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1831
                },
                {
                    "start": 1834,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2034
                },
                {
                    "start": 2037,
                    "end": 2119
                }
            ],
            "ref_mentions": [
                {
                    "start": 138,
                    "end": 156,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 156,
                    "end": 175,
                    "matchedPaperCorpusId": "254408960"
                },
                {
                    "start": 1490,
                    "end": 1508,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1508,
                    "end": 1527,
                    "matchedPaperCorpusId": "212725167"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7783203125
        },
        {
            "corpus_id": "278129791",
            "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review",
            "text": "Hybrid Scale-bining [41] p(y|x) = 1 \n\nin data-scarce scenarios. Non-parametric local recalibration (LoRe) method is a point-wise extension of histogram binning with a local calibration error (LCE) metric that considers bin-wise differences of confidence and accuracy of points with kernel similarity of features to balance the calibration of individual data points and similarly grouped data points [155]. \n\nScale-binning calibrator is a hybrid method that combines parametric and non-parametric calibration approaches [41]. In this method, we apply scaling on one part of the heldout dataset, B similar bins on the second part based on the empirical distribution of function, and HB on the third part. Scale binning can provide more stable measures; however, it only considers raw predictive scores Closed-box calibration. LLMs can be linguistically calibrated to mitigate the inaccurately expressed verbal confidence of the model [169]. A recent study [83] also found that open-box methods have only a narrow gap for addressing uncertainty compared to closed-box linguistics. While it is unclear if CoT prompting improves verbalized calibration, a recent study revealed that linguistic calibration can be more accurate than the numerical methods [200]. [88] stated that calibration metrics rely on the inductive hypothesis that training, test, and calibration data should be sampled from the same distribution, which is neither verifiable nor falsifiable at test time. ECE conflates the uncertainty and depends on accuracy and binning strategy [88]. In [205], authors consider measuring confidence in the wrong prediction for pre-trained language models. \n\nIn [172], the authors measure the fidelity of the answer in a multi-choice setting toward other answers by replacing its expression with the remaining options. If the answer is chosen again after a chain of such modifications, then fidelity is considered high, and confidence is calibrated by multiplying to normalized uncertainty. Providing fact and reflective reasoning was also found to improve the calibration [171]. Self-consistency [85] prompts LLM multiple times and samples answers to score the most frequent ones.",
            "score": 0.4384471241202004,
            "section_title": "Correctness alignment",
            "char_start_offset": 33630,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 38,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 405
                },
                {
                    "start": 408,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1656
                },
                {
                    "start": 1659,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2079
                },
                {
                    "start": 2080,
                    "end": 2181
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 24,
                    "matchedPaperCorpusId": "202718866"
                },
                {
                    "start": 399,
                    "end": 404,
                    "matchedPaperCorpusId": "245144954"
                },
                {
                    "start": 519,
                    "end": 523,
                    "matchedPaperCorpusId": "202718866"
                },
                {
                    "start": 932,
                    "end": 937,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 1555,
                    "end": 1560,
                    "matchedPaperCorpusId": "253244504"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.599609375
        },
        {
            "corpus_id": "257219242",
            "title": "A Survey on Uncertainty Quantification Methods for Deep Learning",
            "text": "In recent years, large language models (LLMs) have demonstrated remarkable capabilities in generating coherent and contextually relevant text across various applications, from natural language processing to decision-making tasks. These models, such as OpenAI's GPT-4 [2], have shown proficiency in tasks ranging from text summarization and translation to more complex problemsolving and creative writing. However, despite their impressive performance, a critical challenge remains: how to ensure accurate uncertainty quantification to enhance the trustworthiness of these models? Existing research in LLMs uncertainty quantification mainly falls into two main categories: uncertainty estimation, and confidence calibration [53]. Uncertainty estimation methods for LLMs includes logit-based methods [43], model internal state-based methods [144], and consistency-based estimation [112]. Logit-based methods use the logits in the model output layer to compute the class entropy of output tokens. Internal state-based methods use the variability of sample embedding from model intermediate layers to quantify uncertainty. The embedding that is far from the majority of others indicates high uncertainty. Consistency-based methods measure the consistency between different model outputs from a sampling process and use the consistency as the uncertainty score [86,99,112]. For uncertainty estimation in LLMs, it is important to consider the semantic equivalence of the generated answers as different model outputs may look different in the generated tokens but similar in semantics. Graph-based [99] and semantic methods [89] propose alternative strategies for confidence estimation based on the semantic pattern, using a semantics similarity graph of different model outputs for uncertainty quantification. For confidence calibration, the goal is to adjust the model so that the quantified uncertainty from the above methods is aligned with prediction accuracy [103,116]. However, the field is still in its infancy. In future research, more",
            "score": 0.4383152295410805,
            "section_title": "UQ for Large Language Models",
            "char_start_offset": 104693,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2037
                }
            ],
            "ref_mentions": [
                {
                    "start": 839,
                    "end": 844,
                    "matchedPaperCorpusId": "252668614"
                },
                {
                    "start": 1963,
                    "end": 1967,
                    "matchedPaperCorpusId": "250073258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8017578125
        },
        {
            "corpus_id": "258866001",
            "title": "Selectively Answering Ambiguous Questions",
            "text": "The most basic method we can use is likelihood-based calibration by computing p(\u03c3|q) = n i=1 p(\u03c3 i |\u03c3 i\u22121 , \u2022 \u2022 \u2022 , \u03c3 n , q) for a sequence sampled from X. This may be used to rank the answers from most confident to least confident based on the model's produced likelihood of that answer. However, in practical settings this method may not accurately reflect the probability of observing an answer for several reasons: \n\n(1) Language models typically incorporate several inference-time biases to improve the quality of the outputs, such as nucleus sampling (Holtzman et al., 2020), top-k sampling (Fan et al., 2018), length penalties, length truncation, and temperature. These techniques affect the output distribution in ways that are hard or impossible to capture in the likelihood score (Zhang et al., 2021a). \n\n(2) Even for decoders that do not incorporate inference-time biases, the likelihood function as defined by the auto-regressive decomposition might leak probability mass to infinite sequences (see Meister et al., 2023, Proposition 2.4). This implies that the model that we sample from might not be a valid probability distribution. \n\n(3) Finally, the likelihood fails to account for the extraction step f , which requires an intractable marginalization step over the model outputs. \n\nIn view of these limitations, it is preferable to use sampling to estimate properties of the resulting marginal distribution. Formally, we can use an Index of Qualitative Variation (IQV) (Wilcox, 1973) to measure the statistical dispersion of discrete samples produced by the model. Practically, there are a many possible IQVs, so we have chosen two fairly different approaches for our experiments. In each case, we generate 10 sampled outputs (at a temperature of 0.5) and use exact match (after lowercasing and removing puncutation) for comparison among outputs. Naturally, there are many other ways to compare outputs, including token-level overlap or an answer-span equivalence model, but we leave these alternatives for future work. See also Kuhn et al. (2023) and Lin et al. (2023) for concurrent work investigating alternative IQVs.",
            "score": 0.438263869208094,
            "section_title": "Likelihood",
            "char_start_offset": 6586,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 418
                },
                {
                    "start": 421,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1145
                },
                {
                    "start": 1148,
                    "end": 1295
                },
                {
                    "start": 1298,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2137
                }
            ],
            "ref_mentions": [
                {
                    "start": 557,
                    "end": 580,
                    "matchedPaperCorpusId": "127986954"
                },
                {
                    "start": 597,
                    "end": 615,
                    "matchedPaperCorpusId": "44134226"
                },
                {
                    "start": 790,
                    "end": 811,
                    "matchedPaperCorpusId": "216056240"
                },
                {
                    "start": 1485,
                    "end": 1499,
                    "matchedPaperCorpusId": "154845301"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2076416015625
        },
        {
            "corpus_id": "252089274",
            "title": "A Principled Evaluation Protocol for Comparative Investigation of the Effectiveness of DNN Classification Models on Similar-but-non-identical Datasets",
            "text": "There are studies that consider softmax outputs as approximations to the probabilities that are generated by DNN classification models: the use of softmax outputs has been considered as a baseline for detecting misclassified and out-of-distribution datapoints by [10], and has been demonstrated to be decent at capturing the uncertainty in the predictions produced by DNN models [11,12]. Our principled evaluation approach and our empirical evaluation results lend support to the use of softmax outputs as a baseline for uncertainty estimation. However, we also align with other works [8,9] that call for caution in the interpretation of softmax outputs as confidence values or probabilities that models have about their predictions, since models are prone to over-confidence. Although well-calibrated probabilities are desirable, our approach is not explicitly aimed at evaluating how calibrated the probabilities are. We aim at investigating how consistent the outputs of a model are. For example, a model that is over-confident but also consistent in multiple environments could still be useful for downstream pipelines, and the probabilities generated by the model could be further calibrated. \n\nCalibration of Models. Model calibration or confidence calibration is the problem of predicting probability estimates that are representative of the true correctness likelihood [8]. Published methods for model calibration involve improving the modeling process [14,15,11,16,17], thereby creating models that inherently produce calibrated probabilities, or utilizing post-processing techniques to improve the outputs of already trained models [18,8]. The Expected Calibration Error [18,8] is a popular metric for evaluating how calibrated models are.",
            "score": 0.43774653213857606,
            "section_title": "Softmax as Baseline for DNN Model Uncertainty.",
            "char_start_offset": 13557,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1197
                },
                {
                    "start": 1200,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1749
                }
            ],
            "ref_mentions": [
                {
                    "start": 263,
                    "end": 267,
                    "matchedPaperCorpusId": "13046179"
                },
                {
                    "start": 379,
                    "end": 383,
                    "matchedPaperCorpusId": "211252346"
                },
                {
                    "start": 585,
                    "end": 588,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 588,
                    "end": 590,
                    "matchedPaperCorpusId": "55700923"
                },
                {
                    "start": 1377,
                    "end": 1380,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1461,
                    "end": 1465,
                    "matchedPaperCorpusId": "49314079"
                },
                {
                    "start": 1465,
                    "end": 1468,
                    "matchedPaperCorpusId": "59608823"
                },
                {
                    "start": 1468,
                    "end": 1471,
                    "matchedPaperCorpusId": "211252346"
                },
                {
                    "start": 1474,
                    "end": 1477,
                    "matchedPaperCorpusId": "227275177"
                },
                {
                    "start": 1642,
                    "end": 1646,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 1646,
                    "end": 1648,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.415771484375
        },
        {
            "corpus_id": "259370520",
            "title": "ETHICIST: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation",
            "text": "(1) With prompt tuning and extra training data, we can better induce large-scale language models to generate their memorized training data and successfully achieves a 9.5% performance improvement on Recall and a 12.4% performance improvement on Recall Early stop .\n\n(2) The proposed smoothing loss can further enhance the ability to extract training data, boosting the Recall score from 60.8% to 62.3%. (3) The calibrated confidence provides a 6.3% improvement on Recall Early stop as expected, demonstrating the importance of calibrating confidence estimation for this task. (4) The smoothing loss is more effective in predicting exact suffixes while the calibrated confidence is more beneficial for identifying highly confident predictions, according to the significant drop in Recall without smoothing and the substantial decrease in Recall Early stop without calibration. (5) The calibrated confidence estimation is effective regardless of whether using prompt tuning. And it demonstrates greater advantages compared to the comparing (LM) baseline in recognizing predictions with higher confidence when using prompt tuning, indicated by increasing Recall Early stop (from 48.7 to 52.4).",
            "score": 0.4356187767802499,
            "section_title": "Main Results",
            "char_start_offset": 19939,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85693359375
        },
        {
            "corpus_id": "270371795",
            "title": "When is Multicalibration Post-Processing Necessary?",
            "text": "We conduct a large-scale evaluation of multicalibration methods, comparing three families of methods: (1) standard ERM, (2) ERM followed by a classical recalibration method (e.g.Platt scaling), and (3) ERM followed by an explicit multicalibration algorithm (e.g. that of H\u00e9bert-Johnson et al. (2018)).\n\nWe find that in practice, this comparison is surprisingly subtle: multicalibration algorithms do not always improve multicalibration (relative to the ERM baseline), for example.From the results of our extensive experiments on tabular, vision, and language tasks (involving running multicalibration algorithms more than 45K times), we extract a number of observations clarifying the utility of multicalibration.Most significantly, we find:\n\n1. ERM alone is often a strong baseline, and can often be remarkably multicalibrated without further postprocessing.In particular, on tabular datasets, multicalibration post-processing does not improve upon worst group calibration error of ERM for simple NNs.\n\n2. Multicalibration algorithms are very sensitive to hyperparameter choices, and can require large parameter sweeps to avoid overfitting.These algorithms tend to be most effective in regimes with large amounts of available data, such as image and language datasets.\n\n3. Traditional calibration methods such as Platt scaling or isotonic regression can sometimes give nearly the same performance as multicalibration algorithms, and are hyperparameter-free.Furthermore, compared to multicalibration post-processing, they are extremely computationally efficient and do not take very long to evaluate.\n\nWe also present numerous practical takeaways for users of multicalibration algorithms, which are not apparent from the existing theoretical literature, but are crucial considerations in practice.We believe that our investigations will not only broaden the practical applicability of multicalibration as a concept and algorithm, but also provide valuable information to the theoretical community as to what barriers multicalibration faces in practice.To both of these ends, all code used in our experiments is publicly accessible1 .\n\nOrganization.",
            "score": 0.43511514504701254,
            "section_title": "Our Contributions",
            "char_start_offset": 7047,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 178,
                    "end": 301
                },
                {
                    "start": 303,
                    "end": 480
                },
                {
                    "start": 480,
                    "end": 713
                },
                {
                    "start": 713,
                    "end": 741
                },
                {
                    "start": 743,
                    "end": 859
                },
                {
                    "start": 859,
                    "end": 1002
                },
                {
                    "start": 1004,
                    "end": 1141
                },
                {
                    "start": 1141,
                    "end": 1269
                },
                {
                    "start": 1271,
                    "end": 1458
                },
                {
                    "start": 1458,
                    "end": 1600
                },
                {
                    "start": 1602,
                    "end": 1797
                },
                {
                    "start": 1797,
                    "end": 2052
                },
                {
                    "start": 2052,
                    "end": 2133
                },
                {
                    "start": 2135,
                    "end": 2148
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.587890625
        },
        {
            "corpus_id": "267770526",
            "title": "Calibrating Large Language Models with Sample Consistency",
            "text": "Accurately gauging the confidence level of Large Language Models' (LLMs) predictions is pivotal for their reliable application. However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale. In this work, we explore the potential of deriving confidence from the distribution of multiple randomly sampled model generations, via three measures of consistency. We perform an extensive evaluation across various open and closed-source models on nine reasoning datasets. Results show that consistency-based calibration methods outperform existing post-hoc approaches. Meanwhile, we find that factors such as intermediate explanations, model scaling, and larger sample sizes enhance calibration, while instruction-tuning makes calibration more difficult. Moreover, confidence scores obtained from consistency have the potential to enhance model performance. Finally, we offer practical guidance on choosing suitable consistency metrics for calibration, tailored to the characteristics of various LMs.",
            "score": 0.43437488186114714,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94921875
        },
        {
            "corpus_id": "258762432",
            "title": "Accurate and Reliable Confidence Estimation Based on Non-Autoregressive End-to-End Speech Recognition System",
            "text": "Confidence estimation -predicting the probability of true correctness likelihood, is important for classification models in many aspects [1,2,3], and also essential and widely adopted in automatic speech recognition (ASR) field since decades ago [4,5,6]. In human-computer interaction flow, dialogue system and machine translation system require confidence score of upstream queries to take different actions; Semi-supervised training and active learning use confidence score to guide the training process [7]; In ASR results rescoring and correction, confidence score plays a role as uncertainty measurement [8,9,10]. At the age of conventional hybrid ASR system (HMM-DNN with external decoder), lattice expanded by transcription contains frame-level phoneme posterior thus it's natural to calculate reliable confidence score which meets the requirements above [8,11,12]. \n\nRecent years have seen dramatic improvement of ASR models in recognition accuracy and various end-to-end (E2E) models predicting the character sequence directly without explicit coupling acoustic model and language model [13,14,15,16]. Meanwhile, the confidence estimation quality collapses as recognition accuracy raises, and overconfidence is widely observed in both time-synchronous models and tokensynchronous models [17,18,19]. As discussed in the work of Li et. al. [20], the posterior given by decoder softmax layer is proved sharp and of poor-quality from the aspect of confidence calibration. Previous works make great efforts to address the overconfidence issue and calculate accurate confidence scores for E2E models. In [9], a sigmoid function is added after BERT [21] in order to enable the pretrained model to estimate token level confidence scores for further rescoing. A series of works is done by Li et. al. with confidence estimation on attention-based models [20,22,23], placing a fully-connected layer after the E2E decoder is proved an efficient way to perform confidence estimation in listen, attend and spell (LAS) model [24].",
            "score": 0.43301166596671214,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 872
                },
                {
                    "start": 875,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 2024
                }
            ],
            "ref_mentions": [
                {
                    "start": 137,
                    "end": 140,
                    "matchedPaperCorpusId": "207158152"
                },
                {
                    "start": 140,
                    "end": 142,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 142,
                    "end": 144,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 246,
                    "end": 249,
                    "matchedPaperCorpusId": "1357669"
                },
                {
                    "start": 249,
                    "end": 251,
                    "matchedPaperCorpusId": "18487363"
                },
                {
                    "start": 251,
                    "end": 253,
                    "matchedPaperCorpusId": "11506631"
                },
                {
                    "start": 506,
                    "end": 509,
                    "matchedPaperCorpusId": "6204581"
                },
                {
                    "start": 609,
                    "end": 612,
                    "matchedPaperCorpusId": "16725919"
                },
                {
                    "start": 612,
                    "end": 614,
                    "matchedPaperCorpusId": "238353904"
                },
                {
                    "start": 862,
                    "end": 865,
                    "matchedPaperCorpusId": "16725919"
                },
                {
                    "start": 865,
                    "end": 868,
                    "matchedPaperCorpusId": "28442245"
                },
                {
                    "start": 868,
                    "end": 871,
                    "matchedPaperCorpusId": "53109587"
                },
                {
                    "start": 1096,
                    "end": 1100,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1100,
                    "end": 1103,
                    "matchedPaperCorpusId": "206742834"
                },
                {
                    "start": 1103,
                    "end": 1106,
                    "matchedPaperCorpusId": "203592075"
                },
                {
                    "start": 1300,
                    "end": 1303,
                    "matchedPaperCorpusId": "5189112"
                },
                {
                    "start": 1303,
                    "end": 1306,
                    "matchedPaperCorpusId": "226200399"
                },
                {
                    "start": 1347,
                    "end": 1351,
                    "matchedPaperCorpusId": "225041216"
                },
                {
                    "start": 1607,
                    "end": 1610,
                    "matchedPaperCorpusId": "238353904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6162109375
        },
        {
            "corpus_id": "235313893",
            "title": "Knowing More About Questions Can Help: Improving Calibration in Question Answering",
            "text": "l et al., 2020) Ours (+ Likelihood) 91.6\u00b10.3 92.9\u00b10.1 11.3% compared to RC, partially because the answer accuracy is substantially lower. For example, with generation based model (T5)'s answer accuracy of 25.5, simply predicting incorrectly for every example will give 74.5 calibration accuracy. In both models, internal confidence scores (Likelihood and Normalized scores) provide reasonable calibrator performance, yet adding our feature set improves the performance. In particular, our calibrator shows a larger gain in the DPR setting. Encouraged by this result, we test our calibrator as an answer candidate reranker for top answer candidates from DPR. Despite high calibration accuracy of generation based approach, selective QA performance (Cov@Acc=80%) is higher with the extractive approach, suggesting comparing calibration performance across models of different accuracy is challenging.\n\nAnswer Reranking Table 6 shows the results of our calibrator as an answer candidate reranker. The calibrator considers the top 1,000 answer candidates (100 retrieved passages, each with top 10 answer spans) and outputs top candidates based on the calibrator score instead of the model score. We show negligible gains in top 1 accuracy but bigger gains in top 5 accuracy. These small but noticeable gains show potential for using calibrators to improve open retrieval QA performances, where  Table 6: Results on open domain question answering in NQ. The calibrator is used as a reranker for selecting the top answer span out of 1,000 answer spans (10 answer spans per each of 100 retrieved passages). multiple answer candidates are considered.  brator uses a standard pretrained language model (BERT) to encode [CLS; (q i , c i )] and takes the final layer hidden representation of the [CLS] token as a feature. Table 7 shows the performance of the [CLS] token classifier. Surprisingly, this calibrator outperforms the MaxProb baseline (in Table 3) in all settings and outperforms Kamath et al. (2020) (in Table 3) in most settings, indicating information about the question and context might be more useful than the QA model's confidence. Using the input example",
            "score": 0.4324581133535743,
            "section_title": "Results",
            "char_start_offset": 19736,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31494140625
        },
        {
            "corpus_id": "231603281",
            "title": "An Evaluation of Word-Level Confidence Estimation for End-to-End Automatic Speech Recognition",
            "text": "In this section we review two lines of research that are related to our work. \n\nConfidence scoring for speech recognition. Most prior work on confidence scoring for ASR targets classical systems based on the HMM-GMM paradigm. These methods first extract a set of features from the decoding lattice, acoustic or language model, and then train a classifier to predict whether the transcription is correct or not. Typical examples of features include log-likelihood of the acoustic realization, language model score, word duration, number of alternatives in the confusion network [8,9,10]. More recently, Swarup et al. have augmented the feature set with deep embeddings of the input audio and the predicted text [11], while Errattahi et al. have shown that the benefits of domain adaptation on the extracted features [12]. The classifiers employed by the confidence scoring methods range from conditional random fields [13,14] and multiple layer perceptrons [15] to bidirectional recurrent neural networks [16,17,18]. \n\nConfidence scoring in end-to-end systems. The baseline method for confidence estimation in neural networks is to use directly the probability of the most-likely prediction [19]. However the neural networks tend to be overconfident and the probability estimates can be improved through temperature scaling [20], which typically leads to better calibration [21,22]. The most promising direction in terms of simplicity and usefulness involves Monte Carlo estimation: Gal and Ghahramani use dropout at test time to obtain multiple predictions, which are then averaged [23], while Lakshminarayanan et al. average the predictions over an ensemble of networks usually trained with different initializations [24]. The latter has been show to be very reliable on challenging out-of-domain datasets [25], but coming at a high cost [22]. The literature on general confidence scoring is rich and continually evolving; the most interesting research avenues involve Bayesian averaging [26], generative models [27,28], input perturbations [29,30], exploiting inner activations [31,32].",
            "score": 0.43191246561227586,
            "section_title": "RELATED WORK",
            "char_start_offset": 3519,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 80,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 1015
                },
                {
                    "start": 1018,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2088
                }
            ],
            "ref_mentions": [
                {
                    "start": 580,
                    "end": 582,
                    "matchedPaperCorpusId": "14011613"
                },
                {
                    "start": 582,
                    "end": 585,
                    "matchedPaperCorpusId": "3127409"
                },
                {
                    "start": 815,
                    "end": 819,
                    "matchedPaperCorpusId": "61810683"
                },
                {
                    "start": 921,
                    "end": 924,
                    "matchedPaperCorpusId": "38857082"
                },
                {
                    "start": 956,
                    "end": 960,
                    "matchedPaperCorpusId": "14484284"
                },
                {
                    "start": 1004,
                    "end": 1008,
                    "matchedPaperCorpusId": "27136093"
                },
                {
                    "start": 1008,
                    "end": 1011,
                    "matchedPaperCorpusId": "13843598"
                },
                {
                    "start": 1011,
                    "end": 1014,
                    "matchedPaperCorpusId": "53110756"
                },
                {
                    "start": 1373,
                    "end": 1377,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1582,
                    "end": 1586,
                    "matchedPaperCorpusId": "160705"
                },
                {
                    "start": 1807,
                    "end": 1811,
                    "matchedPaperCorpusId": "174803437"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.494384765625
        },
        {
            "corpus_id": "247083990",
            "title": "Using calibrator to improve robustness in Machine Reading Comprehension",
            "text": "Calibration in NLP The question of whether a model's posterior probabilites provide an accurate empirical measure of how likely the model is to be correct on a given example has been put forward to examine the reliability of the model [Desai and Durrett, 2020]. A well-calibrated model should have output probabilities align with the true probabilities of its predictions, which means the confidences of their predictions should be more aligned with their accuracies. Previous works have found that the model which gives good confidence estimates on in-domain data is overconfident on OOD data [Desai and Durrett, 2020;Kamath et al., 2020]. \n\nIn MRC, models tend to choose results with max softmax probabilites as final outputs. But out-of-domain data leads to the shift of data distribution, so the softmax probabilies are unreliable estimates of confidence [Kamath et al., 2020;Xin et al., 2021]. Previous works used to apply the calibrator as a threshold to decide whether to abstain the prediction and try to avoid making confident yet incorrect predictions on the preserved examples [Kamath et al., 2020;Xin et al., 2021]. Instead of using it as a threshold, we use the calibrator as a reranker based on the inspiration of [Zhang et al., 2021] and our analysis on bad cases in MRC task.",
            "score": 0.4310919816026011,
            "section_title": "Adversarial Examples Generation Methods",
            "char_start_offset": 4800,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 640
                },
                {
                    "start": 643,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1291
                }
            ],
            "ref_mentions": [
                {
                    "start": 619,
                    "end": 639,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 859,
                    "end": 880,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 880,
                    "end": 897,
                    "matchedPaperCorpusId": "235719735"
                },
                {
                    "start": 1088,
                    "end": 1109,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 1109,
                    "end": 1126,
                    "matchedPaperCorpusId": "235719735"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67822265625
        },
        {
            "corpus_id": "268032879",
            "title": "Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models",
            "text": "Prompting Large Language Models. Recent research (Brown et al., 2020;Kojima et al., 2023) on large language models shows that in-context learning (ICL) achieves great effectiveness in using models as few-shot or zero-shot reasoners. Different styles of prompting such as Knowledge prompting (Liu et al., 2022), Chain of Thought (CoT) prompting (Wei et al., 2022b), Self-Consistency prompting (Wang et al., 2023), Self-ask (Press et al., 2023), Tree-of-Thought prompting (Yao et al., 2023), andSkill-in-Context (Chen et al., 2023) are then proposed to guide the model to elicit its knowledge for reasoning in different ways. \n\nMost previous work mainly focuses on how such a prompting method influences the model performance on various tasks. In this paper, we compare how confidence calibration is influenced by different prompting methods. \n\nConfidence Calibration of LLMs. Extracting honest and constructive confidence scores of large language models is considered an important step towards building faithful and trustworthy AI systems (Desai and Durrett, 2020;Si et al., 2023). Many methods have been proposed recently to get reliable confidence scores with different suffix prompts added after outputting possible answers, such as a follow of True or False multiple choice question (Kadavath et al., 2022), asking models to describe the likelihood of the answer (Lin et al., 2022), and describing the likelihood that the answer is correct with demonstrations (Tian et al., 2023). However, it remains unclear how robust the methods are and how good they are comparatively. Our paper proposes FaR prompting as an orthogonal method to improve calibration and compare different extraction methods with our test bed. \n\nRecently, Yang et al. (2023) discuss the honesty problem of models as part of the alignment. Qian et al. ( 2023) study the confidence change when there is a conflict between in-context and model internal knowledge. Another line of work links model confidence with human confidence (Zhou et al., 2023;Steyvers et al., 2024;Zhou et al., 2024).",
            "score": 0.4307074566937875,
            "section_title": "Related Work",
            "char_start_offset": 23915,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 840
                },
                {
                    "start": 843,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1715
                },
                {
                    "start": 1718,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 69,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 291,
                    "end": 309,
                    "matchedPaperCorpusId": "239016123"
                },
                {
                    "start": 344,
                    "end": 363,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1038,
                    "end": 1063,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 1063,
                    "end": 1079,
                    "matchedPaperCorpusId": "252917981"
                },
                {
                    "start": 1366,
                    "end": 1384,
                    "matchedPaperCorpusId": "249191391"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91015625
        },
        {
            "corpus_id": "225041216",
            "title": "Confidence Estimation for Attention-Based Sequence-to-Sequence Models for Speech Recognition",
            "text": "Since CEM is a model-based approach and the training data for CEM is the same as the ASR, the model is naturally more confident on the training set. Although the mismatch between training and test for CEM is mitigated by having more aggressive augmentation during training and applying PWLMs estimated on dev sets during testing, it is still unclear how well the confidence scores from CEM generalises to data from a mismatched domain. Wall Street Journal (WSJ) [35], a dataset of clean read speech of news articles, is in a moderately mismatched domain compared to LibriSpeech in terms of speaker, style and vocabulary. In Table 4, WSJ eval92 test set is fed into the same setup as in Sec. 4.3, where all models are trained on LibriSpeech. Similar to observations in Table 3, shallow fusion worsens the confidence estimation by softmax probabilities despite reduced WER. CEM improves the quality of confidence estimation significantly with or without LM.",
            "score": 0.43015850038285575,
            "section_title": "Generalisation to a Mismatched Domain",
            "char_start_offset": 15537,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 955
                }
            ],
            "ref_mentions": [
                {
                    "start": 462,
                    "end": 466,
                    "matchedPaperCorpusId": "2618014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.354736328125
        },
        {
            "corpus_id": "273186420",
            "title": "Calibrating Expressions of Certainty",
            "text": "For the second scenario, since the baseline methods only work with scalar confidence scores, we mapped the verbalized phrases to the mean of their corresponding confidence distributions. (Tian et al., 2023) also evaluates histogram binning under the verbalized phrase setup. To ensure a fair comparison, we calculate ECE and Brier Score (BS) for our method the same way as for the baselines, as opposed to following our formulation. Our method is unique in producing calibrated certainty phrases directly, while baselines require conversion to scalar confidence scores during calibration. \n\nTable 1 demonstrates the effectiveness of our optimal transport calibration method in improving the calibration of language models. We show that even after reducing the output natural language expressions of certainty to scalar values (by simply taking the mean of the corresponding confidence distribution), our method remains competitive and does not compromise performance in terms of accuracy. We emphasize that our method's key advantage isn't in outperforming baseline calibration methods, but in directly operating on and producing natural language certainty phrases -a prop-Table 1: Comparison of post-hoc calibration methods for language models expressing confidence as probabilities or certainty phrases. Our optimal transport calibration method, even after reducing output certainty phrases to scalar values (e.g., by taking the mean of the confidence distribution), remains competitive with calibration baselines without compromising accuracy. Unlike traditional methods limited to confidence scores, our approach directly operates on and produces certainty phrases, offering actionable suggestions to improve human calibration. erty that existing methods lack. This makes our approach uniquely suited for improving human calibration in real-world settings, as it provides actionable guidance (e.g., suggesting radiologists use \"May\" instead of \"Present\" in their reporting to mitigate overconfidence) rather than abstract probability adjustments that can't be easily understood by humans.",
            "score": 0.42951653826687963,
            "section_title": "Models",
            "char_start_offset": 24343,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 588
                },
                {
                    "start": 591,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 2092
                }
            ],
            "ref_mentions": [
                {
                    "start": 187,
                    "end": 205,
                    "matchedPaperCorpusId": "258865733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.736328125
        },
        {
            "corpus_id": "256808480",
            "title": "Realistic Conversational Question Answering with Answer Selection based on Calibrated Confidence and Uncertainty Measurement",
            "text": "As it is nearly impossible for models to always make accurate predictions, unreliable predictions become serious issues when deploying machine learning models to realworld settings. Motivated to prevent such a risk, mechanisms of estimating the reliability of model's predictive probabilities based on confidence and uncertainty are recently proposed (Abdar et al., 2021;Houben et al., 2022). We note that confidence is usually measured by the softmax outputs of models (Guo et al., 2017), and that uncertainty can be quantified by Bayesian models, which can be approximated via Monte Carlo (MC) dropout (Gal and Ghahramani, 2016;Kendall and Gal, 2017). With much work on confidence and uncertainty estimations in computer vision tasks (Guillory et al., 2021), related topics have been recently adopted for NLP tasks as well (Shelmanov et al., 2021;Wu et al., 2021;Malinin and Gales, 2021;Vazhentsev et al., 2022). While confidence and uncertainty estimation should also be considered in ConvQA, we believe that this venue is under-explored so far. In particular, since questions are asked sequentially, it is likely that untrustworthy predictions in the conversation history would negatively affect the performance. To tackle this, we propose to exclude low-confident or uncertain predictions when training and evaluating the ConvQA model.\n\nCalibration Confidence and uncertainty help interpret the validity of the model's prediction. However, it is not safe to rely on them when the model is not calibrated, where the correct likelihood does not match the predicted probability (Guo et al., 2017), or the model error does not match the predicted uncertainty (Laves et al., 2019). Since deep neural networks are prone to miscalibration as the number of parameters has much increased, large pre-trained language models are also not free from this problem (Wang et al., 2021;Zhao et al., 2021;Dan and Roth, 2021). One of the most prevalent approaches to calibrating the model is to rescale a logit vector before the softmax function for regularizing the probability, which is known as temperature scaling (Guo et al., 2017). While there exist lots of",
            "score": 0.4286123185651916,
            "section_title": "Confidence and Uncertainty",
            "char_start_offset": 8272,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 351,
                    "end": 371,
                    "matchedPaperCorpusId": "226307260"
                },
                {
                    "start": 604,
                    "end": 630,
                    "matchedPaperCorpusId": "160705"
                },
                {
                    "start": 825,
                    "end": 849,
                    "matchedPaperCorpusId": "231648187"
                },
                {
                    "start": 849,
                    "end": 865,
                    "matchedPaperCorpusId": "237421080"
                },
                {
                    "start": 865,
                    "end": 889,
                    "matchedPaperCorpusId": "231895728"
                },
                {
                    "start": 889,
                    "end": 913,
                    "matchedPaperCorpusId": "248780161"
                },
                {
                    "start": 1660,
                    "end": 1680,
                    "matchedPaperCorpusId": "203593350"
                },
                {
                    "start": 1855,
                    "end": 1874,
                    "matchedPaperCorpusId": "238215193"
                },
                {
                    "start": 1874,
                    "end": 1892,
                    "matchedPaperCorpusId": "231979430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.625
        },
        {
            "corpus_id": "15706225",
            "title": "Active and Semi-Supervised Learning in ASR: Benefits on the Acoustic and Language Models",
            "text": "First, various confidence measures can be used in ASR. A survey of possible confidence measures is given in [8] and several techniques for confidence score calibration have been developed in [9]. Secondly, there are various possible ways of selecting data based on the confidence scores. \n\nSemi-Supervised Training (SST) has also recently received a particular attention in the ASR literature. A method combining multi-system combination with confidence score calibration was proposed in [10]. A large-scale approach based on confidence filtering together with transcript length and transcript flattening heuristics was used in [11]. A cascaded classification scheme based on a set of binary classifiers was proposed in [12]. A reformulation of the Maximum Mutual Information (MMI) criterion used for sequence-discriminative training of Deep Neural Networks (DNN) was described in [13]. A shared hidden layer multi-softmax DNN structure specifically designed for SST purpose was proposed in [14]. The way unsupervised data 1 is selected in this paper is inspired from [11], as it is based on confidence filtering with possible additional constraints on the length and frequency of the transcripts. \n\nThis paper aims at addressing the following questions whose answer is left either open or unclear with regard to the literature: i) Do more sophisticated confidence models help improving data selection? ii) Is AL also beneficial for LM training, and if so to which extent? iii) How do the gains of AL and SST scale up when more and more supervised data is transcribed? iv) Are the improvements similar after cross-entropy and sequence-discriminative training of the DNN AM? \n\nIn most of existing AL and SST studies (e.g. [2,3,6,7,13,14]), the Word Error Rate (WER) typically ranges between 25 and 75%. The baseline model in the present work has a WER of about 12.5%, which makes the application of AL and SST on an industrial task even more challenging. \n\nThe paper is structured as follows. Section 2 presents the approach studied throughout our experiments.",
            "score": 0.4283295975734943,
            "section_title": "Introduction",
            "char_start_offset": 2156,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 287
                },
                {
                    "start": 290,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1197
                },
                {
                    "start": 1200,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1673
                },
                {
                    "start": 1676,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1953
                },
                {
                    "start": 1956,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 111,
                    "matchedPaperCorpusId": "18487363"
                },
                {
                    "start": 191,
                    "end": 194,
                    "matchedPaperCorpusId": "11506631"
                },
                {
                    "start": 488,
                    "end": 492,
                    "matchedPaperCorpusId": "2097513"
                },
                {
                    "start": 881,
                    "end": 885,
                    "matchedPaperCorpusId": "9443248"
                },
                {
                    "start": 1721,
                    "end": 1724,
                    "matchedPaperCorpusId": "1635495"
                },
                {
                    "start": 1724,
                    "end": 1726,
                    "matchedPaperCorpusId": "11364150"
                },
                {
                    "start": 1726,
                    "end": 1728,
                    "matchedPaperCorpusId": "14149578"
                },
                {
                    "start": 1728,
                    "end": 1730,
                    "matchedPaperCorpusId": "27321988"
                },
                {
                    "start": 1730,
                    "end": 1733,
                    "matchedPaperCorpusId": "9443248"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74560546875
        },
        {
            "corpus_id": "259224389",
            "title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
            "text": "Empowering large language models to accurately express confidence in their answers is essential for trustworthy decision-making. Previous confidence elicitation methods, which primarily rely on white-box access to internal model information or model fine-tuning, have become less suitable for LLMs, especially closed-source commercial APIs. This leads to a growing need to explore the untapped area of black-box approaches for LLM uncertainty estimation. To better break down the problem, we define a systematic framework with three components: prompting strategies for eliciting verbalized confidence, sampling methods for generating multiple responses, and aggregation techniques for computing consistency. We then benchmark these methods on two key tasks-confidence calibration and failure prediction-across five types of datasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs including GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights: 1) LLMs, when verbalizing their confidence, tend to be overconfident, potentially imitating human patterns of expressing confidence. 2) As model capability scales up, both calibration and failure prediction performance improve. 3) Employing our proposed strategies, such as human-inspired prompts, consistency among multiple responses, and better aggregation strategies can help mitigate this overconfidence from various perspectives. 4) Comparisons with white-box methods indicate that while white-box methods perform better, the gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements, none of these techniques consistently outperform others, and all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement. We believe this study can serve as a strong baseline and provide insights for eliciting confidence in black-box LLMs.",
            "score": 0.42823585638449285,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6689453125
        },
        {
            "corpus_id": "265212730",
            "title": "On the Calibration of Multilingual Question Answering LLMs",
            "text": "Multilingual pre-trained Large Language Models (LLMs) are incredibly effective at Question Answering (QA), a core task in Natural Language Understanding, achieving high accuracies on several multilingual benchmarks. However, little is known about how well their confidences are calibrated. In this paper, we comprehensively benchmark the calibration of several multilingual LLMs (MLLMs) on a variety of QA tasks. We perform extensive experiments, spanning encoder-only, encoder-decoder, and decoder-only QA models (size varying from 110M to 7B parameters) and diverse languages, including both high- and low-resource ones. We study different dimensions of calibration in in-distribution, out-of-distribution, and cross-lingual transfer settings, and investigate strategies to improve it, including post-hoc methods and regularized fine-tuning. For decoder-only LLMs such as LlaMa2, we additionally find that in-context learning improves confidence calibration on multilingual data. We also conduct several ablation experiments to study the effect of language distances, language corpus size, and model size on calibration, and how multilingual models compare with their monolingual counterparts for diverse tasks and languages. Our experiments suggest that the multilingual QA models are poorly calibrated for languages other than English and incorporating a small set of cheaply translated multilingual samples during fine-tuning/calibration effectively enhances the calibration performance.",
            "score": 0.42758513383228963,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8818359375
        },
        {
            "corpus_id": "273821704",
            "title": "Graph-based Confidence Calibration for Large Language Models",
            "text": "For the Llama3 model, the confidence calibration performance on TriviaQA is shown in Table 1. For the TriviaQA dataset, it can be observed that the likelihood-based method performs poorly on the calibration error (ECE and Brier Score) and AUROC due to unreliable model prediction probability (Zhang et al., 2024). Platt scaling improves the ECE post-calibration and enhances the model's discriminative ability, resulting in higher AUROC results. However, this method cannot capture the semantic equivalence among answers, leading to sub-optimal performance. The Verbalized and Verbalized Qual prompts LLM to output confidence for their answers, improving AUROC by 3 \u2212 5% compared with the likelihood baseline. However, it faces the overconfidence issue; thus, the calibration errors are still high. The GraphSpectral method can produce good confidence estimations, but its calibration performance is poor. Even with the addition of techniques such as Isotonic Calibration or Platt Scaling, this issue can only be partially mitigated. The auxiliary DeBERTa method combines the LLM outputs, Chain-of-Thoughts (CoT) outputs, and verbalized confidence to fine-tune the DeBERTa model for predicting confidence. Our method captures the prediction confidence based on the graph structure of LLM's responses in semantic space and achieves better ECE results. The ECE is reduced from 0.07 to 0.03 and improves the AUROC from 0.72 to 0.86 compared with the baseline calibration methods. The experiment results on TruthfulQA, HotpotQA and CoQA for the Llama3 model are shown in Table 1. These results show a similar trend, with our model achieving superior performance in confidence calibration compared to the baseline methods. Furthermore, we also compare the confidence calibration performance for the Vicuna model on the TriviaQA, CoQA, TruthfulQA and HotpotQA datasets. The results are summarized in Table 2. Our model consistently improves the calibration error compared to the baseline methods. Both GraphSpectral and our method have a similar assumption that the consistency level between responses indicates the confidence levels of these responses.",
            "score": 0.42755302941991663,
            "section_title": "Experiment results",
            "char_start_offset": 23332,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2147
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8955078125
        },
        {
            "corpus_id": "268723623",
            "title": "Few-Shot Recalibration of Language Models",
            "text": "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain. Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.",
            "score": 0.42743692167850017,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.966796875
        },
        {
            "corpus_id": "259095925",
            "title": "Uncertainty in Natural Language Processing: Sources, Quantification, and Applications",
            "text": "Some studies have shown that accurately quantifying uncertainty can help identify situations where the model is uncertain, thereby improving the reliability and interpretability of the output [2,3,4]. \n\nConsidering the unified trends of NLP and the essentiality of uncertainty, a systematic review of uncertaintyrelevant NLP and the corresponding solutions is still lacking, which we aim to fill in this survey. Initially, we arise a research question: what are the sources of uncertainty in the NLP field? Here, we naturally connect uncertainty with multiple stages of an NLP system and summarize arXiv:2306.04459v1 [cs.CL] 5 Jun 2023 the sources into input, system, and output. More concretely, natural language input to NLP system is inherently ambiguous and context-dependent, making it difficult to achieve perfect performance and reliability in many NLP tasks [5,6,7]. Then, the system, such as the network initialization, architecture, and interior computation, etc., could introduce randomness, leading to uncertainty. Lastly, the uncertainties of outputs are due to complicating factors, such as a lack of broader knowledge, unreasonable model parameters, or specification of task output. \n\nIn light of the in-depth analysis of uncertainty sources, we further review the literature about the uncertainty quantification. Uncertainty estimation is ubiquitous in DNNs, and a common example is represented by confidence of the network output. Specifically, the Softmax scores obtained from these networks provide a direct means of estimating confidence values that are easily converted to uncertainty scores, e.g. subtracting the confidence from 1. However, the predictions are typically represented as point estimates [8,9], and mistakes are inevitable. Guo et al. [10] point out that DNNs tend to exhibit excessive confidence, making their confidence scores inaccurate. This is problematic because under-/over-confident in the network can lead to wrong decisions and actions based on overconfident predictions. To address this challenge, researchers have developed various uncertainty estimation methods for DNNs. This survey categorizes uncertainty estimation methods into three groups based on different modeling approaches: (1) calibration confidence-based methods, (2) sampling-based methods, and (3) distribution-based methods.",
            "score": 0.4273001492506785,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2091,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 203,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1197
                },
                {
                    "start": 1200,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2339
                }
            ],
            "ref_mentions": [
                {
                    "start": 192,
                    "end": 195,
                    "matchedPaperCorpusId": "52039644"
                },
                {
                    "start": 195,
                    "end": 197,
                    "matchedPaperCorpusId": "53715584"
                },
                {
                    "start": 866,
                    "end": 869,
                    "matchedPaperCorpusId": "216553205"
                },
                {
                    "start": 871,
                    "end": 873,
                    "matchedPaperCorpusId": "245501917"
                },
                {
                    "start": 1724,
                    "end": 1727,
                    "matchedPaperCorpusId": "3580844"
                },
                {
                    "start": 1727,
                    "end": 1729,
                    "matchedPaperCorpusId": "225094449"
                },
                {
                    "start": 1771,
                    "end": 1775,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54638671875
        },
        {
            "corpus_id": "270286165",
            "title": "Reassessing How to Compare and Improve the Calibration of Machine Learning Models",
            "text": "Comparing these proposed improvements, however, is non-trivial due to the fact that the measurement of calibration in practice is itself an active area of research (Nixon et al., 2019;Kumar et al., 2019;B\u0142asiok et al., 2023), and improvements with respect to one calibration measure do not necessarily indicate improvements with respect to another. \n\nEven if we fix a choice of calibration measure, the matter is further complicated by the existence of trivially calibrated models, such as models whose confidence is always their test accuracy (see Section 3). Most works on improving calibration have approached these issues by choosing to report a number of different calibration metrics along with generalization metrics (e.g. negative log-likelihood or mean-squared error), but these choices vary greatly across works and are always non-exhaustive. \n\nMotivated by this variance in calibration reporting, our work aims to answer the following questions: \n\n\u2022 Do there exist any systematic issues in the reporting of calibration in the recent literature? \n\n\u2022 Is there a theoretically motivated choice of which calibration and generalization measures to report, as well as an efficient means of estimating and visualizing them jointly? \n\n1.1 SUMMARY OF MAIN CONTRIBUTIONS AND TAKEAWAYS In answering these questions, we make the following contributions. \n\n1. We identify multiple issues (identified in Sections 3, 4.1, and 4.2) in how calibration performance is reported in the context of multi-class classification, with the core issue being that many (even very recent) papers report only some notion(s) of calibration error along with test accuracy. With respect to these metrics, the trivial recalibration strategy of always predicting with the mean confidence obtained from a calibration set outperforms the most popular post-training recalibration methods. \n\n2. Based on this observation, we revisit the use of proper scoring rules for measuring calibration. We apply decomposition results for these scoring rules to show how we can motivate a calibration metric based on our choice of generalization metric, and in special cases how we can do the converse as well.",
            "score": 0.42701064393954696,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 348
                },
                {
                    "start": 351,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 852
                },
                {
                    "start": 855,
                    "end": 956
                },
                {
                    "start": 959,
                    "end": 1055
                },
                {
                    "start": 1058,
                    "end": 1235
                },
                {
                    "start": 1238,
                    "end": 1352
                },
                {
                    "start": 1355,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1861
                },
                {
                    "start": 1864,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2170
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 184,
                    "matchedPaperCorpusId": "102486060"
                },
                {
                    "start": 184,
                    "end": 203,
                    "matchedPaperCorpusId": "202718866"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.406005859375
        },
        {
            "corpus_id": "265157516",
            "title": "A Survey of Confidence Estimation and Calibration in Large Language Models",
            "text": "Mielke et al. ( 2022) proposed a calibratorcontrolled method for chatbots, which involves a trained calibrator to return the model confidence score and fine-tuned generative models to enable control over linguistic confidence. Lin et al. (2022) fine-tuned GPT-3 with the human-labeled dataset containing verbalized words and numbers to express uncertainty naturally. Zhou et al. (2023) empirically found that injecting expressions of uncertainty into prompts significantly increases the accuracy of GPT-3's answers and the calibration scores. \n\nDifferent datasets (Amayuelas et al., 2023;Yin et al., 2023;Wang et al., 2023d;Liu et al., 2023a) have been presented on questions that language models cannot answer or for which there is no clear answer. Amayuelas et al. (2023) analyzed how different language models, including both smaller and open-source models, classify a dataset of various unanswerable questions. They observed that LLMs show varying accuracy levels depending on the question type, while smaller and open-source models tend to perform almost randomly. Liu et al. (2023a) evaluated both open-source models like LLaMA-2 (Touvron et al., 2023b), Vicuna (Chiang et al., 2023), and closed-source models such as GPT-3.5 and GPT-4, focusing on their refusal rate, accuracy, and uncertainty in handling unanswerable questions.",
            "score": 0.42665939008705167,
            "section_title": "Improve the linguistic confidence",
            "char_start_offset": 19714,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 542
                },
                {
                    "start": 545,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1336
                }
            ],
            "ref_mentions": [
                {
                    "start": 367,
                    "end": 385,
                    "matchedPaperCorpusId": "257220189"
                },
                {
                    "start": 564,
                    "end": 588,
                    "matchedPaperCorpusId": "258841255"
                },
                {
                    "start": 750,
                    "end": 773,
                    "matchedPaperCorpusId": "258841255"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57275390625
        },
        {
            "corpus_id": "258865733",
            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
            "text": "Real-world prediction systems invariably make errors. However, some mitigation of these errors is possible if the system produces well-calibrated 1 confidence estimates. In this case, the system's least confident predictions correspond to those that are most likely to be incorrect, potentially allowing these predictions to be skipped or overridden by a human. In the context of language models, one consequence of poor calibration may be hallucination, where a language model confidently asserts incorrect facts or reasoning. While the ability of very large LMs to absorb and synthesize knowledge about the outside world has gained significant Figure 1: Verbalized confidence scores (blue) are better-calibrated than log probabilities (orange) for gpt-3.5-turbo. Raw model probabilities (top-left) are consistently over-confident. Verbalized numerical probabilities (bottom) are better-calibrated. Considering more answer choices (bottom-right) further improves verbalized calibration (as in 'Considering the Opposite' in psychology; Lord et al. (1985)). Verbalized expressions of likelihood (top-right) also provide improved calibration. Bar height is average accuracy of predictions in bin. Darker bars mean more predictions fall in that confidence range. Results computed on SciQ. \n\nattention (Brown et al., 2020;Roberts et al., 2020;Bubeck et al., 2023), relatively little attention has been given to their well-calibratedness (Kadavath et al., 2022). Further, most existing analyses of the calibratedness of LLMs focus on models trained with maximum likelihood, while in practice, the most widely-used LLMs (such as ChatGPT) are fine-tuned using methods such as reinforcement learning from human feedback (Christiano et al., 2017). Some findings suggest that RLHF-LMs may sacrifice well-calibrated predictions for the sake of closer adherence to user instructions in dialogue (Kadavath et al., 2022;OpenAI, 2023), as the reinforcement learning objective encourages the model to allocate probability mass to the most preferred answer(s), rather than matching the relative frequency of possible answers.",
            "score": 0.4257132300610431,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 54,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1285
                },
                {
                    "start": 1288,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 2108
                }
            ],
            "ref_mentions": [
                {
                    "start": 1036,
                    "end": 1054,
                    "matchedPaperCorpusId": "24047702"
                },
                {
                    "start": 1298,
                    "end": 1318,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1318,
                    "end": 1339,
                    "matchedPaperCorpusId": "211205183"
                },
                {
                    "start": 1712,
                    "end": 1737,
                    "matchedPaperCorpusId": "4787508"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89501953125
        },
        {
            "corpus_id": "273228940",
            "title": "Calibrating Verbalized Probabilities for Large Language Models",
            "text": "Large Language models (LLMs), such as GPT-4 (OpenAI, 2023), Claude 1 , Mistral (Jiang et al., 2023) have demonstrated to be versatile tools with strong capabilities across many natural language processing (NLP) tasks. Their performance is grounded on very large numbers of parameters and the refinement to align more closely with human preferences using reinforcement learning from human feedback (RLHF;Stiennon et al., 2020), direct preference optimization (DPO; Rafailov et al., 2024), etc. This provides LLMs with the flexibility to adapt to new tasks through natural language instructions (prompts) and contextual data (in-context learning) without requiring further training or parameter updates. \n\nWhile these LLMs achieved impressive performance on a wide range of tasks, recent studies suggest that the RLHF fine-tuned LLMs are poorly calibrated (Tian et al., 2023;Zheng et al., 2023;Zhao et al., 2021;Xiong et al., 2024). This prevents a reliable assessment of the confidences of LLM outputs, which is a key requirement in risksensitive domains such as medicine (Penso et al., 2024) and finance (Bahnsen et al., 2014), where one wants to prefer high-confidence decisions and avoid low-confidence actions. Unfortunately, calibrating LLMs is challenging, particularly for blackbox LLMs, which only provides closed-source APIs and thus users have no access to model output such as token-level probabilities. \n\nRecently, Tian et al. (2023) proposed prompting large language models (LLMs) to directly generate prediction confidence scores, a concept they refer to as verbalized confidence (Lin et al., 2022). In this approach, the language model is instructed to respond with both the predicted label and a numeric confidence score within the range of [0, 1], or with a natural language expression indicating confidence, such as \"highly likely.\" This procedure is known as confidence elicitation. \n\nTheir empirical results show that these verbalized probabilities offer better calibration than con-arXiv:2410.06707v1",
            "score": 0.42562020715159854,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 701
                },
                {
                    "start": 704,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1413
                },
                {
                    "start": 1416,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1900
                },
                {
                    "start": 1903,
                    "end": 2020
                }
            ],
            "ref_mentions": [
                {
                    "start": 403,
                    "end": 425,
                    "matchedPaperCorpusId": "11591887"
                },
                {
                    "start": 464,
                    "end": 486,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 854,
                    "end": 873,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 892,
                    "end": 910,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 1071,
                    "end": 1091,
                    "matchedPaperCorpusId": "266998189"
                },
                {
                    "start": 1104,
                    "end": 1126,
                    "matchedPaperCorpusId": "4344478"
                },
                {
                    "start": 1426,
                    "end": 1444,
                    "matchedPaperCorpusId": "258865733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82275390625
        },
        {
            "corpus_id": "256846523",
            "title": "Bag of Tricks for In-Distribution Calibration of Pretrained Transformers",
            "text": "Therefore, it is necessary to investigate how modern calibration techniques affect PLMs' calibration. \n\nIn this paper, focused on PLMs in multi-class classification tasks, we explore widely used calibration families, including (1) confidence penalty loss functions that can be used instead of cross-entropy loss, (2) data augmentations, and (3) ensemble methods. We consider a low-resource regime since the small size of the training dataset amplifies the miscalibration of models (Rahaman et al., 2021). We also observe PLMs especially produce unreliable predictions in the data scarcity setting (see Figure 1). Contributions. We conduct a comprehensive empirical study for the effectiveness of the above calibration methods. In this study, our findings are as follows: \n\n\u2022 A PLM trained with imposing a strong penalty on the over-confident output shows significant improved calibration performance, but its accuracy can slightly deteriorate. \n\n\u2022 For ensemble methods, Deep Ensemble (Lakshminarayanan et al., 2017) and MIMO (Havasi et al., 2021) increase the diversity of predictions, resulting in the well-calibrated predictions in the data scarcity setting. However, the ensemble methods show insufficient calibration when each ensemble member is overfitted to negative log-likelihood for the training dataset. \n\n\u2022 Data augmentation methods that can expose diverse patterns such as MixUp (Hongyi Zhang, 2018) and EDA (Wei and Zou, 2019) are more effective for calibration in PLMs compared to weak text-augmentation methods (Kolomiyets et al., 2011;Karimi et al., 2021). \n\nBuilding on our findings, we present Calibrated PLM (CALL), a blend of the discussed calibration methods. Numerical experiments demonstrate that the components of CALL complement each other's weaknesses. For instance, data augmentation and ensemble methods offset the accuracy decline caused by the confidence penalty loss, while data augmentation and the confidence penalty loss counteract overfitting in the ensemble model. Through our extensive experiments, we show the CALL's competitiveness on several text classification benchmarks.",
            "score": 0.425494370101124,
            "section_title": "Introduction",
            "char_start_offset": 2015,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 104,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 770
                },
                {
                    "start": 773,
                    "end": 943
                },
                {
                    "start": 946,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1313
                },
                {
                    "start": 1316,
                    "end": 1572
                },
                {
                    "start": 1575,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2113
                }
            ],
            "ref_mentions": [
                {
                    "start": 481,
                    "end": 503,
                    "matchedPaperCorpusId": "220633614"
                },
                {
                    "start": 1025,
                    "end": 1046,
                    "matchedPaperCorpusId": "222341528"
                },
                {
                    "start": 1526,
                    "end": 1551,
                    "matchedPaperCorpusId": "7186165"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.716796875
        },
        {
            "corpus_id": "270214143",
            "title": "Selectively Answering Visual Questions",
            "text": "A QA system is said to be well-calibrated when each prediction has a confidence score which can help assess how often it is correct.Formally, the paradigm of selective QA uses a scoring function s that attaches to each QA pair (q, a) a numeric score s(q, a).The score can be compared with a threshold \u03c4 so that the system answers-aka triggers-when s(q, a) > \u03c4 or abstains otherwise.As shown in Figure 1, various scoring methods can be used.We present first the methods from Cole et al. (2023a) for text-only QA, and then our proposal.\n\nLikelihood The likelihood-based calibration uses the output language model score p(a|q) computed using the chain rule: p(a|q) = n i=1 p(t i |t 1 , \u2022 \u2022 \u2022 , t i\u22121 , q) where the t i are the tokens that form the answer a.\n\nSampling Repetition Based on Wilcox (1973)'s Variation Ratio we compute the frequency of the most sampled output divided by the total number of samples, which coincides with the probability of the mode of the empirical distribution.When more samples agree with each other, the answer can be considered to be more trustworthy.\n\nSampling Diversity Computed as 1 \u2212 #unique #samples , it is inversely proportional to the number of distinct samples and is zero if all samples are different.",
            "score": 0.4254542876825826,
            "section_title": "Methods",
            "char_start_offset": 4880,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 132,
                    "end": 258
                },
                {
                    "start": 258,
                    "end": 382
                },
                {
                    "start": 382,
                    "end": 440
                },
                {
                    "start": 440,
                    "end": 534
                },
                {
                    "start": 536,
                    "end": 754
                },
                {
                    "start": 756,
                    "end": 988
                },
                {
                    "start": 988,
                    "end": 1081
                },
                {
                    "start": 1083,
                    "end": 1241
                }
            ],
            "ref_mentions": [
                {
                    "start": 474,
                    "end": 493,
                    "matchedPaperCorpusId": "258866001"
                },
                {
                    "start": 785,
                    "end": 798,
                    "matchedPaperCorpusId": "154845301"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.468994140625
        },
        {
            "corpus_id": "268253497",
            "title": "Learning to Maximize Mutual Information for Chain-of-Thought Distillation",
            "text": "Calibration measures the alignment between a model's predicted accuracy and its confidence levels.Lee et al. (2022) introduced an innovative perspective on model distillation, positioning the teacher model not only as a source of knowledge but also as a tool for identifying mis-calibration during the training of the student model.This ability to maintain calibration and make reliable predictions is crucial for downstream applications and has been the focus of prior studies (Chen et al., 2023;Lee et al., 2022;Jiang et al., 2021).Here, we apply the Expected Calibration Errors (ECE) and Average Confidence Scores to reflect the alignment between the model's predicted probabilities and the actual outcomes, thereby gauging the reliability and certainty of its predictions.Despite the potential limitations inherent in these metrics, we still employ ECE in our experiments due to its simplicity and popularity, as in previous work on investigating the calibration quality of T5 (Chen et al., 2023;Lee et al., 2022).We employ a 10-bin-based ECE metric and a softmax-based approach to compute average confidence scores from the test outputs across all four datasets.Given that e-SNLI and ANLI essentially represent the same task, we conduct an out-ofdomain experiment by testing the model checkpoint trained on one dataset with the test set of the other.This analysis gives us insights into how well our model generalizes across similar tasks and the robustness of its predictions in out-of-domain scenarios and to assess the calibration quality of the model more comprehensively.\n\nTable 6 presents the results of the distilled model calibration evaluation.Overall, both models report lower ECE and confidence scores on SVAMP and e-SNLI, indicating that these two tasks are more challenging and models are less certain about their prediction.Lower ECE values from our MI-based distillation approach are presented for e-SNLI and ANLI, and their respective out-of-domain tests.Notably, our method achieves an ECE of 4.35 in e-SNLI, significantly lower than DSS's 8.54.",
            "score": 0.42494846222321264,
            "section_title": "Analysis on T5 Calibration",
            "char_start_offset": 15511,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 98,
                    "end": 332
                },
                {
                    "start": 332,
                    "end": 534
                },
                {
                    "start": 534,
                    "end": 776
                },
                {
                    "start": 776,
                    "end": 1018
                },
                {
                    "start": 1018,
                    "end": 1167
                },
                {
                    "start": 1167,
                    "end": 1355
                },
                {
                    "start": 1355,
                    "end": 1581
                },
                {
                    "start": 1583,
                    "end": 1658
                },
                {
                    "start": 1658,
                    "end": 1843
                },
                {
                    "start": 1843,
                    "end": 1976
                },
                {
                    "start": 1976,
                    "end": 2067
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 115,
                    "matchedPaperCorpusId": "253097876"
                },
                {
                    "start": 478,
                    "end": 497,
                    "matchedPaperCorpusId": "258461606"
                },
                {
                    "start": 497,
                    "end": 514,
                    "matchedPaperCorpusId": "253097876"
                },
                {
                    "start": 514,
                    "end": 533,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 981,
                    "end": 1000,
                    "matchedPaperCorpusId": "258461606"
                },
                {
                    "start": 1000,
                    "end": 1017,
                    "matchedPaperCorpusId": "253097876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51513671875
        },
        {
            "corpus_id": "258676559",
            "title": "What are the Desired Characteristics of Calibration Sets? Identifying Correlates on Long Form Scientific Summarization",
            "text": "At a high-level, we fine-tune (FT) language models with standard maximum likelihood estimation (MLE) on each summarization corpus, and then calibration-tune (CT) on a combined objective, which adds a calibration loss (CA) to the MLE loss:\n\n\u03bb M LE , \u03bb CA are scalars controlling the relative weight of objective. For L CT , L M LE acts as a regularizer, as in Liu et al. (2022);Zhao et al. (2022). We describe the setup (objective, metrics, and candidate generation methods) for Relevance Calibration ( \u00a74.1) and Faithful Calibration ( \u00a74.2, before jointly discussing statistics on each setup ( \u00a74.3).",
            "score": 0.4245543537956414,
            "section_title": "Calibration Pipeline",
            "char_start_offset": 8054,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26708984375
        },
        {
            "corpus_id": "273098373",
            "title": "Calibrate to Discriminate: Improve In-Context Learning with Label-Free Comparative Inference",
            "text": "Large Language Models (LLMs) have exhibited emergent capabilities such as advanced creative writing, summarization, translation, arithmetic and commonsense reasoning, etc (Wei et al., 2022;Brown et al., 2020). One of the most fascinating aspects of LLMs is their in-context learning capabilities. In particular, this involves adding pairs of demonstration examples to the prompt, and has been shown to significantly enhance the performance of LLMs (Brown et al., 2020). This capability offers users the significant advantage of utilizing LLMs without the need to train or fine-tune their own models. As a result, there is a growing need for research into calibration techniques (Zhou et al., 2023;Zhao et al., 2021;Jiang et al., 2023;2021) to ensure the reliability of model outputs as well and improve performance. \n\nThe concept of calibration in modern deep learning models was first introduced in (Guo et al., 2017) where the authors also proposed metrics (such as Expected Calibration Errors, reliability diagrams, etc) and methods (such as temperature scaling) for characterizing and mitigating miscalibration issues. The miscalibration of LLMs has also been studied in recent works (Xiong et al., 2023;Tian et al., 2023;Liusie et al., 2024;Zhao et al., 2023;Geng et al., 2023;Kamath et al., 2020). In this work, we show that LLMs with zero-shot and few-shot prompting exhibit a unique miscalibration issue on classification tasks, which we refer to as indiscriminate miscalibration. This phenomenon occurs when models assign equal confidence to correct and incorrect predictions. We show that this phenomenon cannot be quantitatively measured by Expected Calibration Errors (ECE). One alternative metric that can help catch this phenomenon is using Macro-average Calibration Error (MacroCE) proposed in Si et al. (2022). However, it may not depict the phenomenon thoroughly as it only computes the means of the distributions. We further propose a metric to help describe the phenomenon in more details.",
            "score": 0.4240734763474234,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 815
                },
                {
                    "start": 818,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2008
                }
            ],
            "ref_mentions": [
                {
                    "start": 189,
                    "end": 208,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 448,
                    "end": 468,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 697,
                    "end": 715,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 900,
                    "end": 918,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1226,
                    "end": 1246,
                    "matchedPaperCorpusId": "259937561"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59423828125
        },
        {
            "corpus_id": "260704363",
            "title": "Trusting Language Models in Education",
            "text": "Previous works in RC to improve confidence calibration have been done. In ( [10]), several methods for confidence calibration of Deep Neural networks have been discussed. Their best method is the famous Temperature scaling, where the logits before the softmax activation are divided by some constant T, found via NLL optimization on the validation set; this has the effect of increasing the entropy of the model's output probability distribution. However, this doesn't change the AUC scores, as observed in section IV. For the sake of comparison, we also present the difference in results with respect to other approaches proposed in the paper, i.e.: Isotonic regression, Platt scaling and Temperature scaling. \n\nIn ( [13]), a Gradient Boosting Machine is proposed to calibrate the confidences. This GBM is trained on several features, including query and answers token length, softmax probability of start and end of answer spans, and even features based on the Query Embedding Gradients of the model, among others. The problem is that our RC model doesn't work by generating answer spans. Instead, given a query and a paragraph, it outputs the probability that the latter contains a response to the query. Another problem is the increase in complexity for calculating gradients, which is crucial for CK-12 and a lot of systems. \n\nIn ( [12]), the most similar to our approach, they use an XGBoost with multiple features: softmax scores, attention-based, and features related to the query and answer token lengths. Our improvements come from adding relevant attention features by interpreting them as a flow. From the attention flow, one can extract different informations, as presented in section IV. As a side note, in the paper, the authors put emphasis only on knowing when to refrain. We show, additionally, that not only the new calibrator has a better threshold for non-answerability, but the answerable questions also become better calibrated. This is shown in section V, in the reliability plots. Now, we briefly explain the methods used in ( [10]) that were also tested in this work.",
            "score": 0.42340261032411486,
            "section_title": "Related works",
            "char_start_offset": 4757,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 710
                },
                {
                    "start": 713,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1329
                },
                {
                    "start": 1332,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2093
                }
            ],
            "ref_mentions": [
                {
                    "start": 76,
                    "end": 80,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 2052,
                    "end": 2056,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73974609375
        },
        {
            "corpus_id": "265445194",
            "title": "Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex Questions",
            "text": "Estimating the confidence levels associated with the responses of LLMs is an important research topic. In the existing literature, methods can be grouped into three categories: 1) logits-based methods that focus on the probabilities derived from model logits; 2) verbalized confidence that directly asks a model to output its confidence; 3) consistency-based methods that yield multiple responses and use consistency as a surrogate for confidence. \n\nBefore initiating the reasoning framework, we first conducted a pilot study to investigate whether our explanation logits-based confidence is effective. As shown in Table 5, we compare different confidence calibration methods. For each method, given a question, we query LLMs multiple times (decode one response with greedy decoding, and sample another four with temperature t = 0.7), calculate the confidence score for each response, and take the most confident one as the final answer. Better answer F1 indicates better confidence calibration. We randomly sampled 500 questions from HotpotQA, and 500 from MuSiQue to conduct the pilot study. Answer F1 results show that our explanation logitsbased confidence outperforms others, including the model itself estimating its uncertainty in language, and taking answer tokens as part of scoring. \n\nIn addition, we found that the average confidence score for all the 500 examples in HotpotQA is -0.142, and the average confidence score for examples with answer F1 1.0 is -0.109, and that of examples with answer F1 0.0 is -0.176. The low confidence of incorrect answers and high score of correct ones indicates the effect of our method.",
            "score": 0.4233658188536582,
            "section_title": "A Pilot Study for Our Confidence Mechanism",
            "char_start_offset": 29056,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 447
                },
                {
                    "start": 450,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1292
                },
                {
                    "start": 1295,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1632
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8154296875
        },
        {
            "corpus_id": "253098773",
            "title": "On the Calibration of Massively Multilingual Language Models",
            "text": "Massively Multilingual Language Models (MMLMs) like mBERT (Devlin et al., 2019), XLMR (Conneau et al., 2020), mT5 (Xue et al., 2021) and mBART (Liu et al., 2020) have been surprisingly effective at zero-shot cross-lingual transfer i.e. when fine-tuned on an NLP task in one language, they often tend to generalize reasonably well in languages unseen during fine-tuning. \n\nThese models have been evaluated for their performance across a range of multilingual tasks (Pan et al., 2017;Nivre et al., 2018;Conneau et al., 2018) and numerous methods like adapters (Pfeiffer et al., 2020), sparse fine-tuning (Ansell et al., 2022) and few-shot learning (Lauscher et al., 2020) have been proposed to further improve performance of crosslingual transfer. Despite these developments, there has been little to no attention paid to the calibration of these models across languages i.e. how reliable the confidence predictions of these models are. As these models find their way more and more into the real word applications with safety implications, like Hate Speech Detection (Davidson et al., 2017;Deshpande et al., 2022) it becomes important to only take extreme actions for high confidence predictions by the model (Sarkar and KhudaBukhsh, 2021). Hence, calibrated confidences are desirable to have when deploying such systems in practice. Guo et al. (2017) showed that modern neural networks used for Image Recognition (He et al., 2016) though perform much better than the ones introduced decades ago (Lecun et al., 1998), but are significantly worse calibrated and often over-estimate their confidence on incorrect predictions. For NLP tasks specifically, Desai and Durrett (2020) showed that classifiers trained using pre-trained transformer-based models (Devlin et al., 2019) are well calibrated both in-domain and out-of-domain settings compared to non-pre-trained model baselines (Chen et al., 2017).",
            "score": 0.42297237420758294,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 369
                },
                {
                    "start": 372,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1898
                }
            ],
            "ref_mentions": [
                {
                    "start": 58,
                    "end": 79,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 86,
                    "end": 108,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 464,
                    "end": 482,
                    "matchedPaperCorpusId": "29939583"
                },
                {
                    "start": 558,
                    "end": 581,
                    "matchedPaperCorpusId": "218470133"
                },
                {
                    "start": 602,
                    "end": 623,
                    "matchedPaperCorpusId": "238856900"
                },
                {
                    "start": 646,
                    "end": 669,
                    "matchedPaperCorpusId": "226262344"
                },
                {
                    "start": 1065,
                    "end": 1088,
                    "matchedPaperCorpusId": "1733167"
                },
                {
                    "start": 1207,
                    "end": 1237,
                    "matchedPaperCorpusId": "227118923"
                },
                {
                    "start": 1332,
                    "end": 1349,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1412,
                    "end": 1429,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 1494,
                    "end": 1514,
                    "matchedPaperCorpusId": "14542261"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60009765625
        },
        {
            "corpus_id": "265157516",
            "title": "A Survey of Confidence Estimation and Calibration in Large Language Models",
            "text": "When identifying ambiguity in data or unanswerable questions, reliable LLMs are anticipated to refrain from providing answers rather than generating responses arbitrarily (Kamath et al., 2020). Ren et al. (2022) proposed a selective generation method based on relative Mahalanobis distance. Zablotskaia et al. (2023) provided a comprehensive benchmark study that evaluates various calibration methods in neural summarization. Cole et al. ( 2023) and Hou et al. (2023) respectively employed a disambiguate-and-answer approach and input clarification ensembling to measure data uncertainty for detecting ambiguous questions. \n\nUncertainty-guided data exploitation Through measuring data uncertainty, the most representative instances will be selected for few-shot learning (Yu et al., 2022) or human annotation (Su et al., 2022). Regarding the knowledge enhancement to LLMs, Jiang et al. (2023) proposed an adaptive multi-retrieval method that first forecasts future content and retrieves relevant documents stimulated by low-confidence tokens within upcoming sentences. \n\nComprehensive Benchmarks While confidence estimation and calibration have wide-ranging applications, a comprehensive benchmark across tasks and domains is required to better understand and evaluate these techniques' robustness and utility. Addressing this issue requires extensive human efforts to annotate the responses of LLMs, especially in long-form generation (Huang et al., 2024;Mishra et al., 2024). Treating LLMs' long generation for confidence estimation and calibration by parts, instead of as a whole, offers a promising direction for further enhancement. \n\nMulti-modal LLMs By employing additional pre-training with image-text pairings or by finetuning on specialized visual-instruction datasets, LLMs can be transited into the multimodal domain (Dai et al., 2023;Liu et al., 2023b;Zhu et al., 2023b). However, it remains unclear whether these confidence estimation methods are effective for multimodal large language models (MLLMs) and whether these models are well-calibrated.",
            "score": 0.42272367282963075,
            "section_title": "Ambiguity detection and selective generation",
            "char_start_offset": 26128,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 622
                },
                {
                    "start": 625,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1068
                },
                {
                    "start": 1071,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1637
                },
                {
                    "start": 1640,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2061
                }
            ],
            "ref_mentions": [
                {
                    "start": 171,
                    "end": 192,
                    "matchedPaperCorpusId": "219721462"
                },
                {
                    "start": 291,
                    "end": 316,
                    "matchedPaperCorpusId": "258187125"
                },
                {
                    "start": 450,
                    "end": 467,
                    "matchedPaperCorpusId": "265213190"
                },
                {
                    "start": 809,
                    "end": 826,
                    "matchedPaperCorpusId": "252089424"
                },
                {
                    "start": 873,
                    "end": 892,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 1456,
                    "end": 1476,
                    "matchedPaperCorpusId": "266999558"
                },
                {
                    "start": 1829,
                    "end": 1847,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 1865,
                    "end": 1883,
                    "matchedPaperCorpusId": "258291930"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5927734375
        },
        {
            "corpus_id": "225041216",
            "title": "Confidence Estimation for Attention-Based Sequence-to-Sequence Models for Speech Recognition",
            "text": "For various speech-related tasks, confidence scores from a speech recogniser are a useful measure to assess the quality of transcriptions. In traditional hidden Markov model-based automatic speech recognition (ASR) systems, confidence scores can be reliably obtained from word posteriors in decoding lattices. However, for an ASR system with an auto-regressive decoder, such as an attention-based sequence-to-sequence model, computing word posteriors is difficult. An obvious alternative is to use the decoder softmax probability as the model confidence. In this paper, we first examine how some commonly used regularisation methods influence the softmax-based confidence scores and study the overconfident behaviour of end-to-end models. Then we propose a lightweight and effective approach named confidence estimation module (CEM) on top of an existing end-to-end ASR model. Experiments on LibriSpeech show that CEM can mitigate the overconfidence problem and can produce more reliable confidence scores with and without shallow fusion of a language model. Further analysis shows that CEM generalises well to speech from a moderately mismatched domain and can potentially improve downstream tasks such as semi-supervised learning.",
            "score": 0.42227677096392596,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86669921875
        },
        {
            "corpus_id": "258588199",
            "title": "A Hybrid of Generative and Discriminative Models Based on the Gaussian-Coupled Softmax Layer",
            "text": "Moreover, the proposed method blends the discriminative and generative models and can tune the strength of the characteristics of the two models by the prior distribution of model parameters. \n\n2) Energy-based Models: EBMs involve a wide range of applications such as image generation [21], [22], texture generation [23], text generation [24], [25], compositional generation, memory modeling, protein design and folding, outlier detection, confidence calibration, adversarial robustness, semi-supervised learning [16], reinforcement learning, and continual learning. Du et al. [26] improved the training stability of EBMs by introducing KL divergence between the MCMC kernel and model distribution. They also showed that data augmentation and multi-scale processing can be used to improve robustness and generation quality. \n\nThe generative model part of the proposed Gaussiancoupled softmax layer can be regarded as a special case of the EBM when jointly trained with a deep NN. The resulting structure is similar to that of the joint energy-based model (JEM) [16] and details on the differences between them are discussed in the following section. \n\n3) Confidence Calibration: Since calibrated confidence is important in the real-world decision making process, many researchers have investigated confidence calibration. In particular, the confidence calibration for modern deep NNs are explored recently. Guo et al. [12] first explored the confidence calibration for modern deep NNs and found that a simple calibration method using a single parameter called temperature scaling was effective for many datasets. Seo et al. [27] proposed a confidence calibration method for deep NNs based on stochastic inference. Mehrtash et al. [28] proposed a confidence calibration method for medical image segmentation using a deep NN. Wang et al. [29] proposed a confidence calibration method for graph NNs. \n\nMost of the existing methods are called post-hoc calibration, which calibrates the confidence of an already trained model. The proposed method differs from them in that it aims to obtain a calibrated confidence at training time, thereby allowing the use in combination with existing post-hoc calibration methods.",
            "score": 0.42205941578721495,
            "section_title": "D. Related Work",
            "char_start_offset": 12246,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 194,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 823
                },
                {
                    "start": 826,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1896
                },
                {
                    "start": 1899,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2211
                }
            ],
            "ref_mentions": [
                {
                    "start": 285,
                    "end": 289,
                    "matchedPaperCorpusId": "57189202"
                },
                {
                    "start": 291,
                    "end": 295,
                    "matchedPaperCorpusId": "202765664"
                },
                {
                    "start": 316,
                    "end": 320,
                    "matchedPaperCorpusId": "9212174"
                },
                {
                    "start": 338,
                    "end": 342,
                    "matchedPaperCorpusId": "212945787"
                },
                {
                    "start": 344,
                    "end": 348,
                    "matchedPaperCorpusId": "229339738"
                },
                {
                    "start": 513,
                    "end": 517,
                    "matchedPaperCorpusId": "208857409"
                },
                {
                    "start": 577,
                    "end": 581,
                    "matchedPaperCorpusId": "227247944"
                },
                {
                    "start": 1061,
                    "end": 1065,
                    "matchedPaperCorpusId": "208857409"
                },
                {
                    "start": 1418,
                    "end": 1422,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1624,
                    "end": 1628,
                    "matchedPaperCorpusId": "52892244"
                },
                {
                    "start": 1730,
                    "end": 1734,
                    "matchedPaperCorpusId": "208512953"
                },
                {
                    "start": 1836,
                    "end": 1840,
                    "matchedPaperCorpusId": "238215193"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.381591796875
        },
        {
            "corpus_id": "277940122",
            "title": "Beyond One-Hot Labels: Semantic Mixing for Model Calibration",
            "text": "Network calibration (Guo et al., 2017) requires techniques to align model confidence with actual accuracy. Post-hoc methods adjust test-time parameters using a validation set to improve calibration. Notable techniques include Temperature Scaling (TS) (Guo et al., 2017), Histogram Binning (Zadrozny & Elkan, 2001), Beta calibration (Kull et al., 2017) and its extension to Dirichlet calibration (Kull et al., 2019). While simple, post-hoc methods are sensitive to distributional shifts (Ovadia et al., 2019). Training-time regularization methods involve explicit learning constraints and implicit loss functions. Label Smoothing (LS) (M\u00fcller et al., 2019) reduces output entropy by replacing one-hot targets with smoothened labels. Margin-based Label Smoothing (MbLS) (Liu et al., 2022) balances model discriminability and calibration by imposing a margin on logits. Implicit loss functions, such as those optimizing Expected Calibration Error (Karandikar et al., 2021;Kumar et al., 2018), Focal loss (Ross & Doll\u00e1r, 2017;Mukhoti et al., 2020), Inverse Focal loss (Wang et al., 2021), and Mean Square Error (Hui & Belkin, 2020;Liang et al., 2024), are effective objectives to improve calibration. Data-aware calibration primarily involves perturbation methods (Tao et al., 2024) and Mixup-based methods (Zhang et al., 2018;Yun et al., 2019;Hendrycks et al., 2019), originally designed for improved generalization . These techniques have been found effective for calibration (Zhang et al., 2018;Pinto et al., 2022). RegMixup (Pinto et al., 2022) uses Mixup as a regularizer for cross-entropy loss, enhancing both accuracy and uncertainty estimation. RankMixup (Noh et al., 2023) incorporates ordinal ranking relationships among samples to reduce Mixup label bias.",
            "score": 0.4215596941659746,
            "section_title": "Model Calibration",
            "char_start_offset": 4600,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1762
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 38,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 251,
                    "end": 269,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 289,
                    "end": 313,
                    "matchedPaperCorpusId": "9594071"
                },
                {
                    "start": 332,
                    "end": 351,
                    "matchedPaperCorpusId": "217264968"
                },
                {
                    "start": 395,
                    "end": 414,
                    "matchedPaperCorpusId": "202773833"
                },
                {
                    "start": 486,
                    "end": 507,
                    "matchedPaperCorpusId": "174803437"
                },
                {
                    "start": 768,
                    "end": 786,
                    "matchedPaperCorpusId": "244729457"
                },
                {
                    "start": 944,
                    "end": 969,
                    "matchedPaperCorpusId": "236772324"
                },
                {
                    "start": 969,
                    "end": 988,
                    "matchedPaperCorpusId": "49314079"
                },
                {
                    "start": 1001,
                    "end": 1022,
                    "matchedPaperCorpusId": "47252984"
                },
                {
                    "start": 1022,
                    "end": 1043,
                    "matchedPaperCorpusId": "211252346"
                },
                {
                    "start": 1323,
                    "end": 1340,
                    "matchedPaperCorpusId": "152282661"
                },
                {
                    "start": 1494,
                    "end": 1513,
                    "matchedPaperCorpusId": "258508961"
                },
                {
                    "start": 1524,
                    "end": 1544,
                    "matchedPaperCorpusId": "258508961"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5087890625
        },
        {
            "corpus_id": "270199748",
            "title": "SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales",
            "text": "While large language models (LLMs) exhibit remarkable proficiency in reasoning and generating * Equal contribution. \u2020 Corresponding author. \n\neffective responses (OpenAI, 2023;Touvron et al., 2023;Jiang et al., 2023;Wang et al., 2024), they often produce fabricated information (a.k.a, hallucination) and typically hesitate to indicate their uncertainty when faced with unfamiliar questions (Ye et al., 2023;Liu et al., 2023). Determining how to accurately obtain reliable confidence estimates from LLMs is essential (Xiong et al., 2023;Zhou et al., 2023), particularly when the responses are not limited to single tokens1 . \n\nPrevious work on eliciting confidence from LLMs includes prompting-based and trainingbased approaches. Prompting-based methods, such as direct prompting and self-consistency prompting in Figure 1, employ specific prompts to generate confidence scores or use answer consistency as a confidence indicator, even though these can have poor calibration performance or significantly increase inference latency (Tian et al., 2023;Xiong et al., 2023;Diao et al., 2023). Training-based approaches, such as group-based calibration training and R-Tuning in Figure 1, develop specialized datasets for fine-tuning that encourage LLMs to express confidence. However, these methods often provide suboptimal or binary confidence estimates, failing to accurately reflect the models' confidence levels (Lin et al., 2022;Zhang et al., 2023a;Yang et al., 2023). In conclusion, previous work tends to suffer from the following problems: (1) Poor calibration performance; (2) Coarse-grained confidence levels; (3) Long inference latencies. \n\nIn this work, we present SaySelf, a training framework that teaches LLMs to generate more accurate and fine-grained confidence estimates. It successfully tackles the aforementioned problems in previous work. More importantly, SaySelf also goes beyond the confidence elicitation in previous work, and further enables LLMs to Figure 1: The comparison between SaySelf and previous work.",
            "score": 0.41961159014536986,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 139
                },
                {
                    "start": 142,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 624
                },
                {
                    "start": 627,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1644
                },
                {
                    "start": 1647,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2030
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.478271484375
        },
        {
            "corpus_id": "276421463",
            "title": "How does a Language-Specific Tokenizer affect LLMs?",
            "text": "Building upon prior research that incorporates the notion of confidence level in deep learning, such as Inoue [2019], we examine the model's confidence level during the generation of each target token. This approach allows for a more intrinsic evaluation and analysis of the effect of tokenizers on LLMs. Similar to Schuster et al. [2022], Niehues and Pham [2019], where confidence level is used specifically in Language Modeling, we consider the probability calculated using the softmax layer as the confidence level at each time step. Formally, given an input token sequence X = x 1 , . . . , x i\u22121 , Figure 3: Example of the NTP task input in the three different target units. At the token level, the target is tokenized into a single token by both tokenizers. At the character level, the target is segmented into 3 tokens by the base tokenizer but is treated as a single token by the extended tokenizer. At the word level, both tokenizers split the target into multiple tokens. \n\nthe model predicts the next token x i by computing the probabilities of all possible tokens in the vocabulary V . These probabilities are obtained using the softmax function applied to the logits (the raw scores) produced by the model. Let z j denote the logit corresponding to the j-th token in the vocabulary V , where j \u2208 {1, . . . , |V |}. The softmax probability for the j-th token is given by: \n\nThe confidence level at each time step, which is the maximum softmax probability, is then calculated as: \n\nTo ensure more fair and accurate comparison between tokenizers with varying vocabulary sizes, we conduct normalization and additionally compare the normalized confidence levels. This necessity stems from the inherent characteristics of the softmax layer, wherein larger embedding sizes (or vocabulary sizes in this context) often lead to decreased probabilities assigned to each predicted token. To normalize the confidence level at time step t, we first calculate the average confidence level of the previous t \u2212 1 time steps: \n\nThen, we divide the confidence level at the current time step by this average:",
            "score": 0.41950073989395165,
            "section_title": "Confidence Level",
            "char_start_offset": 11885,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 981
                },
                {
                    "start": 984,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1383
                },
                {
                    "start": 1386,
                    "end": 1490
                },
                {
                    "start": 1493,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 2020
                },
                {
                    "start": 2023,
                    "end": 2101
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45654296875
        },
        {
            "corpus_id": "273186420",
            "title": "Calibrating Expressions of Certainty",
            "text": "In this work, we measure and calibrate both humans and computational models that convey their confidence using natural language expressions of certainty. The key idea is to treat certainty phrases as distributions over the probability simplex. Using this construction, we generalize existing estimators for miscalibration metrics, such as the expected calibration error (ECE) (Pakdaman Naeini et al., 2015), and visualization tools, such as the reliability diagrams (Wilks, 2006). To calibrate over certainty phrases, we learn a discrete and possibly stochastic calibration map over a set of these certainty phrases to a potentially different set. This mapping is derived as the solution to an optimal transport problem that minimizes the net change in calibration error. \n\nWe demonstrate our approach by analyzing the calibration of radiologists writing clinical reports, accounting for variables such as the pathology and radiologist's identity. Moreover, we show how Language Model Calibration Language model calibration typically involves defining real-valued confidence scores and using existing tools designed for calibrating classifiers. For instance, confidence scores can represent conditional probability of an answer given the context (Desai & Durrett, 2020) or the average probability across different paraphrases of answer tokens (Jiang et al., 2021). These methods require access to the model's internal state. Our work is closely related to \"verbalized confidence\" (Lin et al., 2022b;Tian et al., 2023), where the model articulates its confidence in token-space as certainty phrases. Rather than focusing solely on the mean, our method offers a more realistic quantification of calibration error by treating each certainty phrase as a distribution. Unlike previous studies that apply calibration techniques designed for classifiers (Desai & Durrett, 2020) or fine-tune the language model for controlled generation (Mielke et al., 2022), we propose a lightweight discrete policy that adjusts the use of certainty phrases to improve calibration.",
            "score": 0.4192780671082533,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2174,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 771
                },
                {
                    "start": 774,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 2058
                }
            ],
            "ref_mentions": [
                {
                    "start": 376,
                    "end": 406,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 466,
                    "end": 479,
                    "matchedPaperCorpusId": "122553878"
                },
                {
                    "start": 1246,
                    "end": 1269,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 1343,
                    "end": 1363,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 1480,
                    "end": 1499,
                    "matchedPaperCorpusId": "249191391"
                },
                {
                    "start": 1499,
                    "end": 1517,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 1847,
                    "end": 1870,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 1929,
                    "end": 1950,
                    "matchedPaperCorpusId": "250073258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.736328125
        },
        {
            "corpus_id": "267412346",
            "title": "Calibration and Correctness of Language Models for Code",
            "text": "LLMs for code are extensively studied [54], [55]. While calibration has a long history in modeling [8], [56], it is not a frequently studied topic in the SE community. Early work moving into modern machine learning studied the calibration of smaller neural models performing classification tasks on text and images; while these early models were poorly calibrated per se, their performance could be improved by simple scaling [22] of their output probabilities. As models became larger, calibration was found to improve [57]. Pre-training was also found to improve calibration [25], [58]; however, these findings have been disputed [47]. \n\nMore recent works evaluated LLM calibration on a wide variety of settings [7], [18], [25], [59]. Desai et al. [25] studied non-code (natural language) tasks such as inference or paraphrasing, with only intrinsic measures using oldergeneration models (BERT and RoBERTA). Jiang et al. [7] studied calibration for natural language question-answering using just intrinsic measures. In contrast, we study calibration for three coding-related tasks, using both artificial and natural code datasets, and both intrinsic and reflective confidence measures, to evaluate calibration in the SE domain. \n\nOther prior work has investigated tokens that might be edited. Vasconcelos et al. [60] discusses code model uncertainty for function-synthesis-style problems, and ran human evaluation of the usefullness of colored highlighting of uncertain tokens. They found highlighting a human-derived groundtruth of which tokens might be edited was helpful, and more useful than raw token probabilities from the model. Johnson et al. [61] developed method of highlighting likely edit tokens via a utility optimization algorithm comparing different file completions. We find exploring more on calibrated uncertainty for local areas be a interesting area for additional work. \n\nLi et al. [49] investigate the calibration of Computer vision (CV) models from an operational perspective i.e., the shift between training input and production inputs, presenting it as a software quality problem that can be addressed using Bayesian approaches.",
            "score": 0.4190658982754809,
            "section_title": "VII. RELATED WORK",
            "char_start_offset": 44335,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 49
                },
                {
                    "start": 50,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1229
                },
                {
                    "start": 1232,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1892
                },
                {
                    "start": 1895,
                    "end": 2155
                }
            ],
            "ref_mentions": [
                {
                    "start": 38,
                    "end": 42,
                    "matchedPaperCorpusId": "255546225"
                },
                {
                    "start": 99,
                    "end": 102,
                    "matchedPaperCorpusId": "122906757"
                },
                {
                    "start": 104,
                    "end": 108,
                    "matchedPaperCorpusId": "23794124"
                },
                {
                    "start": 426,
                    "end": 430,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 577,
                    "end": 581,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 583,
                    "end": 587,
                    "matchedPaperCorpusId": "59336190"
                },
                {
                    "start": 714,
                    "end": 717,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 725,
                    "end": 729,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 750,
                    "end": 754,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 923,
                    "end": 926,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 1314,
                    "end": 1318,
                    "matchedPaperCorpusId": "253393857"
                },
                {
                    "start": 1653,
                    "end": 1657,
                    "matchedPaperCorpusId": "257255061"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50244140625
        },
        {
            "corpus_id": "233204709",
            "title": "Noisy-Labeled NER with Confidence Estimation",
            "text": "Table 1 shows our primary results. We use local and global to denote locally / globally normalized confidence estimation strategies. We use oracle (unavailable in real settings) / searched \u03c4 to denote how we obtain the prior noise \u03c4 . We note that the Self-training baseline (Jie et al., 2019, line 6) is the most comparable baseline since our confidence estimation is directly integrated into it. We primarily compare this baseline with our best configurations (line 10 and 11). We focus on the shaded results as they are the most informative for demonstrating our method.\n\nGeneral Noise. Our methods (both local and global) outperforms the state-of-the-art method (Jie et al., 2019) by a large margin in three datasets (En, Sp, Du, line 10 and 11 v.s. 6), showing the effectiveness of our approach. We observe the oracle \u03c4 does not necessarily give the best performance and an over estimate of confidence could leave a better performance. Ablation results without calibration further show the effectiveness of our calibration methods (line 10 v.s. 14). We note that the CoNLL dataset is an exception where the calibration slightly hurt performance. Otherwise the improvements with calibration is clear in the other 7 datasets.\n\nDistant Supervision. Our method outperforms AutoNER and LRNT without pre-trained language models. Reasons that we are worse than BOND (line 16 v.s. 6) are: (a) many implementation aspects are different, and it is (currently) challenging to transplant their settings to ours; (b) they use multiple tailored techniques for distantly-labeled data (e.g., the adversarial training), while ours is more general-purpose. Though our method does not outperform BOND, it still outperforms AutoNER and LRNT (under the setting all without pretrained model, line 10 and 11 v.s. 7 and 8) and shows promising gain.",
            "score": 0.4182547199799061,
            "section_title": "Results",
            "char_start_offset": 8194,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 666,
                    "end": 683,
                    "matchedPaperCorpusId": "174800839"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56298828125
        },
        {
            "corpus_id": "269157057",
            "title": "Language Model Cascades: Token-level uncertainty and beyond",
            "text": "There has been a large body of work on uncertainty quantification for LMs.We discuss some of the approaches below.They can be broadly divided into the following categories.\n\nConsensus-based.One limitation of Equation 3 is that it considers a single output sequence, e.g., the most likely one.However, as there are many sequences that have similar meaning, it is intuitively more reliable to consider drawing multiple sequences from p(\u2022 | x).One may then assess the consensus in resulting predictions to measure confidence (Wang et al., 2023;Xiao et al., 2021;Chen et al., 2023c); this can help distinguish between models that are locally diffused versus peaked around a candidate sequence.Recently, Yue et al. (2023) explored the use of answer consistency to construct effective cascades.\n\nDeep ensembles and dropout.One approach to measure confidence is to create an ensemble of different models, and suitably aggregate their predictions (e.g., based on disagreement) (Van Landeghem et al., 2022;Wang et al., 2019b;Gleave & Irving, 2022).However, these uncertainty estimation procedures involve additional computation (e.g., multiple inferences with a single model in dropout-based approaches and single inference with multiple models in ensemble-based approaches) compared to simply using softmax probability outputs from a single network.Such approaches are less appealing for use in cascades, where the primary goal is to improve efficiency.\n\nPost-hoc calibration/Answer-and length-bias calibration.For tasks involving question-answering with multiple choices (e.g., (A), (B), (C)), several works have demonstrated that LMs can have prior biases to certain answers, which can be identified and corrected (Zhao et al., 2021;Holtzman et al., 2021;Kumar, 2022;Murray & Chiang, 2018;Mielke et al., 2022;Jiang et al., 2021).\n\nSemantic entropy.Another key challenge in measuring the uncertainty for natural language outputs is that there are a lot of semantic equivalent sentences and hence, the probability can be divided among multiple outputs which mean the exact same thing.",
            "score": 0.4178026946140491,
            "section_title": "Uncertainty Quantification for LMs",
            "char_start_offset": 29245,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 74,
                    "end": 114
                },
                {
                    "start": 114,
                    "end": 172
                },
                {
                    "start": 174,
                    "end": 190
                },
                {
                    "start": 190,
                    "end": 292
                },
                {
                    "start": 292,
                    "end": 441
                },
                {
                    "start": 441,
                    "end": 689
                },
                {
                    "start": 689,
                    "end": 788
                },
                {
                    "start": 790,
                    "end": 817
                },
                {
                    "start": 817,
                    "end": 1039
                },
                {
                    "start": 1039,
                    "end": 1341
                },
                {
                    "start": 1341,
                    "end": 1445
                },
                {
                    "start": 1447,
                    "end": 1503
                },
                {
                    "start": 1503,
                    "end": 1823
                },
                {
                    "start": 1825,
                    "end": 1842
                },
                {
                    "start": 1842,
                    "end": 2076
                }
            ],
            "ref_mentions": [
                {
                    "start": 522,
                    "end": 541,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 969,
                    "end": 997,
                    "matchedPaperCorpusId": "248335524"
                },
                {
                    "start": 997,
                    "end": 1016,
                    "matchedPaperCorpusId": "202539965"
                },
                {
                    "start": 1708,
                    "end": 1727,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 1727,
                    "end": 1749,
                    "matchedPaperCorpusId": "233296182"
                },
                {
                    "start": 1749,
                    "end": 1761,
                    "matchedPaperCorpusId": "248780520"
                },
                {
                    "start": 1761,
                    "end": 1783,
                    "matchedPaperCorpusId": "52132833"
                },
                {
                    "start": 1783,
                    "end": 1803,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 1803,
                    "end": 1822,
                    "matchedPaperCorpusId": "235078802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5166015625
        },
        {
            "corpus_id": "248863170",
            "title": "A Birds Eye View on Knowledge Graph Embeddings, Software Libraries, Applications and Challenges",
            "text": "issues with evaluation metrics and clarified that the currently adopted techniques do not assess KGC, are hard to use for calibration, and cannot reliably differentiate between various models. Calibration is additionally a vital part of KGC that has as of late got consideration [179]. Safavi et al. [180] show that calibration procedures can altogether diminish the alignment error of KGE models in the downstream tasks. Instinctively, calibration is a post-preparing step that changes KGE expectation scores to illustrate real and correct probabilities. \"Treating the likelihood of truth of a triple (\u03c3(\u03c8(s, r, o)) for triple s, r, o) as the certainty of the model for the triple, the model is considered to be calibrated if the certainty lines up with the pace of confirmed realities\" [178]. If certainty is equivalent to 0.5, then around half of triples with this certainty to be valid. Assuming this extent is a long way from 50%, the model isn't calibrated, i.e., the model is underconfident if the extent is greater and overconfident if it is lower. Under an optimal circumstance, if the model 31\n\npredicts that a triple is valid with a 0.9 certainty score, it ought to be right 90% of the time.\n\nModel calibration needs dependable certainty estimation and successful alignment strategies to fix the calibration mistakes. As far as KGE is concerned, two certainty or confidence estimation techniques, SigmoidMax (SIG) and TopKSoftmax (TOP), are exploited in [179] [180]. There are two calibration techniques. Isotonic regression [181] is a non-parametric technique and does take into account sigmoid assumption. It fits an increasing constant function to the model yield and is generally suitable for many examples. However, it is prone to overfit. Platt scaling [182] on the other takes into account the sigmoid function that learns scalar weights to yield a confidence score for each example. It might work better for smaller datasets. Model Calibration has a few advantages. According to the framework's point of view, language processing pipelines that incorporate KG can depend on calib",
            "score": 0.41779559137883543,
            "section_title": "Comparative Analysis",
            "char_start_offset": 85181,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1536,
                    "end": 1541,
                    "matchedPaperCorpusId": "3349576"
                },
                {
                    "start": 1770,
                    "end": 1775,
                    "matchedPaperCorpusId": "56563878"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79638671875
        },
        {
            "corpus_id": "269626095",
            "title": "Selective Classification Under Distribution Shifts",
            "text": "As discussed in Section 2.2, most manually designed confidence scores focus on DNN models and are based on softmax responses (SRs), assuming that SRs closely approximate true posterior probabilities-closing such approximation gaps is the goal of confidence calibration. However, effective confidence calibration remains elusive (Guo et al., 2017;Nixon et al., 2019), and the performance of SR-based score functions is sensitive to the scale of raw logits and hence that of SRs, as explained below.",
            "score": 0.41760213447397965,
            "section_title": "Scale sensitivity of SR-based scores",
            "char_start_offset": 18999,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 497
                }
            ],
            "ref_mentions": [
                {
                    "start": 328,
                    "end": 346,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 346,
                    "end": 365,
                    "matchedPaperCorpusId": "102486060"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.293701171875
        },
        {
            "corpus_id": "273812005",
            "title": "Shortcut Learning in In-Context Learning: A Survey",
            "text": "If the prediction distribution shifts, then this shift is attributed to the corresponding context. Subsequently, during actual predictions, LLMs' outputs are adjusted based on the biased distribution to mitigate biases in the context. \n\nAs research progresses gradually, Contextual Calibration has spawned various calibration methods tailored for different shortcut mitigation scenarios. Prototypical Calibration employs a Gaussian mixture distribution to estimate prototypical clusters for all categories and then utilizes the likelihood of these prototype clusters to calibrate LLMs' predictions (Han et al., 2022). Domain-context Calibration employs randomly selected in-domain words from the task corpus to estimate the label bias of the language model and utilizes this estimation for bias mitigation (Fei et al., 2023). Similarly, PMI DC re-weighs scores based on the likelihood of hypotheses (answers) given premises (questions) in a specific task domain (Holtzman et al., 2021). \n\nBut these calibration schemes may fail due to their inability to effectively estimate contextual bias with only content-free and in-domain random tokens. To address this, Batch Calibration updates the bias as more batches of input data are processed, allowing the bias to stabilize after multiple mini-batches are considered (Zhou et al., 2024a). Generative Calibration adjusts the margin of the labels estimated via Monte-Carlo sampling over the in-context model to simply calibrate the contextual prediction distribution (Jiang et al., 2023). In-Context Calibration calibrates the test distribution of the demonstration using the prior semantics expected from the samples (Jang et al., 2024). PriDe is applied to the task of multiple choice questions by arranging the option content on a small number of test samples to estimate prior information, and then using the estimated prior information to debias the remaining samples (Zheng et al., 2024). Unlike the aforementioned probability-based calibration methods, NOISYICL calibrates language models by adding Gaussian noise to the parameters of LLMs, thereby reducing the prediction bias and unreliable confidence in ICL (Zhao et al., 2024b).",
            "score": 0.417540753090311,
            "section_title": "Model-centric Approach",
            "char_start_offset": 26952,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 234
                },
                {
                    "start": 237,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 806,
                    "end": 824,
                    "matchedPaperCorpusId": "258865519"
                },
                {
                    "start": 962,
                    "end": 985,
                    "matchedPaperCorpusId": "233296182"
                },
                {
                    "start": 1314,
                    "end": 1334,
                    "matchedPaperCorpusId": "263310485"
                },
                {
                    "start": 1663,
                    "end": 1682,
                    "matchedPaperCorpusId": "268385528"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55126953125
        },
        {
            "corpus_id": "270878725",
            "title": "Efficient Nearest Neighbor based Uncertainty Estimation for Natural Language Processing Tasks",
            "text": "Softmax Response (SR) is a trivial baseline, which treats the maximum score from output of the base model's softmax layer as the confidence (Cordella et al., 1995). \n\nTemperature Scaling (TS) is a calibration technique by which the logits are divided by a temperature parameter T before applying the softmax function (Guo et al., 2017). We optimized T by L-BFGS on validation set loss. \n\nLabel Smoothing (LS) is the calibration and generalization technique by introducing a small degree of uncertainty \u03f5 in the target labels during training (Miller et al., 1996;Pereyra et al., 2017). In LS, we optimized \u03f5 \u2208 {0.01, 0.05, 0.1, 0.2, 0.3} by using validation set accuracy when SA and NLI, and validation set F 1 when NER. \n\nMC Dropout is an UE technique by M times stochastic inferences with activating dropout (Gal and Ghahramani, 2016). In our experiments, we set M = 20 for all evaluations, and the dropout rate is 0.1. \n\nSpectral-Normalized Gaussian Process (SNGP) uses spectral normalization of the weights for distance-preserving representation and Gaussian Processes in the output layer for estimating uncertainty (Liu et al., 2020). \n\nPosterior Networks (PN) is one of the methods in the Evidential Deep Learning (EDL) framework (Sensoy et al., 2018) that assumes a probability distribution for class probabilities (Charpentier et al., 2020), which uses normalizing flow (Rezende and Mohamed, 2015) to estimate the density of each class in the latent space.",
            "score": 0.4173154929254704,
            "section_title": "C Details of Baselines",
            "char_start_offset": 25649,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 167,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 385
                },
                {
                    "start": 388,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 719
                },
                {
                    "start": 722,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 920
                },
                {
                    "start": 923,
                    "end": 1138
                },
                {
                    "start": 1141,
                    "end": 1463
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 163,
                    "matchedPaperCorpusId": "8896917"
                },
                {
                    "start": 317,
                    "end": 335,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 541,
                    "end": 562,
                    "matchedPaperCorpusId": "44872810"
                },
                {
                    "start": 562,
                    "end": 583,
                    "matchedPaperCorpusId": "9545399"
                },
                {
                    "start": 809,
                    "end": 835,
                    "matchedPaperCorpusId": "160705"
                },
                {
                    "start": 1119,
                    "end": 1137,
                    "matchedPaperCorpusId": "219792902"
                },
                {
                    "start": 1235,
                    "end": 1256,
                    "matchedPaperCorpusId": "46939880"
                },
                {
                    "start": 1321,
                    "end": 1347,
                    "matchedPaperCorpusId": "219708927"
                },
                {
                    "start": 1377,
                    "end": 1404,
                    "matchedPaperCorpusId": "12554042"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.496337890625
        },
        {
            "corpus_id": "272987064",
            "title": "Calibrating Language Models with Adaptive Temperature Scaling",
            "text": "Architecture. Temperature scaling, while effective in classification settings, struggles to adapt logits well in language modeling as the confidence scores that are most important (such as those that contain actual answers or facts) account for only a small portion of natural language sequences. Therefore, optimizing a single temperature parameter often results in post-RLHF language models still being overconfident post scaling. Additionally, language model miscalibration largely varies based on the type of token being predicted following RLHF. Matrix and vector scaling can in theory perform adaptive confidence prediction by using logits as features; however, they are prone to overfitting, as we find in Section 5. \n\nTo balance regularization with modeling capac-ity in our calibration head, we instead propose to use a head architecture that predicts a singular temperature for every token prediction. For an input pair (x, y), we first produce input-dependent features \u0125 \u2208 R lx+ly,h using the language model \u03c0. \n\nWe then learn a calibration head to produce a temperature vector c \u03b8 ( \u0125) = \u03c4 \u2208 R lx+ly . We exponentiate \u03c4 to ensure positive values then transform logits to yield calibrated logits q = \u1e91 \u2022 e \u03c4 . In practice, we find that directly using the logits \u1e91 as features can be inefficient (with a large vocabulary size) and also less effective compared to hidden states. Therefore, we use the last hidden state of the language model \u03c0 as the features for predicting \u03c4 . With this architecture formulation, we retain the ability to predict confidences adaptively depending on the context, while also never changing the ranking for the possible next token given specific context, as each set of token logits are scaled by only a single value. \n\nLoss function. To improve the process of calibration, we take inspiration from selective classification works (Choi et al., 2023) and use a loss function which adapts targets depending on the correctness of the original language model. For a logit, label pair q \u2208 R v , y \u2208 V , and weighting hyperparameter \u03b1 \u2208 [0, 1] we optimize the following loss function \u2113:",
            "score": 0.4172939294695329,
            "section_title": "Adaptive Temperature Scaling",
            "char_start_offset": 6609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 13
                },
                {
                    "start": 14,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1021
                },
                {
                    "start": 1024,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1757
                },
                {
                    "start": 1760,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1995
                },
                {
                    "start": 1996,
                    "end": 2120
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7490234375
        },
        {
            "corpus_id": "273351420",
            "title": "Fill In The Gaps: Model Calibration and Generalization with Synthetic Data",
            "text": "Among the three aspects of uncertainty management, calibration directly improves model performance by ensuring that model predictions are congruent with empirically observed outcomes. \n\nAI risk management is an emerging field that emphasizes understanding the limitations of model predictions. Model calibration techniques are used to address the fact that high accuracy does not always mean high confidence in a model's predictions. For example, consider a classifier trained to recognize handwritten digits. This model might achieve high accuracy on a test set, but it also provides the predicted probability for each class, which reflects its level of uncertainty. If it classi-Figure 1: The framework of our proposed method involves initially training a downstream model using real data to identify poorly calibrated bins. Data instances, x i , and their prediction probabilities, P (y i |x i ), from these bins are then fed into large language models (LLMs) to generate synthetic data, x syn i , along with their corresponding probabilities, P (y i |x syn i ). This synthetic data, combined with the real data, is used to retrain the downstream model, thereby improving calibration outputs without compromising model performance. \n\nfies a digit as a '3' with 70% probability and as an '8' with 30% probability, it indicates that while the model predicts '3', it lacks high confidence. Understanding this prediction uncertainty has several key benefits: (1) refining decision-making thresholds to improve overall model performance; (2) adjusting models to perform well under different conditions and data distributions; (3) reducing the blackbox nature of machine learning models, fostering greater transparency and trust; and (4) enabling more consistent and reliable decision-making, particularly in risk-sensitive applications where errors can have significant consequences. \n\nModern deep learning neural networks (NN), however, have been shown to be often miscalibrated i.e., while the NN model performs well in classification, the uncertainty around predictions is also high (Wang et al., 2021;Minderer et al., 2021). NLP models trained on classification tasks (such as sentiment analysis) are built on deep learning algorithms, with many hidden layers and regularization steps, and consequently, numerous hyperparameters to be tuned.",
            "score": 0.4167808347632401,
            "section_title": "Introduction",
            "char_start_offset": 2145,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 186,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1234
                },
                {
                    "start": 1237,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1881
                },
                {
                    "start": 1884,
                    "end": 2126
                },
                {
                    "start": 2127,
                    "end": 2343
                }
            ],
            "ref_mentions": [
                {
                    "start": 2084,
                    "end": 2103,
                    "matchedPaperCorpusId": "245011563"
                },
                {
                    "start": 2103,
                    "end": 2125,
                    "matchedPaperCorpusId": "235435823"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66015625
        },
        {
            "corpus_id": "256598179",
            "title": "An Operational Perspective to Fairness Interventions: Where and How to Intervene",
            "text": "These include temperature scaling [28], focal loss [51], differentiable softmax Karandikar et al. [34], etc. \n\n(see [50] for a recap of related calibration techniques for neural networks). For neural networks, we . This has not been as well-studied in boosting frameworks. However, [27] shows that the Brier score is a reasonable choice of loss function for achieving calibration (defined across a variety of CE metrics). \n\n-Multicalibration (Post): Multicalibration [31] and its variant Multiaccuracy [35] have shown promises on improving multi-group fairness by iteratively calibrating subpopulations that are defined by the intersections of features. Presumably, it works better when there exist features which correlate with the protected attribute. \n\nWe implemented various options of this approach as hyperparameters to tune (see A.4.3 for details), and selected the best option to test. \n\n-General calibrators (Post): Post-hoc calibrators apply a confidence mapping  on top of a mis-calibrated scoring classifier p = \u210e() to deliver a calibrated confidence score q = (\u210e()), to improve (average-case) calibration on the overall population [42]. This is in contrast to the per-group calibrator mentioned above which customizes the calibration for each group. These methods include histogram calibration, isotonic regression [57], Platt scaling [61], Bayesian Binning into Quantiles (BBQ) [53], Ensemble of Near-Isotonic Regressions (ENIR) [54], Beta calibration [39], and the Platt scaling and histogram binning hybrid as developed in [40]. \n\nPlease see A.4.4 for detailed descriptions. We test all of the above calibrators based on the implementations made available by in the Netcal Python package [42]. \n\nFeature Selection Methods for Predictive Parity. In the intervention methods discussed above, we have excluded examining the highly practical choice of making the model better by finding more/better features, which has been explored in literature [11,25].",
            "score": 0.41677938787547664,
            "section_title": "Methods for Achieving Predictive Parity",
            "char_start_offset": 27884,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 111,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 421
                },
                {
                    "start": 424,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 753
                },
                {
                    "start": 756,
                    "end": 893
                },
                {
                    "start": 896,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1544
                },
                {
                    "start": 1547,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1709
                },
                {
                    "start": 1712,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1967
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 38,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 51,
                    "end": 55,
                    "matchedPaperCorpusId": "211252346"
                },
                {
                    "start": 98,
                    "end": 102,
                    "matchedPaperCorpusId": "236772324"
                },
                {
                    "start": 116,
                    "end": 120,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 282,
                    "end": 286,
                    "matchedPaperCorpusId": "252368049"
                },
                {
                    "start": 467,
                    "end": 471,
                    "matchedPaperCorpusId": "51880858"
                },
                {
                    "start": 1144,
                    "end": 1148,
                    "matchedPaperCorpusId": "216562522"
                },
                {
                    "start": 1328,
                    "end": 1332,
                    "matchedPaperCorpusId": "207158152"
                },
                {
                    "start": 1348,
                    "end": 1352,
                    "matchedPaperCorpusId": "56563878"
                },
                {
                    "start": 1392,
                    "end": 1396,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 1443,
                    "end": 1447,
                    "matchedPaperCorpusId": "636054"
                },
                {
                    "start": 1466,
                    "end": 1470,
                    "matchedPaperCorpusId": "202773833"
                },
                {
                    "start": 1539,
                    "end": 1543,
                    "matchedPaperCorpusId": "202718866"
                },
                {
                    "start": 1704,
                    "end": 1708,
                    "matchedPaperCorpusId": "216562522"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.515625
        },
        {
            "corpus_id": "247450599",
            "title": "On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency",
            "text": "Model Calibration Calibration on NLU tasks has been widely studied in related literature. Nguyen and O'Connor (2015) provided the method of how to analyze the calibration of non-neural NLP models. Guo et al. (2017) examined the calibration of modern deep neural networks and revealed that techniques such as temperature scaling and dropout affect the calibration on binary/multiclass classification tasks. Wang et al. (2020b) investigated the calibration of neural machine translation models and found that inference suffers from serious miscalibration. Jagannatha and Yu (2020) demonstrated that neural networks show high calibration error on structured predictions such as NER, POS, and QA, and proposed to use a binary class forecaster to calibrate the predictor confidence for a defined output entity of interest. Desai and Durrett (2020) explored pre-trained language models' calibration in combination with temperature scaling and label smoothing both on in-domain and out-of-domain datasets. Jung et al. (2020) jointly optimized two objectives (a cross-entropy loss and a calibration loss) and directly penalized the difference between the predicted and the true posterior probabilities dynamically over the training steps. He et al. (2021) obtained better calibration on natural language understanding tasks by augmenting and training the classifier jointly with an energybased model using noise-contrastive estimation.\n\nMixup Mixup (Zhang et al., 2018) is a method for data augmentation in which additional samples are generated during training by convexly combining random pairs and their associated labels, and aims to alleviate overfitting. Verma et al. (2019) showed that manipulating hidden representations rather than manipulating input-level features on mixup results in better regularization effects due to the fact that it encourages the neural network to focus more on representations of the real training examples in a low dimensional subspace. Many works have empirically noticed regularization effects that improve model performance on deep neural networks. For example, Guo et al. (2019a) explored the NLU specific mixup strategy by using sentence and word embeddings on CNNs and LSTMs to add performance gains in supervised text classification. Chen et al. (2020",
            "score": 0.4164783123796297,
            "section_title": "Related Work",
            "char_start_offset": 5903,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 90,
                    "end": 116,
                    "matchedPaperCorpusId": "2879445"
                },
                {
                    "start": 197,
                    "end": 214,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 406,
                    "end": 425,
                    "matchedPaperCorpusId": "218487046"
                },
                {
                    "start": 554,
                    "end": 578,
                    "matchedPaperCorpusId": "215548393"
                },
                {
                    "start": 999,
                    "end": 1017,
                    "matchedPaperCorpusId": "216867328"
                },
                {
                    "start": 1231,
                    "end": 1247,
                    "matchedPaperCorpusId": "231632895"
                },
                {
                    "start": 1441,
                    "end": 1460,
                    "matchedPaperCorpusId": "3162051"
                },
                {
                    "start": 1653,
                    "end": 1672,
                    "matchedPaperCorpusId": "59604501"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6171875
        },
        {
            "corpus_id": "258187125",
            "title": "On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study",
            "text": "We conduct an extensive study of the popular probabilistic deep learning calibration methods applied to LLM. Consistently positive effect of these techniques is reflected in improved performance in summarization quality, uncertainty calibration, and selective generation. \n\nIn our paper we investigated the effect of most common and widely used probabilistic deep learning methods. Even though we observed a positive effect of calibration on the variety of metrics, that impact wasn't strong enough to change the course of LLM reliability on a large scale and we don't know whether our findings would be general across larger models and different tasks. One explanation as to why the we didn't observe higher level of calibration could be ground in the LLMs training objective. Maximum Likelihood Estimation (MLE) tends to overfit to the training data, because models are faced with a single ground-truth example per input text. In other words, LLMs trained with MLE are more likely to assign high probabilities to examples from the training data, even when they are not representative of the true distribution of the language. Further research is needed to understand how we can best adapt the learning objective in order to take the most advantage from the probabilistic deep learning methods.",
            "score": 0.41623953971295025,
            "section_title": "Conclusion",
            "char_start_offset": 10114,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 271
                },
                {
                    "start": 274,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1295
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5546875
        },
        {
            "corpus_id": "259095925",
            "title": "Uncertainty in Natural Language Processing: Sources, Quantification, and Applications",
            "text": "There are various techniques for evaluating the performance and credibility of PLMs, including self-evaluation techniques, calibration scores, and other methods [26,149,150,151]. These techniques can help to ensure that NLP systems are trustworthy and reliable, and can be used with confidence by users in a range of different applications. \n\nLast but not least, an intuitive question is how to express uncertainty to improve model evaluation and obtain valuable information? This is a natural challenge because language expression itself is inherently uncertain. Quantifying the uncertainty expressed by the model, making effective evaluations, and reducing the interaction bias caused by expression are all important considerations. We provide a more detailed discussion of this challenge in Sec 5.3.",
            "score": 0.4159590897283467,
            "section_title": "Reliability and Trustworthy Assessment",
            "char_start_offset": 76002,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 340
                },
                {
                    "start": 343,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 802
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 169,
                    "matchedPaperCorpusId": "235078802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27001953125
        },
        {
            "corpus_id": "270392060",
            "title": "Large Language Models Must Be Taught to Know What They Don't Know",
            "text": "\"I have high cortisol but low ACTH on a dexamethasone suppression test.What should I do?\"If the answer to such a question is given without associated confidence, it is not actionable, and if the answer is presented with erroneously high confidence, then acting on the answer is dangerous.One of the biggest open questions about whether large language models (LLMs) can benefit society and reliably be used for decision making hinges on whether or not they can accurately represent uncertainty over the correctness of their output.\n\nThere is anything but consensus on whether LLMs accurately represent uncertainty, or even how we should approach uncertainty representation with language models.Claims regarding language models' ability to estimate uncertainty vary widely, with some works suggesting that language models are increasingly capable of estimating their uncertainty directly through prompting, without any fine-tuning or changes to the training data [25,51], and others suggesting that LLMs remain far too overconfident in their predictions [59,60].The task of uncertainty estimation in LLMs is further exacerbated by linguistic variances in freeform generation, all of which cannot be exhaustively accounted for during training.LLM practitioners are therefore faced with the challenge of deciding which estimation method to use.\n\nOne particular dichotomy in uncertainty estimation methods for language models centers around whether the estimates are black-or white-box.Black-box estimates do not require training and can be used with closed-source models like GPT-4 [1] or Gemini [48], while white-box methods require training parameters on a calibration dataset.Although black-box estimates have become popular with Figure 1: Large language models struggle to assign reliable confidence estimates to their generations.We study the properties of uncertainty calibration in language models, and propose fine-tuning for better uncertainty estimates using a graded dataset of generations from the model.We evaluate our methods on a new open-ended variant of MMLU [18].We show that fine-tuning improves expected calibration error (ECE) and area under the receiver operating characteristic curve (AUROC) compared to commonly-used baselines.Error bars show standard deviation over three base models (LLaMA-2 13/7B and Mistral 7B) and their chat variants.",
            "score": 0.41578411964850803,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 71,
                    "end": 89
                },
                {
                    "start": 89,
                    "end": 288
                },
                {
                    "start": 288,
                    "end": 530
                },
                {
                    "start": 532,
                    "end": 693
                },
                {
                    "start": 693,
                    "end": 1060
                },
                {
                    "start": 1060,
                    "end": 1240
                },
                {
                    "start": 1240,
                    "end": 1340
                },
                {
                    "start": 1342,
                    "end": 1481
                },
                {
                    "start": 1481,
                    "end": 1675
                },
                {
                    "start": 1675,
                    "end": 1831
                },
                {
                    "start": 1831,
                    "end": 2012
                },
                {
                    "start": 2012,
                    "end": 2077
                },
                {
                    "start": 2077,
                    "end": 2247
                },
                {
                    "start": 2247,
                    "end": 2360
                }
            ],
            "ref_mentions": [
                {
                    "start": 1056,
                    "end": 1059,
                    "matchedPaperCorpusId": "258959258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67724609375
        },
        {
            "corpus_id": "250526382",
            "title": "Confident Adaptive Language Modeling",
            "text": "Next, we examine the outcomes of the calibration process. Since the obtained risk is guaranteed to be valid (i.e., \u2264 \u03b4 at least 95% of the time), we focus here on efficiency gains per chosen \u03b4. We refer the reader to Appendix B for empirical validation and for additional results and qualitative examples. \n\nTable 2 presents the efficiency gains per choice of \u03b4 for each consistency objective and confidence measure. We examine larger \u03b4 values for textual consistency as this is generally a stricter requirement since the full model's error is not considered. \n\nAcross all, the softmax confidence measure leads to the greatest decrease in number of decoder layers required. Accordingly, softmax mostly enables the highest speedup gains of up to about three times faster than running through all the model's layers. The very lightweight early-exit classifier sometimes provides better gains than softmax, even if more decoding layers are used. Since the speedup is computed over the full generated output, we see more gains on the longer outputs of summarization and translation where the decoding takes most of the time, compared to the short QA outputs where the whole decoding time is not much longer than the encoding time. \n\nThese encouraging efficiency gains are enabled even with the rigorous performance guarantees that are sometimes conservative (e.g., Eq. ( 11)). We note that relaxing these constraints, or tightening the confidence intervals (e.g., with larger calibration sets), can further improve the empirical gains. \n\nThe softmax operation over the full output vocabulary is FLOPs heavy (though, this compute can potentially be paralleled), sometime leading to increased total FLOPs, even with fewer used layers. The state-based and early-exit classifier measures require minimal FLOPs and provide a good alternative with compelling efficiency gains, if total (parallizeable, or not) FLOPs is of concern.",
            "score": 0.4156275673344534,
            "section_title": "Calibrated performance with guaranteed textual or risk consistency",
            "char_start_offset": 30589,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 57
                },
                {
                    "start": 58,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 305
                },
                {
                    "start": 308,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 559
                },
                {
                    "start": 562,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1226
                },
                {
                    "start": 1229,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1531
                },
                {
                    "start": 1534,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1920
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7646484375
        },
        {
            "corpus_id": "258865733",
            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
            "text": "We show that prompting a model to produce several answer choices before giving its confidence scores significantly improves calibration of verbalized probabilities. Combined with temperature scaling (Guo et al., 2017), this approach generally provides better calibration than model probabilities for ChatGPT2 , GPT-43 , and Claude 24 across three datasets, often reducing expected calibration error (ECE) by over 50%. Related Work. Several studies have examined the calibration of large LMs (Lin et al., 2022a;Park and Caragea, 2022;Kadavath et al., 2022;Xiao et al., 2022;Kuhn et al., 2023), finding that combining large pre-trained LMs with temperature scaling (Guo et al., 2017) produces very well-calibrated predictions (Kadavath et al., 2022;Xiao et al., 2022;Kuhn et al., 2023). Other work focuses on the tendency of language and dialogue models to use linguistic expressions of uncertainty in a well-calibrated manner (Zhou et al., 2023;Mielke et al., 2022). However, existing studies focus on LMs trained purely with unsupervised learning (although Kadavath et al. (2022) briefly examine RLHF-LMs), while widely used models in practice are fine-tuned with instruction-tuning or RLHF (Christiano et al., 2017). RLHF has been shown to effectively leverage annotations of human preferences to control sentiment (Ziegler et al., 2020), improve summarization or instruction-following quality (Stiennon et al., 2022;Ouyang et al., 2022), and inject behavioral priors of harmlessness (Bai et al., 2022b,a). However, recent work has raised the question of whether or not RLHF harms calibration (OpenAI, 2023).",
            "score": 0.41523850837678006,
            "section_title": "Introduction",
            "char_start_offset": 3532,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1609
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 217,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 491,
                    "end": 510,
                    "matchedPaperCorpusId": "249191391"
                },
                {
                    "start": 510,
                    "end": 533,
                    "matchedPaperCorpusId": "247450599"
                },
                {
                    "start": 555,
                    "end": 573,
                    "matchedPaperCorpusId": "247613322"
                },
                {
                    "start": 573,
                    "end": 591,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 663,
                    "end": 681,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 747,
                    "end": 765,
                    "matchedPaperCorpusId": "247613322"
                },
                {
                    "start": 765,
                    "end": 783,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 944,
                    "end": 964,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 1191,
                    "end": 1216,
                    "matchedPaperCorpusId": "4787508"
                },
                {
                    "start": 1316,
                    "end": 1338,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1395,
                    "end": 1418,
                    "matchedPaperCorpusId": "232335764"
                },
                {
                    "start": 1418,
                    "end": 1438,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89404296875
        },
        {
            "corpus_id": "269149545",
            "title": "Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation",
            "text": "Confidence Estimation and Calibration for LLMs: With the rapid advancement of LLMs, there has been an increasing focus on estimating and calibrating their prediction confidence, primarily for classification settings (Jiang et al., 2021;Si et al., 2022;2023a;Portillo Wightman et al., 2023).For the more challenging yet pertinent tasks of free-form generation with varying answer lengths, Kuhn et al. (2023) introduce an unsupervised entropy-based metric that captures the uncertainty over meanings rather than sequences, and Liu et al. (2024) present a lightweight training method that learns a bias term added to the output logits for better-calibrated confidence estimates.However, these methods require access to internal model structures or the output logits, which are not available for proprietary LLMs.Recent research endeavors have focused on either estimating confidence based on the probability distribution of the most consistent answer from multiple samples (Wang et al., 2023b;Chen et al., 2023c;Lin et al., 2023) or directly eliciting verbalized confidence (Lin et al., 2022;Tian et al., 2023), and found that prompting or fine-tuning with verbalized expression of uncertainty can lead to increased accuracy or better calibration (Mielke et al., 2022;Zhou et al., 2023).Xiong et al. (2023) present a comprehensive analysis of logit-based, verbalized, and consistency-based methods and suggest that a hybrid approach incorporating verbalized confidence into a consistency-based ensemble is more effective for confidence calibration.\n\nLLM Agent Ensemble: Augmented with external memory system and tool-use abilities, LLMpowered language agents (Park et al., 2023) display great potential to serve as self-consistent human proxies that can plan by self-reflection and refinement (Shinn et al., 2024;Madaan et al., 2024;Sun et al., 2024).",
            "score": 0.4151098633952593,
            "section_title": "RELATED WORK",
            "char_start_offset": 3164,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 290
                },
                {
                    "start": 290,
                    "end": 675
                },
                {
                    "start": 675,
                    "end": 809
                },
                {
                    "start": 809,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1545
                },
                {
                    "start": 1547,
                    "end": 1848
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 236,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 236,
                    "end": 252,
                    "matchedPaperCorpusId": "253098276"
                },
                {
                    "start": 258,
                    "end": 289,
                    "matchedPaperCorpusId": "260063200"
                },
                {
                    "start": 388,
                    "end": 406,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 525,
                    "end": 542,
                    "matchedPaperCorpusId": "264811211"
                },
                {
                    "start": 970,
                    "end": 990,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 1071,
                    "end": 1089,
                    "matchedPaperCorpusId": "249191391"
                },
                {
                    "start": 1244,
                    "end": 1265,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 1265,
                    "end": 1283,
                    "matchedPaperCorpusId": "265150666"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.767578125
        },
        {
            "corpus_id": "247594202",
            "title": "Calibration of Machine Reading Systems at Scale",
            "text": "the confidence of an incorrectly answered question by 46% while decreasing the confidence of a correctly answered question by 38%. RETRIEVER mistakes Another common seen scenario in an open domain setting is when the RETRIEVER is not able to provide any relevant passages. In such cases, because the READER is generally trained on only correct passages, it still produces a high confidence for the incorrect answer. We show that calibration methods that take into account the RETRIEVER confidences can mitigate this by lowering the confidence of the answer. We provide few such examples in  Kamath et al. (2020) study calibration in the context of selective answering, i.e., learning when QA models should abstain from answering questions. They show that training a forecaster to predict the model's confidence can perform well when facing a distributional shift. Su et al. (2019) also investigate selective answering using a probe in the model to determine the model's confidence.\n\nAlso related to our work is uncertainity estimation (Gal and Ghahramani, 2016;Lakshminarayanan et al., 2017) as model uncertainities can be seen as confidence scores. In NLP, Xiao and Wang (2019) propose an approach to characterize model and data uncertainties for various NLP problems.  use uncertainty estimation for confidence estimation in MT. Dong et al. (2018) study confidence estimation for semantic parsing. We are the first to study calibration of open-domain machine reading systems.\n\nOur Gumbel-topk inspired approach of jointly calibrating the READER and RETRIEVER together is interesting in the light of recent open domain QA methods such as Lewis et al. (2020) and Sachan et al. (2021) that train the entire system jointly. As a future work we would also want to compare our approach with these other end-to-end training approaches for the task of calibration.",
            "score": 0.4149965093600271,
            "section_title": "Effect of K on Calibration",
            "char_start_offset": 23307,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 864,
                    "end": 880,
                    "matchedPaperCorpusId": "165163846"
                },
                {
                    "start": 1035,
                    "end": 1061,
                    "matchedPaperCorpusId": "160705"
                },
                {
                    "start": 1158,
                    "end": 1178,
                    "matchedPaperCorpusId": "53715584"
                },
                {
                    "start": 1331,
                    "end": 1349,
                    "matchedPaperCorpusId": "13686145"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59130859375
        },
        {
            "corpus_id": "252368049",
            "title": "Better Uncertainty Calibration via Proper Scores for Classification and Beyond",
            "text": "Since deep neural networks often yield uncalibrated confidence scores [37], a variety of different post-hoc recalibration approaches have been proposed [8,32]. These methods use the validation set to transform predictions returned by a trained neural network such that they become better calibrated. \n\nA key desired property of recalibration methods is to not reduce the accuracy after the transformation. Therefore, most modern approaches are restricted to accuracy preserving transformations of the model outputs [15,43,46,63]. When recalibrating a model, it is crucial to have a reliable estimate of how much the chosen method improves the underlying model. However, when using current estimators for calibration errors, their biased nature results in estimates that are highly sensitive to the number of samples in the test set that are used to compute the calibration error before and after recalibration (Fig. 1; c.f. Section 5). The source code is openly available at https: //github.com/MLO-lab/better_uncertainty_calibration. \n\nOur contributions for better uncertainty calibration are summarized in the following. We... \u2022 ... give an overview of current calibration error literature, place the errors into a taxonomy, and show which are insufficient for calibration quantification. This also includes several theoretical results, which highlight the shortcomings of current approaches. \n\n\u2022 ... introduce the framework of proper calibration errors, which gives important guarantees and relates every element to a proper score. We can reliably estimate the improvement of an injective recalibration method w.r.t. a proper calibration error via its related proper scoreeven in non-classification settings. \n\n\u2022 ... show that common calibration estimators are highly sensitive w.r.t. the test set size. We demonstrate that for commonly used estimators, the estimated improvement of recalibration methods is heavily biased and becomes monotonically worse with fewer test data.",
            "score": 0.4145708385636375,
            "section_title": "Introduction",
            "char_start_offset": 2178,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 299
                },
                {
                    "start": 302,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1034
                },
                {
                    "start": 1037,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1394
                },
                {
                    "start": 1397,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1711
                },
                {
                    "start": 1714,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1979
                }
            ],
            "ref_mentions": [
                {
                    "start": 70,
                    "end": 74,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 152,
                    "end": 155,
                    "matchedPaperCorpusId": "121781338"
                },
                {
                    "start": 155,
                    "end": 158,
                    "matchedPaperCorpusId": "202718866"
                },
                {
                    "start": 515,
                    "end": 519,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 519,
                    "end": 522,
                    "matchedPaperCorpusId": "56563878"
                },
                {
                    "start": 522,
                    "end": 525,
                    "matchedPaperCorpusId": "212725550"
                },
                {
                    "start": 525,
                    "end": 528,
                    "matchedPaperCorpusId": "212725167"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45849609375
        },
        {
            "corpus_id": "277634150",
            "title": "HalluciNot: Hallucination Detection Through Context and Common Knowledge Verification",
            "text": "The approach employs a simple yet effective EigenScore metric that evaluates responses' self-consistency by exploiting the eigenvalues of responses' covariance matrix. SAPLMA (Azaria and Mitchell, 2023) builds a classifier that uses the activation values of the hidden layers of an LLM to determine whether a statement generated by the LLM is truthful or not. LLM-Check (Sriramanan et al., 2024) further refined this approach by analyzing hidden states, attention maps, and output prediction probabilities. Jiang et al. (2021) explored calibration techniques for language model confidence in question answering tasks, demonstrating how properly calibrated confidence scores can correlate better with the likelihood of correctness. Kadavath et al. (2022) demonstrated that larger language models can effectively self-assess their responses' accuracy by predicting the probability that their answers are correct, thereby quantifying their confidence and reducing hallucinations. \n\nThese findings suggest that hallucinations can be identified using model-internal signals without external sources. While these approaches provide valuable insights, they require direct access to generating models, probability distributions or internal activations. \n\nVerification Mechanisms for RAG Systems. Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) aims to ground LLM outputs in external knowledge by retrieving relevant documents. (Shuster et al., 2021) demonstrated that augmenting model responses with retrieved knowledge reduces hallucinations. However, RAG-based approaches do not fully eliminate hallucinations, as models may still generate unsupported statements or misinterpret retrieved content. To improve factuality, CheckRAG (Peng et al., 2023) introduced augmentation of black-box LLM with a set of plug-andplay modules to ground generated responses in in external knowledge. RePlug (Shi et al., 2024) enhances black-box language models by augmenting them with tunable retrieval systems, improving performance on language modeling and QA tasks. Yoran et al. (2024) developed a method to fine-tune a language model to be robust to the presence of irrelevant contexts. Magesh et al. (2024)   hallucinations in the context of RAG and AI legal research applications. \n\nFine-Tuned Classifiers.",
            "score": 0.4128827838125927,
            "section_title": "C Extended Related Work",
            "char_start_offset": 23842,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 976
                },
                {
                    "start": 979,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1244
                },
                {
                    "start": 1247,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2176
                },
                {
                    "start": 2177,
                    "end": 2272
                },
                {
                    "start": 2275,
                    "end": 2298
                }
            ],
            "ref_mentions": [
                {
                    "start": 175,
                    "end": 202,
                    "matchedPaperCorpusId": "258352729"
                },
                {
                    "start": 370,
                    "end": 395,
                    "matchedPaperCorpusId": "276185315"
                },
                {
                    "start": 507,
                    "end": 526,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 731,
                    "end": 753,
                    "matchedPaperCorpusId": "250451161"
                },
                {
                    "start": 1325,
                    "end": 1345,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1429,
                    "end": 1451,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 1893,
                    "end": 1911,
                    "matchedPaperCorpusId": "256389797"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56689453125
        },
        {
            "corpus_id": "270123445",
            "title": "Kernel Language Entropy: Fine-grained Uncertainty Quantification for LLMs from Semantic Similarities",
            "text": "In the context of machine learning, the VNE has been studied theoretically [4], applied to GAN regularization [33], and the exponential of the VNE has been used for effective rank and sample diversity analysis [63,19].\n\nThe first attempts at estimating the entropy of language date back to the 1950s [64], and, today, techniques for uncertainty quantification are widely used in natural language processing.For instance, Desai and Durrett [15] and Jiang et al. [28] presented calibration techniques for classification tasks.Xiao and Wang [72] empirically showed that, for various tasks including sentiment analysis and named entity recognition, measuring model uncertainty can be used to improve performance.Calibration techniques have also been applied in machine translation tasks to improve accuracy [37].\n\nMalinin and Gales [49] discussed the challenges of estimating uncertainty in sequential models.Several previous works have queried LLMs to elicit statements about uncertainty, either via finetuning or by directly including previous LLM generations in the prompt [30,9,53,43,52,21,62,67,12,73,36].Zhang et al. [75] studied UQ for long text generation.Quach et al. [60] used conformal predictions to quantify LLM uncertainty, which is orthogonal to the approach we pursue here.Yang et al. [74] have shown that Bayesian modeling of LLMs using low-rank Laplace approximations improves calibration in small-scale multiple-choice settings.Lin et al. [44] applied spectral graph analysis to graphs of answers for black-box LLMs.Aichberger et al. [2] proposed a new method for sampling diverse answers from LLMs; more diverse sampling strategies could improve KLE as well.\n\nThere are a variety of ways besides model uncertainty to detect hallucinations in LLMs such as querying external knowledge bases [17,42,69], hidden state interventions [76,24,46], using probes [8,41,47], or applying fine-tuning [31,66].KLE is complementary to many of these directions and focuses on estimating more fine-grained semantic uncertainty.",
            "score": 0.4125293982488914,
            "section_title": "Related Work",
            "char_start_offset": 19102,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 220,
                    "end": 407
                },
                {
                    "start": 407,
                    "end": 524
                },
                {
                    "start": 524,
                    "end": 708
                },
                {
                    "start": 708,
                    "end": 808
                },
                {
                    "start": 810,
                    "end": 905
                },
                {
                    "start": 905,
                    "end": 1106
                },
                {
                    "start": 1106,
                    "end": 1160
                },
                {
                    "start": 1160,
                    "end": 1285
                },
                {
                    "start": 1285,
                    "end": 1443
                },
                {
                    "start": 1443,
                    "end": 1531
                },
                {
                    "start": 1531,
                    "end": 1674
                },
                {
                    "start": 1676,
                    "end": 1912
                },
                {
                    "start": 1912,
                    "end": 2026
                }
            ],
            "ref_mentions": [
                {
                    "start": 75,
                    "end": 78,
                    "matchedPaperCorpusId": "246904732"
                },
                {
                    "start": 110,
                    "end": 114,
                    "matchedPaperCorpusId": "257921290"
                },
                {
                    "start": 210,
                    "end": 214,
                    "matchedPaperCorpusId": "12184201"
                },
                {
                    "start": 214,
                    "end": 217,
                    "matchedPaperCorpusId": "252715476"
                },
                {
                    "start": 461,
                    "end": 465,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 538,
                    "end": 542,
                    "matchedPaperCorpusId": "53715584"
                },
                {
                    "start": 1078,
                    "end": 1081,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 1809,
                    "end": 1812,
                    "matchedPaperCorpusId": "263610099"
                },
                {
                    "start": 1872,
                    "end": 1875,
                    "matchedPaperCorpusId": "259088877"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.146728515625
        },
        {
            "corpus_id": "238419596",
            "title": "Improving Confidence Estimation on Out-of-Domain Data for End-to-End Speech Recognition",
            "text": "Although model-based confidence scores are more reliable than softmax probabilities from end-to-end models as confidence estimators for both in-domain and OOD data, the performance on OOD data lags far behind the in-domain scenario. Using pseudo transcriptions to provide binary targets for training model-based confidence estimators and including additional features from an OOD LM are useful for improving the confidence scores on OOD datasets, even when pretraining uses OOD data. By exposing the CEM or R-EBM to OOD data, the calibration performance is also improved. Selecting OOD data using the improved confidence estimators is expected to considerably aid active or semi-supervised learning.",
            "score": 0.4122072847212249,
            "section_title": "CONCLUSION",
            "char_start_offset": 18564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 699
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87939453125
        },
        {
            "corpus_id": "201666124",
            "title": "Calibration of Deep Probabilistic Models with Decoupled Bayesian Neural Networks",
            "text": "For instance, applications where predictions consider different probabilistic models that must be combined, such as neural networks and language models for machine translation [13]; applications with a big mismatch between training and test distributions, as in speaker and language recognition [9,14]; self-driving cars [15]; out-of-distribution sample detection [16]; and so on.\n\nOne classical way of improving calibration is by optimizing an expected value of a proper scoring rule (PSR) [12,17,18], such as the logarithmic scoring rule (whose average value is the cross-entropy or negative log-likelihood, NLL) and the Brier scoring rule (whose average value is an estimate of the mean squared error).However, a proper scoring rule not only measures calibration, but also the ability of a classifier to discriminate between different classes, a magnitude known as discrimination or refinement [19,9,20], which is necessary to achieve good accuracy values [9].Both quantities are indeed additive up to the value of the average PSR.Thus, optimizing the average PSR is not a guarantee of improving calibration, because the optimization process could lead to worse calibration at the benefit of an improved refinement.This effect has been recently pointed-out in DNNs [21], where models trained to optimize the NNL have outstanding accuracy, but are bad calibrated towards the direction of over-confident probabilities.Here, over-confidence means that, for instance, all samples of a given class where the confidence given by the DNN was around 0.99, are correctly classified in much less than 99% of the cases.\n\nMotivated by this observation, several techniques have been recently proposed to improve the calibration of DNNs while aiming at preserving their accuracy [18,21,22,23,24], basing their design choice on point estimate approaches, e.g maximum likelihood.However, as we will justify in the next section, a proper address of uncertainty, as done by Bayesian approaches, is a clear advantage towards reliable probabilistic modelling; a fact that has been recently shown for example in the context of computer vision [25].",
            "score": 0.41140428173624016,
            "section_title": "Introduction",
            "char_start_offset": 1725,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 380
                },
                {
                    "start": 382,
                    "end": 705
                },
                {
                    "start": 705,
                    "end": 963
                },
                {
                    "start": 963,
                    "end": 1034
                },
                {
                    "start": 1034,
                    "end": 1218
                },
                {
                    "start": 1218,
                    "end": 1419
                },
                {
                    "start": 1419,
                    "end": 1611
                },
                {
                    "start": 1613,
                    "end": 1866
                },
                {
                    "start": 1866,
                    "end": 2130
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 180,
                    "matchedPaperCorpusId": "44472515"
                },
                {
                    "start": 298,
                    "end": 301,
                    "matchedPaperCorpusId": "8017769"
                },
                {
                    "start": 364,
                    "end": 368,
                    "matchedPaperCorpusId": "3464416"
                },
                {
                    "start": 491,
                    "end": 495,
                    "matchedPaperCorpusId": "207158152"
                },
                {
                    "start": 495,
                    "end": 498,
                    "matchedPaperCorpusId": "109884250"
                },
                {
                    "start": 498,
                    "end": 501,
                    "matchedPaperCorpusId": "6294674"
                },
                {
                    "start": 897,
                    "end": 901,
                    "matchedPaperCorpusId": "109884250"
                },
                {
                    "start": 903,
                    "end": 906,
                    "matchedPaperCorpusId": "4561530"
                },
                {
                    "start": 1268,
                    "end": 1272,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1768,
                    "end": 1772,
                    "matchedPaperCorpusId": "6294674"
                },
                {
                    "start": 1772,
                    "end": 1775,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1775,
                    "end": 1778,
                    "matchedPaperCorpusId": "49559279"
                },
                {
                    "start": 1778,
                    "end": 1781,
                    "matchedPaperCorpusId": "49314079"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57666015625
        },
        {
            "corpus_id": "251344503",
            "title": "Confidence estimation method for regression neural networks",
            "text": "Introduction: Providing the confidence of an inference result is a desirable property for virtually all neural network based methods. This helps humans to make correct decisions for critical missions (e.g. autonomous driving, medical diagnosis, and so on) and it is also beneficial in integrating possibly contradictory results from different sensors [1]. Therefore, many researchers have attempted to develop methods that provide confidence values as well as predictions. However, most of previous work focused on the classification problem and exploited the properties of softmax layers. That is, the softmax layer is an essential element in classification networks and its output satisfies conditions for the probabilistic interpretation (they are non-negative and sum to unity). Although many methods showed successful results in the classification problem [2,3], it is not straightforward to apply these methods to the regression problem, since softmax layers are not available and determining failures is fuzzy in the regression setting. However, when we consider the importance of the regression problem [4], it is crucial to develop confidence estimation methods for the regression neural networks. \n\nConfidence estimation in the classification problem: Probably, the most intuitive method to get confidence is to use maximum class probability(MCP), which is the maximum of softmax layer outputs. However, this value is prone to over-fitting and a calibration method was proposed in [2]. Precisely, one possible interpretation of confidence is that when you collect a set of samples showing confidence p, then the average accuracy for this set is p, and the authors in [2] proposed a method that calibrates softmax outputs with temperature scaling in order to satisfy the above condition. This idea provides a confidence estimation method without re-training. \n\nIn [3], the authors claimed that true class probability (TCP) reflects the confidence level more faithfully than MCP, and proposed a twoheaded network architecture that estimates TCP values as well as class labels. Arguably, the most ideal confidence value in the classification problem is 0 for incorrect decisions and 1 for correct decisions, and these labels are easily available in the training phase. However, the authors in [3] reported that training using these ideal values as supervised signals did not work well.",
            "score": 0.41089666824588267,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1206
                },
                {
                    "start": 1209,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1867
                },
                {
                    "start": 1870,
                    "end": 2084
                },
                {
                    "start": 2085,
                    "end": 2275
                },
                {
                    "start": 2276,
                    "end": 2392
                }
            ],
            "ref_mentions": [
                {
                    "start": 351,
                    "end": 354,
                    "matchedPaperCorpusId": "53095042"
                },
                {
                    "start": 861,
                    "end": 864,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 864,
                    "end": 866,
                    "matchedPaperCorpusId": "204401816"
                },
                {
                    "start": 1111,
                    "end": 1114,
                    "matchedPaperCorpusId": "1889400"
                },
                {
                    "start": 1491,
                    "end": 1494,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1677,
                    "end": 1680,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1873,
                    "end": 1876,
                    "matchedPaperCorpusId": "204401816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0753173828125
        },
        {
            "corpus_id": "253384402",
            "title": "Calibration Meets Explanation: A Simple and Effective Approach for Model Confidence Estimates",
            "text": "As accurate estimates are required for many difficult or sensitive prediction tasks (Platt, 1999), probability calibration is an important uncertainty estimation task for NLP. Unlike other uncertainty estimation task (e.g., out-of-domain detection, selective inference), calibration focuses on aleatoric uncertainty measured by the probability of the prediction and adjusts the overall model confidence level (Hendrycks and Gimpel, 2017;Pereyra et al., 2017;Guo et al., 2017;Qin et al., 2021). For example, Gal and Ghahramani (2016) propose to adopt multiple predictions with different dropout masks and then combine them to get the confidence estimate. Recently, several works focus on the calibration of PLMs models for NLP tasks (Hendrycks et al., 2019;Desai and Durrett, 2020;Jung et al., 2020;He et al., 2021;Park and Caragea, 2022;Bose et al., 2022). Dan and Roth (2021) investigate the calibration properties of different transformer architectures and sizes of BERT. In line with recent work (Ye and Durrett, 2022), our work focuses on how explanations can help calibration in three NLP tasks. However, we do not need to learn a calibrator by using model interpretations with heuristics, and also do not compare due to its intensive computation cost when generating attributions. In contrast, we explore whether model explanations are useful for calibrating black-box models during training.",
            "score": 0.41045944639441756,
            "section_title": "Related Work",
            "char_start_offset": 12379,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 97,
                    "matchedPaperCorpusId": "56563878"
                },
                {
                    "start": 409,
                    "end": 437,
                    "matchedPaperCorpusId": "13046179"
                },
                {
                    "start": 437,
                    "end": 458,
                    "matchedPaperCorpusId": "9545399"
                },
                {
                    "start": 458,
                    "end": 475,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 475,
                    "end": 492,
                    "matchedPaperCorpusId": "245010935"
                },
                {
                    "start": 507,
                    "end": 532,
                    "matchedPaperCorpusId": "160705"
                },
                {
                    "start": 732,
                    "end": 756,
                    "matchedPaperCorpusId": "59336190"
                },
                {
                    "start": 756,
                    "end": 780,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 780,
                    "end": 798,
                    "matchedPaperCorpusId": "216867328"
                },
                {
                    "start": 798,
                    "end": 814,
                    "matchedPaperCorpusId": "231632895"
                },
                {
                    "start": 814,
                    "end": 837,
                    "matchedPaperCorpusId": "247450599"
                },
                {
                    "start": 857,
                    "end": 876,
                    "matchedPaperCorpusId": "244119588"
                },
                {
                    "start": 999,
                    "end": 1021,
                    "matchedPaperCorpusId": "238856959"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68603515625
        },
        {
            "corpus_id": "258866001",
            "title": "Selectively Answering Ambiguous Questions",
            "text": "Pretrained language models are typically fine-tuned before being used in applications. However, finetuning and other alignment techniques may disturb the model's calibration by emphasizing high performance on specific sets of tasks rather than the full pretraining distribution. 3 Table 4 shows that for Flan-PaLM (Chung et al., 2022), instruction tuning dramatically improves exact match accuracy while simultaneously worsening performance on confidence-based ranking (ROC-AUC) and calibration (ECE), when using model likelihood as the confidence score. However, this miscalibration can be mitigated by using sample-based confidence scores, which dramatically improves the calibration on ambiguous questions (AmbigQA). For the selective-prediction metric C@80, the instructiontuned model with sampling is far ahead of any other method investigated. Note that we investigate Natural Questions and AmbigQA here as they are not in the instruction tuning training data. \n\nWe examine the effect of model scale on calibration, finding that in general, accuracy declines substantially in closed book question answering, but calibration stays roughly constant. See Appendix D for full results. \n\nAs sampling-based approaches require a linear increase in compute for the number of samples, we also examine how calibration scales with the number. In particular, we test three, five, and eight samples and compare that to the original results containing ten samples. Results can be found in Appendix E. Unsurprisingly, more samples seems to improve calibration, though it seems on the unambiguous Natural Questions slice, sampling diversity with a small number of samples works relatively well for the cost.",
            "score": 0.4103219776481557,
            "section_title": "Instuction Tuning and Scaling",
            "char_start_offset": 22910,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 966
                },
                {
                    "start": 969,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1186
                },
                {
                    "start": 1189,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1697
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73681640625
        },
        {
            "corpus_id": "270371795",
            "title": "When is Multicalibration Post-Processing Necessary?",
            "text": "We find that HKRR and HJZ show no improvements to max ECE or max smECE for MLPs, random forests, and logistic regression.Multicalibration gains are also marginal (0 to 0.01) on the Bank Marketing, ACSIncome, and Credit Default datasets (see Appendix F.2). On HMDA, however, multicalibration does seem to provide a noticeable improvement on the order of 0.03-0.05for these models (Figure 21).We believe this is because ERM achieves worse calibration error on HMDA, possibly due to the increased difficulty of the dataset.\n\nObservation 2: HKRR or HJZ post-processing can help un-calibrated models like SVMs or Naive Bayes achieve low group-wise maximum calibration error.Oftentimes, however, similar results can be achieved with traditional calibration methods like isotonic regression (Zadrozny and Elkan, 2002).\n\nAcross our datasets, we find that SVMs, Decision Trees, and Naive Bayes almost always have their max smECE error improve by 0.05 or more using post-processing.We also point out the relatively strong performance of isotonic regression and other traditional calibration methods across datasets and models.For example, isotonic regression provides nearly all the improvements (up to 0.01 error) of the multicalibration algorithms when applied to Naive Bayes in Figure 2. On the Credit default dataset in Figure 22, isotonic regression is -when considering standard deviation -tied with the optimal multicalibration post-processing algorithms for SVM and Naive Bayes.We have similar findings for the MEPS dataset and Random Forests trained on the HMDA dataset.Platt scaling and isotonic regression are desirable because they are parameter-free methods working out of the box without tuning, are simple for practitioners to implement, and further do not require large parameter sweeps to find effective models.Figure 2: Best performing HKRR and HJZ post-processing algorithm hyperparameters (selected based on validation max smECE) compared to ERM on the MEPS dataset.",
            "score": 0.4101619069839303,
            "section_title": "Experiments on Tabular Datasets",
            "char_start_offset": 22341,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 121,
                    "end": 362
                },
                {
                    "start": 362,
                    "end": 391
                },
                {
                    "start": 391,
                    "end": 520
                },
                {
                    "start": 522,
                    "end": 669
                },
                {
                    "start": 669,
                    "end": 811
                },
                {
                    "start": 813,
                    "end": 972
                },
                {
                    "start": 972,
                    "end": 1116
                },
                {
                    "start": 1116,
                    "end": 1476
                },
                {
                    "start": 1476,
                    "end": 1569
                },
                {
                    "start": 1569,
                    "end": 1818
                },
                {
                    "start": 1818,
                    "end": 1976
                }
            ],
            "ref_mentions": [
                {
                    "start": 784,
                    "end": 810,
                    "matchedPaperCorpusId": "3349576"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1900634765625
        },
        {
            "corpus_id": "276742031",
            "title": "Efficient Test-Time Scaling via Self-Calibration",
            "text": "Leveraging additional computation during inference can enhance the quality of responses generated by large language models (LLMs) (Snell et al., 2024a;Yao et al., 2023;Wu et al., 2024;Chen et al., 2025a). Among these methods, repeated sampling (Brown et al., 2024) such 1 Our codes are available at https://github.com/ Chengsong-Huang/Self-Calibration.  as Best-of-N (Cobbe et al., 2021a) and Self-Consistency (Wang et al., 2022b) generate multiple candidate responses and select the final answer by a scoring model or a majority voting rule. While these methods have proven effective, they require a fixed amount of sampled responses for each query regardless of its difficulty and complexity. Although increasing the sample size generally improves performance, it also increases computational costs and inference time (Amini et al., 2024). This is particularly inefficient for simple questions like \"2 + 3 = ?\", where a few samples are sufficient to find the correct solution (Chen et al., 2024b), and extensive sampling is unnecessary. Previous adaptive sampling strategies (Aggarwal et al., 2023;Li et al., 2024;Wan et al., 2024) typically design lightweight stopping criteria to determine whether additional responses should be sampled. However, they often incorporate manually designed features or heuristic rules, such as stopping when the model generates the same response three times consecutively, which can limit their generalizability across different tasks and models. Therefore, it is critical to design a task-independent, model-agnostic approach without heavy reliance on human-designed heuristics. \n\nWe propose an efficient test-time scaling method by using model confidence for dynamically sampling adjustment, since confidence can be seen as an intrinsic measure that directly reflects model uncertainty on different tasks. However, extracting accurate confidence can be challenging since LLMs are known to be overconfident on their own responses (Lin et al., 2022;Xiong et al., 2023;Leng et al., 2024), and their confidence often exceeds the actual accuracy.",
            "score": 0.40928272463092363,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1614
                },
                {
                    "start": 1617,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 2078
                }
            ],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 151,
                    "matchedPaperCorpusId": "271719990"
                },
                {
                    "start": 151,
                    "end": 168,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 244,
                    "end": 263,
                    "matchedPaperCorpusId": "271571035"
                },
                {
                    "start": 410,
                    "end": 430,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 978,
                    "end": 998,
                    "matchedPaperCorpusId": "275133600"
                },
                {
                    "start": 1077,
                    "end": 1100,
                    "matchedPaperCorpusId": "258823191"
                },
                {
                    "start": 1100,
                    "end": 1116,
                    "matchedPaperCorpusId": "267060971"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54833984375
        },
        {
            "corpus_id": "258187125",
            "title": "On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study",
            "text": "Probabilitistic learning for seq2seq models Developed primarily in the context of discriminative models, the state-of-art probabilistic approaches can be applied to large neural models without sacrificing performance (Gal and Ghahramani, 2016;Lakshminarayanan et al., 2017;Wen et al., 2020;Liu et al., 2020). Recently, however, initial investigations into unsupervised uncertainty estimation for structured prediction have appeared, primarily focusing on more basic approaches such as Monte Carlo Dropout (MCD) or Deep Ensemble (DE) (Xiao and Wang, 2019;Wang et al., 2019;Fomicheva et al., 2020;Malinin and Gales, 2021;Lin et al., 2022), with a few work looking into summarization tasks (Xu et al., 2020;Gidiotis and Tsoumakas, 2022). In comparison, this work focuses on an unbiased evaluation of a wide range of state-of-the-art probabilistic methods on tasks with varying difficulty, and reveals failure patterns of classic approaches such as MCD and DE. \n\nCalibration Technique in Language Processing Guo et al. (2017) proposed improving calibration of document classifier using of temperature scaling. M\u00fcller et al. (2019) and Wang et al. (2020) explored improving calibration in neural machine translation using label smoothing. Desai and Durrett (2020) noted that calibration methods can be used to improve the accuracy of pre-trained language models. Jung et al. (2021) proposed a novel training approach to improve calibration by minimizing a combined loss of cross-entropy and calibration In the summarization literature, (Cao and Wang, 2021;Xu et al., 2022;Sun and Li, 2021;Zhao et al., 2022;Liu et al., 2022) explored calibrating model probability using contrastive learning approaches. Most of these techniques focus on deterministic models. They are orthogonal to and can be combined with the probabilistic approaches we explore in this work.",
            "score": 0.40863059184503864,
            "section_title": "Related work",
            "char_start_offset": 3603,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 956
                },
                {
                    "start": 959,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1855
                }
            ],
            "ref_mentions": [
                {
                    "start": 217,
                    "end": 243,
                    "matchedPaperCorpusId": "160705"
                },
                {
                    "start": 243,
                    "end": 273,
                    "matchedPaperCorpusId": "6294674"
                },
                {
                    "start": 290,
                    "end": 307,
                    "matchedPaperCorpusId": "219792902"
                },
                {
                    "start": 533,
                    "end": 554,
                    "matchedPaperCorpusId": "53715584"
                },
                {
                    "start": 572,
                    "end": 595,
                    "matchedPaperCorpusId": "218763134"
                },
                {
                    "start": 595,
                    "end": 619,
                    "matchedPaperCorpusId": "231895728"
                },
                {
                    "start": 619,
                    "end": 636,
                    "matchedPaperCorpusId": "256358379"
                },
                {
                    "start": 687,
                    "end": 704,
                    "matchedPaperCorpusId": "222378161"
                },
                {
                    "start": 704,
                    "end": 733,
                    "matchedPaperCorpusId": "243831075"
                },
                {
                    "start": 1004,
                    "end": 1021,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1131,
                    "end": 1149,
                    "matchedPaperCorpusId": "218487046"
                },
                {
                    "start": 1234,
                    "end": 1258,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 1358,
                    "end": 1376,
                    "matchedPaperCorpusId": "221150972"
                },
                {
                    "start": 1531,
                    "end": 1551,
                    "matchedPaperCorpusId": "237571691"
                },
                {
                    "start": 1551,
                    "end": 1567,
                    "matchedPaperCorpusId": "237439568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1982421875
        },
        {
            "corpus_id": "258967945",
            "title": "Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models",
            "text": "Based on the observations, we hypothesize that preserving the pre-trained features helps calibrate the fine-tuned LMs. \n\nTo validate our hypothesis, we first evaluate the calibration of some previous methods that can preserve pre-trained features, including (1) Parameter-efficient tuning (Houlsby et al., 2019;Hu et al., 2021;Li & Liang, 2021), (2) Pre-trained weight decay, (3) Mixout (Lee et al., 2020). Although these methods were originally designed to improve the performance beyond uncertainty estimation, our experiment demonstrates that these methods outperform vanilla fine-tuning in terms of calibration, especially under out-of-domain settings. Based on our observation that the PLMs are well-calibrated on the MLM task yet the fine-tuned LMs that forget the pre-trained features struggle with overconfidence under domain shift, we propose a simple baseline that utilizes the MLM objective to maintain the consistency between the pre-trained and fine-tuned models. The proposed method achieves the lowest expected calibration error and competitive accuracy compared to existing calibration methods in both ID and OD settings on three NLU tasks, including natural language inference, paraphrase detection, and commonsense reasoning, showing that preserving the pre-trained features is an effective approach for improving the calibration of fine-tuned LMs.",
            "score": 0.4085672232019453,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 121,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1366
                }
            ],
            "ref_mentions": [
                {
                    "start": 289,
                    "end": 311,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 387,
                    "end": 405,
                    "matchedPaperCorpusId": "202750126"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85791015625
        },
        {
            "corpus_id": "259095538",
            "title": "When to Read Documents or QA History: On Unified and Selective Open-domain QA",
            "text": "The key point of our method is to find the effective measurement of the answer confidence, which is essentially the calibration problem. The confidence score P(\u00e2|\u2022) should be able to discern the accurate answer, by comparing the reliability of each knowledge. We propose the way to find such P(\u00e2|\u2022) in the next paragraph, based on our analysis of the important factors on documents and QA-pairs. \n\nPrior work (Hendrycks and Gimpel, 2016) has proposed MaxProb -a method that uses the maximum probability of a classifier as the confidence estimator for selective prediction. For extractive QA, existing works (Zhang et al., 2021;Si et al., 2022) adopt MaxProb as a baseline, by using the sum of the maximum logits of the start and end of the answer span. Meanwhile, we focus on calibrating generative language models, where its output is a token sequence. To apply MaxProb for generative LMs, we select the maximum probability at each step by the argmax function in Eq. ( 3), which can be viewed as greedy decoding. The scores of decoded tokens are aggregate by product, as follows: \n\nwhere P LM ( * ) is the token probabilities obtained from LM head. Since LM tends to underestimate the likelihood of longer texts, length normalization is essential as in (Adiwardana et al., 2020). To normalize as sequence lengths,3 we take the geometric mean of the multiplicative terms, i.e., {P LM (\u00e2|q, c)} 1/|\u00e2| . However, this LM likelihood obtained by Max-Prob has an inevitable problem. MLE loss in Eq. \n\n(2) enforces to train LM solely towards maximizing the likelihoods of observed sequences. Because the observed sequences (or labeled answers) can have diverse surface forms, MLE training inevitably leads to miscalibration. In QA tasks, the sequence likelihood of QA models is reported to be often miscalibrated, or overconfident (Jiang et al., 2021;Kumar and Sarawagi, 2019).",
            "score": 0.4076807721604985,
            "section_title": "Sequence Likelihood of LM",
            "char_start_offset": 10689,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 395
                },
                {
                    "start": 398,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1080
                },
                {
                    "start": 1083,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1493
                },
                {
                    "start": 1496,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1871
                }
            ],
            "ref_mentions": [
                {
                    "start": 607,
                    "end": 627,
                    "matchedPaperCorpusId": "235313893"
                },
                {
                    "start": 627,
                    "end": 643,
                    "matchedPaperCorpusId": "253098276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50390625
        },
        {
            "corpus_id": "274789494",
            "title": "A Survey of Calibration Process for Black-Box LLMs",
            "text": "Additionally, many studies have reviewed various confidence estimation and calibration methods on Language Models(LMs) and LLMs (Geng et al., 2024). However, existing surveys predominantly focus on white-box LLMs, where Calibration Process methods typically encompass logit-based calibration (Bi et al., 2024;Huang et al., 2023;Kuhn et al., 2023), additional parameter layers (Liu et al., 2024b), instruction tuning (Kapoor et al., 2024), and reinforcement learning techniques (Band et al., 2024). \n\nIn contrast to white-box LLMs, research on the Calibration Process for black-box LLMs lacks systematic analysis despite active development. black-box LLMs have recently garnered significant attention due to their exceptional performance across various real-world applications, especially as benchmark judge models for evaluating LLMs output quality and overall performance (Zheng et al., 2023). This role has been widely adopted by small and medium-sized enterprises for product development (Lin et al., 2023;Li et al., 2024b;Chen and Mueller, 2024). Given black-box LLMs' function as critical judge models, enhancing their sensitivity to uncertainty, improving output reliability, and fostering user trust are essential. The Calibration Process has demonstrated substantial potential in meeting these requirements (Wagner et al., 2024), further driving interest in black-box LLMs Calibration Process research (Si et al., 2022a;Ye and Durrett, 2022;Lin et al., 2023;Li et al., 2024b;Chen and Mueller, 2024). \n\nDespite this growing interest, no comprehensive study, to the best of our knowledge, has been dedicated specifically to the Calibration Process for black-box LLMs. This survey aims to bridge this gap by providing a systematic review and analysis of current research progress in this field.",
            "score": 0.4075941385869784,
            "section_title": "Introduction",
            "char_start_offset": 3105,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 497
                },
                {
                    "start": 500,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1507
                },
                {
                    "start": 1510,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1799
                }
            ],
            "ref_mentions": [
                {
                    "start": 128,
                    "end": 147,
                    "matchedPaperCorpusId": "265157516"
                },
                {
                    "start": 376,
                    "end": 395,
                    "matchedPaperCorpusId": "264811211"
                },
                {
                    "start": 416,
                    "end": 437,
                    "matchedPaperCorpusId": "268417346"
                },
                {
                    "start": 873,
                    "end": 893,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 1026,
                    "end": 1049,
                    "matchedPaperCorpusId": "263611057"
                },
                {
                    "start": 1428,
                    "end": 1449,
                    "matchedPaperCorpusId": "252873674"
                },
                {
                    "start": 1483,
                    "end": 1506,
                    "matchedPaperCorpusId": "263611057"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.751953125
        },
        {
            "corpus_id": "274789494",
            "title": "A Survey of Calibration Process for Black-Box LLMs",
            "text": "LLMs. Bias detection and mitigation remain critical challenges in black-box LLMs. The inaccessibility of model internals makes it difficult to identify and address bias, imposing inherent limitations on both confidence estimation and calibration. This often leads to disproportionately high or low calibration errors for specific groups or tasks, undermining the model's reliability and fairness. Traditional methods have limited effectiveness in addressing bias within black-box settings, underscoring the necessity for tailored solutions. Addressing these challenges requires a focus on improving output transparency, integrating external calibration mechanisms, and leveraging user feedback for dynamic confidence adjustment. These strategies not only tackle the unique obstacles of black-box settings but also enhance the calibration performance and practical utility of LLMs, enabling more reliable and interpretable applications across diverse real-world scenarios. \n\nCalibration for Long-form Text. Calibrating long-form text poses significant challenges for both white-box and black-box LLMs. As the demand for LLMs grows, long texts in QA and other generative tasks often encompass multiple claims and facts, making calibration increasingly complex. Assessing whether confidence scores are well-calibrated becomes particularly difficult when only portions of the text are correct, and quality evaluation is further complicated by its inherently subjective nature. Addressing these challenges requires effective methods that incorporate human perception as a metric for correctness. Additionally, it is crucial to develop approaches tailored to black-box settings, where model parameters remain inaccessible.",
            "score": 0.40649313789892605,
            "section_title": "Bias Detection and Mitigation for black-box",
            "char_start_offset": 29097,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 5
                },
                {
                    "start": 6,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 971
                },
                {
                    "start": 974,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1716
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89404296875
        },
        {
            "corpus_id": "268876453",
            "title": "Calibrating the Confidence of Large Language Models by Eliciting Fidelity",
            "text": "Recent work has focused on LLM calibration (Lin et al., 2022a;Kadavath et al., 2022;OpenAI, 2023). \n\nIn this section, we will briefly introduce two mainstream methods for eliciting the confidence from language models, namely the Logit-based Method and the Verbalization-based Method.",
            "score": 0.4064586197702927,
            "section_title": "Related Work",
            "char_start_offset": 5449,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 101,
                    "end": 283
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87158203125
        },
        {
            "corpus_id": "238419596",
            "title": "Improving Confidence Estimation on Out-of-Domain Data for End-to-End Speech Recognition",
            "text": "As end-to-end automatic speech recognition (ASR) models reach promising performance, various downstream tasks rely on good confidence estimators for these systems. Recent research has shown that model-based confidence estimators have a significant advantage over using the output softmax probabilities. If the input data to the speech recogniser is from mismatched acoustic and linguistic conditions, the ASR performance and the corresponding confidence estimators may exhibit severe degradation. Since confidence models are often trained on the same in-domain data as the ASR, generalising to out-of-domain (OOD) scenarios is challenging. By keeping the ASR model untouched, this paper proposes two approaches to improve the model-based confidence estimators on OOD data: using pseudo transcriptions and an additional OOD language model. With an ASR model trained on LibriSpeech, experiments show that the proposed methods can greatly improve the confidence metrics on TED-LIUM and Switchboard datasets while preserving in-domain performance. Furthermore, the improved confidence estimators are better calibrated on OOD data and can provide a much more reliable criterion for data selection.",
            "score": 0.40590371435536876,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85693359375
        },
        {
            "corpus_id": "268249146",
            "title": "Towards Calibrated Deep Clustering Network",
            "text": "To address this, calibration methods in supervised learning are broadly divided into two categories: (1) Post-calibration methods calibrate the model's output after model training. An example is Temperature Scaling (Guo et al., 2017a), which adjusts the softmax using a labeled validation set. \n\n(2) Regularization-based methods penalize the confidence of the model's predictions during the training process via some regularization techniques like Label Smoothing (M\u00fcller et al., 2019), which uses soft labels to reduce overfitting, and Focal Loss (Mukhoti et al., 2020), which emphasizes difficult samples and penalizes overconfident predictions. The L 1 Norm (Joo & Chung, 2020) is also used as an additional regularization to manage confidence levels. Recent studies have also investigated calibration under unsupervised domain adaptation. For instance, (Wang et al., 2020) developed a transferable calibration framework that reduces errors by accurately assessing discrepancies between source and target domains, while (Wang et al., 2023a) employs a differentiable density ratio estimator to manage uncertainties under domain shifts. These studies tackle calibration in the domain adaptation setting, where labeled source data is available. \n\nAs demonstrated in Fig. 1(b) and (c), deep clustering methods exhibit a more pronounced overconfidence issue compared to supervised learning models. This heightened problem stems from stateof-the-art deep clustering techniques relying on pseudo-labeling, where incorrect pseudo-labels exacerbate the overconfidence challenge. Regrettably, past research has largely overlooked this issue. Furthermore, conventional calibration techniques are unsuitable for directly addressing overconfidence in unsupervised clustering because post-calibration methods necessitate a labeled validation set for tuning hyperparameters, while regularization techniques can compromise the quality of selected pseudo-labels and, consequently, impair clustering performance. Consequently, this paper explores the development of a calibrated deep clustering neural network to simultaneously enhance clustering performance and achieve precise confidence estimation.",
            "score": 0.4049952434381493,
            "section_title": "RELATED WORK",
            "char_start_offset": 7366,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 293
                },
                {
                    "start": 296,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1244
                },
                {
                    "start": 1247,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2186
                }
            ],
            "ref_mentions": [
                {
                    "start": 215,
                    "end": 234,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 548,
                    "end": 570,
                    "matchedPaperCorpusId": "211252346"
                },
                {
                    "start": 857,
                    "end": 876,
                    "matchedPaperCorpusId": "220545994"
                },
                {
                    "start": 1023,
                    "end": 1043,
                    "matchedPaperCorpusId": "245502819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.342041015625
        },
        {
            "corpus_id": "5178123",
            "title": "Confidence estimation for translation prediction",
            "text": "Most statistical models used in natural language applications are capable in principle of generating probability estimates for their outputs. However, in practice, these estimates are often quite poor and are usually interpreted simply as scores that are monotonic with probabilities. There are many contexts where good estimates of true probabilities are desirable: \n\n\u2022 in a decision-theoretic setting, posterior probabilities are required in order to choose the lowest-cost output for a given input. \n\n\u2022 when a collection of different models is available for some problem, output probabilities provide a principled and convenient way of combining them; and \n\n\u2022 when multiplying conditional probabilities to compute joint distributions, the accuracy of the result is crucially dependent on the stability of the conditional estimates across different contexts-this is important for applications like speech recognition and machine translation that perform searches over a large space of output sentences, represented as sequences of words. \n\nGiven a statistical model that produces a probabilistic score, a straightforward way of obtaining a true probability is to use the score as input to another model whose output is interpreted as the desired probability. The idea is that the second model can learn how to transform the base model's score by observing its performance on new text, possibly in conjunction with other features. This approach, which is known as confidence estimation (CE), is widely used in speech recognition (Guillevic et al., 2002;Moreno et al., 2001;Sanchis et al., 2003;Stolcke et al., 1997) but is virtually unknown in other areas of natural language progessing (NLP). 1 he alternatives to confidence estimation are traditional smoothing techniques such as backing off to simpler models and cross validation, along with careful marginalization and scaling where applicable to obtain the desired posterior probabilities. There is some evidence (Wessel et al., 2001) that this approach can give results that are at least as good as those obtainable with an external CE model. However, CE as we present it here is not incompatible with traditional techniques, and has several practical advantages. First, it can easily incorporate specialized features that are highly indicative of how well the base model will perform on a given input, but that may be of little use for the task of choosing the output.",
            "score": 0.404973541739567,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 366
                },
                {
                    "start": 369,
                    "end": 501
                },
                {
                    "start": 504,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 1039
                },
                {
                    "start": 1042,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2099
                },
                {
                    "start": 2100,
                    "end": 2220
                },
                {
                    "start": 2221,
                    "end": 2426
                }
            ],
            "ref_mentions": [
                {
                    "start": 1530,
                    "end": 1554,
                    "matchedPaperCorpusId": "14741934"
                },
                {
                    "start": 1574,
                    "end": 1595,
                    "matchedPaperCorpusId": "8463027"
                },
                {
                    "start": 1595,
                    "end": 1616,
                    "matchedPaperCorpusId": "6874209"
                },
                {
                    "start": 1969,
                    "end": 1990,
                    "matchedPaperCorpusId": "1357669"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43994140625
        },
        {
            "corpus_id": "253448663",
            "title": "Trust, but Verify: Using Self-supervised Probing to Improve Trustworthiness",
            "text": "The overconfidence issue [18,16] raises major concerns about deep models' trustworthiness, and has been studied in several related problems: calibration, misclassification detection, and out-of-distribution (OOD) detection [18,14,16]. Calibration algorithms aim to align a model's predictive confidence scores with their ground truth accuracy. Among these methods, a prominent approach is to calibrate the confidence without changing the original prediction, such as Temperature Scaling [14] and Histogram Binning [4]. \n\nFor misclassification and OOD detection, a common approach is to incorporate uncertainty estimation to get a well-grounded confidence score. For example, [25,5] attempt to capture the uncertainty of every sample using a Dirichlet distribution. Ensemble-based methods such as Monte-Carlo Dropout [12] and Deep Ensembles [24] calculate uncertainty from multiple trials either with the Bayesian formalism or otherwise. However, these uncertainty estimation algorithms have a common drawback that they involve modifying the classification architecture, thus often incurring accuracy drops. Besides, ensembling multiple overconfident classifiers can still produce overconfident predictions. \n\nThe practical demand for uncertainty estimation on pretrained models has led to a line of research developing post-hoc methods. Trust Score [21] utilizes neighborhood information as a metric of trustworthiness, assuming that samples in a neighborhood are most likely to have the same class label. True Class Probability [8] aims to train a regressor to capture the softmax output score associated with the true class. \n\nCompared to these works, we introduce probing confidence as a valuable additional source of information for trustworthiness tasks. Rather than replacing existing trustworthiness methods, our approach is complementary to them, flexibly incorporating them into our self-supervised probing framework.",
            "score": 0.4046723718237335,
            "section_title": "Trustworthiness in Deep Learning",
            "char_start_offset": 6014,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1206
                },
                {
                    "start": 1209,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1626
                },
                {
                    "start": 1629,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1926
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 29,
                    "matchedPaperCorpusId": "13046179"
                },
                {
                    "start": 29,
                    "end": 32,
                    "matchedPaperCorpusId": "55700923"
                },
                {
                    "start": 223,
                    "end": 227,
                    "matchedPaperCorpusId": "13046179"
                },
                {
                    "start": 227,
                    "end": 230,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 230,
                    "end": 233,
                    "matchedPaperCorpusId": "55700923"
                },
                {
                    "start": 487,
                    "end": 491,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 514,
                    "end": 517,
                    "matchedPaperCorpusId": "122906757"
                },
                {
                    "start": 679,
                    "end": 681,
                    "matchedPaperCorpusId": "219708927"
                },
                {
                    "start": 840,
                    "end": 844,
                    "matchedPaperCorpusId": "6294674"
                },
                {
                    "start": 1349,
                    "end": 1353,
                    "matchedPaperCorpusId": "44100821"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57470703125
        },
        {
            "corpus_id": "276249126",
            "title": "Learning to Substitute Words with Model-based Score Ranking",
            "text": "where h(\u2022) is the softmax function, s k is the logit corresponding to p \u03b8 (x n = w k |X), and M (X) and M ( Xk ) are the model-based scores of the original and modified sentence, respectively. Recall that for M ( X1 ), the token of interest x n in X has been modified to the top prediction w 1 from the model. Conceptually, L AS in (4) seeks to maximize the (weighted) average of model-based scores from K predictions, while L BS aims to improve the modelbased score of the top prediction with respect to that of a reference model. \n\nWe then combine the margin ranking loss in (3) with the score-improving losses in (4) or (5) as \n\nwhere \u03b3 is a hyperparameter trading off ranking or model-based score improvement. In the experiments we will compare these two approaches to determine empirically whether is better to attempt to improve the scores of K predictions from the model as opposed to improving the score of one of the predictions from the model (the most likely) relative to a baseline prediction obtained from a reference model. Note that is tempting to use ( 5) with all predictions, not just the first one, however, we found empirically that it considerably increases the computational cost without significant performance gains. The cost overhead is caused mainly by the need to evaluate the model-based score K times rather than just 2 in (5). \n\nDirect Preference Optimization Direct Preference Optimization (DPO) (Rafailov et al., 2024) is an efficient technique for aligning large language models with human feedback, which gained popularity due to its simplicity (Miao et al., 2024). \n\nFor instance, it has demonstrated to be effective in chat benchmarks (Tunstall et al., 2023;Zheng et al., 2023). In our case, the model-based score serves as proxy for human feedback DPO (under the Plackett-Luce model) and is formally expressed as \n\nwhere r k = log p \u03b8 ( Xk ) \u2212 log p \u03b8( Xk ), the expectation is over { X1 , . . . , XK , X}, and \u03b8 and \u03b8 denote the parameters of the model being trained and that used for reference, respectively.",
            "score": 0.4045077604799271,
            "section_title": "Preference-Aware Learning",
            "char_start_offset": 19222,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 531
                },
                {
                    "start": 534,
                    "end": 629
                },
                {
                    "start": 632,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1356
                },
                {
                    "start": 1359,
                    "end": 1599
                },
                {
                    "start": 1602,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1849
                },
                {
                    "start": 1852,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2047
                }
            ],
            "ref_mentions": [
                {
                    "start": 1427,
                    "end": 1449,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1579,
                    "end": 1598,
                    "matchedPaperCorpusId": "267657799"
                },
                {
                    "start": 1694,
                    "end": 1713,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.529296875
        },
        {
            "corpus_id": "258865733",
            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
            "text": "In summary, we study the calibration of widely used RLHF-LMs. We first replicate the finding for GPT-4 (OpenAI, 2023) that RLHF can worsen the calibration of a model's conditional probabilities using the open-source Llama-2-70B base and chat models (Figure 2). To mitigate this regression and ease extraction of calibrated confidence scores for models for which log probabilities are not available, we propose and study new methods that can elicit calibrated confidences from RLHF-LMs by prompting the model to verbalize its confidence in token space. We find verbalized probabilities are better-calibrated than conditional probabilities across several closed models, with mixed results for Llama-2-70B-Chat. \n\nOur results raise several questions for future work. Most notably, the difference between GPT-*, Claude-*, and Llama-2's ability to verbalize confidence is significant. What factors are important for learning this skill? Additionally, the 1-stage and 2-stage verbalized numerical confidence prompts sometimes differ drastically in the calibration of their confidences. How can we reduce sensitivity of a model's calibration to the prompt? Going beyond question-answering, can we leverage good calibration in short-answer settings to improve the reliability of long-form generations, perhaps by breaking down long-form generation into a sequence of short questions? Finally, to what extent does a language model's calibration depend on the domain; do our conclusions in the context of factual recall hold in the context of reasoning or arithmetic? Answering these questions provides one path toward building more trustworthy and useful language systems. Limitations. While our work demonstrates a promising new approach to generating calibrated confidences through verbalization, there are limitations that could be addressed in future work. First, our experiments are focused on factual recalloriented problems, and the extent to which our observations would hold for reasoning-heavy settings is an interesting open question.",
            "score": 0.40438637444175385,
            "section_title": "Discussion",
            "char_start_offset": 13186,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 708
                },
                {
                    "start": 711,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2036
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8232421875
        },
        {
            "corpus_id": "273821704",
            "title": "Graph-based Confidence Calibration for Large Language Models",
            "text": "One important approach to improving the reliability of large language models (LLMs) is to provide accurate confidence estimations regarding the correctness of their answers. However, developing a well-calibrated confidence estimation model is challenging, as mistakes made by LLMs can be difficult to detect. We propose a novel method combining the LLM's self-consistency with labeled data and training an auxiliary model to estimate the correctness of its responses to questions. This auxiliary model predicts the correctness of responses based solely on their consistent information. To set up the learning problem, we use a weighted graph to represent the consistency among the LLM's multiple responses to a question. Correctness labels are assigned to these responses based on their similarity to the correct answer. We then train a graph neural network to estimate the probability of correct responses. Experiments demonstrate that the proposed approach substantially outperforms several of the most recent methods in confidence calibration across multiple widely adopted benchmark datasets. Furthermore, the proposed approach significantly improves the generalization capability of confidence calibration on out-of-domain (OOD) data.",
            "score": 0.4043211166024709,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.962890625
        },
        {
            "corpus_id": "267750237",
            "title": "Multi-Perspective Consistency Enhances Confidence Estimation in Large Language Models",
            "text": "Dataset We utilize eight commonly used public datasets for evaluation, including four domain subsets of MMLU (Hendrycks et al., 2021), Chemistry, Computer_Security, Business_Ethics and Anatomy. Other datasets include TruthfulQA (Lin et al., 2022b), CSQA (Talmor et al., 2019), MedQA (Jin et al., 2020) andOBQA (Mihaylov et al., 2018). They provide rich scenarios that allow us to thoroughly evaluate the methods' confidence estimation effectiveness in various specialized fields. Detailed information about datasets is shown in the Appendix B.2. \n\nMetrics we adopt two evaluation metrics: AU-ROC (Area Under the Receiver Operating Characteristic curve) (Hendrycks and Gimpel, 2018;Xiong et al., 2023) and ECE (Expected Calibration Error) (Guo et al., 2017), to comprehensively assess the performance of the method. AUROC is a metric for assessing model discrimination between classes. Correct model predictions are marked positive, incorrect ones negative. The AUROC score spans 0 to 1, with values closer to 1 indicating better performance. ECE quantifies how well the model's predicted probabilities are calibrated, meaning the consistency between predicted probabilities and actual occurrence rates. A lower ECE value suggests that the model's predicted probabil-  Calculate final confidence \n\nend for ities align more closely with the actual outcomes. The specific calculation details are introduced in the Appendix B.3. \n\nBaselines We compare our method with the following strong baselines: \n\n\u2022 Verb (Lin et al., 2022a;Tian et al., 2023). It prompts the LLM to assess its confidence in its answer. By designing the prompt template, it requires LLM to return the answer's confidence score, ranging from 0 to 1. \n\n\u2022 Self-Consistency (Wang et al., 2023;Xiong et al., 2023). It estimates confidence by measuring the consistency among multiple candidate outputs generated by the model. It prompts the model to produce several response candidates and then calculate the consistency score among these candidates. \n\n\u2022 Verb & Surrogate (Shrivastava et al., 2023).",
            "score": 0.4041186417797109,
            "section_title": "Experimental Settings",
            "char_start_offset": 7645,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 545
                },
                {
                    "start": 548,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1294
                },
                {
                    "start": 1297,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1424
                },
                {
                    "start": 1427,
                    "end": 1495
                },
                {
                    "start": 1498,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1714
                },
                {
                    "start": 1717,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2010
                },
                {
                    "start": 2013,
                    "end": 2059
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7587890625
        },
        {
            "corpus_id": "270869459",
            "title": "Calibrated Large Language Models for Binary Question Answering",
            "text": "In terms of calibration performance, the advantage of using Venn-Abers predictors is evident.Figure 2 shows ECE values for both answer-token choices.When using start-of-word tokens ( Yes, No), Softmax-K shows a minimum at a specific temperature (\u03c4 \u2248 1.8) but degrades rapidly as soon as we move away from it.Softmax-2, on the other hand, shows several local minima and a global one for \u03c4 \u2248 33 which outperforms the former model.For the alternative choice (Yes, No), Softmax-2 shows a global minimum at a relatively low temperature, while Softmax-K fails to calibrate the predictions and exhibits high ECE at any temperature.\n\nIn contrast, the Venn-Abers predictors achieve an excellent calibration performance for both token pairs, at any temperature, with the exception of very low values (\u03c4 < 1) where all models seem to struggle (intuitively, lower temperatures push probabilistic predictions towards the extremes 0 and 1, hence there is little room for the scores to be adjusted).\n\nThese findings suggest that while temperature scaling can improve calibration in some cases, it is highly sensitive to the choice of temperature value and may not be effective for all token pairs.On the other hand, the Venn-Abers predictor offers a more reliable and consistent method for obtaining well-calibrated probabilities, making it a promising approach for uncertainty estimation in language models.",
            "score": 0.4028767881723842,
            "section_title": "Calibration results",
            "char_start_offset": 20470,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 93,
                    "end": 149
                },
                {
                    "start": 149,
                    "end": 308
                },
                {
                    "start": 308,
                    "end": 428
                },
                {
                    "start": 428,
                    "end": 624
                },
                {
                    "start": 626,
                    "end": 984
                },
                {
                    "start": 986,
                    "end": 1182
                },
                {
                    "start": 1182,
                    "end": 1393
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7041015625
        },
        {
            "corpus_id": "253244504",
            "title": "A Close Look into the Calibration of Pre-trained Language Models",
            "text": "For basic evaluation, we report accuracy (Acc) and average confidence score (Conf) on the testing set. For calibration evaluation, we report ECE using equal-mass binning and 100 bins following Minderer et al. (2021). Besides, we provide an application-driven perspective to evaluate model calibration, aiming to quantify two unsatisfied scenarios due to miscalibration in practice: (1) Correct predictions (positive) are rejected due to low confidence; (2) Wrong predictions (negative) are accepted due to high confidence. Specifically, we consider the average confidence in correct predictions Conf pos and wrong predictions Conf neg respectively. For unified comparison, we report two calibration error (CErr) cases, CErr pos = 1 \u2212 Conf pos and CErr neg = Conf neg . In principle, we expect calibrated models having both low CErr pos and CErr neg , indicating that they reasonably assign high confidence in correction predictions and low confidence in wrong predictions.  (Raffel et al., 2020), since they represent two classic types of PLMs, namely encoder-only and encoder-decoder models. We experiment with four representative tasks in NLP, including sentiment analysis, natural language inference, news classification, and topic classification. For datasets, we choose SST-2 (Socher et al., 2013a), MNLI (Williams et al., 2018a), AG-News (Zhang et al., 2015), and Yahoo (Zhang et al., 2015) respectively. We employ the prompt-based learning paradigm (Liu et al., 2021) since its superior performance compared to the traditional fine-tuning, especially in the fewshot setting. Specifically, we inherit the masked language modeling task in the pre-training stage and use templates to wrap samples into prompts. We fine-tune the whole PLMs to fill in the [mask] position in the prompt. The manual template and verbalizer for each dataset are listed in Appendix A.",
            "score": 0.4024982055363106,
            "section_title": "EVALUATION METRICS",
            "char_start_offset": 9206,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 215,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 974,
                    "end": 995,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1281,
                    "end": 1303,
                    "matchedPaperCorpusId": "990233"
                },
                {
                    "start": 1310,
                    "end": 1334,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 1344,
                    "end": 1364,
                    "matchedPaperCorpusId": "368182"
                },
                {
                    "start": 1376,
                    "end": 1396,
                    "matchedPaperCorpusId": "368182"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6279296875
        },
        {
            "corpus_id": "222141613",
            "title": "Knowing What You Know: Calibrating Dialogue Belief State Distributions via Ensembles",
            "text": "Since no other belief tracking methods that we are aware of have achieved success in producing wellcalibrated confidence, we look towards methods used in other language tasks. Natural language inference is a related task that also benefits from wellcalibrated confidence in predictions. Desai and Durrett (2020) introduce the use of post-processing techniques such as temperature scaling to produce better-calibrated confidence estimates. Additionally, there have been recent advances in the construction of more adequate loss functions. These methods, including Bayesian matching and prior networks, aim to learn well-calibrated models without the burden of requiring many extra parameters. These methods achieve good calibration in computer vision tasks such as CIFAR (Joo et al., 2020;Malinin and Gales, 2018;Szegedy et al., 2016).\n\nWhen the limitations of a single model still inhibit us from producing more accurate and bettercalibrated models, a popular alternative is to use an ensemble of models. Recently Malinin and Gales (2020) showed the success of using an ensemble of models for machine translation, and in particular utilising accurate confidence predictions for analysing translation quality.",
            "score": 0.4024982055363106,
            "section_title": "Related Work",
            "char_start_offset": 3414,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 788,
                    "end": 812,
                    "matchedPaperCorpusId": "3580844"
                },
                {
                    "start": 812,
                    "end": 833,
                    "matchedPaperCorpusId": "206593880"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68310546875
        },
        {
            "corpus_id": "273821339",
            "title": "Confidence Calibration of Classifiers with Many Classes",
            "text": "The intent is that the confidence score accurately describes whether the prediction is correct, regardless of the class. We show that this novel approach, which we call Top-versus-All (TvA), significantly improves the performance of standard calibration methods: Temperature and Vector Scaling [17], Dirichlet Calibration [29], Histogram Binning [69], Isotonic Regression [70], Beta Calibration [28], and Bayesian Binning into Quantiles [46]. We also introduce a simple regularization for Vector Scaling or Dirichlet Calibration that mitigates overfitting when the number of classes is high relative to the calibration data size. We conduct experiments on multiple image and text classification datasets and many pre-trained models. \n\nOur main contributions are the following: \n\n\u2022 We discuss four issues of the standard approach to confidence calibration. \n\n\u2022 To solve these issues, we develop the Top-versus-All approach to confidence calibration of multiclass classifiers, transforming the problem into a single binary classifier's calibration. This straightforward reformulation enables more efficient use of existing calibration methods, achieved with minimal modifications to the methods' original algorithms. \n\n\u2022 Applied to scaling methods for calibration (such as Temperature Scaling), TvA allows the use of the binary cross-entropy loss, which is more efficient in decreasing the confidence of wrong predictions and leads to stronger gradients in the case of Temperature Scaling. Applied to binary methods for calibration (such as Histogram Binning), TvA significantly improves their performance and makes them accuracy-preserving. \n\n\u2022 We demonstrate our approach's scalability and generality with extensive experiments on image classification with state-of-the-art models for complex datasets and on text classification with Pre-trained Language Models (PLMs) and Large Language Models (LLMs).",
            "score": 0.40237310729555487,
            "section_title": "Introduction",
            "char_start_offset": 4044,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 732
                },
                {
                    "start": 735,
                    "end": 776
                },
                {
                    "start": 779,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1214
                },
                {
                    "start": 1217,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1639
                },
                {
                    "start": 1642,
                    "end": 1902
                }
            ],
            "ref_mentions": [
                {
                    "start": 294,
                    "end": 298,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 322,
                    "end": 326,
                    "matchedPaperCorpusId": "202773833"
                },
                {
                    "start": 346,
                    "end": 350,
                    "matchedPaperCorpusId": "9594071"
                },
                {
                    "start": 372,
                    "end": 376,
                    "matchedPaperCorpusId": "3349576"
                },
                {
                    "start": 395,
                    "end": 399,
                    "matchedPaperCorpusId": "85546658"
                },
                {
                    "start": 437,
                    "end": 441,
                    "matchedPaperCorpusId": "6292807"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92236328125
        },
        {
            "corpus_id": "222341831",
            "title": "Ensemble Distillation for Structured Prediction: Calibrated, Accurate, Fast\u2014Choose Three",
            "text": "For a calibrated model, an event with a forecast confidence p occurs in held-out data with probability p. Calibrated probabilities enable meaningful decision making, either by machines such as downstream probabilistic systems (Nguyen and O'Connor, 2015), or by end-users who must interpret and trust system outputs (Jiang et al., 2012). The calibration of modern neural models has recently received increased attention in both the natural language processing and machine learning communities. A major finding is that modern neural networks do not always produce wellcalibrated predictions (Guo et al., 2017). As a result, much recent work has focused on improving model calibration, predominantly with post-hoc calibration methods (Kuleshov and Liang, 2015). \n\nHowever, post-hoc calibration methods have primarily been developed in the context of classification tasks. Thus, it is unclear how these methods will affect the performance of sequence-level structured prediction tasks (Kumar and Sarawagi, 2019). Additionally, post-hoc calibration methods require a held out calibration dataset, which may not be available in all circumstances. To improve calibration, an alternate approach is model ensembling, which is closely related to approximating the intractable posterior distribution over model parameters (Lakshminarayanan et al., 2017;Pearce et al., 2018;Dusenberry et al., 2020). Although computationally expensive, both at training and inference time, ensembling does not require a separate calibration set. Furthermore, ensembles have been found to be competitive or even outperform other calibration methods, particularly in more challenging settings such as dataset shift (Snoek et al., 2019). \n\nIn this paper, we study ensemble distillation as a means of achieving calibrated and accurate structured models while avoiding the high cost of naive ensembles at inference time (Hinton et al., 2015). Ensemble distillation consists of two stages: In the first stage, we select a base model for the task, such as a recurrent neural network or Transformer, and then train an ensemble of K such models, ensuring diversity either via sub-sampling ( \u00a74) or with different random seeds ( \u00a75).",
            "score": 0.4015339958648617,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 758
                },
                {
                    "start": 761,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1705
                },
                {
                    "start": 1708,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2194
                }
            ],
            "ref_mentions": [
                {
                    "start": 226,
                    "end": 253,
                    "matchedPaperCorpusId": "2879445"
                },
                {
                    "start": 315,
                    "end": 335,
                    "matchedPaperCorpusId": "7716019"
                },
                {
                    "start": 589,
                    "end": 607,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 731,
                    "end": 757,
                    "matchedPaperCorpusId": "2974522"
                },
                {
                    "start": 1311,
                    "end": 1342,
                    "matchedPaperCorpusId": "6294674"
                },
                {
                    "start": 1362,
                    "end": 1386,
                    "matchedPaperCorpusId": "218628552"
                },
                {
                    "start": 1684,
                    "end": 1704,
                    "matchedPaperCorpusId": "174803437"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.421142578125
        },
        {
            "corpus_id": "259095827",
            "title": "Proximity-Informed Calibration for Deep Neural Networks",
            "text": "Confidence Calibration Confidence calibration aims to yield uncertainty estimates via aligning a model's confidence with the accuracy of samples with the same confidence level [10,23,26]. \n\nTo achieve this, Scaling-based methods, such as temperature scaling [10], adjust the predicted probabilities by learning a temperature scalar for all samples. Similarly, parameterized temperature scaling [37] offers improved expressiveness via input-dependent temperature parameterization, and Mix-n-Match [46] adopts ensemble and composition strategies to yield data-efficient and accuracypreserving estimates. Binning-based methods divide samples into multiple bins based on confidence and calibrate each bin. Popular methods include classic histogram binning [44], mutual-informationmaximization-based binning [30], and isotonic regression [45]. However, existing calibration methods overlook the proximity bias issue, which fundamentally limits the methods' capabilities in delivering reliable and interpretable uncertainty estimates. \n\nMulticalibration Multicalibration algorithms [13,19] aim to achieve a certain level of fairness by ensuring that a predictor is well-calibrated for the overall population as well as different computationally-identifiable subgroups. [31] proposes a grouping loss to evaluate subgroup calibration error while we propose a metric to integrate the group cancellation effect into existing calibration loss. [19] focuses on understanding the fundamental trade-offs between group calibration and other fairness criteria, and [13] proposes a conceptual iterative algorithm to learn a multi-calibrated predictor. In this regard, our proposed framework can be considered a specific implementation of the fairness objectives outlined in [13], with a particular focus on proximity-based subgroups. This approach offers easier interpretation and implementation compared to subgroups discussed in [13,19]. \n\n3 What is Proximity Bias? \n\nIn this section, we study the following questions: What is proximity bias? When and why does proximity bias occur? \n\nBackground We consider a supervised multi-class classification problem, where input X \u2208 X and its label Y \u2208 Y = {1, 2, \u2022 \u2022 \u2022 , C} follow a joint distribution \u03c0(X, Y ).",
            "score": 0.4004207468075396,
            "section_title": "Related Work",
            "char_start_offset": 5154,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 190,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1028
                },
                {
                    "start": 1031,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1922
                },
                {
                    "start": 1925,
                    "end": 1950
                },
                {
                    "start": 1953,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2067
                },
                {
                    "start": 2070,
                    "end": 2237
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 180,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 183,
                    "end": 186,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 258,
                    "end": 262,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 394,
                    "end": 398,
                    "matchedPaperCorpusId": "232035835"
                },
                {
                    "start": 496,
                    "end": 500,
                    "matchedPaperCorpusId": "212725167"
                },
                {
                    "start": 752,
                    "end": 756,
                    "matchedPaperCorpusId": "9594071"
                },
                {
                    "start": 833,
                    "end": 837,
                    "matchedPaperCorpusId": "3349576"
                },
                {
                    "start": 1076,
                    "end": 1080,
                    "matchedPaperCorpusId": "51880858"
                },
                {
                    "start": 1080,
                    "end": 1083,
                    "matchedPaperCorpusId": "12845273"
                },
                {
                    "start": 1263,
                    "end": 1267,
                    "matchedPaperCorpusId": "253237320"
                },
                {
                    "start": 1433,
                    "end": 1437,
                    "matchedPaperCorpusId": "12845273"
                },
                {
                    "start": 1549,
                    "end": 1553,
                    "matchedPaperCorpusId": "51880858"
                },
                {
                    "start": 1757,
                    "end": 1761,
                    "matchedPaperCorpusId": "51880858"
                },
                {
                    "start": 1914,
                    "end": 1918,
                    "matchedPaperCorpusId": "51880858"
                },
                {
                    "start": 1918,
                    "end": 1921,
                    "matchedPaperCorpusId": "12845273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40966796875
        },
        {
            "corpus_id": "269004786",
            "title": "Multicalibration for Confidence Scoring in LLMs",
            "text": "Large language models (LLMs) have revolutionized text generation, with applications ranging from code development (Chen et al., 2021) to information retrieval (Zhu et al., 2023).However, alongside their impressive capabilities, LLMs possess a troubling tendency to fabricate information, generating outputs that diverge from factual reality -a phenomenon dubbed \"hallucination\" (Huang et al., 2023a).These hallucinations pose significant challenges to the trustworthiness and ethical deployment of LLMs, demanding the development of robust detection and mitigation strategies.\n\nIn this paper, we leverage recent \"multicalibration\" techniques (H\u00e9bert-Johnson et al., 2018) to produce calibrated probabilities indicating whether a generated response constitutes a hallucination.Unlike conventional calibration Answers are colored from red to green according to their multicalibrated confidence scores of being a hallucination.Multicalibration is performed using Algorithm 5. methods, multicalibrated probabilities are self-consistent not just marginally (i.e. on average over all examples), but also conditionally on various properties of the instance, which allows them to serve as more refined risk measures.Producing \"risk scores\" for hallucinations can provide an interpretable measure of risk which can be exposed to the user (e.g. through a coloring scheme, as in Figure 1) to communicate the risk associated with the generated content.Moreover, when those risk scores are calibrated, they are not only interpretable but \"trustworthy\" in the sense that they can be safely used as if they were true probabilities (Noarov et al., 2023).\n\nOur approach mirrors the robust assurances offered by conformal prediction, where multicalibration (of quantiles) has been used to give group-conditional guarantees (Bastani et al., 2022;Jung et al., 2022;Gibbs et al., 2023).Traditionally multicalibration has been used to give estimates of uncertainty in tabular data settings that hold conditionally on various features that are explicitly present in the data -often demographic attributes like sex or race.A key challenge in applying these techniques to hallucination detection in LLMs is a lack of such explicit features.",
            "score": 0.3997724942089259,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 178,
                    "end": 400
                },
                {
                    "start": 400,
                    "end": 576
                },
                {
                    "start": 578,
                    "end": 776
                },
                {
                    "start": 776,
                    "end": 924
                },
                {
                    "start": 924,
                    "end": 1208
                },
                {
                    "start": 1208,
                    "end": 1440
                },
                {
                    "start": 1440,
                    "end": 1638
                },
                {
                    "start": 1640,
                    "end": 1865
                },
                {
                    "start": 1865,
                    "end": 2099
                },
                {
                    "start": 2099,
                    "end": 2215
                }
            ],
            "ref_mentions": [
                {
                    "start": 1805,
                    "end": 1827,
                    "matchedPaperCorpusId": "249282366"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60986328125
        },
        {
            "corpus_id": "267211649",
            "title": "What large language models know and what people think they know",
            "text": "However, from the perspective of a human interacting with the LLM, this internal model confidence is not usually displayed to human users as part of LLM output. Instead, in current practice, humans rely solely on the language produced by the LLM in order to assess LLM confidence. To contrast with model confidence, in this paper we use the term human confidence to refer to a human's assessment (expressed as a probability) of how likely it is that the LLM's answer is correct based only on the language produced by the LLM without any knowledge of the LLM's internal model confidence. \n\nSurprisingly, studies focused on investigating human confidence in LLM outputs are lacking. In this paper, we take a step in addressing this issue and investigate what we term the calibration gap, namely the difference in the reliability of (i) internal LLM model confidence and (ii) external human confidence. In effect, the calibration gap represents the gap between an LLM's own internal confidence of what it knows and human perception of this confidence. We address two specific research questions in this context: \n\n1. How Large is the Calibration Gap? i.e., is there a significant gap between LLM model confidence and human confidence, in terms of how each assesses the true accuracy of the LLM? \n\n2. Can the Calibration Gap be Reduced? e.g., can the quality of human confidence in an LLM be improved by adapting the textual output of the LLM and by leveraging internal model confidence? \n\nFigure 1: Overview of the evaluation methodology for assessing the calibration gap between model confidence and human confidence in the model. The approach works as follows: \n\n(1) prompt the LLM with a multiple-choice question to obtain the model's internal confidence for each answer choice; \n\n(2) select the most likely answer and prompt the model a second time to generate an explanation for the given answer; \n\n(3) obtain the human confidence by showing users the question and the LLM's explanation and asking users to indicate the probability that the model is correct. In this toy example the model confidence is 0.46 for answer C, whereas the human confidence in 0.95 \n\nThese questions have important implications for the design of reliable LLM assistants.",
            "score": 0.39917133595296084,
            "section_title": "Introduction",
            "char_start_offset": 1894,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 586
                },
                {
                    "start": 589,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1108
                },
                {
                    "start": 1111,
                    "end": 1291
                },
                {
                    "start": 1294,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1483
                },
                {
                    "start": 1486,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1659
                },
                {
                    "start": 1662,
                    "end": 1778
                },
                {
                    "start": 1781,
                    "end": 1898
                },
                {
                    "start": 1901,
                    "end": 2060
                },
                {
                    "start": 2061,
                    "end": 2160
                },
                {
                    "start": 2163,
                    "end": 2249
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60400390625
        },
        {
            "corpus_id": "247594202",
            "title": "Calibration of Machine Reading Systems at Scale",
            "text": "With recent advances in machine reading, there has been a surge of interest in practical applications of the technology such as open-domain question answering (Karpukhin et al., 2020; and claim verification (Thorne et al., 2018b). Due to various scale limitations in practical settings, these systems are seldom trained endto-end. Such systems typically make use of a RE-TRIEVER alongside a READER -the evidence is first retrieved from a large corpus and is then used by a machine reading model to provide an answer.\n\nAs these systems are increasingly being deployed in the real world, it is important that they are not only accurate but also trustworthy. A way to make these systems trustworthy is to indicate when they are likely to be incorrect by providing a calibrated confidence measure in addition to the prediction.\n\nA naive solution for this is to use the system's output probability as the confidence. However, this confidence score is often uncalibrated (Kuleshov and Liang, 2015;Guo et al., 2017); i.e. it is not representative of the true correctness likelihood. 1 Previous work (Jiang et al., 2020;Jagannatha and Yu, 2020;Desai and Durrett, 2020) has shown that large language models especially suffer from miscalibration. Thus, several methods have been proposed to calibrate language models based on gradient-based calibration methods such as temperature scaling (Guo et al., 2017) and featurebased forecasters (Kuleshov and Liang, 2015). While gradient-based calibration is intuitive and easy to implement, feature-based forecasters require manual feature engineering.\n\nIn this work, we contribute a simple method to calibrate practical RETRIEVER -READER machine reading pipelines. These systems typically include a hard retrieval step which makes gradientbased calibration infeasible. Thus, we make use of the Gumbel machinery (Jang et al., 2017;Maddison et al., 2017); specifically the Gumbel top-K procedure of Vieira (2014); Xie and Ermon (2019) to obtain a differentiable sampling routine for the retrieval step. This sampler can then be combined with any gradient-based calibration technique such as Platt's scaling.\n\nWe conduct",
            "score": 0.39870223266158983,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 965,
                    "end": 991,
                    "matchedPaperCorpusId": "2974522"
                },
                {
                    "start": 1092,
                    "end": 1112,
                    "matchedPaperCorpusId": "208513249"
                },
                {
                    "start": 1427,
                    "end": 1453,
                    "matchedPaperCorpusId": "2974522"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73779296875
        },
        {
            "corpus_id": "221808425",
            "title": "Evaluating the Calibration of Knowledge Graph Embeddings for Trustworthy Link Prediction",
            "text": "We implement our methods in an extension of the OpenKE library. 3 To understand \"off-the-shelf\" calibration, we train models with the original loss functions and optimizers in the respective papers. \n\nAppendix A provides details on implementation and model selection. \n\nCalibration error Table 3 gives the ECE of all models before and after calibration using each technique in \u00a7 3.4. Confidence scores prior to calibration are scaled via the softmax. Across datasets, standard techniques calibrate models within 1-2 percentage points of error under the CWA. In most cases, the strongest methods are the multiclass (softmax) approaches. The only exception is matrix scaling on FB15K-Wiki, which overfits due to the large number of classes in the dataset (773 in FB15K-Wiki versus only 11 in WN18RR, Table 2). Evidently, taking the softmax over k classes leads to more discriminative probabilities than setting up k separate one-versus-all calibration problems and performing post-hoc normalization. \n\nWe also observe that off-the-shelf calibration error is correlated with model type, as the bilinear models (DistMult, ComplEx) consistently have lower ECE than the translational models (TransE, 3 https://github.com/thunlp/OpenKE/ TransH). To illustrate these differences, Figure 2 gives reliability diagrams for TransE and Dist-Mult before and after calibration. Reliability diagrams (Guo et al., 2017) bin predictions by confidence level into equally-sized regions of [0, 1] and show the relationship between average confidence level and accuracy in each bin, similar to ECE ( \u00a7 4.1). Without calibration, TransE is underconfident because it scores all predictions nearly the same, whereas DistMult is better calibrated. We observe a similar pattern for TransH and ComplEx. \n\nOne potential explanation for this difference is that multiplicative scoring functions lead to more discriminative scores due to the composition of dot products, which amplify embedding values.",
            "score": 0.398209278486547,
            "section_title": "Results and discussion",
            "char_start_offset": 14932,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 201,
                    "end": 267
                },
                {
                    "start": 270,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 997
                },
                {
                    "start": 1000,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1774
                },
                {
                    "start": 1777,
                    "end": 1970
                }
            ],
            "ref_mentions": [
                {
                    "start": 1384,
                    "end": 1402,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7783203125
        },
        {
            "corpus_id": "215754329",
            "title": "Improving Calibration and Out-of-Distribution Detection in Medical Image Segmentation with Convolutional Neural Networks",
            "text": "All machine learning models are bound to make wrong predictions on a fraction of test data. Nonetheless, one would like the confidence of predictions to be proportional to the probability of being correct. Consider a test set of {x i , y i } i=1:N and suppose for sample x i the model predicts the class \u0177i with a probability pi . In the ideal scenario with perfect confidence calibration, P(\u0177 = y|p = p) = p [23]. \n\nStandard deep learning models have been shown to be poorly calibrated [5]. This should be concerning for safetycritical applications including medicine. A range of methods have been proposed for improving the calibration of deep learning models. For example, it has been shown that calibration can be improved by using a proper scoring rule as the loss function [24], [5], using weight decay and avoiding batch normalization [5]. Training on adversarial examples [25] has also been shown to improve model calibration [24]. Some studies have used the Platt scaling for improving the model calibration [26], [5]. In [27], for instance, after the deep learning network is trained, a model q = Softmax(az + b) with parameters a and b is trained on the logit vector, z, of the trained network to obtain a more calibrated prediction q. Another study proposed to train a separate model, to map the uncalibrated output of a CNN to calibrated probabilities [28]. For this purpose, they used a Bayesian neural network, which they trained after training the main deep learning model. \n\nPrediction uncertainty has also received some attention in medical image segmentation studies. Some studies have proposed methods to estimate the uncertainty [29] or to use the prediction uncertainty to improve the segmentation accuracy [30], [31]. However, little attention has been paid to methods for improving the calibration of CNN-based segmentation models. An example of the latter is the work of [32], where the authors use model ensembles to arrive at better-calibrated models. That study trained an ensemble of CCNs with random initialization of network weights and random shuffling of training data.",
            "score": 0.39796671466648603,
            "section_title": "B. Model calibration and uncertainty estimation",
            "char_start_offset": 6099,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 414
                },
                {
                    "start": 417,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1489
                },
                {
                    "start": 1492,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2102
                }
            ],
            "ref_mentions": [
                {
                    "start": 409,
                    "end": 413,
                    "matchedPaperCorpusId": "9594071"
                },
                {
                    "start": 779,
                    "end": 783,
                    "matchedPaperCorpusId": "6294674"
                },
                {
                    "start": 934,
                    "end": 938,
                    "matchedPaperCorpusId": "6294674"
                },
                {
                    "start": 1031,
                    "end": 1035,
                    "matchedPaperCorpusId": "51920640"
                },
                {
                    "start": 1650,
                    "end": 1654,
                    "matchedPaperCorpusId": "49901497"
                },
                {
                    "start": 1729,
                    "end": 1733,
                    "matchedPaperCorpusId": "198135962"
                },
                {
                    "start": 1735,
                    "end": 1739,
                    "matchedPaperCorpusId": "52270645"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1507568359375
        },
        {
            "corpus_id": "266051956",
            "title": "A Study on the Calibration of In-context Learning",
            "text": "Language models (LMs) that encompass transformer-based architectures (Brown et al., 2020;Chowdhery et al., 2023;OpenAI, 2023) can generate coherent and contextually relevant texts for various use cases. Despite their impressive performance, these models occasionally produce erroneous or overconfident outputs, leading to concerns about their calibration (Dawid, 1982;DeGroot and Fienberg, 1983) which measures how faithful a model's prediction uncertainty is. Such a problem is pressing when users adapt them using a recent paradigm called in-context learning (Brown et al., 2020) to construct performant predictors, especially for applications in safety-critical domains (Bhatt et al., 2021;Pan et al., 2023). \n\nWe provide an in-depth evaluation and analysis of how well these models are calibrated -that is, the alignment between the model's confidence in its predictions and the actual correctness of those predictions. This token-level calibration assessment enables us to measure the discrepancy between the model's perceived and actual performance to assess its accuracy and reliability through a Bayesian uncertainty lens. \n\nWe find that LM such as LLaMA (Touvron et al., 2023a) is poorly calibrated in performant settings and there exists a calibration-accuracy trade-off (Fig. 1) for low-shot settings (k < 4): as we increase the amount of in-context samples, both prediction accuracy and calibration error increase. Such a trade-off can be improved using more ICL examples (k = 8) and larger models. Crucially, this calibration degradation worsens when fine-tuning occurs using specialized data to improve usability, such as curated instructions (Dubois et al., 2023), dialogues (Zheng et al., 2023), or human preference data (Ziegler et al., 2019). Though previous common practice suggests recalibrating models' logits via temperature scaling (Guo et al., 2017), we show that in contrast to classic regimes, the miscalibration issue in ICL can not be easily addressed using such well-established scaling approaches (Platt et al., 1999).",
            "score": 0.39775849733622665,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 711
                },
                {
                    "start": 714,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 2048
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 89,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 89,
                    "end": 112,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 355,
                    "end": 368,
                    "matchedPaperCorpusId": "121781338"
                },
                {
                    "start": 368,
                    "end": 395,
                    "matchedPaperCorpusId": "109884250"
                },
                {
                    "start": 561,
                    "end": 581,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 673,
                    "end": 693,
                    "matchedPaperCorpusId": "226965491"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69140625
        },
        {
            "corpus_id": "215785895",
            "title": "The Right Tool for the Job: Matching Model and Instance Complexities",
            "text": "Since confidence scores play an important role, we use calibration techniques to make them more reliable. Associating classifiers with different layers of the same network allows us to reuse the computation performed by the simple classifiers for the complex ones. See Figure 1 for an illustration. \n\nWe experiment with three text classification benchmarks and two natural language inference (NLI) benchmarks. We consider each of our classifiers with different BERT layers as individual baselines. We find that using our method leads to a consistently better speed/accuracy tradeoff in almost all cases. In particular, in some cases, we obtain similar performance while being as much as five times faster than our strongest baseline (the original BERT-large mode with a single classification layer after the last layer). \n\nOur approach, while allowing substantially faster inference compared to the standard BERTlarge model, is neither slower to fine-tune nor significantly larger in terms of parameters, requiring less than 0.005% additional parameters. Moreover, our method is quite flexible: unlike other approaches for inference speed-up such as model distillation or pruning, which require training a different model for each point along the speed/accuracy curve, our method only requires training a single model, and by setting a single variable at inference time-the confidence threshold-supports each point along that curve. Finally, our method is orthogonal to compression methods such as model distillation (Hinton et al., 2014). Our experiments with a distilled version of BERT (Jiao et al., 2019) show that our method further improves the speed/accuracy curve on top of that model. We publicly release our code.5 2 Premise: Models Vary in Size,",
            "score": 0.3977139741278333,
            "section_title": "Early exit prediction",
            "char_start_offset": 2506,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 298
                },
                {
                    "start": 301,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1756
                }
            ],
            "ref_mentions": [
                {
                    "start": 1517,
                    "end": 1538,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7529296875
        },
        {
            "corpus_id": "247594202",
            "title": "Calibration of Machine Reading Systems at Scale",
            "text": "In this paper, we analyzed how various calibration techniques can be adopted to open-domain machine reading systems which are now being used in user-facing scenarios. We showed that in such systems that include a retriever, calibrating the system's confidence is not trivial and we proposed a technique that allows calibration of the system jointly. Finally, we also provide an analysis on how the calibration techniques can help the model abstain from answering a question especially in settings where the model's prediction can be incorrect due to malformed or out-of-domain questions.\n\nWhile we do not find evidence to prove that one calibration method (e.g. a gradient-based method) is better that the other (e.g. a forecaster approach), it would be important to investigate these questions with more nuanced human studies.",
            "score": 0.3975675409238757,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 25194,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.187255859375
        },
        {
            "corpus_id": "269449363",
            "title": "Improving Automatic Text Recognition with Language Models in the PyLaia Open-Source Library",
            "text": "First, we compare different confidence estimation methods by measuring the correlation between confidence scores and recognition rate.Then, we discuss the benefit of confidence calibration using temperature scaling.\n\nBest method for confidence estimation Table 2 presents the correlation between different computation methods for confidence scores and the recognition rate, defined as 1 \u2212 CER.Both Spearman (rank correlation) and Pearson (linear correlation) coefficients are computed to capture different aspects of the correlation.Unfortunately, we find that the Monte Carlo dropout approach leads to a degradation of recognition quality in addition to high computational cost.Therefore, this method is excluded from our analysis.\n\nThe results show that the most effective method is to take the softmax value for each token after the CTC decoding step and average these values for each line to obtain line-level confidence scores.This approach not only ensures computational efficiency, but also yields a strong correlation with the recognition rate.As a result of these findings, we have incorporated this calculation method into PyLaia.\n\nConfidence score calibration The optimal temperature T is determined using a grid search algorithm on the validation set (from 1 to 6 with a step of 0.5).Once T is found, we compute the correlation between confidence scores and the recognition rate on the test set, with and without temperature scaling.\n\nTable 3 provides insight into the impact of model calibration on the linear correlation between confidence scores and recognition rate on the test set of different datasets.\n\nTable 3: Impact of model calibration using the optimal temperature T on the correlation between confidence scores and the recognition rate.In all experiments, the p-value is always well below 0.05, indicating that the linear correlation is always significant.The results show significant improvements in the linear correlation between confidence scores and recognition rates when temperature scaling is applied.Across different datasets, optimal temperatures ranging from 1.5 to 4.0 yield consistently higher correlations compared to scenarios without scaling.Since temperature scaling only scales the logits without changing the underlying ranking of the predictions, it does not affect the rank correlation.\n\nOur analysis suggests that the default value of 2.0 is a robust choice for calibration, even for datasets where the optimal temperature is not equal to 2.0.",
            "score": 0.3975211698930668,
            "section_title": "Confidence score analysis",
            "char_start_offset": 19580,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 134,
                    "end": 215
                },
                {
                    "start": 217,
                    "end": 393
                },
                {
                    "start": 393,
                    "end": 533
                },
                {
                    "start": 533,
                    "end": 679
                },
                {
                    "start": 679,
                    "end": 732
                },
                {
                    "start": 734,
                    "end": 932
                },
                {
                    "start": 932,
                    "end": 1052
                },
                {
                    "start": 1052,
                    "end": 1140
                },
                {
                    "start": 1142,
                    "end": 1296
                },
                {
                    "start": 1296,
                    "end": 1445
                },
                {
                    "start": 1447,
                    "end": 1620
                },
                {
                    "start": 1622,
                    "end": 1761
                },
                {
                    "start": 1761,
                    "end": 1881
                },
                {
                    "start": 1881,
                    "end": 2033
                },
                {
                    "start": 2033,
                    "end": 2182
                },
                {
                    "start": 2182,
                    "end": 2331
                },
                {
                    "start": 2333,
                    "end": 2489
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97314453125
        },
        {
            "corpus_id": "269484456",
            "title": "Geometric Insights into Focal Loss: Reducing Curvature for Enhanced Model Calibration",
            "text": "In recent years, neural networks have been used successfully in many fields, including computer vision [14,32], natural language processing [8,9], and signal processing [12,15].Such successful results are grounded in the outstanding accuracy of neural networks.However, for real-world applications of machine learning algorithms, merely high accuracy is not sufficient in many cases.Especially in decision-making situations, the confidence level of the model in its own predictions is important.For example, when the output of a machine learning model is used to assist human decision-making rather than being used directly, such as in pathology classification or credit forecasting, the focus is not only on the accuracy of the model itself but also on how much the output of the model can be trusted.In many cases, model confidence is quantified by interpreting the output of the final layer activation function as a classification probability.In fact, the output vector of the softmax function is treated as a probability vector because its value range is [0, 1].However, although the softmax output seems to be regarded as class classification probabilities, it is known that there is a discrepancy with reality.For example, even if a human observer is unsure whether an image is a dog or a cat, the machine learning model may determine that the image is a dog with a 99% probability.This phenomenon is called overconfidence [17,31] and is known to be one of the most important issues to be solved in practical applications of machine learning algorithms.\n\nThe task of addressing these problems is known as model calibration [10,23,30].The goal of model calibration is to ensure that the model output probabilities can be interpreted as confidence levels.There are several metrics for evaluating model calibration, including ECE and its variants, and our objective is to improve them to guarantee the validity of the machine learning model as a confidence level for the output vector of the model.Focal loss [22,23] is one of the most popular techniques used to improve model calibration performance.It was originally proposed as a heuristic for object detection in computer vision, but its effectiveness for model calibration was later reported.",
            "score": 0.3973879940526796,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 177,
                    "end": 261
                },
                {
                    "start": 261,
                    "end": 383
                },
                {
                    "start": 383,
                    "end": 495
                },
                {
                    "start": 495,
                    "end": 802
                },
                {
                    "start": 802,
                    "end": 946
                },
                {
                    "start": 946,
                    "end": 1066
                },
                {
                    "start": 1066,
                    "end": 1216
                },
                {
                    "start": 1216,
                    "end": 1388
                },
                {
                    "start": 1388,
                    "end": 1559
                },
                {
                    "start": 1561,
                    "end": 1640
                },
                {
                    "start": 1640,
                    "end": 1759
                },
                {
                    "start": 1759,
                    "end": 2001
                },
                {
                    "start": 2001,
                    "end": 2104
                },
                {
                    "start": 2104,
                    "end": 2250
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 143,
                    "matchedPaperCorpusId": "8273530"
                },
                {
                    "start": 1429,
                    "end": 1433,
                    "matchedPaperCorpusId": "211258943"
                },
                {
                    "start": 1433,
                    "end": 1436,
                    "matchedPaperCorpusId": "248887336"
                },
                {
                    "start": 1629,
                    "end": 1633,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1633,
                    "end": 1636,
                    "matchedPaperCorpusId": "211252346"
                },
                {
                    "start": 1636,
                    "end": 1639,
                    "matchedPaperCorpusId": "231986167"
                },
                {
                    "start": 2012,
                    "end": 2016,
                    "matchedPaperCorpusId": "47252984"
                },
                {
                    "start": 2016,
                    "end": 2019,
                    "matchedPaperCorpusId": "211252346"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.432373046875
        },
        {
            "corpus_id": "278129791",
            "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review",
            "text": "We next discuss our evaluation's findings and review the limitations and relevance. Key takeaways. We observe that most existing methods for calibrating LLMs are designed primarily for textual and semantic classification tasks and lack scalability for generative tasks [28], [33]. Addressing the applicability of these methods to generative natural language tasks represents a significant research challenge. Recent studies have utilized small auxiliary models to elicit confidence and cluster questions and answers using sentence embedding models to achieve target accuracy [11]. However, we contend that such auxiliary models may be both unreliable and computationally inefficient when applied to diverse tasks with LLMs. Instead, ranking questions and answers present a more practical and scalable alternative. Additionally, the effectiveness of eliciting and improving model confidence is significantly influenced by the quality and relevance of the questions, underscoring the importance of reasoning capabilities in LLMs for providing more reliable confidence assessments across various tasks [4]. \n\nWe also emphasize the importance and reliability of small language models(SLMs) for everyday use and edge intelligence since calibration as the scale of the model are found to be correlated with each other [18], [184]. Lack of intuitive metric. In addition, not all evaluation metrics are suitable for calibration tasks and have limitations in capturing different aspects of uncertainty and calibratedness of the model. The BLEU score cannot measure the semantics and syntactic correctness of the generation. It is also quite sensitive to the size of the generation and struggles to capture the validity of results that are derived from step-by-step expression as in CoT. As a reference-based metric [158], it does not provide a measure of generation quality other than the provided reference answers. We think a robust evaluation metric is still an open question in the LLM literature. Selection bias can be mitigated using calibration [152]; hence, proposing a metric for bias and including it in the supervised fine-tuning stage may be necessary for further research. Better binning for ECE. We observe that a smoothed version of ECE can provide a more concrete error value.",
            "score": 0.39736910179958973,
            "section_title": "A. Discussion",
            "char_start_offset": 49594,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 1103
                },
                {
                    "start": 1106,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2176
                },
                {
                    "start": 2177,
                    "end": 2200
                },
                {
                    "start": 2201,
                    "end": 2283
                }
            ],
            "ref_mentions": [
                {
                    "start": 269,
                    "end": 273,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.469482421875
        },
        {
            "corpus_id": "252780032",
            "title": "Explaining Predictive Uncertainty by Looking Back at Model Explanations",
            "text": "A common way of measuring predictive uncertainty is by calibrating model outputs with the true correctness likelihood, so that the predictive probabilities well represent the confidence of model predictions being correct (Guo et al., 2017;Kong et al., 2020;Desai and Durrett, 2020;Zhao et al., 2021). Lower prediction confidence indicates higher uncertainty (Xu et al., 2020;Jiang et al., 2021). We follow the post-calibration methods and adopt the temperature scaling (Guo et al., 2017;Zhao et al., 2021) to calibrate the pre-trained language models (BERT and RoBERTa) in our experiments. \n\nSpecifically, we use the development set to learn a temperature T which corrects model output probabilities by dividing non-normalized logits before the softmax function. Then the learned T is applied to modify model outputs on the test set. In experiments, we linearly search for an optimal temperature T between [0, 10] with a granularity of 0.01, which empirically performs well. We evaluate model calibration with Expected Calibration Error (ECE) (Guo et al., 2017). The ECE measures the difference between prediction confidence and accuracy, i.e. \n\nwhere the total n predictions are partitioned into K equally-spaced bins, B k represents the predictions fall into the kth bin, acc(\u2022) and conf (\u2022) compute the average accuracy and confidence in each bin respectively. For a perfect calibration, acc(B k ) = conf (B k ), k \u2208 {1, . . . , K}. In this work, we set K = 10. We report the learned temperature scalars and ECEs before and after calibration in Table 6. Temperature scaling performs effectively in decreasing model calibration errors. This enables us to further explain prediction uncertainty based on calibrated confidence. We apply temperature scaling to correct model outputs in experiments.",
            "score": 0.3972033792630074,
            "section_title": "A.2 Posterior Calibration",
            "char_start_offset": 15376,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1143
                },
                {
                    "start": 1146,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1797
                }
            ],
            "ref_mentions": [
                {
                    "start": 221,
                    "end": 239,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 257,
                    "end": 281,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 358,
                    "end": 375,
                    "matchedPaperCorpusId": "222378161"
                },
                {
                    "start": 375,
                    "end": 394,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 469,
                    "end": 487,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1043,
                    "end": 1061,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76171875
        },
        {
            "corpus_id": "247450599",
            "title": "On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency",
            "text": "cy maps can benefit in-domain calibration and by dissimilarity can benefit out-of-domain calibration. To monitor training dynamics, we use the Area Under the Margin (AUM) statistic (Pleiss et al., 2020) which measures how different a true label for a sample is compared to a model's beliefs at each epoch and is calculated as the average difference between the logit values for a sample's assigned class and its highest non-assigned class across training epochs.\n\nMoreover, we combine our mixup with wellknown miscalibration correction methods such as label smoothing and temperature scaling (Guo et al., 2017) to investigate their impact on our proposed mixup. We conduct a comprehensive set of experiments using BERT (Devlin et al., 2019) and RoBERTa  to show the efficacy of our mixup approach by testing on three NLU tasks: natural language inference, paraphrase detection, and commonsense reasoning. We achieve the lowest Expected Calibration Error (ECE) without accuracy drops in comparison with strong baseline methods. Our contributions are as follows:\n\n\u2022 We propose a novel mixup method which is guided by AUM and saliency signals and is targeted at improving model calibration. Specifically, we compare logits to categorize samples into two sets (i.e., a set of easy-to-learn samples and another set of hardto-learn/ambiguous samples), and interpolate samples across these two sets by finding the most similar and most dissimilar samples from the other set while leveraging saliency (to compute sample similarities) for pre-trained language models' calibration on in-domain and out-of-domain data.\n\n\u2022 We combine our method with miscalibration correction techniques (i.e., label smoothing, temperature scaling) to investigate their impact on our proposed mixup.\n\n\u2022 We conduct comprehensive experiments showing that our method achieves the lowest expected calibration errors (ECEs) on both in-domain and out-of-domain samples compared with strong baselines without accuracy drops on multiple NLU tasks, namely, natural language inferences, paraphrase detection, and commonsense reasoning.",
            "score": 0.3964090450116023,
            "section_title": "Introduction",
            "char_start_offset": 3791,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 202,
                    "matchedPaperCorpusId": "210932316"
                },
                {
                    "start": 592,
                    "end": 610,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 719,
                    "end": 740,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73681640625
        },
        {
            "corpus_id": "258865733",
            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
            "text": "Tables 1-5 show the results of evaluating various methods for extracting confidence from RLHF-LMs on gpt-3.5-turbo, gpt-4, claude-1, claude-2, and Llama-2-70b-chat, respectively. We distill several key conclusions from these experiments. 1. Large RLHF-LMs can often directly verbalize better-calibrated confidences (either a numerical confidence probability or an expression such as 'highly likely') than the models' conditional probabilities. 2. Among the methods for verbalizing probabilities directly, we observe that generating and evaluating multiple hypotheses improves calibration (see Figure 1), similarly to humans (Lord et al., 1985), and corroborating a similar finding in LMs (Kadavath et al., 2022). \n\n3. Language models can express their uncertainty with numerical probabilities as well or better than with words, which is surprising in light of longstanding difficulties in representing numbers in language models (Thawani et al., 2021). 4. Chainof-thought prompting does not improve verbalized calibration (see Appendix Figure 5 for additional CoT results). 5. The calibration of both Claude models' conditional probabilities roughly falls between gpt-3.5-turbo and gpt-4; however, while Claude 1 is much weaker at verbalizing its confidence, Claude 2 is generally a bit stronger than gpt-3.5-turbo at verbalizing. The verbal calibration of the open source model Llama-2-70b-chat is generally weaker than that of closed source models but still demonstrates improvement over its conditional probabilities by some metrics, and does so most clearly on TruthfulQA.",
            "score": 0.3960769802229755,
            "section_title": "Results",
            "char_start_offset": 11595,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 712
                },
                {
                    "start": 715,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1576
                }
            ],
            "ref_mentions": [
                {
                    "start": 624,
                    "end": 643,
                    "matchedPaperCorpusId": "24047702"
                },
                {
                    "start": 929,
                    "end": 951,
                    "matchedPaperCorpusId": "232335764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "corpus_id": "249679786",
            "title": "Uncertainty Estimation and Reduction of Pre-trained Models for Text Regression",
            "text": "Uncertainty estimation has been extensively explored in the context of classification (Guo et al., 2017;Vaicenavicius et al., 2019;Desai and Durrett, 2020;Jiang et al., 2020), but is relatively unexplored for regression tasks, due to the complexities in dealing with a continuous target space. The output of a classifier passed through a softmax layer naturally provides a discrete probability distribution, while in a regression setting the output is a single numerical value. \n\nWe compare four well-studied techniques for uncertainty estimation, as applied to pre-trained language models (LMs): Gaussian processes (Shen et al., 2019;Camporeale and Car\u00e8, 2020), Bayesian linear regression (Hern\u00e1ndez-Lobato and Adams, 2015), Bayes by backprop, and Monte Carlo (MC) dropout. To comprehensively assess uncertainty quality, we evaluate results intrinsically using various metrics, and extrinsically with several downstream experiments. Our analysis shows that predictions are highly uncertain and inaccurate in low-resource scenarios. \n\nTwo major types of uncertainty have been identified: aleatoric uncertainty captures noise inherent in the observations; and epistemic uncertainty accounts for uncertainty in the model, which can be explained away given enough data, compensating for limited knowledge (Kendall and Gal, 2017). In other words, uncertainty results primarily from noisy human annotations, insufficient labeled data, and out-of-domain text in practice (Glushkova et al., 2021). We therefore propose a simple method to filter noisy labels and select high-quality instances from an unlabeled data pool based on the predictive confidence, which on the one hand alleviates both aleatoric and epistemic uncertainty, and on the other hand, improves accuracy and generalization thanks to increased training data. \n\nIn this work, we explore how to estimate uncertainty in a regression setting with pre-trained language models, and evaluate estimation quality both intrinsically and extrinsically. Intrinsic uncertainty estimation provides the basis for our proposed data selection strategy: By filtering noise based on confidence thresholding, and mitigating exposure bias, our approach is shown to be effective at improving both performance and generalization in low-resource settings, in self-training, and active learning settings.",
            "score": 0.39588517555852454,
            "section_title": "Introduction",
            "char_start_offset": 2049,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 477
                },
                {
                    "start": 480,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1032
                },
                {
                    "start": 1035,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1818
                },
                {
                    "start": 1821,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2339
                }
            ],
            "ref_mentions": [
                {
                    "start": 86,
                    "end": 104,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 104,
                    "end": 131,
                    "matchedPaperCorpusId": "67749814"
                },
                {
                    "start": 616,
                    "end": 635,
                    "matchedPaperCorpusId": "208331285"
                },
                {
                    "start": 690,
                    "end": 724,
                    "matchedPaperCorpusId": "8645175"
                },
                {
                    "start": 1302,
                    "end": 1325,
                    "matchedPaperCorpusId": "71134"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52490234375
        },
        {
            "corpus_id": "268364098",
            "title": "Uncertainty in Graph Neural Networks: A Survey",
            "text": "This family of methods consists of a single model with deterministic parameters, i.e., each repetition of the method produces the same result. We classify them into direct, Bayesian-based, and Frequentist-based estimation methods. \n\nDirect estimation. The most commonly used uncertainty estimation method is through the softmax probability of prediction outcomes, including maximum softmax probability (Wang et al., 2021) and entropy (Cai et al., 2017). \n\nOther heuristic measures, such as the distance of the sample to each class in the representation space (Xu et al., 2023), are also used to quantify uncertainty. Direct estimation methods integrate uncertainty estimation with prediction and do not require modifications of the original model. \n\nHowever, it has been shown that GNN tends to be under-confident (Wang et al., 2021), which leads to inaccurate estimation of confidence and entropy. Confidence is an indication of how likely (probability) the predictions of a machine learning algorithm are correct, often computed as the maximum softmax class probability. Under-confidence, in other words, refers to that confidence, as the indication of accuracy, falls below the true prediction accuracy. To mitigate the issue of under-confidence, a well-calibrated GNN should produce predicted probabilities that accurately reflect the true likelihood of the predicted outcomes. This is usually evaluated with a smaller Expected Calibration Error (ECE) (Guo et al., 2017). To provide faithful uncertainty estimation, Wang et al. (2021) add a regularizer in the GNN optimization objective, which penalizes the probabilities of the incorrect nodes and encourages the correct nodes to be more confident. Liu et al. (2022d) employ classical post-processing methods to calibrate GNN confidence, including histogram binning, isotonic regression, and Bayesian binning into quantiles. Similarly, Hsu et al. (2022b) propose a post hoc calibration method tailored for GNNs. Compared with uncalibrated direct estimation, the current calibration methods can effectively reduces ECE, but require additional calibration samples.",
            "score": 0.39584276259972123,
            "section_title": "Single Deterministic Model",
            "char_start_offset": 8317,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 230
                },
                {
                    "start": 233,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 453
                },
                {
                    "start": 456,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2117
                }
            ],
            "ref_mentions": [
                {
                    "start": 402,
                    "end": 421,
                    "matchedPaperCorpusId": "238215193"
                },
                {
                    "start": 559,
                    "end": 576,
                    "matchedPaperCorpusId": "260092553"
                },
                {
                    "start": 814,
                    "end": 833,
                    "matchedPaperCorpusId": "238215193"
                },
                {
                    "start": 1456,
                    "end": 1474,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1520,
                    "end": 1538,
                    "matchedPaperCorpusId": "238215193"
                },
                {
                    "start": 1704,
                    "end": 1722,
                    "matchedPaperCorpusId": "249375463"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1636962890625
        },
        {
            "corpus_id": "265157516",
            "title": "A Survey of Confidence Estimation and Calibration in Large Language Models",
            "text": "Given this, confidence estimation and calibration for LLMs is growing as an emerging area of interest (Jiang et al., 2021;Lin et al., 2022Lin et al., , 2023;;Shrivastava et al., 2023). \n\nWhile existing surveys mainly focused on issues such as hallucination and factuality in LLMs (Zhang et al., 2023b;Wang et al., 2023b), there are no comprehensive surveys systematically discussing the technical advancements in LLMs, and here we aim to bridge this gap. We explore the unique challenges posed by LLMs and examine the latest studies addressing these issues. We first discuss key concepts such as confidence, uncertainty, and calibration in the context of neural models, as detailed in Section 2. Then, we pursue two different directions: one addressing confidence estimation and calibration techniques for generation tasks in Section 3, and the other for classification tasks in Section 4. We conclude by exploring their practical applications (Section 5) and looking at potential future research directions (Section 6). Figure 1 provides a comprehensive representation of the survey's structure. By conducting a detailed examination of existing research, our goal is to illuminate this vital facet of LLMs, contributing to the development of more reliable applications.",
            "score": 0.3952572415410156,
            "section_title": "Introduction",
            "char_start_offset": 1833,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 187,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1270
                }
            ],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 122,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 158,
                    "end": 183,
                    "matchedPaperCorpusId": "265213392"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4775390625
        },
        {
            "corpus_id": "278129791",
            "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review",
            "text": "While understanding the trending and important methods for fine-tuning large foundational models (LFMs) is crucial, the fine-tuning techniques of LFMs will be highly related to the contribution one can make via one's local data. This problem can be treated as an uncertainty gap of distribution shift, a small domain difference between training and test datasets, and OOD methods can be used for such considerable differences in samples and features [42]. Estimation and calibration for long sequences. Calibration of longer sequence outputs of models is a relatively new concept [99], [163], as previous studies were focused on short-form generations [10], [200]. Early investigations found that models tend to diffuse probability distribution over the sequence [191]. Calibration for longer generations can be a harder problem. The model can have increased over-confidence or under-confidence at the end token probability of the sentence [188]. Token-level confidence also limits context to the current inference instead of representing the overall predictive uncertainty of a token [31]. Calibration of long-form generation gained attention in the recent works [27], [164]. Another recent study [200] found that verbalized confidences are usually more calibrated than the model's inference probabilities for recent LLMs, though temperature scaling can still be effective for the calibration of LLMs. The trivial calibration method is always outputting the average likelihood of all predictions [26]. However, should a perfectly calibrated model output its average accuracy no matter how the performance is [141]? Better binning strategy. Measuring the original calibration error is impossible with a finite number of bins [41]. One modification is choosing adaptive bin sizes instead of evenly sized bins according to data density [189]. Using Kernels-wise calibration [203], existing methods trade off the calibration for accuracy when applied on the likelihood of all sequences [193] and reduce the categorical discriminative aspect of the model [204]. Individualized calibration of the categories of a dataset is an open challenge. Can we maintain calibration when we do not have labeled examples from the target dataset [41]?",
            "score": 0.3952481407250908,
            "section_title": "E. Applicability in LLMs",
            "char_start_offset": 39627,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2137
                },
                {
                    "start": 2138,
                    "end": 2232
                }
            ],
            "ref_mentions": [
                {
                    "start": 450,
                    "end": 454,
                    "matchedPaperCorpusId": "239049401"
                },
                {
                    "start": 580,
                    "end": 584,
                    "matchedPaperCorpusId": "174802983"
                },
                {
                    "start": 586,
                    "end": 591,
                    "matchedPaperCorpusId": "264811211"
                },
                {
                    "start": 763,
                    "end": 768,
                    "matchedPaperCorpusId": "4375156"
                },
                {
                    "start": 1497,
                    "end": 1501,
                    "matchedPaperCorpusId": "265445593"
                },
                {
                    "start": 1725,
                    "end": 1729,
                    "matchedPaperCorpusId": "202718866"
                },
                {
                    "start": 1834,
                    "end": 1839,
                    "matchedPaperCorpusId": "102486060"
                },
                {
                    "start": 1872,
                    "end": 1877,
                    "matchedPaperCorpusId": "233454709"
                },
                {
                    "start": 1983,
                    "end": 1988,
                    "matchedPaperCorpusId": "15880012"
                },
                {
                    "start": 2051,
                    "end": 2056,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 2227,
                    "end": 2231,
                    "matchedPaperCorpusId": "202718866"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56494140625
        },
        {
            "corpus_id": "174802983",
            "title": "When Does Label Smoothing Help?",
            "text": "Pereyra et al. [17] showed that label smoothing provides consistent gains across many tasks. That work also proposed a new regularizer based on penalizing low entropy predictions, which the authors term \"confidence penalty.\" They show that label smoothing is equivalent to confidence penalty if the order of the KL divergence between uniform distributions and model's outputs is reversed. They also propose to use distributions other than uniform, resulting in unigram label smoothing (see Table 1) which is advantageous when the output labels' distribution is not balanced. Label smoothing also relates to DisturbLabel [18], which can be seen as label dropout, whereas label smoothing is the marginalized version of label dropout. \n\nCalibration of modern neural networks [15] has been widely investigated for image classification, but calibration of sequence models has been investigated only recently. Ott et al. [19] investigate the sequence level calibration of machine translation models and conclude they are remarkably well calibrated. Kumar and Sarawagi [20] investigate calibration of next-token prediction in language translation. They find that calibration of state-of-the-art models can be improved by a parametric model, resulting in a small increase in BLEU score. However, neither work invesigates the relation between label smoothing during training and calibration. For speech recognition, Chorowski and Jaitly [10] investigate the effect of softmax temperature and label smoothing on decoding accuracy. The authors conclude that both temperature scaling and label smoothing improve word error rates after beam-search (label smoothing performs best), but the relation between calibration and label smoothing/temperature scaling is not described. \n\nAlthough we are unaware of any previous work that shows the adverse effect of label smoothing upon distillation, Kornblith et al. [21] previously demonstrated that label smoothing impairs the accuracy of transfer learning, which similarly depends on the presence of non-class-relevant information in the final layers of the network. Additionally, Chelombiev et al. [22] propose an improved mutual information estimator based on binning and show correlation between compression of softmax layer representations and generalization, which may explain why networks trained with label smoothing generalize so well.",
            "score": 0.3948339028929539,
            "section_title": "Related work",
            "char_start_offset": 21423,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 731
                },
                {
                    "start": 734,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1762
                },
                {
                    "start": 1765,
                    "end": 2097
                },
                {
                    "start": 2098,
                    "end": 2374
                }
            ],
            "ref_mentions": [
                {
                    "start": 620,
                    "end": 624,
                    "matchedPaperCorpusId": "10842458"
                },
                {
                    "start": 772,
                    "end": 776,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 915,
                    "end": 919,
                    "matchedPaperCorpusId": "4375156"
                },
                {
                    "start": 1428,
                    "end": 1432,
                    "matchedPaperCorpusId": "9854876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56982421875
        },
        {
            "corpus_id": "247793145",
            "title": "WAVPROMPT: Towards Few-Shot Spoken Language Understanding with Frozen Language Models",
            "text": "During the evaluation, we find that the performances of our models are not stable. We therefore implement the calibration technique reported by [6] to reduce the bias introduced by the language models. We empirically find the calibration brings improvement to the classification accuracy in most cases.",
            "score": 0.3946961894953656,
            "section_title": "Calibration",
            "char_start_offset": 6798,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 302
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 147,
                    "matchedPaperCorpusId": "231979430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72314453125
        },
        {
            "corpus_id": "273228940",
            "title": "Calibrating Verbalized Probabilities for Large Language Models",
            "text": "Calibration with accessible probabilities. Guo et al. (2017) and Mukhoti et al. (2020) showed that the miscalibration of large modern neural networks (NNs) is related to overfitting on the likelihood of the training dataset. To tackle this, many methods have been proposed such as focal loss (Lin et al., 2017), which acts as a maximum entropy regularizer (Mukhoti et al., 2020). Label smoothing (M\u00fcller et al., 2019), Bayesian approach (Maddox et al., 2019), meta-learning (Bohdal et al., 2021), the Gumbel-softmax Trick (Jang et al., 2017;Wang et al., 2021;Pei et al., 2022), and kernelbased methods (Kumar et al., 2018). The recent survey summarizes these approaches (Wang, 2023). \n\nCalibration with inaccessible probabilities. For closed-source LLMs, the aforementioned methods are not applicable. To address this limitation, LLMs are prompted to generate verbalized confidence over their outputs (Lin et al., 2022). This technique was introduced by Tian et al. (2023) to infer probabilities in Question-Answering (QA) tasks, demonstrating better calibration than conditional probabilities. Rivera et al. (2024) integrated this approach with sampling methods for uncertainty quantification. Xiong et al. (2024) further extended it by combining it with sampling strategies to generate multiple responses and employing an aggregation approach for response consistency. \n\nAnother line of work from Ulmer et al. (2024) focuses on training an auxiliary model to calibrate verbalized probabilities. Upon obtaining confidence scores from generation, the authors employ Platt scaling (Platt, 1999;Guo et al., 2017) to adjust these scores. Conceptually different to those work, we focus on prompting LLMs to generate categorical distribution and employing temperature scaling on estimated logits for post-hoc calibration.",
            "score": 0.3946961894953656,
            "section_title": "Related Work",
            "char_start_offset": 19064,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 43,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 683
                },
                {
                    "start": 686,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1370
                },
                {
                    "start": 1373,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1816
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 60,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 65,
                    "end": 86,
                    "matchedPaperCorpusId": "211252346"
                },
                {
                    "start": 292,
                    "end": 310,
                    "matchedPaperCorpusId": "47252984"
                },
                {
                    "start": 356,
                    "end": 378,
                    "matchedPaperCorpusId": "211252346"
                },
                {
                    "start": 396,
                    "end": 417,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 437,
                    "end": 458,
                    "matchedPaperCorpusId": "59608823"
                },
                {
                    "start": 559,
                    "end": 576,
                    "matchedPaperCorpusId": "245501917"
                },
                {
                    "start": 602,
                    "end": 622,
                    "matchedPaperCorpusId": "49314079"
                },
                {
                    "start": 954,
                    "end": 972,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 1580,
                    "end": 1593,
                    "matchedPaperCorpusId": "56563878"
                },
                {
                    "start": 1593,
                    "end": 1610,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82080078125
        },
        {
            "corpus_id": "216867328",
            "title": "Posterior Calibrated Training on Sentence Classification Tasks",
            "text": "Our work is primarily motivated by previous analyses of posterior calibration on modern neural networks. Guo et al. (2017) pointed out that in some cases, as the classification performance of neural networks improves, its posterior output becomes poorly calibrated. There are a few attempts to investigate the effect of posterior calibration on natural language processing (NLP) tasks: Nguyen and O'Connor (2015) empirically tested how classifiers on NLP tasks (e.g., sequence tagging) are calibrated. For instance, compared to the Naive Bayes classifier, logistic regression outputs wellcalibrated posteriors in sentiment classification task. Card and Smith (2018) also mentioned the importance of calibration when generating a training corpus for NLP tasks. \n\nAs noted above, numerous post-processing calibration techniques have been developed: traditional binning methods (Zadrozny andElkan, 2001, 2002) set up bins based on the predicted posterior p, recalculate calibrated posteriors q per each bin on a validation set, and then update every p with q if p falls into the certain bin. On the other hand, scaling methods (Platt et al., 1999;Guo et al., 2017;Kull et al., 2019) re-scale the predicted posterior p from the softmax layer trained on a validation set. Recently, Kumar et al. (2019) pointed out that such re-scaling methods do not actually produce well-calibrated probabilities as reported since the true posterior probability distribution can not be captured with the often low number of samples in the validation set2 . To address the issue, the authors proposed a scaling-binning calibrator, but still rely on the validation set. \n\nIn a broad sense, our end-to-end training with the calibration reduction loss can be seen as sort of regularization designed to mitigate over-fitting. Just as classical explicit regularization techniques such as the lasso (Tibshirani, 1996) penalize models large weights, here we penalize models with posterior outputs that differ substantially from the estimated true posterior.",
            "score": 0.39385257725848993,
            "section_title": "Related Work",
            "char_start_offset": 2448,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1646
                },
                {
                    "start": 1649,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 2028
                }
            ],
            "ref_mentions": [
                {
                    "start": 105,
                    "end": 122,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 386,
                    "end": 412,
                    "matchedPaperCorpusId": "2879445"
                },
                {
                    "start": 644,
                    "end": 665,
                    "matchedPaperCorpusId": "13742568"
                },
                {
                    "start": 1124,
                    "end": 1144,
                    "matchedPaperCorpusId": "56563878"
                },
                {
                    "start": 1144,
                    "end": 1161,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1161,
                    "end": 1179,
                    "matchedPaperCorpusId": "202773833"
                },
                {
                    "start": 1277,
                    "end": 1296,
                    "matchedPaperCorpusId": "202718866"
                },
                {
                    "start": 1871,
                    "end": 1889,
                    "matchedPaperCorpusId": "16162039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69873046875
        },
        {
            "corpus_id": "236447335",
            "title": "Energy-Based Open-World Uncertainty Modeling for Confidence Calibration",
            "text": "Confidence Calibration With the emergence of deep learning technologies and their wide successes, concerns over whether they are reliable to be deployed in practice have also arisen. This is because researchers have found that contemporary deep neural networks (DNNs) often produce overconfident predictions [14], even on input images that are totally unrecognizable to humans [32]. Many approaches for improving confidence calibration have been developed. A widely used method is temperature scaling [14,27,19], which inserts a scaling parameter to the softmax formulation (called 'temperature') and adjusts it in a validation set with a goal to 'soften' the softmax probabilities. Regularization methods, such as label smoothing [36] and Mixup [37], have also been demonstrated effective in improving calibration. In particular, label smoothing modifies the ground-truth labels by fusing them with a uniform distribution, essentially forcing neural networks to produce 'more flattened' probabilities; whereas Mixup is a data augmentation method that randomly mixes two instances at both the image and label space, with a byproduct effect of improving calibration. Bayesian methods have also been explored for calibration. For instance, Monte Carlo Dropout [7] applies dropout in both training and testing; Deep Ensembles [23] uses as prediction the output averaged over an ensemble of models. Adding adversarial perturba-tions to the input has been found effective in smoothing the output probabilities [23,27]. In [26], a GAN model [11] is trained to generate out-of-distribution (OOD) data and the classifier is encouraged to produce low-confidence probabilities on these data. Such an idea has also been investigated in [16] where adversarial perturbations are utilized to synthesize OOD data. In [12], a Joint Energy-based Model (JEM) is proposed to improve calibration by learning the joint distribution based on energy functions [25]. A recent work [38] suggests that calibrating confidence across multiple domains is beneficial to OOD generalization [42]. Studies on why neural networks produce overconfident predictions have also been covered in the literature. In [",
            "score": 0.39358441478656925,
            "section_title": "Related Works",
            "char_start_offset": 4909,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 308,
                    "end": 312,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 501,
                    "end": 505,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 505,
                    "end": 508,
                    "matchedPaperCorpusId": "3526391"
                },
                {
                    "start": 508,
                    "end": 511,
                    "matchedPaperCorpusId": "211506988"
                },
                {
                    "start": 731,
                    "end": 735,
                    "matchedPaperCorpusId": "206593880"
                },
                {
                    "start": 1258,
                    "end": 1261,
                    "matchedPaperCorpusId": "160705"
                },
                {
                    "start": 1323,
                    "end": 1327,
                    "matchedPaperCorpusId": "6294674"
                },
                {
                    "start": 1505,
                    "end": 1509,
                    "matchedPaperCorpusId": "6294674"
                },
                {
                    "start": 1509,
                    "end": 1512,
                    "matchedPaperCorpusId": "3526391"
                },
                {
                    "start": 1517,
                    "end": 1521,
                    "matchedPaperCorpusId": "3464416"
                },
                {
                    "start": 1535,
                    "end": 1539,
                    "matchedPaperCorpusId": "1033682"
                },
                {
                    "start": 1725,
                    "end": 1729,
                    "matchedPaperCorpusId": "55700923"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.407470703125
        },
        {
            "corpus_id": "273228940",
            "title": "Calibrating Verbalized Probabilities for Large Language Models",
            "text": "Calibrating verbalized probabilities presents a novel approach for reliably assessing and leveraging outputs from black-box Large Language Models (LLMs). Recent methods have demonstrated improved calibration by applying techniques like Platt scaling or temperature scaling to the confidence scores generated by LLMs. In this paper, we explore the calibration of verbalized probability distributions for discriminative tasks. First, we investigate the capability of LLMs to generate probability distributions over categorical labels. We theoretically and empirically identify the issue of re-softmax arising from the scaling of verbalized probabilities, and propose using the invert softmax trick to approximate the\"logit\"by inverting verbalized probabilities. Through extensive evaluation on three public datasets, we demonstrate: (1) the robust capability of LLMs in generating class distributions, and (2) the effectiveness of the invert softmax trick in estimating logits, which, in turn, facilitates post-calibration adjustments.",
            "score": 0.3934583508282584,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.923828125
        },
        {
            "corpus_id": "275570800",
            "title": "Rational Tuning of LLM Cascades via Probabilistic Modeling",
            "text": "Forwarding Queries: the models in our cascades decide whether to forward queries by thresholding the calibrated confidence \u03d5 = \u03a6(p raw ), where p raw is the raw confidence signal. We obtain p raw from the modelintrinsic next-token probabilities. On multiple-choice tasks, we take the maximum probability among the answer choices (Hendrycks and Gimpel, 2018;Plaut et al., 2024). In the natural language generation case, we first generate the answer, then send a follow up verification prompt to the model asking \"Is the proposed answer <answer> true? Answer only Y or N.\" We use the probability of the Y token as the confidence signal p raw . Our prompt templates are available in Appendix C. \n\nSince we focus on providing techniques compatible with black-box LLM inference via third-party APIs, we leave consideration of hidden layer-based confidence signals to future work. In addition, we do not consider resampling methods such as semantic entropy (Farquhar et al., 2024). Such methods are compatible with black-box inference, but in the context of LLM cascades, their computational overhead appears prohibitive. For example, at the time of writing inference of Llama3.1 405B typically costs 15 times more than inference of Llama3.1 8B. In this case, it is likely preferable to directly run the 405B model once rather than forward a query based on k \u2248 15 resamples of the 8B model. See Appendix D for a table listing small-large model pairings from Meta, Anthropic, and OpenAI, along with their price differentials. \n\nConfidence Calibration: raw token probabilities of instruction-tuned LLMs are typically poorly calibrated (Ouyang et al., 2022;Brown et al., 2020;OpenAI, 2024;Plaut et al., 2024). However, calibration is important for accurate error prediction. To obtain calibrated confidence scores, we use logistic regression. We favor this approach over temperature scaling since it yields p values and other statistical metrics that are useful for diagnosing calibration issues, especially in a low-data scenario.",
            "score": 0.3933325100681947,
            "section_title": "Methodology",
            "char_start_offset": 17468,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 691
                },
                {
                    "start": 694,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1518
                },
                {
                    "start": 1521,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 2022
                }
            ],
            "ref_mentions": [
                {
                    "start": 951,
                    "end": 974,
                    "matchedPaperCorpusId": "270615909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.484375
        },
        {
            "corpus_id": "260063200",
            "title": "Strength in Numbers: Estimating Confidence of Large Language Models by Prompt Agreement",
            "text": "Does measuring confidence across multiple prompts yield better calibrated confidence scores? A common approach to measuring calibration is expected calibration error (ECE) (Guo et al., 2017), which buckets the prediction probabilities and measures the empirical accuracy of each bucket with its average estimated probability (confidence). 4 The discrepancy between these terms is the calibration gap; lower gaps indicate better calibration. ECE ranges from 0 (perfect calibration) to 1 (lowest calibration). We utilize this method to compare log probabilities obtained from a single prompt to those from multiple prompts. For each dataset, we use 10 evenly-spaced bins and set the min and max of the bins according to the minimum and maximum average log probability in the dataset. \n\nWe measure agreement across prompts using Rand index, which does not give normalized scores that can be interpreted as probabilities. We could convert these scores into probability confidence scores in two ways. 1) Measure the empirical accuracy of different ranges of Rand index on a held-out validation set, then assign confidence scores based on those accuracies. The drawback to this approach is it requires a separate held-out set for calibration, which may be an unrealistic assumption, especially in few-shot settings. 2) Normalize the empirical Rand index scores to form a probability distribution. We experimented with this approach but found that how we bucketed and normalized the scores heavily influenced ECE results, which produced an unfair evaluation setting. \n\nInstead, we view Rand index scores as a relative confidence score between instances, where a higher score means \"more confident.\" We propose an evaluation metric that considers relative confidence of answers between instances. We rank instances in a dataset according to their confidence scores (log probability or Rand index), with the highest scoring instance (e.g., largest log probability or Rand index) at the top of the list. We evaluate each confidence estimation method on how well it ranks correct predictions higher than incorrect ones. \n\nMost evaluation metrics for ranking are geared towards an information retrieval setting where the number of items in the list can vary, different items can be included by each model, only a few items are \"relevant\", or we have close to a total ordering over the ranked items.",
            "score": 0.3930701840530625,
            "section_title": "Evaluation",
            "char_start_offset": 21698,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 781
                },
                {
                    "start": 784,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2108
                },
                {
                    "start": 2111,
                    "end": 2386
                }
            ],
            "ref_mentions": [
                {
                    "start": 172,
                    "end": 190,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79541015625
        },
        {
            "corpus_id": "252564547",
            "title": "Revisiting Softmax for Uncertainty Approximation in Text Classification",
            "text": "In this paper, we perform an in-depth empirical comparison of using the MC dropout method in an efficient setting and the more straightforward softmax method. By doing a thorough empirical analysis of the two methods, shown in Section 4.3.2, using various metrics to measure their performance on both efficiency and performance levels, we see that in our holdout experiments in table 3, that the two methods perform approximately the same. \n\nLooking at the expected calibration error (ECE) experiments, the results again show that the MC dropout and softmax method perform somewhat equally, which we have shown in Section 4.6. We observe differences in the results as we observe a lower accuracy score, which we show in our noise experiment, which is also shown in Section 4.6. \n\nPrior research (Hendrycks and Gimpel, 2017) investigated out-of-distribution analysis and found that softmax, both for sentiment classification and text categorisation tasks, can detect out-ofdistribution data points efficiently. It further showcases that in these two tasks, that the softmax also to some extent, can perform well as a confidence estimator. \n\nWhile we show that the two methods perform almost equally, when comparing the predictive performance, the cost of using MC dropout is at a minimum 10 times that of running softmax even in the efficient setting where only the final layer is dropped out, depending on the post-processing of the uncertainties, as we show in Section 4.4. The post-processing cost of MC dropout can quickly explode when used on larger datasets or if a more expensive method like dropout-entropy is used instead of simpler approaches. \n\nGiven this, when could it be appropriate to use the more efficient softmax over MC dropout for estimating predictive uncertainty? Our results suggest that when the base accuracy of a model is high, the differences in uncertainty estimation between the two methods is relatively low, likely due to the higher confidence of the softmax method. In this case, if latency or resource efficiency is a concern such as on edge devices, it may be appropriate to rely on a quick estimate using softmax as opposed to a more cumbersome method.",
            "score": 0.3930647151080821,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 26412,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 439
                },
                {
                    "start": 442,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 777
                },
                {
                    "start": 780,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1137
                },
                {
                    "start": 1140,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1652
                },
                {
                    "start": 1655,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2186
                }
            ],
            "ref_mentions": [
                {
                    "start": 795,
                    "end": 822,
                    "matchedPaperCorpusId": "13046179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58447265625
        },
        {
            "corpus_id": "277596537",
            "title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
            "text": "and industry applications (Zhu et al., 2025), the ability to accurately express confidence or uncertainty in their outputs is crucial (Su et al., 2024), particularly in high-risk scenarios (Thirunavukarasu et al., 2023;Li et al., 2024c). If self-improvement methods introduce self-bias in confidence estimation, this could pose a significant threat to LLM safety and reliability, creating substantial challenges in the pursuit of trustworthy AI (Sun et al., 2024;Huang et al., 2025). To investigate this, we examine three types of self-improvement methods in our experiments: Basic prompting, Chain-of-Thought (CoT) prompting, and Tuningbased approaches (First et al., 2023;Han et al., 2024;Zhang et al., 2024b;Aky \u00fcrek et al., 2023;Xie et al., 2025). We implement each method and analyze its impact on LLMs' confidence estimation performance. Our results reveal a clear trend of increasing overconfidence as self-improvement iterations progress, leading to a continuously rising Expected Calibration Error (ECE) score (Guo et al., 2017). \n\nAs calibration Guo et al. (2017); Geng et al. (2023); Xie et al. (2024b) serves as an effective technique to align a model's confidence with its correctness and thus improve models confidence estimation, we pose our second research question: How to combine calibration with the self-improvement method to mitigate the confidence estimation bias? To explore this, we examine the compounded effects of calibration and self-improvement. Specifically, we evaluate three experimental settings to analyze their interaction: (1) multiple self-improvement iterations followed by calibration, (2) calibration applied before multiple self-improvement iterations, and (3) iterative calibration and self-improvement at each step. Our results indicate that applying calibration before self-improvement leads to sustained improvements over time. Meanwhile, self-improvement then calibration achieves the best ECE score, resulting in better-calibrated confidence estimates.",
            "score": 0.39294335886687215,
            "section_title": "Introduction",
            "char_start_offset": 1571,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1038
                },
                {
                    "start": 1041,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 1999
                }
            ],
            "ref_mentions": [
                {
                    "start": 134,
                    "end": 151,
                    "matchedPaperCorpusId": "268230867"
                },
                {
                    "start": 189,
                    "end": 219,
                    "matchedPaperCorpusId": "259947046"
                },
                {
                    "start": 219,
                    "end": 236,
                    "matchedPaperCorpusId": "269626089"
                },
                {
                    "start": 654,
                    "end": 674,
                    "matchedPaperCorpusId": "257427444"
                },
                {
                    "start": 674,
                    "end": 691,
                    "matchedPaperCorpusId": "266999677"
                },
                {
                    "start": 711,
                    "end": 733,
                    "matchedPaperCorpusId": "258685337"
                },
                {
                    "start": 1019,
                    "end": 1037,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1056,
                    "end": 1073,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8466796875
        },
        {
            "corpus_id": "273963769",
            "title": "Epistemic Integrity in Large Language Models",
            "text": "In this work, we introduced the problem of epistemic calibration for LLMs: ensuring that the confidence expressed in a model's communication aligns with its underlying reliability. We argued that this normative ideal is critical for LLMs to serve as robust and responsible information sources. Through a decomposition of the problem into external and internal certainty, we developed a framework for understanding and evaluating epistemic calibration in LLMs. Using our new approach that greatly improves fidelity of assertiveness measurements compared to prior models, our empirical investigation of a state-of-the-art modeleveals significant gaps between the model's internal confidence estimates and the assertiveness of its generated language. This miscalibration poses risks to users, who may be misled by overconfident model outputs. \n\nOur work also highlights the need for further research to fully understand and address the challenges of epistemic calibration. One key direction is developing new training and inference techniques to improve the alignment between LLMs' probability estimates and their linguistic expression of confidence. Another is studying the downstream impacts of epistemic miscalibration on user trust, decision making, and information ecosystems, through a combination of user studies and large-scale simulations. We believe that the epistemic calibration framework introduced in this paper provides a valuable foundation for these future efforts. We discuss further applications, to RLHF, silicon sampling, and debate, in Appendix I. Ultimately, achieving epistemic calibration in language models is not just a technical challenge, but a societal imperative. As these models become ever more integrated into our information-seeking and decision-making practices, ensuring that they express confidence in a calibrated and responsible way is essential for mitigating the risks of misinformation, confusion, and unwarranted trust. Figure 4: Our method's predicted assertiveness score is reasonably well aligned with the human scores, while the model's (internal) certainty is not.",
            "score": 0.39269356603445443,
            "section_title": "DISCUSSION AND CONCLUSION",
            "char_start_offset": 27192,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 839
                },
                {
                    "start": 842,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2110
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78173828125
        },
        {
            "corpus_id": "273403321",
            "title": "Atomic Calibration of LLMs in Long-Form Generations",
            "text": "Large language models (LLMs) often suffer from hallucinations, posing significant challenges for real-world applications. Confidence calibration, which estimates the underlying uncertainty of model predictions, is essential to enhance the LLMs' trustworthiness. Existing research on LLM calibration has primarily focused on short-form tasks, providing a single confidence score at the response level (macro calibration). However, this approach is insufficient for long-form generations, where responses often contain more complex statements and may include both accurate and inaccurate information. Therefore, we introduce atomic calibration, a novel approach that evaluates factuality calibration at a fine-grained level by breaking down long responses into atomic claims. We classify confidence elicitation methods into discriminative and generative types and demonstrate that their combination can enhance calibration. Our extensive experiments on various LLMs and datasets show that atomic calibration is well-suited for long-form generation and can also improve macro calibration results. Additionally, atomic calibration reveals insightful patterns in LLM confidence throughout the generation process.",
            "score": 0.39258058258641904,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87939453125
        },
        {
            "corpus_id": "278129791",
            "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review",
            "text": "In this section, we summarize research directions that can impact LLMs' confidence and are necessary to investigate further. We outline avenues as follows: Novel method, metrics, or loss components. UQ methods can be used with LLMs to abstain from answer generation for questions or prompts of incorrect domain [101]. Token-    wise uncertainty quantification may not be efficient for longer sequences, as they only provide the likelihood of the next token without contextual insight [83]. Entropy rates, which are obtained by averaging over the sequence, can be used to calibrate disagreements of long-term generations of LLMs [24]. Hence, preserving the semantic aspect of a sequence, calibration of longer sequences, and sequence-level confidence elicitation is an open research question for the future. Optimizing models with their uncertainty and accuracy can provide better-calibrated and improved confidence [63]. Such loss metrics can also be combined with TS to be robust for distributional shifts [15], [63]. Following this venue, proposing an uncertainty-aware loss that can be used to fine-tune large or pre-train models is important. One example can be using the Kolmogorov-Smirnov (KS) similarity for recalibration [214]. An uncertaintyaware loss function can also provide consistency of uncertainty estimation [13]. Focal loss can be adapted for fine-tuning of models, which can make LLM better calibrated and detect OOD samples accurately compared to TS [215]. Manifold smoothing can regularize LLMs to be more calibrated toward OOD samples [196]. Open question: Is uncertainty toward OOD samples the same as epistemic uncertainty [44]? \n\nAdvanced reasoning to calibrate user input ambiguity. AI agents can converse for improved confidence and a better approach to uncertainty. Recent work [11] found that the model could learn from self-criticism measured by confidence scores from the auxiliary model or from verbally expressed uncertainty to improve the ECE of LLMs. Meanwhile, input ambiguity provided by the user is another form of UQ, which is an open challenge [119]. Inputs in the form of short or obvious questions might be impolite for users, and calibration should consider that aspect [162].",
            "score": 0.3918689431982588,
            "section_title": "B. Future Research Directions",
            "char_start_offset": 52246,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1652
                },
                {
                    "start": 1655,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2219
                }
            ],
            "ref_mentions": [
                {
                    "start": 311,
                    "end": 316,
                    "matchedPaperCorpusId": "250526382"
                },
                {
                    "start": 628,
                    "end": 632,
                    "matchedPaperCorpusId": "189762078"
                },
                {
                    "start": 915,
                    "end": 919,
                    "matchedPaperCorpusId": "227275177"
                },
                {
                    "start": 1007,
                    "end": 1011,
                    "matchedPaperCorpusId": "258960177"
                },
                {
                    "start": 1013,
                    "end": 1017,
                    "matchedPaperCorpusId": "227275177"
                },
                {
                    "start": 1470,
                    "end": 1475,
                    "matchedPaperCorpusId": "211252346"
                },
                {
                    "start": 1647,
                    "end": 1651,
                    "matchedPaperCorpusId": "255287312"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5927734375
        },
        {
            "corpus_id": "85501179",
            "title": "Attended Temperature Scaling: A Practical Approach for Calibrating Deep Neural Networks",
            "text": "In Table 1, the calibration result of TS family methods which have only one parameter for fine-tuning the softmax output layer is compared with Matrix and Vector scaling which apply a linear function on logit layer. TS family methods achieves better calibration results as well as preserving the accuracy rate of the network. It seems, however, Matrix and Vector Scaling can define more complex functions to soften the softmax layer, they suffer from over-fitting to the validation set in both accuracy and confidence. ATS in most cases calibrates the network better than all the others. Especially in the case of highly  accurate networks such as VGG16-CIFAR10, ResNet110-SVHN, ResNet110-CIFAR10 and LeNet5-MNIST the calibration error improvement is more than the moderate accurate networks. This can be explained by the lack of misclassified samples in the validation set which prevents TS from converging to the local optima. Different T values which are found for TS and ATS methods with the selected threshold \u03b8 are provided in Table 2. It shows even a small change in the value of T can improve NLL significantly. \n\nIn Table 3 we provide the calibration result of two post-processing probabilistic approaches. To compare with other measure-based approaches, we also report the results of the same model-datasets in the last three lines of Table 1. For investigating the impact of dropout in making the model calibrated, we have trained LeNet5-MNIST, VGG16-CIFAR10 and VGG16-CIFAR100 model-datasets with and without dropout. The only model that uses dropout during the training is MC-Dropout. It shows MC-Dropout and ensemble can improve the accuracy and calibration of the model simultaneously and make the model more calibrated than other measure-based approaches. However, they increase time and memory complexity and they cannot be used to calibrate an already pretrained models.",
            "score": 0.3916330757044577,
            "section_title": "Calibration",
            "char_start_offset": 20705,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1119
                },
                {
                    "start": 1122,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1888
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1849365234375
        },
        {
            "corpus_id": "5178123",
            "title": "Confidence estimation for translation prediction",
            "text": "Future work will include the search for more relevant confidence features, such as features based on consenus over word-lattices ( (Mangu et al., 2000)), past performance, the use of more appropriate correct/false tagging methods and experiments with different machine learning techniques. Finally, we would like to investigate whether confidence estimation can be used to improve the model prediction accuray, either by using re-scoring techniques or using the confidence estimates during search (decoding).",
            "score": 0.3914622801581408,
            "section_title": "Conclusion",
            "char_start_offset": 24810,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 508
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 151,
                    "matchedPaperCorpusId": "6135726"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.517578125
        },
        {
            "corpus_id": "260886981",
            "title": "Distance Matters For Improving Performance Estimation Under Covariate Shift",
            "text": "Machine learning models are sensitive to variations in their deployment environments [19,63,40,26,57,38]. Due to the unavailability of ground truth labels for continuous performance monitoring at deployment time, real-time and accurate performance estimation is crucial to detect any unexpected behavior or model failure, particularly in distribution-shifted settings. This is especially important for sensitive use cases such as clinical decision making. \n\nThe difficulty in estimating model performance arises from the lack of reliability of model outputs under covariate shift [40,28]. Recently, several attempts have been made at addressing this problem, many of them based on confidence estimates [16,15,32]. For example, Average Thresholded Confidence (ATC) [15] leverages softmax outputs for esti-Figure 1. Performance estimation under covariate shift needs to take into account different sources of errors. Distance to the source distribution in the embedding space matters as confidence estimates become unreliable with increased distance. mating classification accuracy, considering that all outputs whose confidence do not reach a certain threshold are incorrectly classified. While this method has shown to be effective at estimating the performance under mild shifts (e.g. on synthetically corrupted images), experiments show that the method under-performs in more substantial shifts such as natural sub-population shifts. In particular, current approaches tend to overestimate accuracy in natural realworld distribution shift settings [15,23]. This can notably be explained by a deterioration of model calibration when going further from the training distribution [25], with softmax outputs becoming over-confident and unreliable [40]. \n\nIf test samples are too far from training samples, relying on the output of the classification layer for performance estimation is insufficient. From an uncertainty point of view, softmax outputs can been seen as capturing aleatoric uncertainty, arising from overlapping class boundaries [24,9]. However, under dataset shifts, errors may also arise from the fact that the model has never seen this type of input data and does not know how to respond to such inputs.",
            "score": 0.39139573020764107,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 455
                },
                {
                    "start": 458,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1749
                },
                {
                    "start": 1752,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2047
                },
                {
                    "start": 2048,
                    "end": 2217
                }
            ],
            "ref_mentions": [
                {
                    "start": 85,
                    "end": 89,
                    "matchedPaperCorpusId": "220250257"
                },
                {
                    "start": 98,
                    "end": 101,
                    "matchedPaperCorpusId": "173188134"
                },
                {
                    "start": 101,
                    "end": 104,
                    "matchedPaperCorpusId": "235795331"
                },
                {
                    "start": 702,
                    "end": 706,
                    "matchedPaperCorpusId": "235755343"
                },
                {
                    "start": 706,
                    "end": 709,
                    "matchedPaperCorpusId": "245877810"
                },
                {
                    "start": 709,
                    "end": 712,
                    "matchedPaperCorpusId": "250699231"
                },
                {
                    "start": 764,
                    "end": 768,
                    "matchedPaperCorpusId": "245877810"
                },
                {
                    "start": 1549,
                    "end": 1553,
                    "matchedPaperCorpusId": "245877810"
                },
                {
                    "start": 1553,
                    "end": 1556,
                    "matchedPaperCorpusId": "235652267"
                },
                {
                    "start": 1678,
                    "end": 1682,
                    "matchedPaperCorpusId": "246607903"
                },
                {
                    "start": 2044,
                    "end": 2046,
                    "matchedPaperCorpusId": "255287312"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17822265625
        },
        {
            "corpus_id": "235719735",
            "title": "The Art of Abstention: Selective Prediction and Error Regularization for Natural Language Processing",
            "text": "We compare selective prediction performance of different models in Table 1. For each model, we report the performance given by the two confidence estimators, softmax response (SR) and MC-dropout (MC); the results of using PD for confidence estimation are very similar to those of SR, and we report them in Appendix B due to space limitations. The accuracy and the F1 score 3 measure the effectiveness of the classifier f , RPP measures the reliability of the confidence estimatorg, and AUC is a comprehensive metric for both the classifier and the confidence estimator. The choice of confidence estimator does not affect the model's accuracy. We also provide risk-coverage curves (RCCs) of different models and confidence estima-   tors in Figure 2. MC in the table and the figure uses a dropout rate of 0.01 and repetitive runs R = 10. We first notice that models with overall higher accuracy also have better selective prediction performance (lower AUC and RPP). For example, compared with LSTM, BERT-base has higher accuracy and lower AUC/RPP on all datasets, and the same applies to the comparison between BERT-base and BERT-large. Since the classifier's effectiveness does not directly affect RPP, the consistency of RPP's and accuracy's improvement indicates that sophisticated models simultaneously improve both model accuracy and confidence estimation. This is in contrast to the discovery by Guo et al. (2017) that sophisticated neural networks, despite having better accuracy, are more easily overconfident and worse calibrated than simple ones.\n\nWe also notice that MC-dropout performs consistently worse than softmax response, shown by both AUC and RPP. This shows that for NLP tasks and models, model confidence estimated by MCdropout fails to align well with real example difficulty. We further study and visualize in Figure 3 the effect of different dropout rates and different numbers of repetitive runs R on MC-dropout's selective prediction performance. We can see that (1) a dropout rate of 0.01 is a favorable choice: larger dropout rates lead to worse performance while smaller ones do not improve it; (2) MC-dropout needs at least 20 repetitions to obtain results comparable to SR, which is extremely expensive.",
            "score": 0.391174308809954,
            "section_title": "Comparing Different Models",
            "char_start_offset": 14340,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1401,
                    "end": 1418,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7490234375
        },
        {
            "corpus_id": "267938779",
            "title": "Interpreting Predictive Probabilities: Model Confidence or Human Label Variation?",
            "text": "From a statistical perspective, most NLP systems are trained on single annotations using regularised maximum likelihood estimation (MLE), without mechanism or incentive to represent uncertainty about their own correctness (MLE recovers a single realisation of the map x \u2192 f (\u2022; x)). This is unlike, for instance, Bayesian estimation (where the map x \u2192 f (\u2022; x) is given random treatment; more in Section 4). \n\nIn addition, regardless of whether models represent uncertainty about their own correctness, calibration metrics, and ECE in particular, are known to have limitations, e.g., problems with binning (Nixon et al., 2019;Vaicenavicius et al., 2019;Gupta et al., 2021), evaluating only the mode probability rather than the entire distribution (Kumar et al., 2019;Vaicenavicius et al., 2019;Widmann et al., 2019;Kull et al., 2019), and being minimised by global label frequencies (Nixon et al., 2019). Moreover, Baan et al. (2022) recently demonstrate that ECE disregards plausible instance-level label variation and pose that such calibration metrics are ill-suited for tasks with human label variation. \n\nFinally, the sense of trustworthiness from verifying that Equation (1) holds (for a given confidence level q) in a given dataset, might not transfer to any one future prediction in isolation. Though some studies examine the effect of communicating predictive probability to human decision makers (Zhang et al., 2020;Wang and Yin, 2021;Vodrahalli et al., 2022;Vasconcelos et al., 2023;Dhuliawala et al., 2023), to the best of our knowledge, none verified the user-impact of models with various calibration scores, raising the question: can calibration metrics like ECE discriminate systems perceived as more trustworthy?",
            "score": 0.39095517209201064,
            "section_title": "P1: Uncertainty about Model Error",
            "char_start_offset": 9124,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 407
                },
                {
                    "start": 410,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1729
                }
            ],
            "ref_mentions": [
                {
                    "start": 606,
                    "end": 626,
                    "matchedPaperCorpusId": "102486060"
                },
                {
                    "start": 626,
                    "end": 653,
                    "matchedPaperCorpusId": "67749814"
                },
                {
                    "start": 653,
                    "end": 672,
                    "matchedPaperCorpusId": "219981518"
                },
                {
                    "start": 747,
                    "end": 767,
                    "matchedPaperCorpusId": "202718866"
                },
                {
                    "start": 767,
                    "end": 794,
                    "matchedPaperCorpusId": "67749814"
                },
                {
                    "start": 815,
                    "end": 833,
                    "matchedPaperCorpusId": "202773833"
                },
                {
                    "start": 883,
                    "end": 903,
                    "matchedPaperCorpusId": "102486060"
                },
                {
                    "start": 915,
                    "end": 933,
                    "matchedPaperCorpusId": "253224378"
                },
                {
                    "start": 1406,
                    "end": 1426,
                    "matchedPaperCorpusId": "210023849"
                },
                {
                    "start": 1445,
                    "end": 1469,
                    "matchedPaperCorpusId": "233224125"
                },
                {
                    "start": 1494,
                    "end": 1518,
                    "matchedPaperCorpusId": "264405995"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11163330078125
        },
        {
            "corpus_id": "259095925",
            "title": "Uncertainty in Natural Language Processing: Sources, Quantification, and Applications",
            "text": "In this section, we review the applications of uncertainty in the output stage of NLP system. Here the main insight is that can we fully trust the output of a neural network? The answer is obviously no. The main reason is that its output relies on probability and thereby mistakes are inevitable. The question can be converted to how much can we trust the output? In other words, it means reliability or trustworthy extents. Since uncertainty is reflected as a numerical value, which naturally helps us to assess the quality of the model or output. By quantifying and analyzing uncertainty, we can gain valuable insights into the strengths and weaknesses of our NLP models, and better understand the risks and limitations of the decisions we make based on them. For instance, if an NLP model has high uncertainty in its predictions for certain data points or scenarios, we expect to investigate further or use additional information to improve the performance of the model. Uncertainty can be used to evaluate models' performance. Confidence scores can be compared to the actual correctness of decisions made by the model. This approach enables us to determine how reliable and trustworthy the model is in terms of making accurate decisions. In particular, some metrics given in Sec 3.3 can be exploited to evaluate the confidence calibration and uncertainty classification ability of the model, and then reflect the advantages and disadvantages of the model. Furthermore, assessing the reliability of generated content is crucial. Especially in the exponential search space of the generative model, the use of uncertainty for model and output analysis is valuable. Table 3 shows that in recent years, research on uncertainty estimation in MT and QA has gradually increased, and various evaluation indicators have increased to explain the model or output results. Typically, multiple candidate translations can be generated in MT, making it difficult to determine the best translation. Estimating the uncertainty of each candidate translation can help researchers choose the translation with the lowest uncertainty, leading to better translation quality [18,93,148]. Recently, as PLMs have become more powerful and widely used, it has become increasingly important to be able to explain how they arrive at answers and provide some degree of uncertainty or confidence in predictions. There are various techniques for evaluating the performance and credibility of PLMs, including self-evaluation techniques, calibration scores, and other methods [26,149,150,151].",
            "score": 0.3909037940335889,
            "section_title": "Reliability and Trustworthy Assessment",
            "char_start_offset": 73619,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2166
                },
                {
                    "start": 2167,
                    "end": 2382
                },
                {
                    "start": 2383,
                    "end": 2561
                }
            ],
            "ref_mentions": [
                {
                    "start": 2154,
                    "end": 2158,
                    "matchedPaperCorpusId": "254096143"
                },
                {
                    "start": 2158,
                    "end": 2161,
                    "matchedPaperCorpusId": "11492268"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.375244140625
        },
        {
            "corpus_id": "271962723",
            "title": "Explain then Rank: Scale Calibration of Neural Rankers Using Natural Language Explanations from LLMs",
            "text": "Based on prior studies that LLM confidence scores are well-calibrated (Kadavath et al., 2022;Tian et al., 2023), we use the LLM as a zero-shot binary classifier and take the average predictions over 20 samples as the confidence score. We then use Platt scaling (Platt, 2000) to map these scores to the target scale. \n\nFull calibration with NLEs (NLE). We first acquire meta NLEs for candidate query-document pairs and then fine-tune a BERT model3 using the NLEs as inputs with the calibrated softmax loss with the same restrictions as FC. This forms a direct comparison with FC, as they use the same initialization weights, labels and training objectives, but different inputs.",
            "score": 0.3908534054751986,
            "section_title": "Baselines",
            "char_start_offset": 20359,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 315
                },
                {
                    "start": 318,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 677
                }
            ],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 274,
                    "matchedPaperCorpusId": "56563878"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.669921875
        },
        {
            "corpus_id": "267770526",
            "title": "Calibrating Large Language Models with Sample Consistency",
            "text": "Large Language Models (LLMs) excel in various tasks, yet it is hard to know when they err. A first step towards making LLMs more trustworthy is for them to provide a confidence estimate with predictions (Papadopoulos et al., 2001). This estimate needs to be calibrated, meaning that the confidence level is aligned with the likelihood of the prediction being correct (Brier, 1950). A well-calibrated system can enable model developers to provide selective predictions, help users decide when to trust or distrust model responses, and potentially facilitate performance improvement through human intervention or self-refinement (Madaan et al., 2023;Shridhar et al., 2023). * Equal contribution; Qing Lyu did her work while interning at Allen Institute for AI.",
            "score": 0.3908534054751986,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 758
                }
            ],
            "ref_mentions": [
                {
                    "start": 648,
                    "end": 670,
                    "matchedPaperCorpusId": "262466051"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78271484375
        },
        {
            "corpus_id": "271218084",
            "title": "Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models",
            "text": "Calibration is essential for ensuring that large language models (LLMs) are honest and dependable.Studies by Kadavath et al. [12] and Plaut et al. [14] demonstrated that larger models tend to be better calibrated, particularly in multiple-choice contexts, and showed a strong correlation between softmax probabilities and the correctness of answers.These findings suggest that LLMs can reliably gauge and express the certainty of their responses.\n\nContrasting these results, our research reveals that the calibration of LLMs in multiple-choice scenarios is inherently fragile and can be easily manipulated to skew confidence levels.This highlights a significant vulnerability in using these models for assessments, indicating that reliance on simple calibration metrics might not adequately reflect an LLM's true capabilities.Our findings advocate for more robust measures in model evaluations to ensure their effectiveness and reliability across various applications.",
            "score": 0.3908534054751986,
            "section_title": "Calibration in LLMs",
            "char_start_offset": 22752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 98,
                    "end": 349
                },
                {
                    "start": 349,
                    "end": 446
                },
                {
                    "start": 448,
                    "end": 632
                },
                {
                    "start": 632,
                    "end": 826
                },
                {
                    "start": 826,
                    "end": 968
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71337890625
        },
        {
            "corpus_id": "275570800",
            "title": "Rational Tuning of LLM Cascades via Probabilistic Modeling",
            "text": "We favor this approach over temperature scaling since it yields p values and other statistical metrics that are useful for diagnosing calibration issues, especially in a low-data scenario. \n\nUnfortunately, the overconfidence of the raw token probabilities makes the distribution of raw confidence signals highly peaked. The raw token probabilities accumulate near 1.0, making tiny changes in confidence (for example, p raw = 0.98 vs p raw = 0.99) highly consequential. To enhance the calibration performance of logistic regression, as a pre-processing step we apply hyperparameter-free feature transformations that spread out the overconfident probabilities via asymptotes near p raw = 0.0 and p raw = 1.0. Following Zellinger and Thomson (2024), on multiple-choice tasks we use the transformation \n\nwhereas on natural language generation tasks, we use \n\nImportantly, these feature transformations do not require any hyperparameter tuning. \n\nUnfortunately, models sometimes return perfect certainty p raw = 1.0 or p raw = 0.0, making ( 16) and ( 17) blow up. To address this problem, we reassign all observations with infinite \u03be to the maximum of the finite values of \u03be. In other words, we define \n\nwhere D is the training set consisting of pairs (p raw , y), and y \u2208 {0, 1} indicates correctness of the model's answer. 2 We set all observations where \u03be = \u221e to \u03be max , and treat \u03be min analogously. \n\nBenchmarks: we evaluate our probabilistic model and the cost-error curves of LLM cascades on six language modeling benchmarks including MMLU (Hendrycks et al., 2021); MedMCQA (Pal et al., 2022); TriviaQA (Joshi et al., 2017); XSum (Narayan et al., 2018); GSM8K (Cobbe et al., 2021); and TruthfulQA (Lin et al., 2022b). These tasks include general-purpose knowledge and reasoning, domain-specific QA, open-domain QA, summarization, mathematical reasoning, and truthfulness in the face of adversarially chosen questions.",
            "score": 0.3908534054751986,
            "section_title": "Methodology",
            "char_start_offset": 19302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 191,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 797
                },
                {
                    "start": 800,
                    "end": 852
                },
                {
                    "start": 855,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1196
                },
                {
                    "start": 1199,
                    "end": 1397
                },
                {
                    "start": 1400,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1918
                }
            ],
            "ref_mentions": [
                {
                    "start": 1541,
                    "end": 1565,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 1575,
                    "end": 1593,
                    "matchedPaperCorpusId": "247763070"
                },
                {
                    "start": 1604,
                    "end": 1624,
                    "matchedPaperCorpusId": "26501419"
                },
                {
                    "start": 1631,
                    "end": 1653,
                    "matchedPaperCorpusId": "215768182"
                },
                {
                    "start": 1698,
                    "end": 1717,
                    "matchedPaperCorpusId": "237532606"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4658203125
        },
        {
            "corpus_id": "265212730",
            "title": "On the Calibration of Multilingual Question Answering LLMs",
            "text": "In this section, we perform ablations to answer the following research questions: \n\n\u2022 the relationship between linguistic and nonlinguistic features, and the transferability of calibration performance from English to other languages. \n\n\u2022 the effect of model size on calibration, and accuracy, for a particular model family.9 \n\n6.1 Investigating the effects of language distance and corpus size on calibration \n\nOne important aspect of the multilingual model analysis is the factors that affect the model's crosslingual ability (Jones et al., 2021;Jiang et al., 2022;Ahuja et al., 2022). Here we focus on analyzing both linguistic (distances between languages) and non-linguistic factors (pre-training data size) for calibration performance. Lingustic Features Following previous work (Ahuja et al., 2022;Jones et al., 2021), we load the syntactic and genetic distances that are precomputed by the URIEL project (Littell et al., 2017), as the distance measurements between English and other target languages. The syntactic distance measures the structural differences between various languages. The genetic distance quantifies the genetic relation between languages according to the Glottolog tree. We investigate whether the closeness of the target language to the source language, English, implies better calibration performance. To measure this we compute Pearson's correlation coefficient between the language distance and ECE of the standard models. Table 5 shows that the calibration performance is highly correlated with the syntactic distances between English and the corresponding languages. \n\nNon-Lingustic Features We also explore the impact of non-linguistic features from training. In this section, we compute the correlation between pretraining data size and calibration performance and Table 5 indicates that the size of different languages in the pre-training influences the cross-lingual calibration of QA models. 6) 10 . The table shows that the accuracy increases with model size, as expected. Further, we note that confidence calibration also improves with increasing model size, which also supports prior findings on the calibration of monolingual encoder LLMs in (Dan and Roth, 2021).",
            "score": 0.3908534054751986,
            "section_title": "Additional Ablation Experiments and Discussion",
            "char_start_offset": 19466,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 84,
                    "end": 233
                },
                {
                    "start": 236,
                    "end": 324
                },
                {
                    "start": 327,
                    "end": 408
                },
                {
                    "start": 411,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1599
                },
                {
                    "start": 1602,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2011
                },
                {
                    "start": 2012,
                    "end": 2205
                }
            ],
            "ref_mentions": [
                {
                    "start": 547,
                    "end": 566,
                    "matchedPaperCorpusId": "250551977"
                },
                {
                    "start": 911,
                    "end": 933,
                    "matchedPaperCorpusId": "17625727"
                },
                {
                    "start": 2184,
                    "end": 2204,
                    "matchedPaperCorpusId": "244119588"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54541015625
        },
        {
            "corpus_id": "270062675",
            "title": "Multicalibration for Modeling Censored Survival Data with Universal Adaptability",
            "text": "The essence of multicalibration boosting lies in its ability to transfer weak learners to stronger learners by auditing the residuals, enabling enhancement in predictive accuracy across diverse subpopulations, which in turn improves the accuracy of statistical estimates within the target domain. How multicalibration leads to precise predictions under covariate shift needs to be elucidated in a formal mathematical language. Our main results relate the prediction error (7) obtained using multicalibration to that of ipsw. Specifically, we reveal that under proper conditions, a multicalibrated function m (k) (\u2022; t) leads to the property of universal adaptability. Recall the collection of propensity score odds H defined in (8) and the specification error d(h, w) defined in (6), with them in place, we are ready to state the theorem below that establishes universal adaptability from multicalibration. Theorem 6. Under the same conditions outlined in Theorem 3, suppose m (k) (\u2022; t) : X \u2192 [0, C] is (H, \u03b1 \u2032 )-multicalibrated over the source distribution D S , then for any target distribution D T , and any w \u2208 \u03a3, m (k) (X; t) satisfies \n\nTheorem 6 also has implications beyond universal adaptability. It demonstrates that multicalibration in the source domain not only ensures universal adaptability but retains some degree of multicalibration over the target domain as well. Its statement can be simply understood as providing an adaptive guarantee under different degrees of shift: 1. If m (k) is well calibrated on the source domain, it will only lose a small amount in the multicalibration guarantee on targets close by. A simple instance is that H(\u03a3) = {Id(x)} in a scenario without any distribution shift. 2. Under extreme shifts, shifts that require the full power of the class to account for H(\u03a3), the universal adaptability and calibration guarantee weakens unless \u03a3 is well specified and A is capable of learning H(\u03a3) or H(\u03a3)\u2297C agnostically.",
            "score": 0.39064154548038443,
            "section_title": "Universal adaptability",
            "char_start_offset": 26870,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1141
                },
                {
                    "start": 1144,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1957
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2017822265625
        },
        {
            "corpus_id": "277596537",
            "title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
            "text": "As calibration serves as an effective technique to align a model's confidence with its correctness and thus improve models confidence estimation, we propose three experiments using the basic prompting approach to investigate the RQ2. The results highlight notable commonalities and distinctions between the LLMs, as shown in Figure 5. \n\nECE can be diminished when combined with self-improvement after calibration. Multi self-improvement-then-calibration methods yield reduced ECE, with the latter achieving a markedly lower ECE compared to the other two approaches. Despite performing calibration after each round, the iterative method continues to exhibit relatively high ECE, possibly because the alternating introduction of self-improvement dilutes the calibration effect and consequently compromises alignment between confidence and accuracy. Furthermore, both Note. Cms means calibration then multi self-improvement and msc is multi self-improvement then calibration. Ics stands for iterative calibration and self-improvement the \"calibration then multi self-improvement\" and \"iterative\" methods produce relatively high ECE-particularly in Llama, where ECE increases substantially compared to selfimprovement alone. One explanation for this phenomenon is that calibration is primarily intended to align the model's confidence with its actual accuracy. However, during selfimprovement, the model refines its responses based on self-generated feedback, which can shift its confidence distribution. As a result, the self-bias dominates over calibration effect when the calibration is performed at the beginning. \n\nCalibration can serve as a better foundation in self-improvement for stronger LLM. \n\nThe \"calibration then multi self-improvement\" strategy in Llama-deepseek shows steady improvement of ACC, surpassing the performance of Llama-deepseek's longer CoT in pure self-improvement setting. Additionally, unlike basic prompting-based self-improvement, this method does not exhibit an initial accuracy drop. In Llama-deepseek, the multi selfimprovement-then-calibration approach effectively rectifies errors in earlier stages while maintaining a relatively stable ECE; however, it manifests some fluctuations of ACC in later stages, suggesting a pronounced impact on model reasoning ability.",
            "score": 0.3903258185178451,
            "section_title": "RQ 2: What are the compounded effects of marrying calibration and self-improvement on model performance?",
            "char_start_offset": 22448,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 334
                },
                {
                    "start": 337,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1613
                },
                {
                    "start": 1616,
                    "end": 1698
                },
                {
                    "start": 1701,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2014
                },
                {
                    "start": 2015,
                    "end": 2298
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83203125
        },
        {
            "corpus_id": "17101144",
            "title": "Reliable Confidence Estimation via Online Learning",
            "text": "Assessing uncertainty within machine learning systems is an important step towards ensuring their safety and reliability. Indeed, in many applications of machine learning -including medical diagnosis [1], natural language understanding [2], speech recognition [3], weather forecasting [4], econometric analysis [5] -assessing confidence can be as important as obtaining high accuracy. This work explores robust, online confidence estimation methods for classification problems. \n\nAn important limitation of existing methods is the assumption that data is sampled i.i.d. from a distribution P(x, y); when test-time data is distributed according to a different P * , these methods may become overly confident on incorrect predictions. Here, we introduce new, robust uncertainty estimation algorithms guaranteed to produce reliable confidence estimates on out-of-distribution input, including input generated by an adversary. Our algorithms are based on online learning techniques and, unlike earlier methods, they do not require data to be set aside solely for uncertainty estimation. \n\nWe focus primarily on the classification setting, where the most common and natural way of measuring an algorithm's uncertainty is via calibrated probability estimates that match the true empirical frequencies of an outcome. For example, if an algorithm predicted a 60% chance of rain 100 times in a given year, we would say that its forecast was calibrated if it rained on about 60 of those 100 days. Calibrated confidence estimates are typically constructed via recalibration, using methods such as Platt scaling or isotonic regression. In the context of binary classification, these methods reduce recalibration to a one-dimensional regression problem that, given data (x i , y i ) n i=1 , trains a model g(s) (e.g. logisitc regression) to predict probabilities p i = g(s i ) from uncalibrated scores s i = h(x i ) produced by a classifier h (e.g. SVM margins). Fitting g is equivalent to performing density estimation targeting P(Y = 1|h(X) = s i ) and hence may fail on out-of-distribution testing data. \n\nThe methods we introduce in this work are instead based on calibration techniques developed in the literature on online learning in mathematical games [6,7].",
            "score": 0.39029092017923656,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 477
                },
                {
                    "start": 480,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1082
                },
                {
                    "start": 1085,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2093
                },
                {
                    "start": 2096,
                    "end": 2253
                }
            ],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 203,
                    "matchedPaperCorpusId": "7716019"
                },
                {
                    "start": 260,
                    "end": 263,
                    "matchedPaperCorpusId": "11506631"
                },
                {
                    "start": 285,
                    "end": 288,
                    "matchedPaperCorpusId": "15880012"
                },
                {
                    "start": 311,
                    "end": 314,
                    "matchedPaperCorpusId": "123181502"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33251953125
        },
        {
            "corpus_id": "239011778",
            "title": "Context-Aware Selective Label Smoothing for Calibrating Sequence Recognition Model",
            "text": "Instead of trivially applying non-sequential calibration methods on each token, some methods attempt to exploit the intrinsic characteristics to facilitate sequence-level calibration. Kuleshov and Liang [19] firstly consider confidence calibration in the context of structure prediction problem, which demonstrates that utilizing structural features will produce better calibration performance on structured prediction such as sequence. Recent methods for sequential data can be categorized into two directions. Based on temperature scaling [10], Leathart et al. propose a sequential length-related calibration method [22]. As calibration error varies with sequential length, different temperature values are set for calibration. Ding et al. obtain the adaptive temperature values for different pixels or voxels for semantic segmentation [6]. Other methods are inspired by label smoothing [38]. In natural language processing, Elbayad et al. complete token-level smoothing with sequence-level smoothing methods. The nearby target sequence is sampled according to the BLEU score [30] to conduct sequence-level loss smoothing [7]. Lukasik et al. utilize n-gram overlapping with the target sequence in the label smoothing method to guarantee the semantic similarity of translated sequence [25]. Song et al. smooth the loss function according to the occurrence of the n-gram context in candidate word set as the weight [37]. Although better calibration performance could be achieved on sequential data, few methods consider the uneven distribution of dataset to flexibly calibrate the prediction of different labels, and additionally, the potential contextual information is not fully explored to calibrate on the recognition of sequential data tasks.",
            "score": 0.38965781865827465,
            "section_title": "Calibration on Sequential Data",
            "char_start_offset": 6388,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 207,
                    "matchedPaperCorpusId": "2974522"
                },
                {
                    "start": 541,
                    "end": 545,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 889,
                    "end": 893,
                    "matchedPaperCorpusId": "206593880"
                },
                {
                    "start": 1078,
                    "end": 1082,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 1124,
                    "end": 1127,
                    "matchedPaperCorpusId": "44004989"
                },
                {
                    "start": 1286,
                    "end": 1290,
                    "matchedPaperCorpusId": "222379592"
                },
                {
                    "start": 1415,
                    "end": 1419,
                    "matchedPaperCorpusId": "216505957"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60205078125
        },
        {
            "corpus_id": "253098773",
            "title": "On the Calibration of Massively Multilingual Language Models",
            "text": "We briefly review some commonly used methods for calibrating neural network based classifiers. 1. Temperature Scaling (TS and Self-TS) (Guo et al., 2017) is applied by scaling the output logits using a temperature parameter T before applying softmax i.e. : \n\n, where o k denotes the logits corresponding to the k th class. T is a learnable parameter obtained posttraining by maximizing the log-likelihood on the dev set while keeping other network parameters fixed. We experiment with two settings for improving calibration on a target language: using dev data in English to perform temperature scaling (TS) and using the target language's dev data (Self-TS). 2. Label Smoothing (LS) (Pereyra et al., 2017) is a regularization technique that penalizes low entropy distributions by using soft labels that are obtained by assigning a fixed probability q = 1 \u2212 \u03b1 to the true label (0 < \u03b1 < 1), and distributing the remaining probability mass uniformly across the remaining classes. Label smoothing has been empirically shown to be competitive with temperature scaling for calibration (M\u00fcller et al., 2019) especially in out of domain settings (Desai and Durrett, 2020) 3. Few-Shot Learning (FSL) We also investigate if fine-tuning the MMLM on a few examples in a target language in addition to the data in English, leads to any improvement in calibration as it does in the performance (Lauscher et al., 2020). Since these models are expected to be calibrated worse for out-of-domain data compared to in-domain data (Desai and Durrett, 2020), we try to improve calibration by reducing the domain shift through fewshot learning. \n\nApart from these, we also consider combinations of different calibration methods in our experiments, including Label Smoothing with Temperature Scaling (TS + LS or Self-TS + LS) and Few-Shot Learning with Label Smoothing (FSL + LS).",
            "score": 0.3894575548324393,
            "section_title": "Calibration Methods",
            "char_start_offset": 4891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 256
                },
                {
                    "start": 259,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1623
                },
                {
                    "start": 1626,
                    "end": 1858
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 153,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1081,
                    "end": 1102,
                    "matchedPaperCorpusId": "174802983"
                },
                {
                    "start": 1140,
                    "end": 1165,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 1382,
                    "end": 1405,
                    "matchedPaperCorpusId": "226262344"
                },
                {
                    "start": 1512,
                    "end": 1537,
                    "matchedPaperCorpusId": "212747810"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.875
        },
        {
            "corpus_id": "270392060",
            "title": "Large Language Models Must Be Taught to Know What They Don't Know",
            "text": "Question answering evaluations.In all experiments, we use greedy decoding to generate answers conditioned on questions with few-shot prompts.We then label the generated answers as correct or incorrect and independently generate P pcorrectq using one of the uncertainty estimators.For evaluation, we primarily use the popular MMLU dataset [18], which covers 57 subjects including STEM, humanities, and social sciences.Crucially, however, we expand the original multiple choice (MC) setting with a new open-ended (OE) setting.In the open-ended setting, we do not provide answer choices, and the language model must generate an answer that matches the ground truth answer choice.We determine a correct match by grading with a strong auxiliary language model (Appendix A.2). We verify that grading via language models provides a cheap and effective proxy for the gold standard human grading (Appendix A.3), consistent with related findings [10].\n\nMetrics.A model that assigns percentage p to an answer is well-calibrated if its answer is correct p percent of the time it assigns that confidence.Calibration is typically measured using expected calibration error (ECE) [37], which compares empirical frequences with estimated probabilities through binning (Appendix A.4).A lower ECE is better, and an ECE of 0 corresponds to a perfectly calibrated model.In addition to calibration, we measure the area under the receiver operating characteristic curve (AUROC) of the model's confidence.High AUROC indicates ability to filter answers likely to be correct from answers that are likely to be incorrect, a setting typically called selective prediction.\n\nTemperature scaling.Temperature scaling [42,17] improves the calibration of a classifier by scaling its logits by 1 T (where T is the temperature) before applying the softmax function.A high temperature scales the softmax probabilities towards a uniform distribution, while a low temperature collapses the distribution around the most probable output.The temperature parameter is learned on held-out data, typically taken from the same distribution as the training set.\n\n4 Do We Get Good Uncertainties Out-of-the-Box?",
            "score": 0.3893646611884649,
            "section_title": "Preliminaries",
            "char_start_offset": 8766,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 31,
                    "end": 141
                },
                {
                    "start": 141,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 417
                },
                {
                    "start": 417,
                    "end": 524
                },
                {
                    "start": 524,
                    "end": 676
                },
                {
                    "start": 676,
                    "end": 941
                },
                {
                    "start": 943,
                    "end": 951
                },
                {
                    "start": 951,
                    "end": 1091
                },
                {
                    "start": 1091,
                    "end": 1266
                },
                {
                    "start": 1266,
                    "end": 1349
                },
                {
                    "start": 1349,
                    "end": 1481
                },
                {
                    "start": 1481,
                    "end": 1643
                },
                {
                    "start": 1645,
                    "end": 1665
                },
                {
                    "start": 1665,
                    "end": 1829
                },
                {
                    "start": 1829,
                    "end": 1996
                },
                {
                    "start": 1996,
                    "end": 2114
                },
                {
                    "start": 2116,
                    "end": 2162
                }
            ],
            "ref_mentions": [
                {
                    "start": 936,
                    "end": 940,
                    "matchedPaperCorpusId": "258461287"
                },
                {
                    "start": 1164,
                    "end": 1168,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 1685,
                    "end": 1689,
                    "matchedPaperCorpusId": "56563878"
                },
                {
                    "start": 1689,
                    "end": 1692,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73388671875
        },
        {
            "corpus_id": "269149545",
            "title": "Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation",
            "text": "Uncertainty estimation is a significant issue for current large language models (LLMs) that are generally poorly calibrated and over-confident, especially with reinforcement learning from human feedback (RLHF). Unlike humans, whose decisions and confidences not only stem from intrinsic beliefs but can also be adjusted through daily observations, existing calibration methods for LLMs focus on estimating or eliciting individual confidence without taking full advantage of the\"Collective Wisdom\": the interaction among multiple LLMs that can collectively improve both accuracy and calibration. In this work, we propose Collaborative Calibration, a post-hoc training-free calibration strategy that leverages the collaborative and expressive capabilities of multiple tool-augmented LLM agents in a simulated group deliberation process. We demonstrate the effectiveness of Collaborative Calibration on generative QA tasks across various domains, showing its potential in harnessing the rationalization of collectively calibrated confidence assessments and improving the reliability of model predictions.",
            "score": 0.38920239527821626,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90673828125
        },
        {
            "corpus_id": "270258126",
            "title": "Cycles of Thought: Measuring LLM Confidence through Stable Explanations",
            "text": "Recently much attention has been devoted to measuring uncertainty specifically in LLMs [16,20].Since LLMs are generative models, uncertainty may be measured with respect to an infinite set of text sequences as opposed to a fixed number of classification labels [4].Many works, however, use multiple choice question answering tasks to evaluate LLMs using standard classification methodologies [24,43], and we will follow a similar approach in this work.Issues with using token logits directly to compute confidence are well known.Recent works [2,24,38] show that larger models are typically better calibrated on multiple choice datasets than smaller ones, but are still sensitive to question reformulations as well as typical RLHF training strategies.Another recent work [48] notes that language models fail to identify unanswerable questions at a higher rate than humans.At a high level, existing techniques for LLM confidence elicitation can be classified as either whitebox, requiring access to internal model weights and token probabilities, or black-box, using only samples from the model [16].We choose to summarize inference time interventions below, as training time interventions are often computationally expensive and require strict inductive assumptions.\n\nWhite-box Methods.Access to the last activation layer of the LLM (token logits) admits calculating token and token sequence probabilities via the softmax function.One can incorporate text sequence probabilities to implement conformal prediction [27] methods, or adjust them based on semantic importance of individual tokens to improve calibration [13].Surrogate models can also serve as an effective substitute if access the original model is restricted-access [36].Internal activations can also be observed to determine if certain feature directions are more or less truthful [3,6].\n\nBlack-box Methods.Black-box confidence typically uses one or both of the following approaches: Sample+aggregate methods involve analyzing the distributions of multiple responses sampled from the model [46].Responses can be generated in a variety of ways, such as using chain-of-thought prompting [43], asking for multiple answers in a single response [40], or perturbing the question in-between samples [28].",
            "score": 0.389095069814705,
            "section_title": "Uncertainty Quantification in LLMs",
            "char_start_offset": 8049,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 95,
                    "end": 265
                },
                {
                    "start": 265,
                    "end": 452
                },
                {
                    "start": 452,
                    "end": 529
                },
                {
                    "start": 529,
                    "end": 750
                },
                {
                    "start": 750,
                    "end": 871
                },
                {
                    "start": 871,
                    "end": 1098
                },
                {
                    "start": 1098,
                    "end": 1265
                },
                {
                    "start": 1267,
                    "end": 1285
                },
                {
                    "start": 1285,
                    "end": 1430
                },
                {
                    "start": 1430,
                    "end": 1619
                },
                {
                    "start": 1619,
                    "end": 1733
                },
                {
                    "start": 1733,
                    "end": 1850
                },
                {
                    "start": 1852,
                    "end": 1870
                },
                {
                    "start": 1870,
                    "end": 2058
                },
                {
                    "start": 2058,
                    "end": 2260
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.599609375
        },
        {
            "corpus_id": "263611057",
            "title": "Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness",
            "text": "Even when trained in a supervised manner, neural network models pose a unique set of challenges for measuring uncertainty and improving calibration (Papadopoulos et al., 2001;Riquelme et al., 2018). Much of this work stems from the field of computer vision, where distinct frameworks have been proposed by: Blundell et al. (2015); Gal & Ghahramani (2016a) to approximate Bayesian inference, Lakshminarayanan et al. (2017); Jain et al. (2020) to rely on straightforward deep ensembles, Liang et al. (2017); Papernot & McDaniel (2018) to detect Out-of-Distribution training samples. Parallel ideas for uncertainty estimation with supervised neural works have been developed in natural language processing (Fortunato et al., 2017;Gal & Ghahramani, 2016b;Kuleshov et al., 2018). However, these techniques are not directly applicable to today's best LLMs which are behind black-box APIs with unknown training data. \n\nUncertainty Estimation for LLMs. For estimating the confidence levels tied to responses output by large language models, Kuhn et al. (2023) introduce semantic entropy, incorporating linguistic invariances created by shared meanings. However their approach requires access to token-level probabilities from the LLM, which is often not accessible with today's black-box APIs. Kadavath et al. (2022) prompt the models to self-evaluate their answers and directly ask the LLM to produce the likelihood P (Answer is True) -also fine-tuning the model to output better values for its stated likelihood. Relatedly, Lin et al. (2022) prompt LLMs to generate both an answer and a level of confidence. Manakul et al. (2023) propose a sampling-based approach to detect hallucinated facts. All of these aforementioned approaches train additional models via supervised learning, unlike BSDETECTOR which does not employ any additional training. More recently, Tian et al. (2023) conduct evaluations of computationally feasible methods to extract confidence scores from the probabilities output by LLMs trained via Reinforcement Learning with Human Feedback.",
            "score": 0.3887262442695258,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3990,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 909
                },
                {
                    "start": 912,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 2053
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 175,
                    "matchedPaperCorpusId": "1285801"
                },
                {
                    "start": 175,
                    "end": 197,
                    "matchedPaperCorpusId": "3502463"
                },
                {
                    "start": 331,
                    "end": 355,
                    "matchedPaperCorpusId": "160705"
                },
                {
                    "start": 423,
                    "end": 441,
                    "matchedPaperCorpusId": "189998986"
                },
                {
                    "start": 727,
                    "end": 751,
                    "matchedPaperCorpusId": "15953218"
                },
                {
                    "start": 751,
                    "end": 773,
                    "matchedPaperCorpusId": "49559279"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.486572265625
        },
        {
            "corpus_id": "267682039",
            "title": "Uncertainty Quantification for In-Context Learning of Large Language Models",
            "text": "Earlier works (Xiao and Wang, 2019;Desai and Durrett, 2020;Jiang et al., 2021) on uncertainty in language models have focused on the calibration of classifiers (e.g., applying dropout to the model parameters or leveraging ensemble voting) to better assess the confidence of the generated output. When it comes to the era of LLMs, multiple works (Xiao and Wang, 2021;Xiao et al., 2022;Lin et al., 2022;Yu et al., 2022;Lin et al., 2023;Kuhn et al., 2023;Fadeeva et al., 2023) have been proposed to measure the uncertainty of LLM's prediction in multiple aspects (e.g., lexical uncertainty, text uncertainty, and semantic uncertainty) for multiple NLP tasks. Another line of works (Kadavath et al., 2022;Zhou et al., 2023;Amayuelas et al., 2023;Chen et al., 2024) instead tries to analyze how to extract knowledge from a language model correctly and self-evaluate the correctness with a confidence score. However, despite these commendable efforts, existing methods still lack an effective way to directly quantify and decompose the uncertainty inherent in the outputs of LLMs with in-context learning. \n\nWe evaluate the uncertainty decomposition procedure on realistic natural language understanding problems. By comparing state-of-the-art uncertainty quantification methods, we aim to examine what type of uncertainty is the most promising indicator of high confidence for LLMs. In addition, we also provide generalization analysis and two specific out-of-distribution detection applications. Due to the space limit, extra experiments and experiment settings are provided in the Appendix.",
            "score": 0.3877333817244647,
            "section_title": "Related Works",
            "char_start_offset": 14498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1099
                },
                {
                    "start": 1102,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1587
                }
            ],
            "ref_mentions": [
                {
                    "start": 14,
                    "end": 35,
                    "matchedPaperCorpusId": "53715584"
                },
                {
                    "start": 401,
                    "end": 417,
                    "matchedPaperCorpusId": "252626745"
                },
                {
                    "start": 452,
                    "end": 473,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 742,
                    "end": 760,
                    "matchedPaperCorpusId": "250390527"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.384521484375
        },
        {
            "corpus_id": "253384402",
            "title": "Calibration Meets Explanation: A Simple and Effective Approach for Model Confidence Estimates",
            "text": "Calibration strengthens the trustworthiness of black-box models by producing better accurate confidence estimates on given examples. However, little is known about if model explanations can help confidence calibration. Intuitively, humans look at important features attributions and decide whether the model is trustworthy. Similarly, the explanations may tell us when the model might know and when it does not. Inspired by this, we propose a method named CME that leverages model explanations to make the model less confident with non-inductive attributions. The idea is that when the model is not highly confident, it is difficult to identify strong indications of any class, and the tokens accordingly do not have high attribution scores for any class and vice versa. We conduct extensive experiments on six datasets with two popular pre-trained language models in the in-domain and out-of-domain settings. The results show that CME improves calibration performance in all settings. The expected calibration errors are further reduced when combined with temperature scaling. Our findings highlight that model explanations can help calibrate posterior estimates.",
            "score": 0.38762789913485074,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.890625
        },
        {
            "corpus_id": "278326843",
            "title": "Always Tell Me The Odds: Fine-grained Conditional Probability Estimation",
            "text": "Uncertainty in LLMs Like other modern neural networks (Guo et al., 2017), transformerbased language models are often overconfident in their predictions (Desai & Durrett, 2020;Jiang et al., 2022). While larger LLMs are generally better calibrated, instruction tuning can worsen miscalibration (Achiam et al., 2023;Hendrycks et al., 2020). To address this, various methods have been proposed to improve uncertainty estimation, such as analyzing logits (Zhao et al., 2021b), prompting LLMs to verbalize confidence (Mielke et al., 2022;Tian et al., 2023), or probing internal representations (CH- Wang et al., 2024). Of these, verbalized uncertainty is the most calibrated (Tian et al., 2023), though it can still be overconfident (Tanneru et al., 2023;Xiong et al., 2024;Chen et al., 2023) and often relies on a small set of common discrete outputs (e.g., 10%, 75%) (Cruz et al., 2024). \n\nPart of the problem is that existing resources for training are predominantly hard labeled, making model calibration and its evaluation particularly challenging (Kumar et al., 2019;Nixon et al., 2019). Recently, the field has seen growing interest in modeling the complete label distribution (Meissner et al., 2021;Liu et al., 2023;Zhou et al., 2021;Cheng et al., 2024), and different protocols have been proposed for curating datasets to enable such training and evaluation (Nie et al., 2020b;Chen et al., 2020). However, these datasets tend to be limited in size and are subject to ongoing debate over whether the distributions reflect true aleatoric uncertainty or merely label noise (Jiang & Marneffe, 2022;Baan et al., 2022;Wang et al., 2022;Weber-Genzel et al., 2024;Baan et al., 2024).",
            "score": 0.3875528020145898,
            "section_title": "Related Work",
            "char_start_offset": 4717,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 883
                },
                {
                    "start": 886,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1678
                }
            ],
            "ref_mentions": [
                {
                    "start": 54,
                    "end": 72,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 152,
                    "end": 175,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 450,
                    "end": 470,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 511,
                    "end": 532,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 532,
                    "end": 550,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 669,
                    "end": 688,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 749,
                    "end": 768,
                    "matchedPaperCorpusId": "259224389"
                },
                {
                    "start": 768,
                    "end": 786,
                    "matchedPaperCorpusId": "270285522"
                },
                {
                    "start": 863,
                    "end": 882,
                    "matchedPaperCorpusId": "271329113"
                },
                {
                    "start": 1047,
                    "end": 1067,
                    "matchedPaperCorpusId": "202718866"
                },
                {
                    "start": 1067,
                    "end": 1086,
                    "matchedPaperCorpusId": "102486060"
                },
                {
                    "start": 1361,
                    "end": 1380,
                    "matchedPaperCorpusId": "222177851"
                },
                {
                    "start": 1380,
                    "end": 1398,
                    "matchedPaperCorpusId": "202230727"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58984375
        },
        {
            "corpus_id": "271270595",
            "title": "Robust Calibration of Large Vision-Language Adapters",
            "text": "Calibrating the confidence of deep learning models is paramount in developing reliable solutions, as the confidence is expected to correlate with correctness.\n\nGiven the importance and the potential impact of miscalibration, a growing literature to address this issue has emerged in the last years.Post-processing techniques have been widely used to achieve calibration, wherein a linear transformation [11,20,54] is applied to the predicted logits before converting it to softmax.Nevertheless, an important limitation is that these transformations are obtained using held-out validation data, which is assumed to follow the same distribution as the test data, limiting their applicability in the presence of domain drifts [40].A popular alternative consists in calibrating the networks at training time.This can be achieved by incorporating explicit penalties that either penalize overconfident softmax predictions [4,25,41,43] or encourage small logit differences [29,30,37].Furthermore, [35,36] demonstrated that popular classification losses, such as Focal Loss [28] or Label Smoothing [52], integrate an implicit term that maximizes the entropy of the network predictions, thus favoring low-confidence models.Other works to improve the accuracy of the uncertainty estimates during training include the use of MixUp [53,62], or enforcing a constant vector norm on the logits [56], among others.Nevertheless, all these methods have been proposed in the context of fully-supervised models, and the calibration of Prompt Learning and Adapter-based methods for CLIP remains unexplored in the literature.\n\n3 Background",
            "score": 0.38750880956394324,
            "section_title": "Model calibration",
            "char_start_offset": 8489,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 160,
                    "end": 298
                },
                {
                    "start": 298,
                    "end": 481
                },
                {
                    "start": 481,
                    "end": 728
                },
                {
                    "start": 728,
                    "end": 804
                },
                {
                    "start": 804,
                    "end": 977
                },
                {
                    "start": 977,
                    "end": 1214
                },
                {
                    "start": 1214,
                    "end": 1398
                },
                {
                    "start": 1398,
                    "end": 1603
                },
                {
                    "start": 1605,
                    "end": 1617
                }
            ],
            "ref_mentions": [
                {
                    "start": 403,
                    "end": 407,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 407,
                    "end": 410,
                    "matchedPaperCorpusId": "250492910"
                },
                {
                    "start": 410,
                    "end": 413,
                    "matchedPaperCorpusId": "232035835"
                },
                {
                    "start": 723,
                    "end": 727,
                    "matchedPaperCorpusId": "174803437"
                },
                {
                    "start": 916,
                    "end": 919,
                    "matchedPaperCorpusId": "249948182"
                },
                {
                    "start": 919,
                    "end": 922,
                    "matchedPaperCorpusId": "245424614"
                },
                {
                    "start": 922,
                    "end": 925,
                    "matchedPaperCorpusId": "261075925"
                },
                {
                    "start": 925,
                    "end": 928,
                    "matchedPaperCorpusId": "9545399"
                },
                {
                    "start": 966,
                    "end": 970,
                    "matchedPaperCorpusId": "244729457"
                },
                {
                    "start": 970,
                    "end": 973,
                    "matchedPaperCorpusId": "254044617"
                },
                {
                    "start": 973,
                    "end": 976,
                    "matchedPaperCorpusId": "257496829"
                },
                {
                    "start": 990,
                    "end": 994,
                    "matchedPaperCorpusId": "211252346"
                },
                {
                    "start": 1066,
                    "end": 1070,
                    "matchedPaperCorpusId": "47252984"
                },
                {
                    "start": 1090,
                    "end": 1094,
                    "matchedPaperCorpusId": "206593880"
                },
                {
                    "start": 1320,
                    "end": 1324,
                    "matchedPaperCorpusId": "166228660"
                },
                {
                    "start": 1324,
                    "end": 1327,
                    "matchedPaperCorpusId": "231918640"
                },
                {
                    "start": 1379,
                    "end": 1383,
                    "matchedPaperCorpusId": "248887336"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59033203125
        },
        {
            "corpus_id": "272310114",
            "title": "Assessing Generative Language Models in Classification Tasks: Performance and Self-evaluation Capabilities in the Environmental and Climate Change Domain",
            "text": "Given the considerable amount of work and vertiginous growth of material on the topic of LLMs, we report a limited number of studies relevant to our research. LLMs in Classification Task Text Classification, a fundamental NLP task, has progressed from traditional methods such as Na\u00efve Bayes [18], to advanced models like BERT [9,17]. Recent studies on LLMs in text classification present mixed outcomes when compared to traditional supervised methods. For instance, [34] explores the performance of different classes of language models in text classification, often favoring smaller models. [2] investigates LLMs across languages and tasks without a clear best performer, while [6] discusses ChatGPT's limitations in biomedical fields compared to domain-specific models. Nonetheless, advances in prompting techniques, such as Clues and Reasoning prompts [6], and strategic prompts [7], show potential in improving LLMs' classification abilities. [24] highlights GPT-4's comparable performance in propaganda detection to the current state-of-the-art. LLMs Self Evaluation The significant hype around LLMs has encouraged numerous studies on their capabilities [5] and self-evaluation skills, including calibration-the alignment between a model's confidence and its prediction accuracy. Calibration is crucial for a model's reliability, especially in determining when to defer to an expert [29]. Studies [29,16] indicate LLMs' verbalized confidences are often better calibrated than conditional probabilities, yet [33,21] highlight their tendency towards overconfidence. Our research evaluates the accuracy and calibration quality of LLMs' verbalized confidence scores. LLMs in Environmental Domain Regarding the intersection of LLMs and the environmental domain, similar to other NLP subfields, efforts in this direction have been focused mostly on climate change (CC) topics rather than the broader environmental and ecological fields (which still include CC).",
            "score": 0.38746797264495647,
            "section_title": "Related Works",
            "char_start_offset": 4429,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1960
                }
            ],
            "ref_mentions": [
                {
                    "start": 292,
                    "end": 296,
                    "matchedPaperCorpusId": "7311285"
                },
                {
                    "start": 327,
                    "end": 330,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 679,
                    "end": 682,
                    "matchedPaperCorpusId": "257952480"
                },
                {
                    "start": 855,
                    "end": 858,
                    "matchedPaperCorpusId": "257952480"
                },
                {
                    "start": 1406,
                    "end": 1409,
                    "matchedPaperCorpusId": "249191391"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.391845703125
        },
        {
            "corpus_id": "231933881",
            "title": "Don't Just Blame Over-parametrization for Over-confidence: Theoretical Analysis of Calibration in Binary Classification",
            "text": "Algorithms for model calibration Practitioners have observed and dealt with the overconfidence of logistic regression long ago. Recalibration algorithms fix this by adjusting the output of a well-trained model, and dates back to the classical methods of Platt scaling (Platt et al., 1999), histogram binning (Zadrozny and Elkan) and isotonic regression (Zadrozny and Elkan, 2002). Platt et al. (1999) also uses a particular kind of label smoothing as a way of mitigating the over-confidence in logistic regression. Guo et al. (2017) show that temperature scaling, a simple method that learns a rescaling factor for the logits, is a competitive method for calibrating neural networks. A number of recent recalibration methods further improve the performances over these approaches (Kull et al., 2017(Kull et al., , 2019;;Ding et al., 2020;Rahimi et al., 2020;Zhang et al., 2020). Another line of work improves calibration by aggregating the probabilisitic predictions over multiple models, using either an ensemble of models (Lakshminarayanan et al., 2016;Malinin et al., 2019;Wen et al., 2020;Tran et al., 2020), or randomized predictions such as Bayesian neural networks (Gal and Ghahramani, 2016;Gal et al., 2017;Maddox et al., 2019;Dusenberry et al., 2020). Finally, there are techniques for improving the calibration of a single neural network during training (Thulasidasan et al., 2019;Mukhoti et al., 2020;Liu et al., 2020). Kumar et al. (2019) show that continuous rescaling methods such as temperature scaling is less calibrated than reported, and proposed a method that combines temperature scaling and histogram binning. Gupta et al. (2020) study the relationship between calibration and other notions of uncertainty such as confidence intervals. Shabat et al. (2020); Jung et al. (2020) study the sample complexity of estimating the multicalibration error (group calibration).",
            "score": 0.3873885757253008,
            "section_title": "Related work",
            "char_start_offset": 4817,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1887
                }
            ],
            "ref_mentions": [
                {
                    "start": 780,
                    "end": 798,
                    "matchedPaperCorpusId": "217264968"
                },
                {
                    "start": 838,
                    "end": 858,
                    "matchedPaperCorpusId": "212725550"
                },
                {
                    "start": 1076,
                    "end": 1093,
                    "matchedPaperCorpusId": "218628552"
                },
                {
                    "start": 1172,
                    "end": 1198,
                    "matchedPaperCorpusId": "160705"
                },
                {
                    "start": 1215,
                    "end": 1235,
                    "matchedPaperCorpusId": "59608823"
                },
                {
                    "start": 1235,
                    "end": 1259,
                    "matchedPaperCorpusId": "218628552"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.203125
        },
        {
            "corpus_id": "269456957",
            "title": "Calibration of Large Language Models on Code Summarization",
            "text": "We now study how calibration changes with rescaling.Table IV and Figure 5 show Brier scores, ECE, and skill scores at different setups.Following the prior research question,   We also observe that the ECE greatly reduces with rescaling.To summarize, the thresholded raw average token probability is not well-calibrated, but the thresholded rescaled average token probability is well-calibrated.Therefore, the reliability of the model increases with rescaling.\n\nCalibration changes with ASAP.Table V presents the results comparing calibration applying ASAP.We observe that, augmented with static analysis products, more of the model's output exceeds the threshold (higher success rate) in all cases except for Python with the GPT-3 Turbo model.Turning now to the calibration, for Java, the skill score decreases with ASAP, whereas for Python, the skill score increases for the CodeLlama-70B and deepseek-coder-33binstruct models.In these two cases, the skill score increases alongside success rate.It's noteworthy that Java is a strictly typed and verbose language; the additional information provided by ASAP in the prompts may be redundant, and distort the model's confidence.However, for Python, ASAP may verbalized confidence.In the x-axis, we present the verbalized confidence scores, and on the y-axis is accuracy with respect to optimal F1 thresholded Sentence-BERT.For both zero-shot and few-shot settings, we found that the models were similarly not well-calibrated.Note that we tried a higher temperature and generated multiple samples.Majority voting from multiple samples did not change the calibration outcome.\n\nTherefore, we can conclude that the models are not wellcalibrated for any of the self-reflective measures studied.",
            "score": 0.38726858554596943,
            "section_title": "B. RQ2: Platt Rescaled vs. Raw Calibration",
            "char_start_offset": 34784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 52,
                    "end": 135
                },
                {
                    "start": 135,
                    "end": 236
                },
                {
                    "start": 236,
                    "end": 394
                },
                {
                    "start": 394,
                    "end": 459
                },
                {
                    "start": 461,
                    "end": 491
                },
                {
                    "start": 491,
                    "end": 556
                },
                {
                    "start": 556,
                    "end": 743
                },
                {
                    "start": 743,
                    "end": 928
                },
                {
                    "start": 928,
                    "end": 997
                },
                {
                    "start": 997,
                    "end": 1177
                },
                {
                    "start": 1177,
                    "end": 1229
                },
                {
                    "start": 1229,
                    "end": 1372
                },
                {
                    "start": 1372,
                    "end": 1474
                },
                {
                    "start": 1474,
                    "end": 1545
                },
                {
                    "start": 1545,
                    "end": 1622
                },
                {
                    "start": 1624,
                    "end": 1738
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.405517578125
        },
        {
            "corpus_id": "251881442",
            "title": "Why is the video analytics accuracy fluctuating, and what can we do about it?",
            "text": "In general, we use softmax confidence output as the correctness probability estimate of the prediction. However, many of these neural networks are poorly calibrated [16,32]. The uncertainty in softmax confidence scores from poorly calibrated NN can potentially worsen the robustness of video analytics performance. \n\nTo mitigate this, we can employ several post-hoc methods on SOTA models to improve the softmax estimates, e.g., via averaging predictions obtained from bag of models (e.g., detectors, classifiers) [25], platt scaling [16], isotonic regression [34], etc. We can also adapt the confidence threshold to filter out the low-confidence mispredictions. This confidence threshold value can be adapted based on the difficulty level to detect in a certain frame. We leave the investigation of neural network calibration and confidence threshold adaptation as future work.",
            "score": 0.3870749592190254,
            "section_title": "Calibrating softmax confidence scores",
            "char_start_offset": 28079,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 314
                },
                {
                    "start": 317,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 878
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 169,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 169,
                    "end": 172,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 534,
                    "end": 538,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 560,
                    "end": 564,
                    "matchedPaperCorpusId": "234349945"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1458740234375
        },
        {
            "corpus_id": "270737848",
            "title": "On Calibration of Speech Classification Models: Insights from Energy-Based Model Investigations",
            "text": "The results are summarized in Table 3 alongside ECEs.It is evident that these two post-processing calibrators provide limited improvement in model calibration.This restricted effectiveness may stem from potential disparities between the auxiliary data and the target distribution, resulting in suboptimal calibration adjustments.In contrast, EBMs consistently demonstrate superior performance, resulting in significant reductions in the ECE, without the requirement of supplementary training data.Why softmax-based models are poorly calibrated?As illustrated in Figure 2, softmax models prioritize optimizing accuracy over achieving minimizing NLL.This observation aligns with the findings in [3], which suggest that modern neural networks can overfit to NLL without overfitting accuracy.This phenomenon indicates that while softmax models may achieve high classification accuracy, they may not necessarily provide well-calibrated probability estimates.In other words, the pursuit of higher accuracy values can sometimes come at the cost of the model's ability to accurately reflect confidence in its predictions, thereby compromising calibration quality.In contrast, EBMs excel in reducing NLL while maintaining high accuracy.Despite slower convergence during EBM training, they achieve lower NLL and higher confidence levels.\n\nConfidence distribution.We analyze the problem of model overconfidence by visualizing the confidence distribution in Figure 3.It demonstrates a significant prevalence of excessively high confidence levels for incorrect predictions across three speech tasks, resulting in unreliable confidence estimates for softmax-based models.For example, within the confidence range of 0.9-1, the softmax-based model for age recognition yields 300 misclassified samples, whereas EBMs show only 10 misclassifications.It is noteworthy that EBMs exhibit a reduction in the confidence range of incorrect predictions across the three speech tasks, with accurate predictions predominantly falling within higher confidence intervals, leading to reliable confidence.",
            "score": 0.38704866882156796,
            "section_title": "Performance analysis and discussion",
            "char_start_offset": 10942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 53,
                    "end": 159
                },
                {
                    "start": 159,
                    "end": 329
                },
                {
                    "start": 329,
                    "end": 497
                },
                {
                    "start": 497,
                    "end": 544
                },
                {
                    "start": 544,
                    "end": 648
                },
                {
                    "start": 648,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 953
                },
                {
                    "start": 953,
                    "end": 1155
                },
                {
                    "start": 1155,
                    "end": 1227
                },
                {
                    "start": 1227,
                    "end": 1327
                },
                {
                    "start": 1329,
                    "end": 1353
                },
                {
                    "start": 1353,
                    "end": 1455
                },
                {
                    "start": 1455,
                    "end": 1657
                },
                {
                    "start": 1657,
                    "end": 1831
                },
                {
                    "start": 1831,
                    "end": 2073
                }
            ],
            "ref_mentions": [
                {
                    "start": 693,
                    "end": 696,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75146484375
        },
        {
            "corpus_id": "268510586",
            "title": "Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection",
            "text": "Confidence Calibration of LLM. Confidence calibration has been previously studies in neural networks (Guo et al., 2017) and applied in NLP models (Desai and Durrett, 2020;Dan and Roth, 2021;Hu et al., 2023). After the advent of LLM, many confidence calibration methods utilize the output token probability, such as semantic uncertainty (Kuhn et al., 2023), temperature scaling (Shih et al., 2023), entropy-based (Huang et al., 2023c), semantic significance (Duan et al., 2023), and fine-tuning for calibration (Jiang et al., 2021;Lin et al., 2022a). Zhang et al. (2023b) also employ model ensemble for calibration. Our research is orthogonal to them, since we focus on black-box API LLM itself. Other recent work suitable for black-box LLM includes fidelity elicitation (Zhang et al., 2024), fact elicitation (Zhao et al., 2024) and perturbation generation (Gao et al., 2024). \n\nSelf-Evaluation of LLM. LLM self-evaluation often focuses on specific domains, e.g., code generation (Zhou et al., 2023), natural language generation (Lin et al., 2023) and fact checking (Manakul et al., 2023). The general methods include P(True) (Kadavath et al., 2022) and directly asking LLM (Li et al., 2024b). Feng et al. ( 2024) also performs answer reflection and employs model collaboration, yet they still focus on answers generated by LLM. Note that self-detect (Zhao et al., 2023c) is also a general self-evaluation method following the three-step confidence calibration pipeline.",
            "score": 0.38704866882156796,
            "section_title": "Related Work",
            "char_start_offset": 16210,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 876
                },
                {
                    "start": 879,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1470
                }
            ],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 119,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 171,
                    "end": 190,
                    "matchedPaperCorpusId": "244119588"
                },
                {
                    "start": 336,
                    "end": 355,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 510,
                    "end": 530,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 530,
                    "end": 548,
                    "matchedPaperCorpusId": "249191391"
                },
                {
                    "start": 550,
                    "end": 570,
                    "matchedPaperCorpusId": "265019095"
                },
                {
                    "start": 1066,
                    "end": 1088,
                    "matchedPaperCorpusId": "257557820"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5517578125
        },
        {
            "corpus_id": "273345872",
            "title": "LLM$\\times$MapReduce: Simplified Long-Sequence Processing using Large Language Models",
            "text": "When long sequences are split into multiple chunks, conflicts may arise due to incomplete information. The aforementioned information structure provides a confidence score for the answer from each chunk. During the collapse and reduce stages, confidence scores are crucial for guiding the merging of information and generating both the rationale and the final answer. \n\nTo make the confidence scores across different chunks comparable, we propose to calibrate them through in-context learning, without adjusting model parameters. Specifically, we provide confidence estimation principles alongside a typical example for different levels of confidence score. By referring to the principles and the examples, the model is expected to apply a consistent criterion when processing different chunks. Figure 2 provides an example of the calibration prompt. We can customize different calibration prompts for various tasks. Claims fully supported by the provided text are assigned high confidence, while those inferred by the model receive medium confidence. Claims not related to the provided text are assigned low confidence. Experiments in Section 3.3 demonstrate the necessity of the proposed in-context confidence calibration mechanism. \n\nAssign a confidence score (out of 5) to your answer based on the completeness and reliability of the extracted information and your rationale. The following is some assigning scoring cases: <Text: [ Jerry is 18 years old this year. He can swim and wants to be an athlete. ]. Examples of confidence estimation: [ Jerry can swim, 5 points; Jerry will become an athlete in the future, 3.5 points; Jerry will become a swimming athlete in the future, 3 points; Jerry is strong, 3 points; Jerry can play chess, 0 points; Jerry likes talking, 0 points ] >. We employ vLLM4 for model inference, and the decoding temperature is set to 0.7. \n\nEvaluation We evaluate the performance of the involved models and methods on In-finiteBench (Zhang et al., 2024a), where the average input length exceeds 100K tokens. This benchmark assesses the long-text capabilities of LLMs across several dimensions, including longrange retrieval, language comprehension, code understanding, and mathematical problem-solving. We exclude the subsets Code.Run and Math.Calc, as nearly all models achieve less than 5% accuracy on these tasks, making it difficult to differentiate performance among the models.",
            "score": 0.3866801894713876,
            "section_title": "In-Context Confidence Calibration",
            "char_start_offset": 13184,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 367
                },
                {
                    "start": 370,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1234
                },
                {
                    "start": 1237,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1867
                },
                {
                    "start": 1870,
                    "end": 2036
                },
                {
                    "start": 2037,
                    "end": 2231
                },
                {
                    "start": 2232,
                    "end": 2412
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7255859375
        },
        {
            "corpus_id": "254044617",
            "title": "Class Adaptive Network Calibration",
            "text": "Another popular direction is to directly address miscalibration during training by introducing an additional penalty or supervision regarding model calibration with the standard training loss. In [18], the authors introduced a trainable calibration measure based on RKHS kernels, while [16] proposed a differential calibration loss based on a soft version of the binning operation in the ECE metric. In [5], two types of binary pairwise calibration constraints were proposed as additional penalty terms during training. Other methods try to decrease over-fitting on the cross-entropy loss, which has been demonstrated to be the main reason of over-confidence [11,31]. In [38] an explicit confidence penalty (ECP) is proposed to maximize the entropy and reduce over-fitting, while Label Smoothing [42] has also been shown to implicitly improve the calibration [32] by softening the hard one-hot targets in the cross-entropy. The Focal Loss [21], originally proposed to tackle class imbalance, can also be effective for calibration [31], as it implicitly minimizes the Kullback-Leibler (KL) divergence between the uniform distribution and the network softmax probabilities, thereby increasing the entropy of the predictions. As an extension the Sample-Dependent Focal Loss (FLSD) was also proposed in [31] to further boost calibration performance.",
            "score": 0.3865832140409814,
            "section_title": "Learning-based methods",
            "char_start_offset": 9247,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 196,
                    "end": 200,
                    "matchedPaperCorpusId": "49314079"
                },
                {
                    "start": 286,
                    "end": 290,
                    "matchedPaperCorpusId": "236772324"
                },
                {
                    "start": 403,
                    "end": 406,
                    "matchedPaperCorpusId": "249948182"
                },
                {
                    "start": 659,
                    "end": 663,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 663,
                    "end": 666,
                    "matchedPaperCorpusId": "211252346"
                },
                {
                    "start": 671,
                    "end": 675,
                    "matchedPaperCorpusId": "9545399"
                },
                {
                    "start": 796,
                    "end": 800,
                    "matchedPaperCorpusId": "206593880"
                },
                {
                    "start": 939,
                    "end": 943,
                    "matchedPaperCorpusId": "47252984"
                },
                {
                    "start": 1030,
                    "end": 1034,
                    "matchedPaperCorpusId": "211252346"
                },
                {
                    "start": 1299,
                    "end": 1303,
                    "matchedPaperCorpusId": "211252346"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62060546875
        },
        {
            "corpus_id": "226283839",
            "title": "Probabilistic Extension of Precision, Recall, and F1 Score for More Thorough Evaluation of Classification Models",
            "text": "It was demonstrated empirically by Wang et al. (2013) and Dinga et al. (2019) that probabilistic metrics are more reliable in discriminating between models, since they leverage the most information from the model's output. This does not necessarily make them better metrics, as we stated earlier how modeling decisions are closely tied to operating conditions, but allows them to be more data-efficient (require less data to reach statistically significant results). Recent developments in Transfer Learning (Pan and Yang, 2010;Conneau et al., 2020) demonstrated impressive ability to learn from small training sets (often referred to as Few-Shot Learning), showing a wide NLP community interest in improving data-efficiency during model training, but we have not found any publication related to data-efficient model testing. Usually the lack of training data would also imply a lack of test data, as they would be caused by the same underlying factor (expensive data collection and/or labelling, low-resource language), which highlights the value in developing ways to compare models with minimal test data requirements. As part of our investigation in this subject, we empirically show that our proposed metrics are more data efficient than their threshold-based counterparts, as they allow for modeling decisions with smaller test-sets. \n\nOn a side note, some of the cases in which model confidence assignments are used in production require the scores to be probabilistically calibrated, as they are interpreted by the users as probabilities of events happening (e.g. disease prevention, weather forecasts). Probabilistic calibration refers to the reliability of the scores in reflecting the true probability of the predictions being correct (e.g. if a calibrated model predicted in n cases that event X will happen with probability p, then event X should happen in approximately p*n of those cases). The proposed metrics do not evaluate for probabilistic calibration. For such use-cases we suggest the combined usage of a probability calibration measure (e.g. the Expected Calibration Error (Guo et al., 2017), the reliability component of Brierscore (Murphy, 1973)) along with the proposed metrics, for a thorough evaluation of both performance and calibration.",
            "score": 0.38656199707081257,
            "section_title": "Error correction:",
            "char_start_offset": 6116,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1340
                },
                {
                    "start": 1343,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2268
                }
            ],
            "ref_mentions": [
                {
                    "start": 35,
                    "end": 53,
                    "matchedPaperCorpusId": "18496289"
                },
                {
                    "start": 58,
                    "end": 77,
                    "matchedPaperCorpusId": "202017839"
                },
                {
                    "start": 508,
                    "end": 528,
                    "matchedPaperCorpusId": "740063"
                },
                {
                    "start": 528,
                    "end": 549,
                    "matchedPaperCorpusId": "207880568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.609375
        },
        {
            "corpus_id": "267760274",
            "title": "Probabilities of Chat LLMs Are Miscalibrated but Still Predict Correctness on Multiple-Choice Q&A",
            "text": "For each combination of LLM, classifier, and c \u2208 {1, 2}, Table 2 reports the scores obtained by the base LLM and by our method on the test set, where the our method used the threshold determined by training set. Figure 9 shows each model's score across the entire range of possible thresholds. \n\nOur method outperformed or matched the base LLM in nearly all conditions and substantially outperformed the base LLM on the conservative score metric. (The sole exception is the Llama 3.1 8B, Max Logit, on the balanced score metric, which exhibits a tiny drop in score of 0.3). As expected, models with low initial scores exhibited the most dramatic improvements. For example, any model with a negative initial score can trivially improve to 0 by abstaining on every question. (Table 5 shows the abstention frequency for each modelclassifier pair.) More generally, the higher the fraction of correct answers, the more likely the model is to accidentally abstain on a correct answer. As a result, it is unsurprising that models with high initial scores showed only modest improvements on the balanced score metric. \n\nThese results show how the uncertainty signals from softmax probabilities and/or logits can be leveraged to improve performance on practical language tasks. Further details on our Q&A-withabstention results can be found in Appendix A.3.",
            "score": 0.38656145345580045,
            "section_title": "Results",
            "char_start_offset": 23902,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 293
                },
                {
                    "start": 296,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1109
                },
                {
                    "start": 1112,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1348
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.414306640625
        },
        {
            "corpus_id": "258378137",
            "title": "Triple-Hybrid Energy-based Model Makes Better Calibrated Natural Language Understanding Models",
            "text": "Since many industrial applications involve safety -critical domains such as healthcare (Li et al., 2019;Blinov et al., 2020;Li et al., 2020;Rasmy et al., 2021;Sarabadani, 2019), anticipating credit card defaults (Sun and Vasarhalyi, 2021) and selfdriving (Khaitan et al., 2021), it's essential for machine learning systems to provide not only accurate but also well-calibrated predictions (Li et al., 2019), which can help to decide whether it can be trusted. \n\nHowever, models achieving high accuracy usually lead to overconfidence and miscalibration (Guo et al., 2017;Thulasidasan et al., 2019;Ovadia et al., 2019). This motivates an interesting and important area that attempts to achieve a better trade-off between accuracy and calibration. In addition to ID calibration, it's more important for machine learning models to produce high uncertainty when OOD data is observed, rather than to produce wrong yet wildly confident predictions. Related works. To overcome the problem of miscalibration, numerous methods have been proposed. The natural way is post-hoc calibration that transforms the output of the original network into calibrated confidence scores while maintaining the network's accuracy (Guo et al., 2017;Rahimi et al., 2020;Jung et al., 2020). The second method to mitigate miscalibration is to add regularizations during training such as label smoothing (Wang et al., 2020), Mixup (Zhang et al., 2018). Desai and Durrett (2020) and Kong et al. (2020) further conveys that the aforementioned methods can be applied to improve the calibration of pre-trained language models on NLU tasks. The third way is to design a specific loss function to minimize the discrepancy between accuracy and confidence. For example, Kong et al. (2020) lately propose the ID and OOD regularizer to leverage the relationship between accuracy and uncertainty, and it obtains a significant improvement over previous methods in ID calibration and OOD detection. Energy-based Models.",
            "score": 0.3861705174407548,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 459
                },
                {
                    "start": 462,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 1974
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 104,
                    "matchedPaperCorpusId": "202567532"
                },
                {
                    "start": 104,
                    "end": 124,
                    "matchedPaperCorpusId": "220525684"
                },
                {
                    "start": 124,
                    "end": 140,
                    "matchedPaperCorpusId": "212622878"
                },
                {
                    "start": 140,
                    "end": 159,
                    "matchedPaperCorpusId": "218889776"
                },
                {
                    "start": 159,
                    "end": 176,
                    "matchedPaperCorpusId": "203396356"
                },
                {
                    "start": 212,
                    "end": 238,
                    "matchedPaperCorpusId": "56917503"
                },
                {
                    "start": 255,
                    "end": 277,
                    "matchedPaperCorpusId": "224814527"
                },
                {
                    "start": 389,
                    "end": 406,
                    "matchedPaperCorpusId": "202567532"
                },
                {
                    "start": 552,
                    "end": 570,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 570,
                    "end": 596,
                    "matchedPaperCorpusId": "166228660"
                },
                {
                    "start": 596,
                    "end": 616,
                    "matchedPaperCorpusId": "174803437"
                },
                {
                    "start": 1203,
                    "end": 1221,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1221,
                    "end": 1241,
                    "matchedPaperCorpusId": "212725550"
                },
                {
                    "start": 1241,
                    "end": 1259,
                    "matchedPaperCorpusId": "216867328"
                },
                {
                    "start": 1372,
                    "end": 1391,
                    "matchedPaperCorpusId": "218487046"
                },
                {
                    "start": 1399,
                    "end": 1419,
                    "matchedPaperCorpusId": "3162051"
                },
                {
                    "start": 1421,
                    "end": 1445,
                    "matchedPaperCorpusId": "212747810"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72119140625
        },
        {
            "corpus_id": "233296030",
            "title": "Learning to Cascade: Confidence Calibration for Improving the Accuracy and Computational Cost of Cascade Inference Systems",
            "text": "Confidence Calibration Guo et al. (Guo et al. 2017) point out that recent deep learning models are overconfident. Starting with this paper, several confidence calibration methods are proposed (Zhang, Dalca, and Sabuncu 2019;Thulasidasan et al. 2019;Wan et al. 2018;Kull et al. 2019). These studies conduct evaluations using some metrics such as Expected Calibration Error (ECE). As these metrics are calculated solely from the prediction results and labels, it does not evaluate the performance of systems that use confidence scores. It is not clear whether these calibration methods can improve the performance of systems such as a decision-making system and cascade inference systems.\n\nIndividually optimized models may not be optimal for systems that use confidence scores. Bansal et al. (Bansal et al. 2020) propose a method for training models to optimize the performance of the entire decision-making system. They assume 100% human accuracy and evaluate only simple machine learning models and binary classification tasks. In practice, human accuracy is less than 100% for the multi-classification tasks commonly used in deep learning (e.g., 94% for CIFAR-10 (Krizhevsky, Hinton et al. 2009)).\n\nSeveral confidence calibration methods for out-ofdistribution (OoD) detection have also been studied (Liang, Li, and Srikant 2018;Hsu et al. 2020;Chen et al. 2020;DeVries and Taylor 2018;). These studies calibrate confidence scores to detect OoD data. Although the problem setting is different from this paper, the perspective of confidence calibration to improve the performance of the task (or system) is the same.",
            "score": 0.3860696003027465,
            "section_title": "Related Works",
            "char_start_offset": 5210,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 50,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 224,
                    "end": 249,
                    "matchedPaperCorpusId": "166228660"
                },
                {
                    "start": 249,
                    "end": 265,
                    "matchedPaperCorpusId": "52286225"
                },
                {
                    "start": 265,
                    "end": 282,
                    "matchedPaperCorpusId": "202773833"
                },
                {
                    "start": 1302,
                    "end": 1331,
                    "matchedPaperCorpusId": "3526391"
                },
                {
                    "start": 1331,
                    "end": 1347,
                    "matchedPaperCorpusId": "211506988"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36669921875
        },
        {
            "corpus_id": "273638092",
            "title": "LLM Tree Search",
            "text": "Model Confidence: This approach relies on the language model's own probability estimates. The confidence score can be calculated using methods such as the sum of log probabilities (similar to beam search) [7], geometric mean, or average of the probabilities. However, this approach has limitations. High confidence scores from the model do not always correlate with the correctness of the completion. For instance, run-away completions with repeated tokens or characters can result in high confidence but incorrect or nonsensical outputs. \n\nThe trained evaluator model approach addresses the limitations of relying solely on model confidence for determining the quality of text completions. It involves training a separate evaluator model on a dataset comprising humangenerated text, randomly sampled completions, and LLM-generated completions. The evaluator model learns to predict whether a given completion is human-generated or non-human [4]. During the search process, each completion is fed to the trained evaluator model, which assigns a confidence score based on its prediction of the completion's human-likeness. This confidence score serves as a proxy for the quality of the completion and is used to guide the search process, prioritizing the exploration of promising paths. By leveraging the evaluator model's ability to distinguish between human-like and non-humanlike text, this approach provides more accurate confidence scores compared to model confidence alone, ultimately improving the quality of the generated text.",
            "score": 0.38575775003614277,
            "section_title": "How can we determine a confidence score for a given leaf?",
            "char_start_offset": 2407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 538
                },
                {
                    "start": 541,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1534
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7470703125
        },
        {
            "corpus_id": "272987064",
            "title": "Calibrating Language Models with Adaptive Temperature Scaling",
            "text": "Large language models (LLMs) have become a cornerstone of modern artificial intelligence, offering impressive capabilities in natural language processing tasks. However, the reliability of LLMs is intertwined with their ability to generate confidence scores that accurately reflect the likelihood of their outputs being correct. This calibration, aligning a model's confidence with its accuracy, is essential, especially when LLMs are deployed in real-world scenarios where decisions based on incorrect outputs can have significant consequences. \n\nWhile unsupervised pre-training methods have shown success in producing well-calibrated LLMs, * Equal contribution. a challenge arises when these models undergo finetuning through reinforcement learning from human feedback (RLHF). While RLHF fine-tuning is effective in enhancing model performance on specific tasks and aligning outputs with human preferences, recent studies indicate a notable degradation in the calibration of LLMs post-RLHF finetuning (Achiam et al., 2023;Tian et al., 2023;Kadavath et al., 2022). This degradation compromises the model's ability to provide reliable confidence scores, an issue that becomes critical when these models are applied to tasks requiring high levels of trust and accuracy. An important question arises: how can we maintain the performance gains achieved through RLHF fine-tuning while ensuring that the model's confidence scores remain reliable? \n\nTo address this challenge, our work introduces Adaptive Temperature Scaling (ATS), a post-hoc calibration technique that predicts a temperature scaling parameter for each token prediction based on a language model's hidden features. Basic temperature scaling is a widely-used calibration method that applies a single temperature parameter across all outputs of a model. This technique, while effective in some contexts, assumes uniform calibration needs across all inputs, which is often not the case for complex models like LLMs. ATS, in contrast, predicts a unique temperature scaling parameter for each set of token predictions. This input-specific approach allows ATS to refine the calibration process, addressing the varying degrees of calibration shift that can occur after RLHF fine-tuning.",
            "score": 0.38573729386420696,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 545
                },
                {
                    "start": 548,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1441
                },
                {
                    "start": 1444,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2241
                }
            ],
            "ref_mentions": [
                {
                    "start": 1024,
                    "end": 1042,
                    "matchedPaperCorpusId": "258865733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93994140625
        },
        {
            "corpus_id": "263829211",
            "title": "Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU",
            "text": "Few shot performance Providing several questions and the answer key in prompts has been re- ported to improve model performance (Karimi Mahabadi et al., 2022;Hendrycks et al., 2021). We run similar experiments with our top-4 best opensource models and observe mixed outcomes in Figure 7. 9 Few-shot inference does not yield improvements in instruction-tuned models like mT0 and BLOOMZ, as evidenced by a decrease in accuracy. \n\nIn contrast, the pure LLMs Falcon and LLaMA show better performance with few-shot inference compared to zero-shot. These findings align with those of Liu et al. (2023b); Li et al. (2023b), where few-shot prompts may lead to unnatural inferences for instruction-tuned models. \n\nModel confidence Given the top three models in Table 3, we assess whether their confidence predictions (i.e. the predicted likelihood of the predicted answer being correct) corresponds to the actual accuracy across 64 tasks. This uncertainty calibration gives us hints about the model's reliability and how to use them appropriately in real-world settings. For mT0 and BLOOMZ, the confidence score is determined through softmax normalization over probabilities of the multiple-choice options. For GPT-3.5, we adopt the approach described by Si et al. (2022); Wang et al. (2022), using a hightemperature value (0.7) during decoding. For each question, we generate n different outputs and measure self-consistency. The probability of a multiplechoice option is calculated based on the output frequency. In this experiment, we use n = 7, and choose the most frequently-occurring answer as the final prediction. \n\nWe average the confidence scores across the 64 tasks, and display the calibration of mT0, BLOOMZ, and GPT-3.5 in Figure 8. We observe that all three models are well-calibrated, with correlation scores of r > 0.85. \n\nAdditionally, we examine the relationship between confidence scores and question length, as depicted in Figure 9. We found a very weak correlation for both mT0 and BLOOMZ.",
            "score": 0.3853037015902381,
            "section_title": "Analysis",
            "char_start_offset": 17201,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 425
                },
                {
                    "start": 428,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1612
                },
                {
                    "start": 1615,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1828
                },
                {
                    "start": 1831,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2002
                }
            ],
            "ref_mentions": [
                {
                    "start": 128,
                    "end": 158,
                    "matchedPaperCorpusId": "247939864"
                },
                {
                    "start": 158,
                    "end": 181,
                    "matchedPaperCorpusId": "221516475"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3818359375
        },
        {
            "corpus_id": "263671892",
            "title": "A Formalism and Approach for Improving Robustness of Large Language Models Using Risk-Adjusted Confidence Scores",
            "text": "This paper proposes such a general formalism, including identifying two different risk categories that are relevant to LLMs, and novel metrics for measuring risk. \n\nInitial research on LLMs' 'self-understanding' of their own uncertainty, especially in the deep learning literature, has predominantly relied on interpreting raw softmax probabilities of the final output layer as 'confidence' scores (Vasudevan, Sethy, and Ghias 2019). While studies such as (Guo et al. 2017) have flagged these scores as potentially misleading and not genuinely capturing the model's true uncertainty, (Kadavath et al. 2022) highlighted that genera-tive LLMs exhibit commendable calibration properties facing certain situations. These models can accurately predict which questions they will be able to answer correctly on diverse NLI tasks based on their confidence scores. Nevertheless, a quantitative assessment of risk, even for such models, has been lacking. (Jiang et al. 2021) suggest that, when faced with uncertain situations, LLMs can sometimes be poorly calibrated, with the confidence score estimation barely being correlated with the likelihood of the output being correct. Building on these observations, studies by (Jagannatha and Yu 2020;Jiang et al. 2021;Kuhn, Gal, and Farquhar 2022) have endeavored to re-calibrate confidence, using mechanisms like entropy, or by crafting binary classifiers based on the given confidence scores. More recently, (Zhou, Jurafsky, and Hashimoto 2023) focus on generative models, probing linguistic hedge markers in the models' outputs to evaluate their ability to discern uncertain situations. A further strand of research, such as by (Yin et al. 2023), proposes benchmarks aiming to spotlight areas of knowledge where LLMs grapple with uncertainty. Concurrently, efforts by (Collins et al. 2023) seek to minimize uncertainty in human-AI contexts by addressing risks originating from human errors. \n\nIn our work, we propose a novel re-adjusted confidence calibration for both generative and discriminative LLMs, aimed at discerning uncertain situations. We utilize a binary classifier calibration, previously shown to have exceptional performance (Kamath, Jia, and Liang 2020;Jiang et al. 2021), as an experimental baseline.",
            "score": 0.3851989224831885,
            "section_title": "Related Work",
            "char_start_offset": 5060,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 165,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1928
                },
                {
                    "start": 1931,
                    "end": 2084
                },
                {
                    "start": 2085,
                    "end": 2255
                }
            ],
            "ref_mentions": [
                {
                    "start": 398,
                    "end": 432,
                    "matchedPaperCorpusId": "155100063"
                },
                {
                    "start": 456,
                    "end": 473,
                    "matchedPaperCorpusId": "220250257"
                },
                {
                    "start": 945,
                    "end": 964,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 1211,
                    "end": 1235,
                    "matchedPaperCorpusId": "215548393"
                },
                {
                    "start": 1235,
                    "end": 1253,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 1253,
                    "end": 1282,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 1666,
                    "end": 1683,
                    "matchedPaperCorpusId": "258959258"
                },
                {
                    "start": 1806,
                    "end": 1826,
                    "matchedPaperCorpusId": "257687699"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79345703125
        },
        {
            "corpus_id": "273501766",
            "title": "Accounting for Sycophancy in Language Model Uncertainty Estimation",
            "text": "Objective and Evaluation We assume a setting where a model (and user) are faced with a problem statement q that has some ground-truth answer a * . Example problem domains are given in \u00a7 2.2. In uncertainty estimation, the goal is to predict probability of correctness for the question q, given a model answer a. Commonly, uncertainty estimates are evaluated as probabilistic classifiers (Kadavath et al., 2022;Tian et al., 2023;Sicilia et al., 2024), which accounts for the interpretation of the estimate as a signal of model confidence (Guo et al., 2017). In this setting, an estimate Pqa for the probability of correctness is evaluated by a proper scoring rule (Br\u00f6cker, 2009), which ranks estimates based on how well they match the true probability of correctness. Among these, we use the Brier Score, averaged over questions: \n\nwhere ACC qa is a binary indicator of model correctness. Since squared probabilities are not easy to interpret, we also report the Brier Skill Score: \n\nwhere \u00b5 is the average accuracy. Brier Score represents a mean squared error for the probability estimate Pqa in predicting correctness, while Brier Skill Score represents a percent of variance in correctness explained by the prediction Pqa . It measures the information gain of the uncertainty estimate (relative to \u00b5) as a predictor for correctness. \n\nMethodology Methods for language model uncertainty estimation tend to follow a consistent format (Guo et al., 2017;Kadavath et al., 2022;Mielke et al., 2022;Tian et al., 2023): \n\n1. collect derivatives from the model, which correlate with answer uncertainty; then, 2. transform the value of the derivative to an actual probability of correctness. Given a floating point model derivative \u1e90qa , Platt Scaling (Platt et al., 1999) provides an effective strategy to produce an estimate Pqa . It assumes \n\nselecting parameters \u03b1, \u03b2 using MLE with a small amount of data (e.g., n < 100). Sicilia et al. (2024) show this strategy generalizes (or beats) other similar estimation techniques for language models.",
            "score": 0.3848765807742557,
            "section_title": "Uncertainty Estimation (UE)",
            "char_start_offset": 3982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 829
                },
                {
                    "start": 832,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 981
                },
                {
                    "start": 984,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1335
                },
                {
                    "start": 1338,
                    "end": 1514
                },
                {
                    "start": 1517,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1836
                },
                {
                    "start": 1839,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2040
                }
            ],
            "ref_mentions": [
                {
                    "start": 537,
                    "end": 555,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 663,
                    "end": 678,
                    "matchedPaperCorpusId": "15880012"
                },
                {
                    "start": 1435,
                    "end": 1453,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1475,
                    "end": 1495,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 1745,
                    "end": 1765,
                    "matchedPaperCorpusId": "56563878"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.363037109375
        },
        {
            "corpus_id": "247778884",
            "title": "Towards Improving Selective Prediction Ability of NLP Systems",
            "text": "we annotate instances of a held-out dataset conditioned on the model's predictive correctness (computed using difficulty score and prediction confidence) and then train a calibrator using these instances. This annotation score represents the likelihood of correctness of the model's prediction. Finally, the trained calibrator predicts this likelihood value for test instances and is used as the confidence estimator for selective prediction.\n\nTo evaluate the efficacy of our method, we conduct comprehensive experiments in In-Domain (IID) and Out-of-Domain (OOD) settings for Natural Language Inference (NLI) and Duplicate Detection (DD) tasks. We also compare its performance with existing calibration techniques. On the NLI task, our method achieves 15.81% and 5.64% im-provement on AUC of risk-coverage curve over MaxProb in IID and OOD setting respectively. Furthermore, on the DD task, it achieves 6.19% and 13.9% improvement in IID and OOD setting respectively. Finally, we hope that our work will facilitate development of more robust and reliable AI systems making their wide adoption in real-world applications possible.",
            "score": 0.3842302801836498,
            "section_title": "Introduction",
            "char_start_offset": 2046,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83544921875
        },
        {
            "corpus_id": "276407550",
            "title": "A Study of Calibration as a Measurement of Trustworthiness of Large Language Models in Biomedical Research",
            "text": "Objectives To assess the calibration of 9 large language models (LLMs) within biomedical natural language processing (BioNLP) tasks, furthering understanding of trustworthiness and reliability in real-world settings. Materials and Methods For each LLM, we collected responses and corresponding confidence scores for all 13 datasets (grouped into 6 tasks) of the Biomedical Language Understanding & Reasoning Benchmark (BLURB). Confidence scores were assigned using 3 strategies: Verbal, Self-consistency, Hybrid. For evaluation, we introduced Flex-ECE (Flexible Expected Calibration Error): a novel adaptation of ECE that accounts for partial correctness in model responses, allowing for a more realistic assessment of calibration in language-based settings. Two post-hoc calibration techniques\u2014isotonic regression and histogram binning\u2014were evaluated. Results Across tasks, mean calibration ranged from 23.9% (Population-Intervention-Comparison-Outcome extraction) to 46.6% (Relation Extraction). Across LLMs, Medicine-Llama3-8B had the best mean overall calibration (29.8%); Flan-T5-XXL had the highest ranking on 5/13 datasets. Across strategies, self-consistency (mean: 27.3%) had better calibration than Verbal (mean: 42.0%) and Hybrid (mean: 44.2%). Post-hoc methods substantially improved calibration, with best mean calibrated Flex-ECEs ranging from 0.1% to 4.1%. Discussion The poor out-of-the-box calibration of LLMs poses a risk to trustworthy deployment of such models in real-world BioNLP applications. Calibration can be improved post-hoc and is a recommended practice. Non-binary metrics for LLM evaluation such as Flex-ECE provide a more realistic assessment of trustworthiness of LLMs, and indeed any model that can be partially right/wrong. Conclusion This study shows that out-of-the-box calibration of LLMs is very poor, but traditional post-hoc calibration techniques are useful to calibrate LLMs.",
            "score": 0.3841778028017505,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7861328125
        },
        {
            "corpus_id": "252735023",
            "title": "To Softmax, or not to Softmax: that is the question when applying Active Learning for Transformer Models",
            "text": "Despite achieving state-of-the-art results in nearly all Natural Language Processing applications, fine-tuning Transformer-based language models still requires a significant amount of labeled data to work. A well known technique to reduce the amount of human effort in acquiring a labeled dataset is \\textit{Active Learning} (AL): an iterative process in which only the minimal amount of samples is labeled. AL strategies require access to a quantified confidence measure of the model predictions. A common choice is the softmax activation function for the final layer. As the softmax function provides misleading probabilities, this paper compares eight alternatives on seven datasets. Our almost paradoxical finding is that most of the methods are too good at identifying the true most uncertain samples (outliers), and that labeling therefore exclusively outliers results in worse performance. As a heuristic we propose to systematically ignore samples, which results in improvements of various methods compared to the softmax function.",
            "score": 0.3840820884712089,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67041015625
        },
        {
            "corpus_id": "248863170",
            "title": "A Birds Eye View on Knowledge Graph Embeddings, Software Libraries, Applications and Challenges",
            "text": "other takes into account the sigmoid function that learns scalar weights to yield a confidence score for each example. It might work better for smaller datasets. Model Calibration has a few advantages. According to the framework's point of view, language processing pipelines that incorporate KG can depend on calibrated scores to determine which KGE forecasts to trust. According to a research point of view, incorporating calibration helps determine the output predictions for acknowledging KGE models. As given in figure 18, we can easily analyze how a very much calibrated model resembles. A straight spotted line addresses an ideal calibrated model though the red shaded line addresses an uncalibrated model. Models after calibration procedures work impressively better compared to uncalibrated strategies [44]. Nonetheless, two primary issues are confining the viability of the probabilistic calibration techniques for link prediction tasks [183]. Firstly, the lack of suitable confidence estimation. The evaluation techniques [] revolve around the ideal score and contrast it with different scores in the score sequence. High ideal scores lead to high certainty yet do not accomplish a similar level of precision since the score of each triple shows its relative ordering among other triples in a single prediction. Secondly, the inconsistent and unreliable calibration metrics. Expected Calibration Error (ECE) [184] is ordinarily used to assess the impact of calibration, yet is not appropriate for downstream tasks such as link prediction. Benefitted from a causal inference analysis, Kai Wang et al. [45] proposed a novel neighborhood intervention consistency (NIC) method that can effectively intercede the scoring cycle of KGE models. In particular, it creates a progression of neighborhood vectors for an input element by changing the entity vector in various dimensions and inspecting whether the model's output changes or matches the initial one. On this premise, the authors also designed neighborhood intervention values and a dimension selection system for high-dimensional KGE models to focus on efficiency. However, there is a tradeoff between the number of dimensions incorporated and the accuracy achieved. Selecting a proper neighborhood is very much needed to focus on both efficiency and predictive power.",
            "score": 0.3839167728218912,
            "section_title": "Comparative Analysis",
            "char_start_offset": 86964,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1419,
                    "end": 1424,
                    "matchedPaperCorpusId": "207158152"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58447265625
        },
        {
            "corpus_id": "231632895",
            "title": "Joint Energy-based Model Training for Better Calibrated Natural Language Understanding Models",
            "text": "Calibration refers to how well a classification model's confidence (reflected by its output posterior probability) aligns with its actual accuracy. As deep learning models achieve amazing accuracy in computer vision (He et al., 2015) or natural language processing (NLP) (Liu et al., 2019;Devlin et al., 2018), more research attention has been drawn to the calibration aspect of these models. As shown by Guo et al. (2017), the high accuracy from deep models does not always lead to better calibration. This motivates an important line of works attempting to achieve a better trade-off between accuracy and calibration.\n\nMost existing calibration methods (Guo et al., 2017;Kumar et al., 2019;Zadrozny and Elkan, 2001) generally rescale the posterior distribution predicted from the classifier after training. Such post-processing methods require a held-out development set with a decent number of samples to be Bryan McCann contributed to this work while he was at Salesforce Research.\n\navailable. To overcome this constraint, Jung et al.\n\n(2020) uses a penalty term to encourage better calibration during training.\n\nIn another line of work, Grathwohl et al. (2019) shows that one can jointly train an energy-based model (EBM) during the standard training of a neural classifier. Although calibration is not explicitly addressed during EBM training, the calibration of the resulting model is shown to be greatly improved. Some intuitions of the underlying reasons will be given in Section 2.3. However, the training framework proposed by Grathwohl et al. (2019) is designed for image classifiers, and it can not be readily applied to discrete text data.\n\nIn this work, we propose a framework that uses noise contrastive estimation (NCE) to jointly train an energy-based model during the finetuning of pretrained text encoders (e.g., BERT (Devlin et al., 2018) or Roberta (Liu et al., 2019)) for NLU tasks. We compare several variants of energy functions that can be defined on top of the encoder. Our experiments show that the resulting models achieve competitive calibration results comparing to strong baselines, with little or no loss",
            "score": 0.3835859878641696,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 405,
                    "end": 422,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 655,
                    "end": 673,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 692,
                    "end": 717,
                    "matchedPaperCorpusId": "9594071"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79345703125
        },
        {
            "corpus_id": "264811211",
            "title": "LitCab: Lightweight Language Model Calibration over Short- and Long-form Responses",
            "text": "We compare LITCAB with two popular calibration methods for neural networks on the widely-used Llama2-7B7 , which is one of the most recent open-source LMs: \n\n\u2022 Temperature Scaling (Liang et al., 2018): A temperature constant is used to scale the logits before computing the softmax. We employ gradient descent optimization to search for the optimal temperature on the training set. \u2022 Label Smoothing (Szegedy et al., 2016): As a training-based method, the entire LM is finetuned on training set with label smoothing. To train Llama2-7B, we adopt LoRA (Hu et al., 2022) to enable the fine-tuning process on a single GPU. \n\nAdditionally, we also compare LITCAB with recent confidence estimation methods that are specifically designed for LMs, including: \n\n\u2022 Verbalization prompts the LLM to provide its own confidence in a given output. We directly reuse the prompt provided in Tian et al. (2023). \u2022 P(IK) (Kadavath et al., 2022): A linear layer is stacked on top of the LM last-layer's hidden state that corresponds to the question's last token. The added layer learns to predict whether the model can accurately answer the question. We keep the parameters of the LM fixed and only fine-tune the linear layer. \u2022 Self-Consistency (Tian et al., 2023;Xiong et al., 2023) relies on the hypothesis that confident responses will appear more frequently when sampling from the model. As applying majority voting directly is not straightforward over long-form generations, we rely on a natural language inference model. Details about this method are listed in Appendix C. \n\nWe note that LM confidence estimation methods, i.e. Verbalization, P(IK) and Self-Consistency, can only yield a single unified score for the entire generation, which cannot be used to assess calibration at the claim level since each generation normally contains multiple claims. Therefore, we do not list the results for paragraph-level tasks by these comparison methods. To construct training instances for label smoothing and temperature paragraph-level tasks, we employ the same generation process used for constructing training instances for LITCAB, but only keep the accurate claims.",
            "score": 0.383477256249633,
            "section_title": "BASELINES",
            "char_start_offset": 18925,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 158,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 619
                },
                {
                    "start": 622,
                    "end": 751
                },
                {
                    "start": 754,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1561
                },
                {
                    "start": 1564,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2152
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 200,
                    "matchedPaperCorpusId": "3526391"
                },
                {
                    "start": 400,
                    "end": 422,
                    "matchedPaperCorpusId": "206593880"
                },
                {
                    "start": 551,
                    "end": 568,
                    "matchedPaperCorpusId": "235458009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.794921875
        },
        {
            "corpus_id": "253510101",
            "title": "Calibrated Interpretation: Confidence Estimation in Semantic Parsing",
            "text": "Given the utility of calibrated models in decisionmaking, a large body of research has focused on describing the calibration characteristics of different architectures and models, with some work finding neural networks to be relatively well-calibrated (Niculescu-Mizil and Caruana, 2005;Minderer et al., 2021;Carrell et al., 2022) -including neural encoders pre-trained on text data (Desai and Durrett, 2020) -and other research indicating they are not (Guo et al., 2017;Wang et al., 2020;Si et al., 2022). These mixed results preclude drawing general conclusions about neural models' calibration, and motivate studies like ours documenting the calibration characteristics of standard models. \n\nPast work has examined a variety of classification problems, often focusing on binary or multiclass classification (Naeini et al., 2015;Guo et al., 2017;Minderer et al., 2021;Khojah et al., 2022). Some papers have addressed sequential NLP tasks: e.g. Jagannatha and Yu (2020) address calibration in structured prediction tasks. More related to our sequence generation setting, Kumar and Sarawagi (2019) and Wang et al. (2020) examine calibration in machine translation, both finding models to be over-confident. Measuring calibration in translation tasks is limited by the metrics used, which are noisy proxies for accuracy and have well-documented limitations (Callison-Burch et al., 2006;Mathur et al., 2020). In semantic parsing specifically, past work has focused on improving confidence estimation for certain parsers. Dong et al. (2018) propose a confidence estimation method based on model and input features. Similarly, Chen et al. (2022) introduce an additional confidence-estimation model for semantic parses. In our work, we extract confidence scores from the same model used for parsing, and focus on analyzing calibration rather than improving it.",
            "score": 0.3832814172581209,
            "section_title": "Calibration in NLP",
            "char_start_offset": 5469,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 692
                },
                {
                    "start": 695,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1855
                }
            ],
            "ref_mentions": [
                {
                    "start": 252,
                    "end": 287,
                    "matchedPaperCorpusId": "207158152"
                },
                {
                    "start": 287,
                    "end": 309,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 383,
                    "end": 408,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 453,
                    "end": 471,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 471,
                    "end": 489,
                    "matchedPaperCorpusId": "218487046"
                },
                {
                    "start": 489,
                    "end": 505,
                    "matchedPaperCorpusId": "253098276"
                },
                {
                    "start": 810,
                    "end": 831,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 831,
                    "end": 848,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 848,
                    "end": 870,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 870,
                    "end": 890,
                    "matchedPaperCorpusId": "252847587"
                },
                {
                    "start": 946,
                    "end": 970,
                    "matchedPaperCorpusId": "215548393"
                },
                {
                    "start": 1102,
                    "end": 1120,
                    "matchedPaperCorpusId": "218487046"
                },
                {
                    "start": 1519,
                    "end": 1537,
                    "matchedPaperCorpusId": "13686145"
                },
                {
                    "start": 1623,
                    "end": 1641,
                    "matchedPaperCorpusId": "253098276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.599609375
        },
        {
            "corpus_id": "268723623",
            "title": "Few-Shot Recalibration of Language Models",
            "text": "Calibration is a key tool for knowing when language model predictions can be trusted and when they should abstain or defer to experts.However, calibration on an individual domain can be much worse than the aggregate data distribution (Yu et al., 2022;Hebert-Johnson et al., 2018).In this paper, we show that large language models suffer from the same calibration failure.While LMs appear to be well-calibrated on average, they are significantly miscalibrated in finer-grained settings.\n\nWe study LM calibration for multiclass classification: let x be an input query drawn from some query distribution p(x) and y \u2208 {1, \u2022 \u2022 \u2022 , K} be the output class.Let p LM (y | x) denote the model probability, which is also the model's confidence.Let \u0177 = arg max y p LM (y | x) be the model's prediction, and y * be the ground truth label.",
            "score": 0.3832814172581209,
            "section_title": "The Illusion of LM Calibration",
            "char_start_offset": 4586,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 134,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 371
                },
                {
                    "start": 371,
                    "end": 485
                },
                {
                    "start": 487,
                    "end": 649
                },
                {
                    "start": 649,
                    "end": 733
                },
                {
                    "start": 733,
                    "end": 825
                }
            ],
            "ref_mentions": [
                {
                    "start": 251,
                    "end": 279,
                    "matchedPaperCorpusId": "51880858"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7333984375
        },
        {
            "corpus_id": "249674500",
            "title": "Emergent Abilities of Large Language Models",
            "text": "Finally, an important direction for deployment of language models studies is calibration, which measures whether models can predict which questions they will be able to answer correctly. Kadavath et al. (2022) compared two ways of measuring calibration: a True/False technique, where models first propose answers and then evaluate the probability \"P(True)\" that their answers are correct, and more-standard methods of calibration, which use the probability of the correct answer compared with other answer options. As shown in Figure 3D, the superiority of the True/False technique only emerges when scaled to the largest model scale of \u223c3 \u2022 10 23 training FLOPs (52B parameters).",
            "score": 0.3832814172581209,
            "section_title": "Model calibration.",
            "char_start_offset": 9268,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 680
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43359375
        }
    ],
    "quotes": {
        "cost": 0.19052999999999998,
        "quotes": [
            {
                "idx": 0,
                "key": "[235078802 | Jiang et al. | 2020 | Citations: 436]",
                "snippets": "Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, \"How can we know when language models know, with confidence, the answer to a particular query?\" We examine this question from the point of view of calibration, the property of a probabilistic model's predicted probabilities actually being well correlated with the probabilities of correctness.\n\nTo alleviate this problem, we propose methods to make LMs' confidence scores correlate better with the likelihood of model prediction being correct. We examined both fine-tuning methods that modify LMs' parameters and post-hoc methods that keep LMs fixed and only manipulate the confidence values or inputs. Specifically, we fine-tune the LM using softmax-or marginbased objective functions based on multiple candidate answers. For post-hoc calibration, we examined temperature-based scaling and feature-based decision trees that take prediction probability and input-related features as input and produce calibrated confidence (Jagannatha and Yu, 2020;Desai and Durrett, 2020;Kamath et al., 2020). We also study the sensitivity of LMs' confidence estimation with respect to language variation by paraphrasing candidate answers and augmenting questions using retrieved context.\n\nBoth softmax-based and margin-based fine-tuning, which explicitly compare and adjust the probability of candidate answers, can further improve ECE on multiplechoice datasets. We argue that the softmax-based and margin-based objective functions are better suited for questions with potential candidates.\n\nPost-processing the raw confidence either solely based on confidence or other features is effective across all datasets, which is consistent with the conclusion on other tasks such as structured prediction and natural language inference (Jagannatha and Yu, 2020; Desai and Durrett, 2020). We demonstrate the histogram of confidence before and after applying temperature-based scaling or feature-based decision trees in Figure 2. LMs tend to be over-confident, with most predictions having either extremely high or low confidence. Both methods can successfully re-scale the confidence to reasonable ranges, thus improving the calibration performance.\n\nThe improvement brought by using paraphrasing is significant on multiple-choice datasets, demonstrating that using diverse expressions can indeed improve confidence estimation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, \"How can we know when language models know, with confidence, the answer to a particular query?\" We examine this question from the point of view of calibration, the property of a probabilistic model's predicted probabilities actually being well correlated with the probabilities of correctness.\n\nTo alleviate this problem, we propose methods to make LMs' confidence scores correlate better with the likelihood of model prediction being correct. We examined both fine-tuning methods that modify LMs' parameters and post-hoc methods that keep LMs fixed and only manipulate the confidence values or inputs. Specifically, we fine-tune the LM using softmax-or marginbased objective functions based on multiple candidate answers. For post-hoc calibration, we examined temperature-based scaling and feature-based decision trees that take prediction probability and input-related features as input and produce calibrated confidence (Jagannatha and Yu, 2020;Desai and Durrett, 2020;Kamath et al., 2020). We also study the sensitivity of LMs' confidence estimation with respect to language variation by paraphrasing candidate answers and augmenting questions using retrieved context.\n\nBoth softmax-based and margin-based fine-tuning, which explicitly compare and adjust the probability of candidate answers, can further improve ECE on multiplechoice datasets. We argue that the softmax-based and margin-based objective functions are better suited for questions with potential candidates.\n\nPost-processing the raw confidence either solely based on confidence or other features is effective across all datasets, which is consistent with the conclusion on other tasks such as structured prediction and natural language inference (Jagannatha and Yu, 2020; Desai and Durrett, 2020). We demonstrate the histogram of confidence before and after applying temperature-based scaling or feature-based decision trees in Figure 2. LMs tend to be over-confident, with most predictions having either extremely high or low confidence. Both methods can successfully re-scale the confidence to reasonable ranges, thus improving the calibration performance.\n\nThe improvement brought by using paraphrasing is significant on multiple-choice datasets, demonstrating that using diverse expressions can indeed improve confidence estimation.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[238419596 | Li et al. | 2021 | Citations: 11]",
                "snippets": "Recent research has shown that model-based confidence estimators have a significant advantage over using the output softmax probabilities. If the input data to the speech recogniser is from mismatched acoustic and linguistic conditions, the ASR performance and the corresponding confidence estimators may exhibit severe degradation...Using pseudo transcriptions to provide binary targets for training model-based confidence estimators and including additional features from an OOD LM are useful for improving the confidence scores on OOD datasets, even when pretraining uses OOD data. By exposing the CEM or R-EBM to OOD data, the calibration performance is also improved.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 164,
                        "end": 495,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Recent research has shown that model-based confidence estimators have a significant advantage over using the output softmax probabilities. If the input data to the speech recogniser is from mismatched acoustic and linguistic conditions, the ASR performance and the corresponding confidence estimators may exhibit severe degradation"
                    },
                    {
                        "section_title": "CONCLUSION",
                        "pdf_hash": "",
                        "start": 233,
                        "end": 572,
                        "sentence_offsets": [
                            {
                                "start": 233,
                                "end": 483
                            },
                            {
                                "start": 484,
                                "end": 571
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Using pseudo transcriptions to provide binary targets for training model-based confidence estimators and including additional features from an OOD LM are useful for improving the confidence scores on OOD datasets, even when pretraining uses OOD data. By exposing the CEM or R-EBM to OOD data, the calibration performance is also improved."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[253098773 | Ahuja et al. | 2022 | Citations: 16]",
                "snippets": "We briefly review some commonly used methods for calibrating neural network based classifiers. 1. Temperature Scaling (TS and Self-TS) (Guo et al., 2017) is applied by scaling the output logits using a temperature parameter T before applying softmax i.e. : \n\n, where o k denotes the logits corresponding to the k th class. T is a learnable parameter obtained posttraining by maximizing the log-likelihood on the dev set while keeping other network parameters fixed. We experiment with two settings for improving calibration on a target language: using dev data in English to perform temperature scaling (TS) and using the target language's dev data (Self-TS). 2. Label Smoothing (LS) (Pereyra et al., 2017) is a regularization technique that penalizes low entropy distributions by using soft labels that are obtained by assigning a fixed probability q = 1 \u2212 \u03b1 to the true label (0 < \u03b1 < 1), and distributing the remaining probability mass uniformly across the remaining classes. Label smoothing has been empirically shown to be competitive with temperature scaling for calibration (M\u00fcller et al., 2019) especially in out of domain settings (Desai et al., 2020) 3. Few-Shot Learning (FSL) We also investigate if fine-tuning the MMLM on a few examples in a target language in addition to the data in English, leads to any improvement in calibration as it does in the performance (Lauscher et al., 2020). Since these models are expected to be calibrated worse for out-of-domain data compared to in-domain data (Desai et al., 2020), we try to improve calibration by reducing the domain shift through fewshot learning. \n\nApart from these, we also consider combinations of different calibration methods in our experiments, including Label Smoothing with Temperature Scaling (TS + LS or Self-TS + LS) and Few-Shot Learning with Label Smoothing (FSL + LS).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[174802983 | Muller et al. | 2019 | Citations: 1954]": "The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions.",
                    "[212747810 | Desai et al. | 2020 | Citations: 301]": "Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain.",
                    "[226262344 | Lauscher et al. | 2020 | Citations: 315]": "Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions.",
                    "[28671436 | Guo et al. | 2017 | Citations: 5869]": "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."
                },
                "metadata": [
                    {
                        "section_title": "Calibration Methods",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1858,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 94
                            },
                            {
                                "start": 95,
                                "end": 256
                            },
                            {
                                "start": 259,
                                "end": 322
                            },
                            {
                                "start": 323,
                                "end": 465
                            },
                            {
                                "start": 466,
                                "end": 659
                            },
                            {
                                "start": 660,
                                "end": 978
                            },
                            {
                                "start": 979,
                                "end": 1406
                            },
                            {
                                "start": 1407,
                                "end": 1623
                            },
                            {
                                "start": 1626,
                                "end": 1858
                            }
                        ],
                        "ref_mentions": [
                            "28671436",
                            "174802983",
                            "212747810",
                            "226262344",
                            "212747810"
                        ],
                        "quote": "We briefly review some commonly used methods for calibrating neural network based classifiers. 1. Temperature Scaling (TS and Self-TS) (Guo et al., 2017) is applied by scaling the output logits using a temperature parameter T before applying softmax i.e. : \n\n, where o k denotes the logits corresponding to the k th class. T is a learnable parameter obtained posttraining by maximizing the log-likelihood on the dev set while keeping other network parameters fixed. We experiment with two settings for improving calibration on a target language: using dev data in English to perform temperature scaling (TS) and using the target language's dev data (Self-TS). 2. Label Smoothing (LS) (Pereyra et al., 2017) is a regularization technique that penalizes low entropy distributions by using soft labels that are obtained by assigning a fixed probability q = 1 \u2212 \u03b1 to the true label (0 < \u03b1 < 1), and distributing the remaining probability mass uniformly across the remaining classes. Label smoothing has been empirically shown to be competitive with temperature scaling for calibration (M\u00fcller et al., 2019) especially in out of domain settings (Desai et al., 2020) 3. Few-Shot Learning (FSL) We also investigate if fine-tuning the MMLM on a few examples in a target language in addition to the data in English, leads to any improvement in calibration as it does in the performance (Lauscher et al., 2020). Since these models are expected to be calibrated worse for out-of-domain data compared to in-domain data (Desai et al., 2020), we try to improve calibration by reducing the domain shift through fewshot learning. \n\nApart from these, we also consider combinations of different calibration methods in our experiments, including Label Smoothing with Temperature Scaling (TS + LS or Self-TS + LS) and Few-Shot Learning with Label Smoothing (FSL + LS)."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[258865733 | Tian et al. | 2023 | Citations: 355]",
                "snippets": "Among the methods for verbalizing probabilities directly, we observe that generating and evaluating multiple hypotheses improves calibration (see Figure 1), similarly to humans (Lord et al., 1984), and corroborating a similar finding in LMs (Kadavath et al., 2022)...We show that prompting a model to produce several answer choices before giving its confidence scores significantly improves calibration of verbalized probabilities. Combined with temperature scaling (Guo et al., 2017), this approach generally provides better calibration than model probabilities for ChatGPT, GPT-4, and Claude 2 across three datasets, often reducing expected calibration error (ECE) by over 50%.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[24047702 | Lord et al. | 1984 | Citations: 663]": "It is proposed that several biases in social judgment result from a failure--first noted by Francis Bacon--to consider possibilities at odds with beliefs and perceptions of the moment. Individuals who are induced to consider the opposite, therefore, should display less bias in social judgment. In two separate but conceptually parallel experiments, this reasoning was applied to two domains--biased assimilation of new evidence on social issues and biased hypothesis testing of personality impressions. Subjects were induced to consider the opposite in two ways: through explicit instructions to do so and through stimulus materials that made opposite possibilities more salient. In both experiments the induction of a consider-the-opposite strategy had greater corrective effect than more demand-laden alternative instructions to be as fair and unbiased as possible. The results are viewed as consistent with previous research on perseverance, hindsight, and logical problem solving, and are thought to suggest an effective method of retraining social judgment.",
                    "[28671436 | Guo et al. | 2017 | Citations: 5869]": "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."
                },
                "metadata": [
                    {
                        "section_title": "Results",
                        "pdf_hash": "",
                        "start": 447,
                        "end": 711,
                        "sentence_offsets": [
                            {
                                "start": 444,
                                "end": 712
                            }
                        ],
                        "ref_mentions": [
                            "24047702"
                        ],
                        "quote": "Among the methods for verbalizing probabilities directly, we observe that generating and evaluating multiple hypotheses improves calibration (see Figure 1), similarly to humans (Lord et al., 1984), and corroborating a similar finding in LMs (Kadavath et al., 2022)"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 413,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 164
                            },
                            {
                                "start": 165,
                                "end": 417
                            }
                        ],
                        "ref_mentions": [
                            "28671436"
                        ],
                        "quote": "We show that prompting a model to produce several answer choices before giving its confidence scores significantly improves calibration of verbalized probabilities. Combined with temperature scaling (Guo et al., 2017), this approach generally provides better calibration than model probabilities for ChatGPT, GPT-4, and Claude 2 across three datasets, often reducing expected calibration error (ECE) by over 50%."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[258967945 | He et al. | 2023 | Citations: 22]",
                "snippets": "Based on the observations, we hypothesize that preserving the pre-trained features helps calibrate the fine-tuned LMs. \n\nTo validate our hypothesis, we first evaluate the calibration of some previous methods that can preserve pre-trained features, including (1) Parameter-efficient tuning (Houlsby et al., 2019)Hu et al., 2021;Li & Liang, 2021), (2) Pre-trained weight decay, (3) Mixout (Lee et al., 2019). Although these methods were originally designed to improve the performance beyond uncertainty estimation, our experiment demonstrates that these methods outperform vanilla fine-tuning in terms of calibration, especially under out-of-domain settings. Based on our observation that the PLMs are well-calibrated on the MLM task yet the fine-tuned LMs that forget the pre-trained features struggle with overconfidence under domain shift, we propose a simple baseline that utilizes the MLM objective to maintain the consistency between the pre-trained and fine-tuned models. The proposed method achieves the lowest expected calibration error and competitive accuracy compared to existing calibration methods in both ID and OD settings on three NLU tasks, including natural language inference, paraphrase detection, and commonsense reasoning, showing that preserving the pre-trained features is an effective approach for improving the calibration of fine-tuned LMs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[202750126 | Lee et al. | 2019 | Citations: 209]": "In natural language processing, it has been observed recently that generalization could be greatly improved by finetuning a large-scale language model pretrained on a large unlabeled corpus. Despite its recent success and wide adoption, finetuning a large pretrained language model on a downstream task is prone to degenerate performance when there are only a small number of training instances available. In this paper, we introduce a new regularization technique, to which we refer as \"mixout\", motivated by dropout. Mixout stochastically mixes the parameters of two models. We show that our mixout technique regularizes learning to minimize the deviation from one of the two models and that the strength of regularization adapts along the optimization trajectory. We empirically evaluate the proposed mixout and its variants on finetuning a pretrained language model on downstream tasks. More specifically, we demonstrate that the stability of finetuning and the average accuracy greatly increase when we use the proposed approach to regularize finetuning of BERT on downstream tasks in GLUE.",
                    "[59599816 | Houlsby et al. | 2019 | Citations: 4518]": "Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1366,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 118
                            },
                            {
                                "start": 121,
                                "end": 406
                            },
                            {
                                "start": 407,
                                "end": 656
                            },
                            {
                                "start": 657,
                                "end": 976
                            },
                            {
                                "start": 977,
                                "end": 1366
                            }
                        ],
                        "ref_mentions": [
                            "59599816",
                            "202750126"
                        ],
                        "quote": "Based on the observations, we hypothesize that preserving the pre-trained features helps calibrate the fine-tuned LMs. \n\nTo validate our hypothesis, we first evaluate the calibration of some previous methods that can preserve pre-trained features, including (1) Parameter-efficient tuning (Houlsby et al., 2019)Hu et al., 2021;Li & Liang, 2021), (2) Pre-trained weight decay, (3) Mixout (Lee et al., 2019). Although these methods were originally designed to improve the performance beyond uncertainty estimation, our experiment demonstrates that these methods outperform vanilla fine-tuning in terms of calibration, especially under out-of-domain settings. Based on our observation that the PLMs are well-calibrated on the MLM task yet the fine-tuned LMs that forget the pre-trained features struggle with overconfidence under domain shift, we propose a simple baseline that utilizes the MLM objective to maintain the consistency between the pre-trained and fine-tuned models. The proposed method achieves the lowest expected calibration error and competitive accuracy compared to existing calibration methods in both ID and OD settings on three NLU tasks, including natural language inference, paraphrase detection, and commonsense reasoning, showing that preserving the pre-trained features is an effective approach for improving the calibration of fine-tuned LMs."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[259859030 | Chen et al. | 2023 | Citations: 0]",
                "snippets": "Data augmentation (Hendrycks et al., 2019)(Wang et al., 2021) and model ensemble (Gal and Ghahramani, 2016;(Lakshminarayanan et al., 2016) have been empirically proven to be successful in computer vision. However, they cannot bring the same benefits in NLP according to the empirical study in Chen et al. (2022b)...Temperature scaling (Platt, 1999)Guo et al., 2017) readjusts the output logits in a posterior way according to the calibration performance on a held-out set. Label smoothing (Szegedy et al., 2015) imposes a confidence regularization during the training process, discouraging models from being overconfident in their predictions.\n\nRecently, there is an emergent trend in NLP, motivating to directly collect data for training models to have reasonable confidence estimations. Kadavath et al. (2022) assume that the last hidden states of PLMs contain the uncertainty information, and directly apply a multi-layer perceptron on them to perform confidence estimations. Lin et al. (2022) also show that PLMs can be directly trained to give their confidence estimations by words. These two methods are proven to be successful in significantly reducing the overconfidence issue and are further extended to exploit the potential in this kind of methods (Chen et al., 2022b).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[206593880 | Szegedy et al. | 2015 | Citations: 27419]": "Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.",
                    "[208637407 | Hendrycks et al. | 2019 | Citations: 1308]": "Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AugMix, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AugMix significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half.",
                    "[239885364 | Wang et al. | 2021 | Citations: 113]": "Data augmentation is a simple yet effective way to improve the robustness of deep neural networks (DNNs). Diversity and hardness are two complementary dimensions of data augmentation to achieve robustness. For example, AugMix explores random compositions of a diverse set of augmentations to enhance broader coverage, while adversarial training generates adversarially hard samples to spot the weakness. Motivated by this, we propose a data augmentation framework, termed AugMax, to unify the two aspects of diversity and hardness. AugMax first randomly samples multiple augmentation operators and then learns an adversarial mixture of the selected operators. Being a stronger form of data augmentation, AugMax leads to a significantly augmented input distribution which makes model training more challenging. To solve this problem, we further design a disentangled normalization module, termed DuBIN (Dual-Batch-and-Instance Normalization), that disentangles the instance-wise feature heterogeneity arising from AugMax. Experiments show that AugMax-DuBIN leads to significantly improved out-of-distribution robustness, outperforming prior arts by 3.03%, 3.49%, 1.82% and 0.71% on CIFAR10-C, CIFAR100-C, Tiny ImageNet-C and ImageNet-C. Codes and pretrained models are available: https://github.com/VITA-Group/AugMax.",
                    "[6294674 | Lakshminarayanan et al. | 2016 | Citations: 5843]": "Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 152,
                        "end": 463,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "208637407",
                            "239885364",
                            "6294674"
                        ],
                        "quote": "Data augmentation (Hendrycks et al., 2019)(Wang et al., 2021) and model ensemble (Gal and Ghahramani, 2016;(Lakshminarayanan et al., 2016) have been empirically proven to be successful in computer vision. However, they cannot bring the same benefits in NLP according to the empirical study in Chen et al. (2022b)"
                    },
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 591,
                        "end": 1564,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "56563878",
                            "206593880"
                        ],
                        "quote": "Temperature scaling (Platt, 1999)Guo et al., 2017) readjusts the output logits in a posterior way according to the calibration performance on a held-out set. Label smoothing (Szegedy et al., 2015) imposes a confidence regularization during the training process, discouraging models from being overconfident in their predictions.\n\nRecently, there is an emergent trend in NLP, motivating to directly collect data for training models to have reasonable confidence estimations. Kadavath et al. (2022) assume that the last hidden states of PLMs contain the uncertainty information, and directly apply a multi-layer perceptron on them to perform confidence estimations. Lin et al. (2022) also show that PLMs can be directly trained to give their confidence estimations by words. These two methods are proven to be successful in significantly reducing the overconfidence issue and are further extended to exploit the potential in this kind of methods (Chen et al., 2022b)."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[267617073 | Huang et al. | 2024 | Citations: 12]",
                "snippets": "Improving Calibration Traditional calibration methods focus on post-processing logits (Guo et al., 2017), but with LLMs generating unbounded text, logits could fall short. Thus, extracting better confidence scores (i.e., confidence elicitation) has become crucial for improving calibration. These include: verbalization, which directly asks the model to output its confidence (Lin et al., 2022), consistency, which uses the uniformity of multiple responses to gauge confidence (Kadavath et al., 2022, Kuhn et al., 2023, Cole et al., 2023, Chen et al., 2023c, Tian et al., 2023a, Lin et al., 2023), and the hybrid of both (Xiong et al., 2023, Tian et al., 2023b, Chen and Mueller, 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[28671436 | Guo et al. | 2017 | Citations: 5869]": "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1026,
                        "end": 1712,
                        "sentence_offsets": [
                            {
                                "start": 1026,
                                "end": 1197
                            },
                            {
                                "start": 1198,
                                "end": 1316
                            },
                            {
                                "start": 1317,
                                "end": 1712
                            }
                        ],
                        "ref_mentions": [
                            "28671436"
                        ],
                        "quote": "Improving Calibration Traditional calibration methods focus on post-processing logits (Guo et al., 2017), but with LLMs generating unbounded text, logits could fall short. Thus, extracting better confidence scores (i.e., confidence elicitation) has become crucial for improving calibration. These include: verbalization, which directly asks the model to output its confidence (Lin et al., 2022), consistency, which uses the uniformity of multiple responses to gauge confidence (Kadavath et al., 2022, Kuhn et al., 2023, Cole et al., 2023, Chen et al., 2023c, Tian et al., 2023a, Lin et al., 2023), and the hybrid of both (Xiong et al., 2023, Tian et al., 2023b, Chen and Mueller, 2023)."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[268032879 | Zhao et al. | 2024 | Citations: 7]",
                "snippets": "Confidence Calibration of LLMs. Extracting honest and constructive confidence scores of large language models is considered an important step towards building faithful and trustworthy AI systems (Desai et al., 2020)(Si et al., 2022). Many methods have been proposed recently to get reliable confidence scores with different suffix prompts added after outputting possible answers, such as a follow of True or False multiple choice question (Kadavath et al., 2022), asking models to describe the likelihood of the answer (Lin et al., 2022), and describing the likelihood that the answer is correct with demonstrations (Tian et al., 2023). However, it remains unclear how robust the methods are and how good they are comparatively. Our paper proposes FaR prompting as an orthogonal method to improve calibration and compare different extraction methods with our test bed.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[212747810 | Desai et al. | 2020 | Citations: 301]": "Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain.",
                    "[249191391 | Lin et al. | 2022 | Citations: 424]": "We show that a GPT-3 model can learn to express uncertainty about its own answers in natural language -- without use of model logits. When given a question, the model generates both an answer and a level of confidence (e.g.\"90% confidence\"or\"high confidence\"). These levels map to probabilities that are well calibrated. The model also remains moderately calibrated under distribution shift, and is sensitive to uncertainty in its own answers, rather than imitating human examples. To our knowledge, this is the first time a model has been shown to express calibrated uncertainty about its own answers in natural language. For testing calibration, we introduce the CalibratedMath suite of tasks. We compare the calibration of uncertainty expressed in words (\"verbalized probability\") to uncertainty extracted from model logits. Both kinds of uncertainty are capable of generalizing calibration under distribution shift. We also provide evidence that GPT-3's ability to generalize calibration depends on pre-trained latent representations that correlate with epistemic uncertainty over its answers.",
                    "[252917981 | Si et al. | 2022 | Citations: 302]": "Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3's reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM's factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions. Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 843,
                        "end": 1715,
                        "sentence_offsets": [
                            {
                                "start": 843,
                                "end": 874
                            },
                            {
                                "start": 875,
                                "end": 1080
                            },
                            {
                                "start": 1081,
                                "end": 1483
                            },
                            {
                                "start": 1484,
                                "end": 1575
                            },
                            {
                                "start": 1576,
                                "end": 1715
                            }
                        ],
                        "ref_mentions": [
                            "212747810",
                            "252917981",
                            "249191391"
                        ],
                        "quote": "Confidence Calibration of LLMs. Extracting honest and constructive confidence scores of large language models is considered an important step towards building faithful and trustworthy AI systems (Desai et al., 2020)(Si et al., 2022). Many methods have been proposed recently to get reliable confidence scores with different suffix prompts added after outputting possible answers, such as a follow of True or False multiple choice question (Kadavath et al., 2022), asking models to describe the likelihood of the answer (Lin et al., 2022), and describing the likelihood that the answer is correct with demonstrations (Tian et al., 2023). However, it remains unclear how robust the methods are and how good they are comparatively. Our paper proposes FaR prompting as an orthogonal method to improve calibration and compare different extraction methods with our test bed."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[268723623 | Li et al. | 2024 | Citations: 5]",
                "snippets": "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain. Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1281,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain. Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[269004786 | Detommaso et al. | 2024 | Citations: 19]",
                "snippets": "We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and \"self-annotation\" - querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 243,
                        "end": 917,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and \"self-annotation\" - querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[269449363 | Tarride et al. | 2024 | Citations: 4]",
                "snippets": "The results show that the most effective method is to take the softmax value for each token after the CTC decoding step and average these values for each line to obtain line-level confidence scores. This approach not only ensures computational efficiency, but also yields a strong correlation with the recognition rate. As a result of these findings, we have incorporated this calculation method into PyLaia.\n\nTable 3 provides insight into the impact of model calibration on the linear correlation between confidence scores and recognition rate on the test set of different datasets...The results show significant improvements in the linear correlation between confidence scores and recognition rates when temperature scaling is applied. Across different datasets, optimal temperatures ranging from 1.5 to 4.0 yield consistently higher correlations compared to scenarios without scaling. Since temperature scaling only scales the logits without changing the underlying ranking of the predictions, it does not affect the rank correlation.\n\nOur analysis suggests that the default value of 2.0 is a robust choice for calibration, even for datasets where the optimal temperature is not equal to 2.0.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The results show that the most effective method is to take the softmax value for each token after the CTC decoding step and average these values for each line to obtain line-level confidence scores. This approach not only ensures computational efficiency, but also yields a strong correlation with the recognition rate. As a result of these findings, we have incorporated this calculation method into PyLaia.\n\nTable 3 provides insight into the impact of model calibration on the linear correlation between confidence scores and recognition rate on the test set of different datasets",
                        "pdf_hash": ""
                    },
                    {
                        "section_title": "Confidence score analysis",
                        "pdf_hash": "",
                        "start": 1553,
                        "end": 2164,
                        "sentence_offsets": [
                            {
                                "start": 1447,
                                "end": 1620
                            },
                            {
                                "start": 1622,
                                "end": 1761
                            },
                            {
                                "start": 1761,
                                "end": 1881
                            },
                            {
                                "start": 1881,
                                "end": 2033
                            },
                            {
                                "start": 2033,
                                "end": 2182
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The results show significant improvements in the linear correlation between confidence scores and recognition rates when temperature scaling is applied. Across different datasets, optimal temperatures ranging from 1.5 to 4.0 yield consistently higher correlations compared to scenarios without scaling. Since temperature scaling only scales the logits without changing the underlying ranking of the predictions, it does not affect the rank correlation.\n\nOur analysis suggests that the default value of 2.0 is a robust choice for calibration, even for datasets where the optimal temperature is not equal to 2.0."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[270620078 | Liu et al. | 2024 | Citations: 6]",
                "snippets": "Popular LM calibration methods include training-free, inference-only approaches through verbalization (Tian et al., 2023a) or selfconsistency measurements (Wang et al., 2022) and training-based methods, such as tuning temperature parameters (Liang et al., 2017) or learning uncertainty estimations from LMs' logits (Liu et al., 2023).Despite their potential compatibility with many LMs, training-free methods are limited by the models' instruction-following capabilities and can be computationally expensive during inference.\n\nTraining-based methods, on the other hand, directly learn to output model's uncertainty.For instance, (Kuhn et al., 2023) leverages the token logits predicted by the model to estimate the confidence of the entire response.Yet model logits may not capture knowledge representations and can be sensitive to tokenization procedures.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[247595263 | Wang et al. | 2022 | Citations: 3734]": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
                    "[257039062 | Kuhn et al. | 2023 | Citations: 310]": "We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of\"semantic equivalence\"-- different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy -- an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines.",
                    "[264811211 | Liu et al. | 2023 | Citations: 27]": "A model is considered well-calibrated when its probability estimate aligns with the actual likelihood of the output being correct. Calibrating language models (LMs) is crucial, as it plays a vital role in detecting and mitigating hallucinations of LMs as well as building more trustworthy models. However, standard calibration techniques may not be suited for LM calibration. For instance, post-processing methods such as temperature scaling do not reorder the candidate generations. On the other hand, training-based methods require fine-tuning the entire model, which is impractical for LMs of large scale. We present LitCab, a lightweight calibration mechanism consisting of a single linear layer that takes the input text representation and predicts a bias term, which is then added to the LM output logits. LitCab improves model calibration by only adding<2% of the original model parameters. For evaluation, we construct CaT, a benchmark consisting of eight text generation tasks, covering responses ranging from short phrases to paragraphs. We test LitCab with Llama2-7B, where it improves calibration across all tasks, reducing the average ECE score by as large as 30%. We further conduct a comprehensive evaluation with multiple popular open-sourced LMs from GPT and LLaMA families, yielding the following key findings: (i) Larger models within the same family exhibit better calibration on tasks with short generation tasks, but not necessarily for longer ones. (ii) GPT-family models show superior calibration compared to LLaMA, Llama2, and Vicuna models, despite having much fewer parameters. (iii) Fine-tuning pretrained model (e.g., LLaMA) with samples of limited purpose (e.g., conversations) may lead to worse calibration, highlighting the importance of fine-tuning setups for calibrating LMs.",
                    "[3526391 | Liang et al. | 2017 | Citations: 2082]": "We consider the problem of detecting out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10) when the true positive rate is 95%."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 583,
                        "end": 1438,
                        "sentence_offsets": [
                            {
                                "start": 583,
                                "end": 917
                            },
                            {
                                "start": 917,
                                "end": 1108
                            },
                            {
                                "start": 1110,
                                "end": 1198
                            },
                            {
                                "start": 1198,
                                "end": 1331
                            },
                            {
                                "start": 1331,
                                "end": 1438
                            }
                        ],
                        "ref_mentions": [
                            "247595263",
                            "3526391",
                            "264811211",
                            "257039062"
                        ],
                        "quote": "Popular LM calibration methods include training-free, inference-only approaches through verbalization (Tian et al., 2023a) or selfconsistency measurements (Wang et al., 2022) and training-based methods, such as tuning temperature parameters (Liang et al., 2017) or learning uncertainty estimations from LMs' logits (Liu et al., 2023).Despite their potential compatibility with many LMs, training-free methods are limited by the models' instruction-following capabilities and can be computationally expensive during inference.\n\nTraining-based methods, on the other hand, directly learn to output model's uncertainty.For instance, (Kuhn et al., 2023) leverages the token logits predicted by the model to estimate the confidence of the entire response.Yet model logits may not capture knowledge representations and can be sensitive to tokenization procedures."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[272689376 | Sun et al. | 2024 | Citations: 1]",
                "snippets": "We evaluate four methods for estimating confidence scores based on softmax, raw token scores, verbalized confidences, and a combination of these methods, using the area under the curve (AUC) metric to assess calibration, with higher AUC indicating better calibration. We also enhance these with a self-probing mechanism, proposed for closed models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 672,
                        "end": 1020,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We evaluate four methods for estimating confidence scores based on softmax, raw token scores, verbalized confidences, and a combination of these methods, using the area under the curve (AUC) metric to assess calibration, with higher AUC indicating better calibration. We also enhance these with a self-probing mechanism, proposed for closed models."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[272987064 | Xie et al. | 2024 | Citations: 17]",
                "snippets": "In this work, we introduce Adaptive Temperature Scaling (ATS), a post-hoc calibration method that predicts a temperature scaling parameter for each token prediction. The predicted temperature values adapt based on token-level features and are fit over a standard supervised fine-tuning (SFT) dataset. The adaptive nature of ATS addresses the varying degrees of calibration shift that can occur after RLHF fine-tuning. ATS improves calibration by over 10-50% across three downstream natural language evaluation benchmarks compared to prior calibration methods and does not impede performance improvements from RLHF.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this work, we introduce Adaptive Temperature Scaling (ATS), a post-hoc calibration method that predicts a temperature scaling parameter for each token prediction. The predicted temperature values adapt based on token-level features and are fit over a standard supervised fine-tuning (SFT) dataset. The adaptive nature of ATS addresses the varying degrees of calibration shift that can occur after RLHF fine-tuning. ATS improves calibration by over 10-50% across three downstream natural language evaluation benchmarks compared to prior calibration methods and does not impede performance improvements from RLHF.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[273228151 | Manggala et al. | 2024 | Citations: 3]",
                "snippets": "Group Notions of Calibration. Previous works highlight the limitations of average-case calibration. Group-wise calibration, which uses predefined groupings (Kleinberg et al., 2016)(Pleiss et al., 2017), has been adapted for language models (LMs). Li et al. ( 2024) train a model that approximates the precision-threshold curve for a given group by using few-shot samples to predict the LM's empirical precision at various confidence thresholds. Ulmer et al. (2024) train an auxiliary model using accuracy per group as target to predict an LM's confidence based on textual input and output. Detommaso et al. (2024) achieves multicalibration -simultaneous calibration across various intersecting groupings of the data. Our work complements multicalibration, and our methods could extend to this by adapting Algorithm 3 in Detommaso et al. (2024). Luo et al. (2022) measure calibration over a set of similar predictions, quantified by a kernel function on feature space.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[12845273 | Kleinberg et al. | 2016 | Citations: 1776]": "Recent discussion in the public sphere about algorithmic classification has involved tension between competing notions of what it means for a probabilistic classification to be fair to different groups. We formalize three fairness conditions that lie at the heart of these debates, and we prove that except in highly constrained special cases, there is no method that can satisfy these three conditions simultaneously. Moreover, even satisfying all three conditions approximately requires that the data lie in an approximate version of one of the constrained special cases identified by our theorem. These results suggest some of the ways in which key notions of fairness are incompatible with each other, and hence provide a framework for thinking about the trade-offs between them.",
                    "[75455 | Pleiss et al. | 2017 | Citations: 880]": "The machine learning community has become increasingly concerned with the potential for bias and discrimination in predictive models. This has motivated a growing line of work on what it means for a classification procedure to be \"fair.\" In this paper, we investigate the tension between minimizing error disparity across different population groups while maintaining calibrated probability estimates. We show that calibration is compatible only with a single error constraint (i.e. equal false-negatives rates across groups), and show that any algorithm that satisfies this relaxation is no better than randomizing a percentage of predictions for an existing classifier. These unsettling findings, which extend and generalize existing results, are empirically confirmed on several datasets."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 1005,
                        "end": 1971,
                        "sentence_offsets": [
                            {
                                "start": 1005,
                                "end": 1034
                            },
                            {
                                "start": 1035,
                                "end": 1104
                            },
                            {
                                "start": 1105,
                                "end": 1250
                            },
                            {
                                "start": 1251,
                                "end": 1448
                            },
                            {
                                "start": 1449,
                                "end": 1593
                            },
                            {
                                "start": 1594,
                                "end": 1720
                            },
                            {
                                "start": 1721,
                                "end": 1848
                            },
                            {
                                "start": 1849,
                                "end": 1971
                            }
                        ],
                        "ref_mentions": [
                            "12845273",
                            "75455"
                        ],
                        "quote": "Group Notions of Calibration. Previous works highlight the limitations of average-case calibration. Group-wise calibration, which uses predefined groupings (Kleinberg et al., 2016)(Pleiss et al., 2017), has been adapted for language models (LMs). Li et al. ( 2024) train a model that approximates the precision-threshold curve for a given group by using few-shot samples to predict the LM's empirical precision at various confidence thresholds. Ulmer et al. (2024) train an auxiliary model using accuracy per group as target to predict an LM's confidence based on textual input and output. Detommaso et al. (2024) achieves multicalibration -simultaneous calibration across various intersecting groupings of the data. Our work complements multicalibration, and our methods could extend to this by adapting Algorithm 3 in Detommaso et al. (2024). Luo et al. (2022) measure calibration over a set of similar predictions, quantified by a kernel function on feature space."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[273228940 | Wang et al. | 2024 | Citations: 0]",
                "snippets": "Recent methods have demonstrated improved calibration by applying techniques like Platt scaling or temperature scaling to the confidence scores generated by LLMs. In this paper, we explore the calibration of verbalized probability distributions for discriminative tasks. First, we investigate the capability of LLMs to generate probability distributions over categorical labels. We theoretically and empirically identify the issue of re-softmax arising from the scaling of verbalized probabilities, and propose using the invert softmax trick to approximate the\"logit\"by inverting verbalized probabilities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 154,
                        "end": 759,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Recent methods have demonstrated improved calibration by applying techniques like Platt scaling or temperature scaling to the confidence scores generated by LLMs. In this paper, we explore the calibration of verbalized probability distributions for discriminative tasks. First, we investigate the capability of LLMs to generate probability distributions over categorical labels. We theoretically and empirically identify the issue of re-softmax arising from the scaling of verbalized probabilities, and propose using the invert softmax trick to approximate the\"logit\"by inverting verbalized probabilities."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[273821339 | LeCoz et al. | 2024 | Citations: 1]",
                "snippets": "To address this issue, we transform the problem of calibrating a multiclass classifier into calibrating a single surrogate binary classifier. This approach allows for more efficient use of standard calibration methods. We evaluate our approach on numerous neural networks used for image or text classification and show that it significantly enhances existing calibration methods.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "To address this issue, we transform the problem of calibrating a multiclass classifier into calibrating a single surrogate binary classifier. This approach allows for more efficient use of standard calibration methods. We evaluate our approach on numerous neural networks used for image or text classification and show that it significantly enhances existing calibration methods.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[273821704 | Li et al. | 2024 | Citations: 2]",
                "snippets": "(Mielke et al., 2020) train a calibrator to predict the correctness of a response for a given question. With a similar idea, Ulmer et al. (2024) train a language model (e.g., DeBERTa) based on question-response pairs to predict the probability of responses' correctness. Based on SelfCheckGPT (Manakul et al., 2023) and JAFC (Tian et al., 2023), Chen et al. (2024) train supervised models to reduce grouping losses and improve the confidence estimation. The method by Liu et al. (2024) uses an LLM's latent representations to predict the correctness of responses. Detommaso et al. (2024) use the \"multicalibration\" technique to calibrate the probability of correctness. Fadeeva et al. (2023) offer a detailed comparative study of various confidence estimation methods, providing empirical evidence on their effectiveness across different tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258865733 | Tian et al. | 2023 | Citations: 355]": "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%.",
                    "[250073258 | Mielke et al. | 2020 | Citations: 169]": "Abstract While improving neural dialogue agents\u2019 factual accuracy is the object of much research, another important aspect of communication, less studied in the setting of neural dialogue, is transparency about ignorance. In this work, we analyze to what extent state-of-the-art chit-chat models are linguistically calibrated in the sense that their verbalized expression of doubt (or confidence) matches the likelihood that the model\u2019s responses are factually incorrect (or correct). We find that these models are poorly calibrated, yet we show that likelihood of correctness can accurately be predicted. By incorporating such metacognitive features into the training of a controllable generation model, we obtain a dialogue agent with greatly improved linguistic calibration.",
                    "[257557820 | Manakul et al. | 2023 | Citations: 444]": "Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 301,
                        "end": 1144,
                        "sentence_offsets": [
                            {
                                "start": 301,
                                "end": 403
                            },
                            {
                                "start": 404,
                                "end": 570
                            },
                            {
                                "start": 571,
                                "end": 753
                            },
                            {
                                "start": 754,
                                "end": 863
                            },
                            {
                                "start": 864,
                                "end": 969
                            },
                            {
                                "start": 970,
                                "end": 1144
                            }
                        ],
                        "ref_mentions": [
                            "250073258",
                            "257557820",
                            "258865733"
                        ],
                        "quote": "(Mielke et al., 2020) train a calibrator to predict the correctness of a response for a given question. With a similar idea, Ulmer et al. (2024) train a language model (e.g., DeBERTa) based on question-response pairs to predict the probability of responses' correctness. Based on SelfCheckGPT (Manakul et al., 2023) and JAFC (Tian et al., 2023), Chen et al. (2024) train supervised models to reduce grouping losses and improve the confidence estimation. The method by Liu et al. (2024) uses an LLM's latent representations to predict the correctness of responses. Detommaso et al. (2024) use the \"multicalibration\" technique to calibrate the probability of correctness. Fadeeva et al. (2023) offer a detailed comparative study of various confidence estimation methods, providing empirical evidence on their effectiveness across different tasks."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[274789494 | Xie et al. | 2024 | Citations: 4]",
                "snippets": "Calibration aims to align confidence scores with correctness. Commonly used calibration methods include temperature scaling (Guo et al., 2017), label smoothing (Szegedy et al., 2016), and logistic calibration (Kull et al., 2017). However, methods like label smoothing and logistic calibration are unsuitable for black-box LLMs, as they require internal model access or fine-tuning, whereas temperature scaling, which operates on the logits of model outputs, is better suited for gray-box models. In black-box LLMs, common calibration methods include post-processing or transforming the model into a gray-box. Due to the near-well-calibrated nature of Confidence Estimation in black-box LLMs, calibration pressure is reduced.\n\nIn post-processing methods, popular techniques include Histogram Binning (Zadrozny and Elkan, 2001) and Isotonic Regression (Jiang et al., 2011). Histogram Binning (Zadrozny and Elkan, 2001) adjusts confidence values within different intervals by assessing the correctness of predictions in each interval, providing a simple but potentially limited calibration when confidence distributions are uneven. Isotonic Regression (Jiang et al., 2011), on the other hand, seeks an optimal monotonically increasing function f (c) to map the model's confidence c 1 , c 2 , . . . , c n to correctness. f (c) need to satisfy the condition in Formula 2, ensuring that the confidence ranking aligns with the correctness.\n\nTo address more complex scenarios, some methods introduce additional constraints. For instance, H\u00e9bert-Johnson et al. ( 2018) introduced a multicalibration method, ensuring not only overall alignment but also calibration across multiple subgroups S j through iterative alignment. The overall goal can be expressed as Formula 3.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Calibration aims to align confidence scores with correctness. Commonly used calibration methods include temperature scaling (Guo et al., 2017), label smoothing (Szegedy et al., 2016), and logistic calibration (Kull et al., 2017). However, methods like label smoothing and logistic calibration are unsuitable for black-box LLMs, as they require internal model access or fine-tuning, whereas temperature scaling, which operates on the logits of model outputs, is better suited for gray-box models. In black-box LLMs, common calibration methods include post-processing or transforming the model into a gray-box. Due to the near-well-calibrated nature of Confidence Estimation in black-box LLMs, calibration pressure is reduced.\n\nIn post-processing methods, popular techniques include Histogram Binning (Zadrozny and Elkan, 2001) and Isotonic Regression (Jiang et al., 2011). Histogram Binning (Zadrozny and Elkan, 2001) adjusts confidence values within different intervals by assessing the correctness of predictions in each interval, providing a simple but potentially limited calibration when confidence distributions are uneven. Isotonic Regression (Jiang et al., 2011), on the other hand, seeks an optimal monotonically increasing function f (c) to map the model's confidence c 1 , c 2 , . . . , c n to correctness. f (c) need to satisfy the condition in Formula 2, ensuring that the confidence ranking aligns with the correctness.\n\nTo address more complex scenarios, some methods introduce additional constraints. For instance, H\u00e9bert-Johnson et al. ( 2018) introduced a multicalibration method, ensuring not only overall alignment but also calibration across multiple subgroups S j through iterative alignment. The overall goal can be expressed as Formula 3.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[277150701 | Liu et al. | 2025 | Citations: 8]",
                "snippets": "Calibration refers to closing the gap between the confidence score and the expected correctness conditioned on confidence score. It has a long history preceding LLM or even modern machine learning (Degroot et al., 1983)(Murphy et al., 1977)(Murphy et al., 1984), but bears slightly different meanings in NLP. In general, we could define a perfectly calibrated confidence measure to achieve the following: \n\nwhere the expectation is taken over the joint distribution of x and generation s. A lot of papers focus on evaluating the calibration quality of specific LMs and tasks (Khojah et al., 2022)63,(Wang et al., 2020). Evaluation typically relies on variants of Expected Calibration Error (ECE) [63,71,(Tian et al., 2023). Oftentimes confidence scores from classification could be directly applied (Jiang et al., 2020)(Stengel-Eskin et al., 2022)(Zhao et al., 2021) in order to evaluate whether an LM is overor under-confident, especially for de facto classification tasks like sentiment analysis or multiple-choice QA. \n\nAs uncertainty and confidence are often intertwined, many approaches used in uncertainty quantification have their counterpart in confidence estimation. For example, for black-box methods, Lin et al. (Lin et al., 2023) computes a similarity matrix of sampled responses and derives confidence estimates for each generation via its degree or distance derived from the graph Laplacian, before using these scores to compute uncertainty. Zhang et al. (Zhang et al., 2024) extends such black-box methods to longer generations. For logit-based methods, Malinin and Gales (Malinin et al., 2021) normalize Eq. ( 1) with the length of s. Further improvements include replacing the logit-sum or mean with weighted sum, by attention values on the important tokens [78] or by importance inferred from natural language inference (NLI) models (Duan et al., 2023). Such variants of sequence likelihood could then be fed for (entropy-style) uncertainty computation (Kuhn et al., 2023)[78]. \n\nAnother popular approach is asking the LM itself whether a particular free-form generation is correct [59]. However, this formulation also poses a restriction on the confidence estimation method, as it is essentially a scalar logit. Thus, many extensions focus on applying calibration methods from classification to calibrate such self-evaluation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[235078802 | Jiang et al. | 2020 | Citations: 436]": "Abstract Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, \u201cHow can we know when language models know, with confidence, the answer to a particular query?\u201d We examine this question from the point of view of calibration, the property of a probabilistic model\u2019s predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models\u2014T5, BART, and GPT-2\u2014and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github.com/jzbjyb/lm-calibration.",
                    "[258865733 | Tian et al. | 2023 | Citations: 355]": "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%.",
                    "[109884250 | Degroot et al. | 1983 | Citations: 745]": "In this paper we present methods for comparing and evaluating forecasters whose predictions are presented as their subjective probability distributions of various random variables that will be observed in the future, e.g. weather forecasters who each day must specify their own probabilities that it will rain in a particular location. We begin by reviewing the concepts of calibration and refinement, and describiing the relationship between this notion of refinement and the notion of sufficiency in the comparison of statistical experiments. We also consider the question of interrelationships among forecasters and discuss methods by which an observer should combine the predictions from two or more different forecasters. Then we turn our attention to the concept of a proper scoring rule for evaluating forecasters, relating it to the concepts of calibration and refinement. Finally, we discuss conditions under which one fore- caster can exploit the predictions of another forecaster to obtain a better score. In this paper we describe some concepts and methods appropriate for evaluating and com- paring forecasters who repeatedly present their predictions of whether or not various events will occur in terms of their subjective probabilities of those events. The ideas we describe here are relevant in almost any situation in which forecasters must repeatedly make such probabilistic predictions, regardless of the particular subject matter or substantive area of the events being forecast. The forecaster might be an economist who at the beginning of each quarterly period makes predictions about unemployment, the rate of inflation, or Gross National Product in that quarter based on the values of various economic indicators; the forecaster might even make predictions using a large-scale econometric model of the United States economy based on hundreds of variables and econometric relations. In a different field, the forecaster might be the weatherman for a television station who at the beginning of each day must announce his probability that it will rain during the day. For ease of exposition, we present our discussions here in the context of such a weather forecaster who day after day must specify his subjective probability x that there will be at least a certain amount of rain at some given location during a specified time interval of the day. We refer to the occurrence of this well-specified event simply as \"rain\". Thus, at the begin- ning of each day the forecaster must specify his probability of rain and at the end of each day he observes whether or not rain actually occurred. The probability x specified by the forecaster on any particular day is called his prediction for that day. We shall make the realistic, and simultaneously simplifying, assumption that the",
                    "[122150550 | Murphy et al. | 1984 | Citations: 338]": "Abstract Efforts to quantify the uncertainty in weather forecasts began more than 75 years ago, and many studies and experiments involving objective and subjective probability forecasting have been conducted in meteorology in the intervening period. Moreover, the U.S. National Weather Service (NWS) initiated a nationwide program in 1965 in which precipitation probability forecasts were formulated on an operational basis and routinely disseminated to the general public. In addition, the NWS now prepares objective probability forecasts for many variables, using statistical procedures. Hence probability forecasting in meteorology is unique in that very large sets of probability forecasts that have been subjected to detailed evaluation are available. This article has four objectives: (a) to review the history of probability forecasting in meteorology to acquaint statisticians with this body of literature; (b) to describe recent methodological, experimental, and operational activities in this field; (c) to exa...",
                    "[127203550 | Murphy et al. | 1977 | Citations: 387]": "SUMMARY This paper briefly describes some results of operational and experimental programmes in the United States involving subjective probability forecasts of precipitation occurrence and of maximum and minimum temperatures. These results indicate that weather forecasters can formulate such forecasts in a reliable manner.",
                    "[218487046 | Wang et al. | 2020 | Citations: 82]": "Confidence calibration, which aims to make model predictions equal to the true correctness measures, is important for neural machine translation (NMT) because it is able to offer useful indicators of translation errors in the generated output. While prior studies have shown that NMT models trained with label smoothing are well-calibrated on the ground-truth training data, we find that miscalibration still remains a severe challenge for NMT during inference due to the discrepancy between training and inference. By carefully designing experiments on three language pairs, our work provides in-depth analyses of the correlation between calibration and translation performance as well as linguistic properties of miscalibration and reports a number of interesting findings that might help humans better analyze, understand and improve NMT models. Based on these observations, we further propose a new graduated label smoothing method that can improve both inference calibration and translation performance.",
                    "[231895728 | Malinin et al. | 2021 | Citations: 265]": "Uncertainty estimation is important for ensuring safety and robustness of AI systems. While most research in the area has focused on un-structured prediction tasks, limited work has investigated general uncertainty estimation approaches for structured prediction. Thus, this work aims to investigate uncertainty estimation for structured prediction tasks within a single unified and interpretable probabilistic ensemble-based framework. We consider: uncertainty estimation for sequence data at the token-level and complete sequence-level; interpretations for, and applications of, various measures of uncertainty; and discuss both the theoretical and practical challenges associated with obtaining them. This work also provides baselines for token-level and sequence-level error detection, and sequence-level out-of-domain input detection on the WMT\u201914 English-French and WMT\u201917 English-German translation and LibriSpeech speech recognition datasets.",
                    "[231979430 | Zhao et al. | 2021 | Citations: 1428]": "GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as \"N/A\". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt.",
                    "[252847587 | Khojah et al. | 2022 | Citations: 3]": "A Natural Language Understanding (NLU) component can be used in a dialogue system to perform intent classification, returning an N-best list of hypotheses with corresponding confidence estimates. We perform an in-depth evaluation of 5 NLUs, focusing on confidence estimation. We measure and visualize calibration for the 10 best hypotheses on model level and rank level, and also measure classification performance. The results indicate a trade-off between calibration and performance. In particular, Rasa (with Sklearn classifier) had the best calibration but the lowest performance scores, while Watson Assistant had the best performance but a poor calibration.",
                    "[253510101 | Stengel-Eskin et al. | 2022 | Citations: 25]": "Abstract Sequence generation models are increasingly being used to translate natural language into programs, i.e., to perform executable semantic parsing. The fact that semantic parsing aims to predict programs that can lead to executed actions in the real world motivates developing safe systems. This in turn makes measuring calibration\u2014a central component to safety\u2014particularly important. We investigate the calibration of popular generation models across four popular semantic parsing datasets, finding that it varies across models and datasets. We then analyze factors associated with calibration error and release new confidence-based challenge splits of two parsing datasets. To facilitate the inclusion of calibration in semantic parsing evaluations, we release a library for computing calibration metrics.1",
                    "[257039062 | Kuhn et al. | 2023 | Citations: 310]": "We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of\"semantic equivalence\"-- different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy -- an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines.",
                    "[258967487 | Lin et al. | 2023 | Citations: 154]": "Large language models (LLMs) specializing in natural language generation (NLG) have recently started exhibiting promising capabilities across a variety of domains. However, gauging the trustworthiness of responses generated by LLMs remains an open challenge, with limited research on uncertainty quantification (UQ) for NLG. Furthermore, existing literature typically assumes white-box access to language models, which is becoming unrealistic either due to the closed-source nature of the latest LLMs or computational constraints. In this work, we investigate UQ in NLG for *black-box* LLMs. We first differentiate *uncertainty* vs *confidence*: the former refers to the ``dispersion'' of the potential predictions for a fixed input, and the latter refers to the confidence on a particular prediction/generation. We then propose and compare several confidence/uncertainty measures, applying them to *selective NLG* where unreliable results could either be ignored or yielded for further assessment. Experiments were carried out with several popular LLMs on question-answering datasets (for evaluation purposes). Results reveal that a simple measure for the semantic dispersion can be a reliable predictor of the quality of LLM responses, providing valuable insights for practitioners on uncertainty management when adopting LLMs. The code to replicate our experiments is available at https://github.com/zlin7/UQ-NLG.",
                    "[268793903 | Zhang et al. | 2024 | Citations: 39]": "Large Language Models (LLMs) have demonstrated remarkable capability in a variety of NLP tasks. However, LLMs are also prone to generate nonfactual content. Uncertainty Quantification (UQ) is pivotal in enhancing our understanding of a model\u2019s confidence on its generation, thereby aiding in the mitigation of nonfactual outputs. Existing research on UQ predominantly targets short text generation, typically yielding brief, word-limited responses. However, real-world applications frequently necessitate much longer responses. Our study first highlights the limitations of current UQ methods in handling long text generation. We then introduce Luq and its two variations, a series of novel sampling-based UQ approaches specifically designed for long text. Our findings reveal that Luq outperforms existing baseline methods in correlating with the model\u2019s factuality scores (negative coefficient of -0.85 observed for Gemini Pro). To further improve the factuality of LLM responses, we propose Luq-Ensemble, a method that ensembles responses from multiple models and selects the response with the lowest uncertainty. The ensembling method greatly improves the response factuality upon the best standalone LLM.",
                    "[270095084 | Duan et al. | 2023 | Citations: 50]": "Large Language Models (LLMs) show promising results in language generation and instruction following but frequently\"hallucinate\", making their outputs less reliable. Despite Uncertainty Quantification's (UQ) potential solutions, implementing it accurately within LLMs is challenging. Our research introduces a simple heuristic: not all tokens in auto-regressive LLM text equally represent the underlying meaning, as\"linguistic redundancy\"often allows a few keywords to convey the essence of long sentences. However, current methods underestimate this inequality when assessing uncertainty, causing tokens with limited semantics to be equally or excessively weighted in UQ. To correct this, we propose Shifting Attention to more Relevant (SAR) components at both token- and sentence-levels for better UQ. We conduct extensive experiments involving a range of popular\"off-the-shelf\"LLMs, such as Vicuna, WizardLM, and LLaMA-2-chat, with model sizes extending up to 33B parameters. We evaluate various free-form question-answering tasks, encompassing domains such as reading comprehension, science Q&A, and medical Q&A. Our experimental results, coupled with a comprehensive demographic analysis, demonstrate the superior performance of SAR. The code is available at https://github.com/jinhaoduan/SAR."
                },
                "metadata": [
                    {
                        "section_title": "Uncertainty and Confidence in LLMs",
                        "pdf_hash": "",
                        "start": 2,
                        "end": 2113,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 130
                            },
                            {
                                "start": 131,
                                "end": 256
                            },
                            {
                                "start": 257,
                                "end": 352
                            },
                            {
                                "start": 355,
                                "end": 436
                            },
                            {
                                "start": 437,
                                "end": 535
                            },
                            {
                                "start": 536,
                                "end": 624
                            },
                            {
                                "start": 625,
                                "end": 866
                            },
                            {
                                "start": 869,
                                "end": 1021
                            },
                            {
                                "start": 1022,
                                "end": 1287
                            },
                            {
                                "start": 1288,
                                "end": 1360
                            },
                            {
                                "start": 1361,
                                "end": 1449
                            },
                            {
                                "start": 1450,
                                "end": 1655
                            },
                            {
                                "start": 1656,
                                "end": 1763
                            },
                            {
                                "start": 1766,
                                "end": 1873
                            },
                            {
                                "start": 1874,
                                "end": 1998
                            },
                            {
                                "start": 1999,
                                "end": 2113
                            }
                        ],
                        "ref_mentions": [
                            "109884250",
                            "127203550",
                            "122150550",
                            "252847587",
                            "218487046",
                            "258865733",
                            "235078802",
                            "253510101",
                            "231979430",
                            "258967487",
                            "268793903",
                            "231895728",
                            "270095084",
                            "257039062"
                        ],
                        "quote": "Calibration refers to closing the gap between the confidence score and the expected correctness conditioned on confidence score. It has a long history preceding LLM or even modern machine learning (Degroot et al., 1983)(Murphy et al., 1977)(Murphy et al., 1984), but bears slightly different meanings in NLP. In general, we could define a perfectly calibrated confidence measure to achieve the following: \n\nwhere the expectation is taken over the joint distribution of x and generation s. A lot of papers focus on evaluating the calibration quality of specific LMs and tasks (Khojah et al., 2022)63,(Wang et al., 2020). Evaluation typically relies on variants of Expected Calibration Error (ECE) [63,71,(Tian et al., 2023). Oftentimes confidence scores from classification could be directly applied (Jiang et al., 2020)(Stengel-Eskin et al., 2022)(Zhao et al., 2021) in order to evaluate whether an LM is overor under-confident, especially for de facto classification tasks like sentiment analysis or multiple-choice QA. \n\nAs uncertainty and confidence are often intertwined, many approaches used in uncertainty quantification have their counterpart in confidence estimation. For example, for black-box methods, Lin et al. (Lin et al., 2023) computes a similarity matrix of sampled responses and derives confidence estimates for each generation via its degree or distance derived from the graph Laplacian, before using these scores to compute uncertainty. Zhang et al. (Zhang et al., 2024) extends such black-box methods to longer generations. For logit-based methods, Malinin and Gales (Malinin et al., 2021) normalize Eq. ( 1) with the length of s. Further improvements include replacing the logit-sum or mean with weighted sum, by attention values on the important tokens [78] or by importance inferred from natural language inference (NLI) models (Duan et al., 2023). Such variants of sequence likelihood could then be fed for (entropy-style) uncertainty computation (Kuhn et al., 2023)[78]. \n\nAnother popular approach is asking the LM itself whether a particular free-form generation is correct [59]. However, this formulation also poses a restriction on the confidence estimation method, as it is essentially a scalar logit. Thus, many extensions focus on applying calibration methods from classification to calibrate such self-evaluation."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[278129791 | Abbasli et al. | 2025 | Citations: 0]",
                "snippets": "Post-hoc or post-training methods, such as TS (Guo et al., 2017), Platt scaling (Platt, 1999), histogram binning (Zadrozny et al., 2001), and training with label smoothing (M\u00fcller et al., 2019), are classical methods of calibration. Mixup training with learning the complexity of samples was proposed to calibrate the pre-trained language models [100]. We note that open-box methods of calibration can be complex and difficult to apply to LLMs, but they can be more informative of inner states than post-hoc methods [101].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[174802983 | Muller et al. | 2019 | Citations: 1954]": "The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions.",
                    "[28671436 | Guo et al. | 2017 | Citations: 5869]": "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."
                },
                "metadata": [
                    {
                        "section_title": "A. Model Calibration",
                        "pdf_hash": "",
                        "start": 1644,
                        "end": 2107,
                        "sentence_offsets": [
                            {
                                "start": 1644,
                                "end": 1817
                            },
                            {
                                "start": 1818,
                                "end": 1937
                            },
                            {
                                "start": 1938,
                                "end": 2107
                            }
                        ],
                        "ref_mentions": [
                            "28671436",
                            "56563878",
                            "9594071",
                            "174802983"
                        ],
                        "quote": "Post-hoc or post-training methods, such as TS (Guo et al., 2017), Platt scaling (Platt, 1999), histogram binning (Zadrozny et al., 2001), and training with label smoothing (M\u00fcller et al., 2019), are classical methods of calibration. Mixup training with learning the complexity of samples was proposed to calibrate the pre-trained language models [100]. We note that open-box methods of calibration can be complex and difficult to apply to LLMs, but they can be more informative of inner states than post-hoc methods [101]."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.03237,
        "cot": "To effectively organize the quotes and answer the user's query about the most effective methods for improving the calibration of language model confidence scores, I'll create dimensions that address different aspects of calibration methods, their implementations, and comparisons.\n\n1. First, I'll include a \"Background and Importance of Calibration\" section to provide context on what calibration is and why it matters for language models. This will be in synthesis format to give readers a coherent understanding of the foundational concepts.\n\n2. Next, I'll create a \"Softmax-based Calibration Methods\" dimension to cover temperature scaling and other softmax-based techniques that appear frequently in the quotes. This will be in synthesis format to explain how these methods work conceptually.\n\n3. For \"Post-hoc Calibration Techniques\", I'll use a list format to enumerate various post-processing methods that can be applied after model training. The quotes mention several distinct techniques that would be better presented as a list for clarity.\n\n4. A \"Training-based Calibration Methods\" dimension will cover approaches that involve modifying the training process, like label smoothing. This will be in synthesis format to explain the relationships between these approaches.\n\n5. \"Multicalibration and Group-wise Calibration\" will focus specifically on these advanced approaches that appear in several quotes. Synthesis format is appropriate here to explain these complex concepts.\n\n6. \"Verbalization and Consistency-based Methods\" will cover techniques specific to LLMs that involve directly asking models for confidence. This will be in synthesis format to explain the common principles.\n\n7. \"Comparative Effectiveness of Calibration Methods\" will synthesize the quotes that directly compare different calibration approaches, addressing the user's question about how these techniques compare in practice.\n\n8. \"Adaptive and Domain-specific Calibration\" will be in synthesis format to explain approaches that adapt calibration to specific domains or contexts.\n\n9. \"Implementation Considerations\" will use list format to provide practical considerations when implementing calibration methods.\n\nThis organization ensures we cover all the major calibration methods mentioned in the quotes while directly addressing the user's query about effectiveness and practical comparisons.",
        "plan": {
            "Background and Importance of Calibration (synthesis)": [
                0,
                7,
                19
            ],
            "Softmax-based Calibration Methods (synthesis)": [
                0,
                2,
                10,
                15,
                16
            ],
            "Post-hoc Calibration Techniques (list)": [
                0,
                1,
                2,
                3,
                5,
                6,
                13,
                18
            ],
            "Training-based Calibration Methods (synthesis)": [
                0,
                1,
                2,
                4,
                5
            ],
            "Multicalibration and Group-wise Calibration (synthesis)": [
                8,
                9,
                14,
                18
            ],
            "Verbalization and Consistency-based Methods (synthesis)": [
                3,
                5,
                6,
                7,
                11,
                12,
                15
            ],
            "Comparative Effectiveness of Calibration Methods (synthesis)": [
                3,
                10,
                11,
                12,
                13,
                17
            ],
            "Adaptive and Domain-specific Calibration (synthesis)": [
                8,
                13,
                20
            ],
            "Implementation Considerations (list)": [
                1,
                10,
                18,
                20
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Background and Importance of Calibration",
                "tldr": "Calibration in language models refers to how well a model's confidence scores correlate with its actual likelihood of being correct. Well-calibrated models are essential for building trustworthy AI systems that can accurately indicate when they are uncertain about their outputs. (9 sources)",
                "text": "\nCalibration has a long history in probability theory and statistics, predating modern machine learning approaches <Paper corpusId=\"109884250\" paperTitle=\"(Degroot et al., 1983)\" isShortName></Paper> <Paper corpusId=\"122150550\" paperTitle=\"(Murphy et al., 1984)\" isShortName></Paper> <Paper corpusId=\"127203550\" paperTitle=\"(Murphy et al., 1977)\" isShortName></Paper>. In the context of language models (LMs), calibration refers to the alignment between a model's confidence scores and the probability that its predictions are correct <Paper corpusId=\"235078802\" paperTitle=\"(Jiang et al., 2020)\" isShortName></Paper>. A perfectly calibrated confidence measure should satisfy the condition that the expected correctness conditioned on a particular confidence score equals that confidence score <Paper corpusId=\"277150701\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>.\n\nThe importance of calibration has grown with the increasing deployment of language models across various applications. As Jiang et al. note, \"How can we know when language models know, with confidence, the answer to a particular query?\" <Paper corpusId=\"235078802\" paperTitle=\"(Jiang et al., 2020)\" isShortName></Paper>. This question highlights a fundamental issue: despite impressive capabilities, no model is perfect, and they often fail to provide appropriate answers. Poorly calibrated models may be overconfident in incorrect predictions or underconfident in correct ones, potentially leading to unreliable system behavior.\n\nWell-calibrated confidence scores are increasingly recognized as essential for building trustworthy AI systems <Paper corpusId=\"268032879\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"252917981\" paperTitle=\"(Si et al., 2022)\" isShortName></Paper>. Properly calibrated models can:\n\n1. Enable more reliable decision-making in real-world applications\n2. Facilitate better human-AI collaboration by indicating when a model is uncertain\n3. Help identify when a model should defer to human experts or additional information sources\n4. Improve safety and robustness in deployment scenarios\n\nThe evaluation of calibration quality typically relies on metrics such as Expected Calibration Error (ECE), which measures the discrepancy between confidence scores and empirical accuracy <Paper corpusId=\"258865733\" paperTitle=\"(Tian et al., 2023)\" isShortName></Paper> <Paper corpusId=\"277150701\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>. Notably, recent research has shown that while pre-trained language models can be reasonably well-calibrated for in-domain tasks, their calibration often deteriorates when faced with out-of-domain examples or after fine-tuning processes like reinforcement learning from human feedback (RLHF) <Paper corpusId=\"212747810\" paperTitle=\"(Desai et al., 2020)\" isShortName></Paper> <Paper corpusId=\"258865733\" paperTitle=\"(Tian et al., 2023)\" isShortName></Paper>.\n\nGiven these challenges, researchers have developed various methods to improve calibration, ranging from fine-tuning approaches that modify model parameters to post-hoc techniques that adjust confidence scores without changing the underlying model <Paper corpusId=\"235078802\" paperTitle=\"(Jiang et al., 2020)\" isShortName></Paper>. These calibration methods will be explored in detail in subsequent sections.",
                "citations": [
                    {
                        "id": "(Degroot et al., 1983)",
                        "snippets": [
                            "In this paper we present methods for comparing and evaluating forecasters whose predictions are presented as their subjective probability distributions of various random variables that will be observed in the future, e.g. weather forecasters who each day must specify their own probabilities that it will rain in a particular location. We begin by reviewing the concepts of calibration and refinement, and describiing the relationship between this notion of refinement and the notion of sufficiency in the comparison of statistical experiments. We also consider the question of interrelationships among forecasters and discuss methods by which an observer should combine the predictions from two or more different forecasters. Then we turn our attention to the concept of a proper scoring rule for evaluating forecasters, relating it to the concepts of calibration and refinement. Finally, we discuss conditions under which one fore- caster can exploit the predictions of another forecaster to obtain a better score. In this paper we describe some concepts and methods appropriate for evaluating and com- paring forecasters who repeatedly present their predictions of whether or not various events will occur in terms of their subjective probabilities of those events. The ideas we describe here are relevant in almost any situation in which forecasters must repeatedly make such probabilistic predictions, regardless of the particular subject matter or substantive area of the events being forecast. The forecaster might be an economist who at the beginning of each quarterly period makes predictions about unemployment, the rate of inflation, or Gross National Product in that quarter based on the values of various economic indicators; the forecaster might even make predictions using a large-scale econometric model of the United States economy based on hundreds of variables and econometric relations. In a different field, the forecaster might be the weatherman for a television station who at the beginning of each day must announce his probability that it will rain during the day. For ease of exposition, we present our discussions here in the context of such a weather forecaster who day after day must specify his subjective probability x that there will be at least a certain amount of rain at some given location during a specified time interval of the day. We refer to the occurrence of this well-specified event simply as \"rain\". Thus, at the begin- ning of each day the forecaster must specify his probability of rain and at the end of each day he observes whether or not rain actually occurred. The probability x specified by the forecaster on any particular day is called his prediction for that day. We shall make the realistic, and simultaneously simplifying, assumption that the"
                        ],
                        "paper": {
                            "corpus_id": 109884250,
                            "title": "The Comparison and Evaluation of Forecasters.",
                            "authors": [
                                {
                                    "authorId": "39416713",
                                    "name": "M. Degroot"
                                },
                                {
                                    "authorId": "1684961",
                                    "name": "S. Fienberg"
                                }
                            ],
                            "year": 1983,
                            "venue": "",
                            "n_citations": 745
                        },
                        "score": 0
                    },
                    {
                        "id": "(Murphy et al., 1984)",
                        "snippets": [
                            "Abstract Efforts to quantify the uncertainty in weather forecasts began more than 75 years ago, and many studies and experiments involving objective and subjective probability forecasting have been conducted in meteorology in the intervening period. Moreover, the U.S. National Weather Service (NWS) initiated a nationwide program in 1965 in which precipitation probability forecasts were formulated on an operational basis and routinely disseminated to the general public. In addition, the NWS now prepares objective probability forecasts for many variables, using statistical procedures. Hence probability forecasting in meteorology is unique in that very large sets of probability forecasts that have been subjected to detailed evaluation are available. This article has four objectives: (a) to review the history of probability forecasting in meteorology to acquaint statisticians with this body of literature; (b) to describe recent methodological, experimental, and operational activities in this field; (c) to exa"
                        ],
                        "paper": {
                            "corpus_id": 122150550,
                            "title": "Probability Forecasting in Meteorology",
                            "authors": [
                                {
                                    "authorId": "32620901",
                                    "name": "A. H. Murphy"
                                },
                                {
                                    "authorId": "1830895",
                                    "name": "R. L. Winkler"
                                }
                            ],
                            "year": 1984,
                            "venue": "",
                            "n_citations": 338
                        },
                        "score": 0
                    },
                    {
                        "id": "(Murphy et al., 1977)",
                        "snippets": [
                            "SUMMARY This paper briefly describes some results of operational and experimental programmes in the United States involving subjective probability forecasts of precipitation occurrence and of maximum and minimum temperatures. These results indicate that weather forecasters can formulate such forecasts in a reliable manner."
                        ],
                        "paper": {
                            "corpus_id": 127203550,
                            "title": "Reliability of Subjective Probability Forecasts of Precipitation and Temperature",
                            "authors": [
                                {
                                    "authorId": "32620901",
                                    "name": "A. H. Murphy"
                                },
                                {
                                    "authorId": "1830895",
                                    "name": "R. L. Winkler"
                                }
                            ],
                            "year": 1977,
                            "venue": "",
                            "n_citations": 387
                        },
                        "score": 0
                    },
                    {
                        "id": "(Jiang et al., 2020)",
                        "snippets": [
                            "Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, \"How can we know when language models know, with confidence, the answer to a particular query?\" We examine this question from the point of view of calibration, the property of a probabilistic model's predicted probabilities actually being well correlated with the probabilities of correctness.\n\nTo alleviate this problem, we propose methods to make LMs' confidence scores correlate better with the likelihood of model prediction being correct. We examined both fine-tuning methods that modify LMs' parameters and post-hoc methods that keep LMs fixed and only manipulate the confidence values or inputs. Specifically, we fine-tune the LM using softmax-or marginbased objective functions based on multiple candidate answers. For post-hoc calibration, we examined temperature-based scaling and feature-based decision trees that take prediction probability and input-related features as input and produce calibrated confidence (Jagannatha and Yu, 2020;Desai and Durrett, 2020;Kamath et al., 2020). We also study the sensitivity of LMs' confidence estimation with respect to language variation by paraphrasing candidate answers and augmenting questions using retrieved context.\n\nBoth softmax-based and margin-based fine-tuning, which explicitly compare and adjust the probability of candidate answers, can further improve ECE on multiplechoice datasets. We argue that the softmax-based and margin-based objective functions are better suited for questions with potential candidates.\n\nPost-processing the raw confidence either solely based on confidence or other features is effective across all datasets, which is consistent with the conclusion on other tasks such as structured prediction and natural language inference (Jagannatha and Yu, 2020; Desai and Durrett, 2020). We demonstrate the histogram of confidence before and after applying temperature-based scaling or feature-based decision trees in Figure 2. LMs tend to be over-confident, with most predictions having either extremely high or low confidence. Both methods can successfully re-scale the confidence to reasonable ranges, thus improving the calibration performance.\n\nThe improvement brought by using paraphrasing is significant on multiple-choice datasets, demonstrating that using diverse expressions can indeed improve confidence estimation."
                        ],
                        "paper": {
                            "corpus_id": 235078802,
                            "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
                            "authors": [
                                {
                                    "authorId": "2669515",
                                    "name": "Zhengbao Jiang"
                                },
                                {
                                    "authorId": "50007145",
                                    "name": "J. Araki"
                                },
                                {
                                    "authorId": "47929135",
                                    "name": "Haibo Ding"
                                },
                                {
                                    "authorId": "1700325",
                                    "name": "Graham Neubig"
                                }
                            ],
                            "year": 2020,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 436
                        },
                        "score": 0.95947265625
                    },
                    {
                        "id": "(Liu et al., 2025)",
                        "snippets": [
                            "Calibration refers to closing the gap between the confidence score and the expected correctness conditioned on confidence score. It has a long history preceding LLM or even modern machine learning (Degroot et al., 1983)(Murphy et al., 1977)(Murphy et al., 1984), but bears slightly different meanings in NLP. In general, we could define a perfectly calibrated confidence measure to achieve the following: \n\nwhere the expectation is taken over the joint distribution of x and generation s. A lot of papers focus on evaluating the calibration quality of specific LMs and tasks (Khojah et al., 2022)63,(Wang et al., 2020). Evaluation typically relies on variants of Expected Calibration Error (ECE) [63,71,(Tian et al., 2023). Oftentimes confidence scores from classification could be directly applied (Jiang et al., 2020)(Stengel-Eskin et al., 2022)(Zhao et al., 2021) in order to evaluate whether an LM is overor under-confident, especially for de facto classification tasks like sentiment analysis or multiple-choice QA. \n\nAs uncertainty and confidence are often intertwined, many approaches used in uncertainty quantification have their counterpart in confidence estimation. For example, for black-box methods, Lin et al. (Lin et al., 2023) computes a similarity matrix of sampled responses and derives confidence estimates for each generation via its degree or distance derived from the graph Laplacian, before using these scores to compute uncertainty. Zhang et al. (Zhang et al., 2024) extends such black-box methods to longer generations. For logit-based methods, Malinin and Gales (Malinin et al., 2021) normalize Eq. ( 1) with the length of s. Further improvements include replacing the logit-sum or mean with weighted sum, by attention values on the important tokens [78] or by importance inferred from natural language inference (NLI) models (Duan et al., 2023). Such variants of sequence likelihood could then be fed for (entropy-style) uncertainty computation (Kuhn et al., 2023)[78]. \n\nAnother popular approach is asking the LM itself whether a particular free-form generation is correct [59]. However, this formulation also poses a restriction on the confidence estimation method, as it is essentially a scalar logit. Thus, many extensions focus on applying calibration methods from classification to calibrate such self-evaluation."
                        ],
                        "paper": {
                            "corpus_id": 277150701,
                            "title": "Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2346334761",
                                    "name": "Xiaoou Liu"
                                },
                                {
                                    "authorId": "2278853459",
                                    "name": "Tiejin Chen"
                                },
                                {
                                    "authorId": "1387492282",
                                    "name": "Longchao Da"
                                },
                                {
                                    "authorId": "150946029",
                                    "name": "Chacha Chen"
                                },
                                {
                                    "authorId": "2351350698",
                                    "name": "Zhen Lin"
                                },
                                {
                                    "authorId": "2277576540",
                                    "name": "Hua Wei"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.8603515625
                    },
                    {
                        "id": "(Zhao et al., 2024)",
                        "snippets": [
                            "Confidence Calibration of LLMs. Extracting honest and constructive confidence scores of large language models is considered an important step towards building faithful and trustworthy AI systems (Desai et al., 2020)(Si et al., 2022). Many methods have been proposed recently to get reliable confidence scores with different suffix prompts added after outputting possible answers, such as a follow of True or False multiple choice question (Kadavath et al., 2022), asking models to describe the likelihood of the answer (Lin et al., 2022), and describing the likelihood that the answer is correct with demonstrations (Tian et al., 2023). However, it remains unclear how robust the methods are and how good they are comparatively. Our paper proposes FaR prompting as an orthogonal method to improve calibration and compare different extraction methods with our test bed."
                        ],
                        "paper": {
                            "corpus_id": 268032879,
                            "title": "Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "1500662261",
                                    "name": "Xinran Zhao"
                                },
                                {
                                    "authorId": "2279761345",
                                    "name": "Hongming Zhang"
                                },
                                {
                                    "authorId": "2243367575",
                                    "name": "Xiaoman Pan"
                                },
                                {
                                    "authorId": "2087264100",
                                    "name": "Wenlin Yao"
                                },
                                {
                                    "authorId": "2256336899",
                                    "name": "Dong Yu"
                                },
                                {
                                    "authorId": "2287820683",
                                    "name": "Tongshuang Wu"
                                },
                                {
                                    "authorId": "2108276402",
                                    "name": "Jianshu Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 7
                        },
                        "score": 0.91015625
                    },
                    {
                        "id": "(Si et al., 2022)",
                        "snippets": [
                            "Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3's reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM's factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions. Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3."
                        ],
                        "paper": {
                            "corpus_id": 252917981,
                            "title": "Prompting GPT-3 To Be Reliable",
                            "authors": [
                                {
                                    "authorId": "152358188",
                                    "name": "Chenglei Si"
                                },
                                {
                                    "authorId": "144702900",
                                    "name": "Zhe Gan"
                                },
                                {
                                    "authorId": "2149231840",
                                    "name": "Zhengyuan Yang"
                                },
                                {
                                    "authorId": "2992833",
                                    "name": "Shuohang Wang"
                                },
                                {
                                    "authorId": "2124948371",
                                    "name": "Jianfeng Wang"
                                },
                                {
                                    "authorId": "1389036863",
                                    "name": "Jordan L. Boyd-Graber"
                                },
                                {
                                    "authorId": "29957038",
                                    "name": "Lijuan Wang"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 302
                        },
                        "score": 0
                    },
                    {
                        "id": "(Tian et al., 2023)",
                        "snippets": [
                            "Among the methods for verbalizing probabilities directly, we observe that generating and evaluating multiple hypotheses improves calibration (see Figure 1), similarly to humans (Lord et al., 1984), and corroborating a similar finding in LMs (Kadavath et al., 2022)",
                            "We show that prompting a model to produce several answer choices before giving its confidence scores significantly improves calibration of verbalized probabilities. Combined with temperature scaling (Guo et al., 2017), this approach generally provides better calibration than model probabilities for ChatGPT, GPT-4, and Claude 2 across three datasets, often reducing expected calibration error (ECE) by over 50%."
                        ],
                        "paper": {
                            "corpus_id": 258865733,
                            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
                            "authors": [
                                {
                                    "authorId": "2218407338",
                                    "name": "Katherine Tian"
                                },
                                {
                                    "authorId": "49688913",
                                    "name": "E. Mitchell"
                                },
                                {
                                    "authorId": "2064472884",
                                    "name": "Allan Zhou"
                                },
                                {
                                    "authorId": "50465276",
                                    "name": "Archit Sharma"
                                },
                                {
                                    "authorId": "102801230",
                                    "name": "Rafael Rafailov"
                                },
                                {
                                    "authorId": "18307037",
                                    "name": "Huaxiu Yao"
                                },
                                {
                                    "authorId": "46881670",
                                    "name": "Chelsea Finn"
                                },
                                {
                                    "authorId": "144783904",
                                    "name": "Christopher D. Manning"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 355
                        },
                        "score": 0.89501953125
                    },
                    {
                        "id": "(Desai et al., 2020)",
                        "snippets": [
                            "Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain."
                        ],
                        "paper": {
                            "corpus_id": 212747810,
                            "title": "Calibration of Pre-trained Transformers",
                            "authors": [
                                {
                                    "authorId": "120777041",
                                    "name": "Shrey Desai"
                                },
                                {
                                    "authorId": "1814094",
                                    "name": "Greg Durrett"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 301
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Softmax-based Calibration Methods",
                "tldr": "Softmax-based calibration methods directly manipulate the probability distribution produced by language models to achieve better alignment between confidence scores and actual correctness. Temperature scaling is particularly effective, requiring only a single parameter adjustment to significantly improve calibration across various tasks. (8 sources)",
                "text": "\nSoftmax-based methods represent one of the most widely used approaches for calibrating language model confidence scores. These techniques typically focus on adjusting the softmax function that converts raw model logits into probability distributions. The most prominent among these is temperature scaling, which involves dividing logits by a temperature parameter T before applying the softmax function <Paper corpusId=\"28671436\" paperTitle=\"(Guo et al., 2017)\" isShortName></Paper> <Paper corpusId=\"253098773\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper>. This simple yet effective approach has been shown to significantly improve calibration across various language tasks without changing the relative ranking of predictions.\n\nTemperature scaling is particularly appealing because it requires optimizing only a single parameter, which can be learned by maximizing log-likelihood on a validation set while keeping the other model parameters fixed <Paper corpusId=\"253098773\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper>. Empirical studies have demonstrated that optimal temperature values typically range from 1.5 to 4.0, with a default value of 2.0 often serving as a robust choice across different datasets <Paper corpusId=\"269449363\" paperTitle=\"(Tarride et al., 2024)\" isShortName></Paper>. Researchers have observed that language models tend to be overconfident in their predictions, with most confidence scores clustered at extremely high or low values. Temperature scaling effectively redistributes these scores to better reflect actual correctness probabilities <Paper corpusId=\"235078802\" paperTitle=\"(Jiang et al., 2020)\" isShortName></Paper>.\n\nAnother softmax-based approach is label smoothing, which penalizes low entropy distributions by assigning a fixed probability to the true label and distributing the remaining probability mass uniformly across other classes <Paper corpusId=\"253098773\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper>. This technique has proven competitive with temperature scaling for calibration <Paper corpusId=\"174802983\" paperTitle=\"(Muller et al., 2019)\" isShortName></Paper>, particularly in out-of-domain settings <Paper corpusId=\"212747810\" paperTitle=\"(Desai et al., 2020)\" isShortName></Paper>.\n\nRecent advancements in softmax-based calibration include the \"invert softmax trick,\" which approximates logits by inverting verbalized probabilities to address issues arising from the scaling of these probabilities <Paper corpusId=\"273228940\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. Another innovative approach transforms the multiclass calibration problem into calibrating a single surrogate binary classifier, enabling more efficient use of standard calibration methods <Paper corpusId=\"273821339\" paperTitle=\"(LeCoz et al., 2024)\" isShortName></Paper>.\n\nFor multiple-choice questions, fine-tuning language models with softmax-based or margin-based objective functions that explicitly compare and adjust the probabilities of candidate answers has been shown to improve expected calibration error (ECE) <Paper corpusId=\"235078802\" paperTitle=\"(Jiang et al., 2020)\" isShortName></Paper>. These methods can be combined with other techniques, such as using temperature scaling after label smoothing or applying few-shot learning with label smoothing, to further enhance calibration performance <Paper corpusId=\"253098773\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Guo et al., 2017)",
                        "snippets": [
                            "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."
                        ],
                        "paper": {
                            "corpus_id": 28671436,
                            "title": "On Calibration of Modern Neural Networks",
                            "authors": [
                                {
                                    "authorId": "144993411",
                                    "name": "Chuan Guo"
                                },
                                {
                                    "authorId": "10804137",
                                    "name": "Geoff Pleiss"
                                },
                                {
                                    "authorId": "2117103358",
                                    "name": "Yu Sun"
                                },
                                {
                                    "authorId": "7446832",
                                    "name": "Kilian Q. Weinberger"
                                }
                            ],
                            "year": 2017,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 5869
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ahuja et al., 2022)",
                        "snippets": [
                            "We briefly review some commonly used methods for calibrating neural network based classifiers. 1. Temperature Scaling (TS and Self-TS) (Guo et al., 2017) is applied by scaling the output logits using a temperature parameter T before applying softmax i.e. : \n\n, where o k denotes the logits corresponding to the k th class. T is a learnable parameter obtained posttraining by maximizing the log-likelihood on the dev set while keeping other network parameters fixed. We experiment with two settings for improving calibration on a target language: using dev data in English to perform temperature scaling (TS) and using the target language's dev data (Self-TS). 2. Label Smoothing (LS) (Pereyra et al., 2017) is a regularization technique that penalizes low entropy distributions by using soft labels that are obtained by assigning a fixed probability q = 1 \u2212 \u03b1 to the true label (0 < \u03b1 < 1), and distributing the remaining probability mass uniformly across the remaining classes. Label smoothing has been empirically shown to be competitive with temperature scaling for calibration (M\u00fcller et al., 2019) especially in out of domain settings (Desai et al., 2020) 3. Few-Shot Learning (FSL) We also investigate if fine-tuning the MMLM on a few examples in a target language in addition to the data in English, leads to any improvement in calibration as it does in the performance (Lauscher et al., 2020). Since these models are expected to be calibrated worse for out-of-domain data compared to in-domain data (Desai et al., 2020), we try to improve calibration by reducing the domain shift through fewshot learning. \n\nApart from these, we also consider combinations of different calibration methods in our experiments, including Label Smoothing with Temperature Scaling (TS + LS or Self-TS + LS) and Few-Shot Learning with Label Smoothing (FSL + LS)."
                        ],
                        "paper": {
                            "corpus_id": 253098773,
                            "title": "On the Calibration of Massively Multilingual Language Models",
                            "authors": [
                                {
                                    "authorId": "52154863",
                                    "name": "Kabir Ahuja"
                                },
                                {
                                    "authorId": "3010457",
                                    "name": "Sunayana Sitaram"
                                },
                                {
                                    "authorId": "34725175",
                                    "name": "Sandipan Dandapat"
                                },
                                {
                                    "authorId": "143990839",
                                    "name": "M. Choudhury"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 16
                        },
                        "score": 0.875
                    },
                    {
                        "id": "(Tarride et al., 2024)",
                        "snippets": [
                            "The results show that the most effective method is to take the softmax value for each token after the CTC decoding step and average these values for each line to obtain line-level confidence scores. This approach not only ensures computational efficiency, but also yields a strong correlation with the recognition rate. As a result of these findings, we have incorporated this calculation method into PyLaia.\n\nTable 3 provides insight into the impact of model calibration on the linear correlation between confidence scores and recognition rate on the test set of different datasets",
                            "The results show significant improvements in the linear correlation between confidence scores and recognition rates when temperature scaling is applied. Across different datasets, optimal temperatures ranging from 1.5 to 4.0 yield consistently higher correlations compared to scenarios without scaling. Since temperature scaling only scales the logits without changing the underlying ranking of the predictions, it does not affect the rank correlation.\n\nOur analysis suggests that the default value of 2.0 is a robust choice for calibration, even for datasets where the optimal temperature is not equal to 2.0."
                        ],
                        "paper": {
                            "corpus_id": 269449363,
                            "title": "Improving Automatic Text Recognition with Language Models in the PyLaia Open-Source Library",
                            "authors": [
                                {
                                    "authorId": "116700157",
                                    "name": "Sol\u00e8ne Tarride"
                                },
                                {
                                    "authorId": "2298900821",
                                    "name": "Yoann Schneider"
                                },
                                {
                                    "authorId": "2298903215",
                                    "name": "Marie Generali-Lince"
                                },
                                {
                                    "authorId": "1380222130",
                                    "name": "M\u00e9lodie Boillet"
                                },
                                {
                                    "authorId": "1491425010",
                                    "name": "Bastien Abadie"
                                },
                                {
                                    "authorId": "2285800930",
                                    "name": "Christopher Kermorvant"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE International Conference on Document Analysis and Recognition",
                            "n_citations": 4
                        },
                        "score": 0.97314453125
                    },
                    {
                        "id": "(Jiang et al., 2020)",
                        "snippets": [
                            "Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, \"How can we know when language models know, with confidence, the answer to a particular query?\" We examine this question from the point of view of calibration, the property of a probabilistic model's predicted probabilities actually being well correlated with the probabilities of correctness.\n\nTo alleviate this problem, we propose methods to make LMs' confidence scores correlate better with the likelihood of model prediction being correct. We examined both fine-tuning methods that modify LMs' parameters and post-hoc methods that keep LMs fixed and only manipulate the confidence values or inputs. Specifically, we fine-tune the LM using softmax-or marginbased objective functions based on multiple candidate answers. For post-hoc calibration, we examined temperature-based scaling and feature-based decision trees that take prediction probability and input-related features as input and produce calibrated confidence (Jagannatha and Yu, 2020;Desai and Durrett, 2020;Kamath et al., 2020). We also study the sensitivity of LMs' confidence estimation with respect to language variation by paraphrasing candidate answers and augmenting questions using retrieved context.\n\nBoth softmax-based and margin-based fine-tuning, which explicitly compare and adjust the probability of candidate answers, can further improve ECE on multiplechoice datasets. We argue that the softmax-based and margin-based objective functions are better suited for questions with potential candidates.\n\nPost-processing the raw confidence either solely based on confidence or other features is effective across all datasets, which is consistent with the conclusion on other tasks such as structured prediction and natural language inference (Jagannatha and Yu, 2020; Desai and Durrett, 2020). We demonstrate the histogram of confidence before and after applying temperature-based scaling or feature-based decision trees in Figure 2. LMs tend to be over-confident, with most predictions having either extremely high or low confidence. Both methods can successfully re-scale the confidence to reasonable ranges, thus improving the calibration performance.\n\nThe improvement brought by using paraphrasing is significant on multiple-choice datasets, demonstrating that using diverse expressions can indeed improve confidence estimation."
                        ],
                        "paper": {
                            "corpus_id": 235078802,
                            "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
                            "authors": [
                                {
                                    "authorId": "2669515",
                                    "name": "Zhengbao Jiang"
                                },
                                {
                                    "authorId": "50007145",
                                    "name": "J. Araki"
                                },
                                {
                                    "authorId": "47929135",
                                    "name": "Haibo Ding"
                                },
                                {
                                    "authorId": "1700325",
                                    "name": "Graham Neubig"
                                }
                            ],
                            "year": 2020,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 436
                        },
                        "score": 0.95947265625
                    },
                    {
                        "id": "(Muller et al., 2019)",
                        "snippets": [
                            "The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions."
                        ],
                        "paper": {
                            "corpus_id": 174802983,
                            "title": "When Does Label Smoothing Help?",
                            "authors": [
                                {
                                    "authorId": "2114054259",
                                    "name": "Rafael M\u00fcller"
                                },
                                {
                                    "authorId": "40464924",
                                    "name": "Simon Kornblith"
                                },
                                {
                                    "authorId": "1695689",
                                    "name": "Geoffrey E. Hinton"
                                }
                            ],
                            "year": 2019,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 1954
                        },
                        "score": 0
                    },
                    {
                        "id": "(Desai et al., 2020)",
                        "snippets": [
                            "Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain."
                        ],
                        "paper": {
                            "corpus_id": 212747810,
                            "title": "Calibration of Pre-trained Transformers",
                            "authors": [
                                {
                                    "authorId": "120777041",
                                    "name": "Shrey Desai"
                                },
                                {
                                    "authorId": "1814094",
                                    "name": "Greg Durrett"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 301
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Recent methods have demonstrated improved calibration by applying techniques like Platt scaling or temperature scaling to the confidence scores generated by LLMs. In this paper, we explore the calibration of verbalized probability distributions for discriminative tasks. First, we investigate the capability of LLMs to generate probability distributions over categorical labels. We theoretically and empirically identify the issue of re-softmax arising from the scaling of verbalized probabilities, and propose using the invert softmax trick to approximate the\"logit\"by inverting verbalized probabilities."
                        ],
                        "paper": {
                            "corpus_id": 273228940,
                            "title": "Calibrating Verbalized Probabilities for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2214132154",
                                    "name": "Cheng Wang"
                                },
                                {
                                    "authorId": "72270481",
                                    "name": "Gyuri Szarvas"
                                },
                                {
                                    "authorId": "2325041176",
                                    "name": "Georges Balazs"
                                },
                                {
                                    "authorId": "70407105",
                                    "name": "Pavel Danchenko"
                                },
                                {
                                    "authorId": "2325095919",
                                    "name": "Patrick Ernst"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.923828125
                    },
                    {
                        "id": "(LeCoz et al., 2024)",
                        "snippets": [
                            "To address this issue, we transform the problem of calibrating a multiclass classifier into calibrating a single surrogate binary classifier. This approach allows for more efficient use of standard calibration methods. We evaluate our approach on numerous neural networks used for image or text classification and show that it significantly enhances existing calibration methods."
                        ],
                        "paper": {
                            "corpus_id": 273821339,
                            "title": "Confidence Calibration of Classifiers with Many Classes",
                            "authors": [
                                {
                                    "authorId": "2329178942",
                                    "name": "Adrien LeCoz"
                                },
                                {
                                    "authorId": "2300096291",
                                    "name": "St\u00e9phane Herbin"
                                },
                                {
                                    "authorId": "7167973",
                                    "name": "Faouzi Adjed"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 1
                        },
                        "score": 0.92236328125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Post-hoc Calibration Techniques",
                "tldr": "Post-hoc calibration techniques improve model confidence estimates without modifying the underlying language model. These methods include temperature scaling, histogram binning, isotonic regression, feature-based decision trees, and multi-hypothesis generation, which effectively redistribute confidence scores to better align with actual prediction accuracy. (10 sources)",
                "text": "\nPost-hoc calibration methods are particularly valuable because they can be applied to pre-trained language models without requiring expensive retraining or access to model internals. These techniques focus on manipulating the confidence values or inputs while keeping the underlying model fixed <Paper corpusId=\"235078802\" paperTitle=\"(Jiang et al., 2020)\" isShortName></Paper>.\n\n## Temperature Scaling\n\nTemperature scaling remains one of the most widely used post-hoc calibration methods, where logits are divided by a temperature parameter before applying the softmax function. This simple approach has proven remarkably effective at calibrating predictions across various tasks <Paper corpusId=\"28671436\" paperTitle=\"(Guo et al., 2017)\" isShortName></Paper>. Recent innovations include Adaptive Temperature Scaling (ATS), which predicts a temperature scaling parameter for each token prediction rather than using a single global parameter. ATS has been shown to improve calibration by 10-50% across several natural language benchmarks, particularly addressing calibration shifts that occur after RLHF fine-tuning <Paper corpusId=\"272987064\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>.\n\n## Histogram-Based Methods\n\n* **Histogram Binning**: This method adjusts confidence values within different intervals by assessing the correctness of predictions in each bin, providing a simple calibration approach, though potentially limited when confidence distributions are uneven <Paper corpusId=\"274789494\" paperTitle=\"(Xie et al._1, 2024)\" isShortName></Paper>.\n\n* **Isotonic Regression**: This technique seeks an optimal monotonically increasing function to map model confidence to correctness while preserving the relative ranking of predictions <Paper corpusId=\"274789494\" paperTitle=\"(Xie et al._1, 2024)\" isShortName></Paper>.\n\n## Feature-Based Approaches\n\n* **Feature-Based Decision Trees**: These methods incorporate not only confidence scores but also input-related features to produce calibrated confidence estimates. Research has shown that post-processing raw confidence based on additional features is effective across various tasks, including structured prediction and natural language inference <Paper corpusId=\"235078802\" paperTitle=\"(Jiang et al., 2020)\" isShortName></Paper> <Paper corpusId=\"212747810\" paperTitle=\"(Desai et al., 2020)\" isShortName></Paper>.\n\n* **Confidence Estimation Models (CEM)**: Model-based confidence estimators trained on pseudo transcriptions can significantly improve calibration for out-of-domain (OOD) data, especially when trained with additional features from OOD language models <Paper corpusId=\"238419596\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>.\n\n## Multi-Hypothesis Generation\n\nGenerating and evaluating multiple hypotheses before providing confidence scores has been shown to significantly improve calibration of verbalized probabilities. When combined with temperature scaling, this approach can reduce expected calibration error (ECE) by over 50% for models like ChatGPT, GPT-4, and Claude 2 across multiple datasets <Paper corpusId=\"258865733\" paperTitle=\"(Tian et al., 2023)\" isShortName></Paper> <Paper corpusId=\"24047702\" paperTitle=\"(Lord et al., 1984)\" isShortName></Paper>.\n\n## Data Augmentation and Ensemble Methods\n\nWhile data augmentation and model ensemble approaches have been successful in computer vision, they have shown limited benefits for NLP tasks according to empirical studies <Paper corpusId=\"259859030\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. Nevertheless, exploring diverse expressions through paraphrasing has demonstrated significant improvements in confidence estimation, particularly for multiple-choice datasets <Paper corpusId=\"235078802\" paperTitle=\"(Jiang et al., 2020)\" isShortName></Paper>.\n\n## Multicalibration\n\nMulticalibration methods ensure not only overall alignment between confidence scores and correctness but also calibration across multiple subgroups through an iterative alignment process <Paper corpusId=\"274789494\" paperTitle=\"(Xie et al._1, 2024)\" isShortName></Paper>. This approach is particularly valuable for ensuring fair calibration across different demographic or linguistic groups <Paper corpusId=\"274789494\" paperTitle=\"(Xie et al._1, 2024)\" isShortName></Paper>.\n\nPost-hoc calibration techniques are often preferred for black-box language models where internal model access or fine-tuning is not possible. These methods can successfully re-scale confidence from extremely high or low values to more reasonable ranges, thus improving overall calibration performance <Paper corpusId=\"235078802\" paperTitle=\"(Jiang et al., 2020)\" isShortName></Paper> <Paper corpusId=\"267617073\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Jiang et al., 2020)",
                        "snippets": [
                            "Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, \"How can we know when language models know, with confidence, the answer to a particular query?\" We examine this question from the point of view of calibration, the property of a probabilistic model's predicted probabilities actually being well correlated with the probabilities of correctness.\n\nTo alleviate this problem, we propose methods to make LMs' confidence scores correlate better with the likelihood of model prediction being correct. We examined both fine-tuning methods that modify LMs' parameters and post-hoc methods that keep LMs fixed and only manipulate the confidence values or inputs. Specifically, we fine-tune the LM using softmax-or marginbased objective functions based on multiple candidate answers. For post-hoc calibration, we examined temperature-based scaling and feature-based decision trees that take prediction probability and input-related features as input and produce calibrated confidence (Jagannatha and Yu, 2020;Desai and Durrett, 2020;Kamath et al., 2020). We also study the sensitivity of LMs' confidence estimation with respect to language variation by paraphrasing candidate answers and augmenting questions using retrieved context.\n\nBoth softmax-based and margin-based fine-tuning, which explicitly compare and adjust the probability of candidate answers, can further improve ECE on multiplechoice datasets. We argue that the softmax-based and margin-based objective functions are better suited for questions with potential candidates.\n\nPost-processing the raw confidence either solely based on confidence or other features is effective across all datasets, which is consistent with the conclusion on other tasks such as structured prediction and natural language inference (Jagannatha and Yu, 2020; Desai and Durrett, 2020). We demonstrate the histogram of confidence before and after applying temperature-based scaling or feature-based decision trees in Figure 2. LMs tend to be over-confident, with most predictions having either extremely high or low confidence. Both methods can successfully re-scale the confidence to reasonable ranges, thus improving the calibration performance.\n\nThe improvement brought by using paraphrasing is significant on multiple-choice datasets, demonstrating that using diverse expressions can indeed improve confidence estimation."
                        ],
                        "paper": {
                            "corpus_id": 235078802,
                            "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
                            "authors": [
                                {
                                    "authorId": "2669515",
                                    "name": "Zhengbao Jiang"
                                },
                                {
                                    "authorId": "50007145",
                                    "name": "J. Araki"
                                },
                                {
                                    "authorId": "47929135",
                                    "name": "Haibo Ding"
                                },
                                {
                                    "authorId": "1700325",
                                    "name": "Graham Neubig"
                                }
                            ],
                            "year": 2020,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 436
                        },
                        "score": 0.95947265625
                    },
                    {
                        "id": "(Guo et al., 2017)",
                        "snippets": [
                            "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."
                        ],
                        "paper": {
                            "corpus_id": 28671436,
                            "title": "On Calibration of Modern Neural Networks",
                            "authors": [
                                {
                                    "authorId": "144993411",
                                    "name": "Chuan Guo"
                                },
                                {
                                    "authorId": "10804137",
                                    "name": "Geoff Pleiss"
                                },
                                {
                                    "authorId": "2117103358",
                                    "name": "Yu Sun"
                                },
                                {
                                    "authorId": "7446832",
                                    "name": "Kilian Q. Weinberger"
                                }
                            ],
                            "year": 2017,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 5869
                        },
                        "score": 0
                    },
                    {
                        "id": "(Xie et al., 2024)",
                        "snippets": [
                            "In this work, we introduce Adaptive Temperature Scaling (ATS), a post-hoc calibration method that predicts a temperature scaling parameter for each token prediction. The predicted temperature values adapt based on token-level features and are fit over a standard supervised fine-tuning (SFT) dataset. The adaptive nature of ATS addresses the varying degrees of calibration shift that can occur after RLHF fine-tuning. ATS improves calibration by over 10-50% across three downstream natural language evaluation benchmarks compared to prior calibration methods and does not impede performance improvements from RLHF."
                        ],
                        "paper": {
                            "corpus_id": 272987064,
                            "title": "Calibrating Language Models with Adaptive Temperature Scaling",
                            "authors": [
                                {
                                    "authorId": "2285068366",
                                    "name": "Johnathan Xie"
                                },
                                {
                                    "authorId": "2111073657",
                                    "name": "Annie S. Chen"
                                },
                                {
                                    "authorId": "2260289273",
                                    "name": "Yoonho Lee"
                                },
                                {
                                    "authorId": "2260106796",
                                    "name": "Eric Mitchell"
                                },
                                {
                                    "authorId": "2279838067",
                                    "name": "Chelsea Finn"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 17
                        },
                        "score": 0.93994140625
                    },
                    {
                        "id": "(Xie et al._1, 2024)",
                        "snippets": [
                            "Calibration aims to align confidence scores with correctness. Commonly used calibration methods include temperature scaling (Guo et al., 2017), label smoothing (Szegedy et al., 2016), and logistic calibration (Kull et al., 2017). However, methods like label smoothing and logistic calibration are unsuitable for black-box LLMs, as they require internal model access or fine-tuning, whereas temperature scaling, which operates on the logits of model outputs, is better suited for gray-box models. In black-box LLMs, common calibration methods include post-processing or transforming the model into a gray-box. Due to the near-well-calibrated nature of Confidence Estimation in black-box LLMs, calibration pressure is reduced.\n\nIn post-processing methods, popular techniques include Histogram Binning (Zadrozny and Elkan, 2001) and Isotonic Regression (Jiang et al., 2011). Histogram Binning (Zadrozny and Elkan, 2001) adjusts confidence values within different intervals by assessing the correctness of predictions in each interval, providing a simple but potentially limited calibration when confidence distributions are uneven. Isotonic Regression (Jiang et al., 2011), on the other hand, seeks an optimal monotonically increasing function f (c) to map the model's confidence c 1 , c 2 , . . . , c n to correctness. f (c) need to satisfy the condition in Formula 2, ensuring that the confidence ranking aligns with the correctness.\n\nTo address more complex scenarios, some methods introduce additional constraints. For instance, H\u00e9bert-Johnson et al. ( 2018) introduced a multicalibration method, ensuring not only overall alignment but also calibration across multiple subgroups S j through iterative alignment. The overall goal can be expressed as Formula 3."
                        ],
                        "paper": {
                            "corpus_id": 274789494,
                            "title": "A Survey of Calibration Process for Black-Box LLMs",
                            "authors": [
                                {
                                    "authorId": "2336030822",
                                    "name": "Liangru Xie"
                                },
                                {
                                    "authorId": "2326538488",
                                    "name": "Hui Liu"
                                },
                                {
                                    "authorId": "2276423058",
                                    "name": "Jingying Zeng"
                                },
                                {
                                    "authorId": "2313288320",
                                    "name": "Xianfeng Tang"
                                },
                                {
                                    "authorId": "2336079495",
                                    "name": "Yan Han"
                                },
                                {
                                    "authorId": "2305051640",
                                    "name": "Chen Luo"
                                },
                                {
                                    "authorId": "2336079074",
                                    "name": "Jing Huang"
                                },
                                {
                                    "authorId": "2313299851",
                                    "name": "Zhen Li"
                                },
                                {
                                    "authorId": "2257362558",
                                    "name": "Suhang Wang"
                                },
                                {
                                    "authorId": "2315065019",
                                    "name": "Qi He"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.93408203125
                    },
                    {
                        "id": "(Desai et al., 2020)",
                        "snippets": [
                            "Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain."
                        ],
                        "paper": {
                            "corpus_id": 212747810,
                            "title": "Calibration of Pre-trained Transformers",
                            "authors": [
                                {
                                    "authorId": "120777041",
                                    "name": "Shrey Desai"
                                },
                                {
                                    "authorId": "1814094",
                                    "name": "Greg Durrett"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 301
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2021)",
                        "snippets": [
                            "Recent research has shown that model-based confidence estimators have a significant advantage over using the output softmax probabilities. If the input data to the speech recogniser is from mismatched acoustic and linguistic conditions, the ASR performance and the corresponding confidence estimators may exhibit severe degradation",
                            "Using pseudo transcriptions to provide binary targets for training model-based confidence estimators and including additional features from an OOD LM are useful for improving the confidence scores on OOD datasets, even when pretraining uses OOD data. By exposing the CEM or R-EBM to OOD data, the calibration performance is also improved."
                        ],
                        "paper": {
                            "corpus_id": 238419596,
                            "title": "Improving Confidence Estimation on Out-of-Domain Data for End-to-End Speech Recognition",
                            "authors": [
                                {
                                    "authorId": "1522347808",
                                    "name": "Qiujia Li"
                                },
                                {
                                    "authorId": "2153632494",
                                    "name": "Yu Zhang"
                                },
                                {
                                    "authorId": "150080713",
                                    "name": "David Qiu"
                                },
                                {
                                    "authorId": "2145999837",
                                    "name": "Yanzhang He"
                                },
                                {
                                    "authorId": "48749954",
                                    "name": "Liangliang Cao"
                                },
                                {
                                    "authorId": "1716393",
                                    "name": "P. Woodland"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                            "n_citations": 11
                        },
                        "score": 0.87939453125
                    },
                    {
                        "id": "(Tian et al., 2023)",
                        "snippets": [
                            "Among the methods for verbalizing probabilities directly, we observe that generating and evaluating multiple hypotheses improves calibration (see Figure 1), similarly to humans (Lord et al., 1984), and corroborating a similar finding in LMs (Kadavath et al., 2022)",
                            "We show that prompting a model to produce several answer choices before giving its confidence scores significantly improves calibration of verbalized probabilities. Combined with temperature scaling (Guo et al., 2017), this approach generally provides better calibration than model probabilities for ChatGPT, GPT-4, and Claude 2 across three datasets, often reducing expected calibration error (ECE) by over 50%."
                        ],
                        "paper": {
                            "corpus_id": 258865733,
                            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
                            "authors": [
                                {
                                    "authorId": "2218407338",
                                    "name": "Katherine Tian"
                                },
                                {
                                    "authorId": "49688913",
                                    "name": "E. Mitchell"
                                },
                                {
                                    "authorId": "2064472884",
                                    "name": "Allan Zhou"
                                },
                                {
                                    "authorId": "50465276",
                                    "name": "Archit Sharma"
                                },
                                {
                                    "authorId": "102801230",
                                    "name": "Rafael Rafailov"
                                },
                                {
                                    "authorId": "18307037",
                                    "name": "Huaxiu Yao"
                                },
                                {
                                    "authorId": "46881670",
                                    "name": "Chelsea Finn"
                                },
                                {
                                    "authorId": "144783904",
                                    "name": "Christopher D. Manning"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 355
                        },
                        "score": 0.89501953125
                    },
                    {
                        "id": "(Lord et al., 1984)",
                        "snippets": [
                            "It is proposed that several biases in social judgment result from a failure--first noted by Francis Bacon--to consider possibilities at odds with beliefs and perceptions of the moment. Individuals who are induced to consider the opposite, therefore, should display less bias in social judgment. In two separate but conceptually parallel experiments, this reasoning was applied to two domains--biased assimilation of new evidence on social issues and biased hypothesis testing of personality impressions. Subjects were induced to consider the opposite in two ways: through explicit instructions to do so and through stimulus materials that made opposite possibilities more salient. In both experiments the induction of a consider-the-opposite strategy had greater corrective effect than more demand-laden alternative instructions to be as fair and unbiased as possible. The results are viewed as consistent with previous research on perseverance, hindsight, and logical problem solving, and are thought to suggest an effective method of retraining social judgment."
                        ],
                        "paper": {
                            "corpus_id": 24047702,
                            "title": "Considering the opposite: a corrective strategy for social judgment.",
                            "authors": [
                                {
                                    "authorId": "6802871",
                                    "name": "C. G. Lord"
                                },
                                {
                                    "authorId": "81476483",
                                    "name": "M. Lepper"
                                },
                                {
                                    "authorId": "40299604",
                                    "name": "Elizabeth L. Preston"
                                }
                            ],
                            "year": 1984,
                            "venue": "Journal of Personality and Social Psychology",
                            "n_citations": 663
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "Data augmentation (Hendrycks et al., 2019)(Wang et al., 2021) and model ensemble (Gal and Ghahramani, 2016;(Lakshminarayanan et al., 2016) have been empirically proven to be successful in computer vision. However, they cannot bring the same benefits in NLP according to the empirical study in Chen et al. (2022b)",
                            "Temperature scaling (Platt, 1999)Guo et al., 2017) readjusts the output logits in a posterior way according to the calibration performance on a held-out set. Label smoothing (Szegedy et al., 2015) imposes a confidence regularization during the training process, discouraging models from being overconfident in their predictions.\n\nRecently, there is an emergent trend in NLP, motivating to directly collect data for training models to have reasonable confidence estimations. Kadavath et al. (2022) assume that the last hidden states of PLMs contain the uncertainty information, and directly apply a multi-layer perceptron on them to perform confidence estimations. Lin et al. (2022) also show that PLMs can be directly trained to give their confidence estimations by words. These two methods are proven to be successful in significantly reducing the overconfidence issue and are further extended to exploit the potential in this kind of methods (Chen et al., 2022b)."
                        ],
                        "paper": {
                            "corpus_id": 259859030,
                            "title": "Making Pre-trained Language Models both Task-solvers and Self-calibrators",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Yangyi Chen"
                                },
                                {
                                    "authorId": "2144803999",
                                    "name": "Xingyao Wang"
                                },
                                {
                                    "authorId": "2072975661",
                                    "name": "Heng Ji"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 0
                        },
                        "score": 0.9130859375
                    },
                    {
                        "id": "(Huang et al., 2024)",
                        "snippets": [
                            "Improving Calibration Traditional calibration methods focus on post-processing logits (Guo et al., 2017), but with LLMs generating unbounded text, logits could fall short. Thus, extracting better confidence scores (i.e., confidence elicitation) has become crucial for improving calibration. These include: verbalization, which directly asks the model to output its confidence (Lin et al., 2022), consistency, which uses the uniformity of multiple responses to gauge confidence (Kadavath et al., 2022, Kuhn et al., 2023, Cole et al., 2023, Chen et al., 2023c, Tian et al., 2023a, Lin et al., 2023), and the hybrid of both (Xiong et al., 2023, Tian et al., 2023b, Chen and Mueller, 2023)."
                        ],
                        "paper": {
                            "corpus_id": 267617073,
                            "title": "Calibrating Long-form Generations from Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2145952181",
                                    "name": "Yukun Huang"
                                },
                                {
                                    "authorId": "2283783652",
                                    "name": "Yixin Liu"
                                },
                                {
                                    "authorId": "2283771339",
                                    "name": "Raghuveer Thirukovalluru"
                                },
                                {
                                    "authorId": "2266838179",
                                    "name": "Arman Cohan"
                                },
                                {
                                    "authorId": "2060730422",
                                    "name": "Bhuwan Dhingra"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 12
                        },
                        "score": 0.9609375
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Training-based Calibration Methods",
                "tldr": "Training-based calibration methods improve confidence score alignment by directly modifying language model parameters during the training or fine-tuning process. These approaches include label smoothing, parameter-efficient tuning, and specialized objective functions that preserve pre-trained features while optimizing for calibration. (10 sources)",
                "text": "\nTraining-based calibration methods involve directly modifying language model parameters to achieve better calibration, as opposed to post-hoc techniques that adjust confidence scores after training. These approaches integrate calibration goals directly into the training or fine-tuning process.\n\nLabel smoothing is one of the most effective training-based calibration techniques. This regularization method penalizes low entropy distributions by assigning a fixed probability to the true label and distributing the remaining probability mass uniformly across other classes <Paper corpusId=\"253098773\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper> <Paper corpusId=\"259859030\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. Studies have demonstrated that label smoothing is competitive with temperature scaling for calibration <Paper corpusId=\"253098773\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper> <Paper corpusId=\"174802983\" paperTitle=\"(Muller et al., 2019)\" isShortName></Paper>, especially in out-of-domain settings <Paper corpusId=\"212747810\" paperTitle=\"(Desai et al., 2020)\" isShortName></Paper>.\n\nFor multiple-choice questions, fine-tuning with specialized objective functions has proven effective. Both softmax-based and margin-based fine-tuning, which explicitly compare and adjust the probabilities of candidate answers, can significantly improve expected calibration error (ECE) on multiple-choice datasets <Paper corpusId=\"235078802\" paperTitle=\"(Jiang et al., 2020)\" isShortName></Paper>. These objective functions are particularly well-suited for questions with potential candidate answers.\n\nA promising recent finding suggests that preserving pre-trained features helps calibrate fine-tuned language models. Several approaches achieve this goal:\n\n1. **Parameter-efficient tuning** methods, which were originally designed to improve performance rather than calibration, have been shown to outperform vanilla fine-tuning in terms of calibration, especially under out-of-domain settings <Paper corpusId=\"258967945\" paperTitle=\"(He et al., 2023)\" isShortName></Paper> <Paper corpusId=\"59599816\" paperTitle=\"(Houlsby et al., 2019)\" isShortName></Paper>.\n\n2. **Pre-trained weight decay** and **Mixout** <Paper corpusId=\"202750126\" paperTitle=\"(Lee et al., 2019)\" isShortName></Paper>, which regularizes learning to minimize deviation from pre-trained weights, also help maintain better calibration <Paper corpusId=\"258967945\" paperTitle=\"(He et al., 2023)\" isShortName></Paper>.\n\n3. **Masked Language Modeling (MLM) objectives** can be incorporated during fine-tuning to maintain consistency between pre-trained and fine-tuned models, achieving the lowest expected calibration error and competitive accuracy compared to existing calibration methods in both in-domain and out-of-domain settings <Paper corpusId=\"258967945\" paperTitle=\"(He et al., 2023)\" isShortName></Paper>.\n\nFew-shot learning combined with calibration techniques offers another approach. Fine-tuning on a small number of examples in a target language, in addition to data in the source language, can improve calibration by reducing domain shift <Paper corpusId=\"253098773\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper> <Paper corpusId=\"226262344\" paperTitle=\"(Lauscher et al., 2020)\" isShortName></Paper>.\n\nFor out-of-domain scenarios, training confidence estimation models (CEMs) using pseudo transcriptions has proven effective. Including additional features from out-of-domain language models can significantly improve confidence scores on out-of-domain datasets <Paper corpusId=\"238419596\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>.\n\nUnlike in computer vision, data augmentation and model ensemble approaches have not shown the same benefits for NLP tasks according to empirical studies <Paper corpusId=\"259859030\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. However, an emerging trend involves directly collecting data to train models for reasonable confidence estimations, applying multi-layer perceptrons on the last hidden states of pre-trained language models or training them to verbalize their confidence levels <Paper corpusId=\"259859030\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Ahuja et al., 2022)",
                        "snippets": [
                            "We briefly review some commonly used methods for calibrating neural network based classifiers. 1. Temperature Scaling (TS and Self-TS) (Guo et al., 2017) is applied by scaling the output logits using a temperature parameter T before applying softmax i.e. : \n\n, where o k denotes the logits corresponding to the k th class. T is a learnable parameter obtained posttraining by maximizing the log-likelihood on the dev set while keeping other network parameters fixed. We experiment with two settings for improving calibration on a target language: using dev data in English to perform temperature scaling (TS) and using the target language's dev data (Self-TS). 2. Label Smoothing (LS) (Pereyra et al., 2017) is a regularization technique that penalizes low entropy distributions by using soft labels that are obtained by assigning a fixed probability q = 1 \u2212 \u03b1 to the true label (0 < \u03b1 < 1), and distributing the remaining probability mass uniformly across the remaining classes. Label smoothing has been empirically shown to be competitive with temperature scaling for calibration (M\u00fcller et al., 2019) especially in out of domain settings (Desai et al., 2020) 3. Few-Shot Learning (FSL) We also investigate if fine-tuning the MMLM on a few examples in a target language in addition to the data in English, leads to any improvement in calibration as it does in the performance (Lauscher et al., 2020). Since these models are expected to be calibrated worse for out-of-domain data compared to in-domain data (Desai et al., 2020), we try to improve calibration by reducing the domain shift through fewshot learning. \n\nApart from these, we also consider combinations of different calibration methods in our experiments, including Label Smoothing with Temperature Scaling (TS + LS or Self-TS + LS) and Few-Shot Learning with Label Smoothing (FSL + LS)."
                        ],
                        "paper": {
                            "corpus_id": 253098773,
                            "title": "On the Calibration of Massively Multilingual Language Models",
                            "authors": [
                                {
                                    "authorId": "52154863",
                                    "name": "Kabir Ahuja"
                                },
                                {
                                    "authorId": "3010457",
                                    "name": "Sunayana Sitaram"
                                },
                                {
                                    "authorId": "34725175",
                                    "name": "Sandipan Dandapat"
                                },
                                {
                                    "authorId": "143990839",
                                    "name": "M. Choudhury"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 16
                        },
                        "score": 0.875
                    },
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "Data augmentation (Hendrycks et al., 2019)(Wang et al., 2021) and model ensemble (Gal and Ghahramani, 2016;(Lakshminarayanan et al., 2016) have been empirically proven to be successful in computer vision. However, they cannot bring the same benefits in NLP according to the empirical study in Chen et al. (2022b)",
                            "Temperature scaling (Platt, 1999)Guo et al., 2017) readjusts the output logits in a posterior way according to the calibration performance on a held-out set. Label smoothing (Szegedy et al., 2015) imposes a confidence regularization during the training process, discouraging models from being overconfident in their predictions.\n\nRecently, there is an emergent trend in NLP, motivating to directly collect data for training models to have reasonable confidence estimations. Kadavath et al. (2022) assume that the last hidden states of PLMs contain the uncertainty information, and directly apply a multi-layer perceptron on them to perform confidence estimations. Lin et al. (2022) also show that PLMs can be directly trained to give their confidence estimations by words. These two methods are proven to be successful in significantly reducing the overconfidence issue and are further extended to exploit the potential in this kind of methods (Chen et al., 2022b)."
                        ],
                        "paper": {
                            "corpus_id": 259859030,
                            "title": "Making Pre-trained Language Models both Task-solvers and Self-calibrators",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Yangyi Chen"
                                },
                                {
                                    "authorId": "2144803999",
                                    "name": "Xingyao Wang"
                                },
                                {
                                    "authorId": "2072975661",
                                    "name": "Heng Ji"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 0
                        },
                        "score": 0.9130859375
                    },
                    {
                        "id": "(Muller et al., 2019)",
                        "snippets": [
                            "The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions."
                        ],
                        "paper": {
                            "corpus_id": 174802983,
                            "title": "When Does Label Smoothing Help?",
                            "authors": [
                                {
                                    "authorId": "2114054259",
                                    "name": "Rafael M\u00fcller"
                                },
                                {
                                    "authorId": "40464924",
                                    "name": "Simon Kornblith"
                                },
                                {
                                    "authorId": "1695689",
                                    "name": "Geoffrey E. Hinton"
                                }
                            ],
                            "year": 2019,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 1954
                        },
                        "score": 0
                    },
                    {
                        "id": "(Desai et al., 2020)",
                        "snippets": [
                            "Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain."
                        ],
                        "paper": {
                            "corpus_id": 212747810,
                            "title": "Calibration of Pre-trained Transformers",
                            "authors": [
                                {
                                    "authorId": "120777041",
                                    "name": "Shrey Desai"
                                },
                                {
                                    "authorId": "1814094",
                                    "name": "Greg Durrett"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 301
                        },
                        "score": 0
                    },
                    {
                        "id": "(Jiang et al., 2020)",
                        "snippets": [
                            "Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, \"How can we know when language models know, with confidence, the answer to a particular query?\" We examine this question from the point of view of calibration, the property of a probabilistic model's predicted probabilities actually being well correlated with the probabilities of correctness.\n\nTo alleviate this problem, we propose methods to make LMs' confidence scores correlate better with the likelihood of model prediction being correct. We examined both fine-tuning methods that modify LMs' parameters and post-hoc methods that keep LMs fixed and only manipulate the confidence values or inputs. Specifically, we fine-tune the LM using softmax-or marginbased objective functions based on multiple candidate answers. For post-hoc calibration, we examined temperature-based scaling and feature-based decision trees that take prediction probability and input-related features as input and produce calibrated confidence (Jagannatha and Yu, 2020;Desai and Durrett, 2020;Kamath et al., 2020). We also study the sensitivity of LMs' confidence estimation with respect to language variation by paraphrasing candidate answers and augmenting questions using retrieved context.\n\nBoth softmax-based and margin-based fine-tuning, which explicitly compare and adjust the probability of candidate answers, can further improve ECE on multiplechoice datasets. We argue that the softmax-based and margin-based objective functions are better suited for questions with potential candidates.\n\nPost-processing the raw confidence either solely based on confidence or other features is effective across all datasets, which is consistent with the conclusion on other tasks such as structured prediction and natural language inference (Jagannatha and Yu, 2020; Desai and Durrett, 2020). We demonstrate the histogram of confidence before and after applying temperature-based scaling or feature-based decision trees in Figure 2. LMs tend to be over-confident, with most predictions having either extremely high or low confidence. Both methods can successfully re-scale the confidence to reasonable ranges, thus improving the calibration performance.\n\nThe improvement brought by using paraphrasing is significant on multiple-choice datasets, demonstrating that using diverse expressions can indeed improve confidence estimation."
                        ],
                        "paper": {
                            "corpus_id": 235078802,
                            "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
                            "authors": [
                                {
                                    "authorId": "2669515",
                                    "name": "Zhengbao Jiang"
                                },
                                {
                                    "authorId": "50007145",
                                    "name": "J. Araki"
                                },
                                {
                                    "authorId": "47929135",
                                    "name": "Haibo Ding"
                                },
                                {
                                    "authorId": "1700325",
                                    "name": "Graham Neubig"
                                }
                            ],
                            "year": 2020,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 436
                        },
                        "score": 0.95947265625
                    },
                    {
                        "id": "(He et al., 2023)",
                        "snippets": [
                            "Based on the observations, we hypothesize that preserving the pre-trained features helps calibrate the fine-tuned LMs. \n\nTo validate our hypothesis, we first evaluate the calibration of some previous methods that can preserve pre-trained features, including (1) Parameter-efficient tuning (Houlsby et al., 2019)Hu et al., 2021;Li & Liang, 2021), (2) Pre-trained weight decay, (3) Mixout (Lee et al., 2019). Although these methods were originally designed to improve the performance beyond uncertainty estimation, our experiment demonstrates that these methods outperform vanilla fine-tuning in terms of calibration, especially under out-of-domain settings. Based on our observation that the PLMs are well-calibrated on the MLM task yet the fine-tuned LMs that forget the pre-trained features struggle with overconfidence under domain shift, we propose a simple baseline that utilizes the MLM objective to maintain the consistency between the pre-trained and fine-tuned models. The proposed method achieves the lowest expected calibration error and competitive accuracy compared to existing calibration methods in both ID and OD settings on three NLU tasks, including natural language inference, paraphrase detection, and commonsense reasoning, showing that preserving the pre-trained features is an effective approach for improving the calibration of fine-tuned LMs."
                        ],
                        "paper": {
                            "corpus_id": 258967945,
                            "title": "Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models",
                            "authors": [
                                {
                                    "authorId": "2218509878",
                                    "name": "Guande He"
                                },
                                {
                                    "authorId": "2276707",
                                    "name": "Jianfei Chen"
                                },
                                {
                                    "authorId": "2155220672",
                                    "name": "Jun Zhu"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 22
                        },
                        "score": 0.85791015625
                    },
                    {
                        "id": "(Houlsby et al., 2019)",
                        "snippets": [
                            "Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task."
                        ],
                        "paper": {
                            "corpus_id": 59599816,
                            "title": "Parameter-Efficient Transfer Learning for NLP",
                            "authors": [
                                {
                                    "authorId": "2815290",
                                    "name": "N. Houlsby"
                                },
                                {
                                    "authorId": "1911881",
                                    "name": "A. Giurgiu"
                                },
                                {
                                    "authorId": "40569328",
                                    "name": "Stanislaw Jastrzebski"
                                },
                                {
                                    "authorId": "68973833",
                                    "name": "Bruna Morrone"
                                },
                                {
                                    "authorId": "51985388",
                                    "name": "Quentin de Laroussilhe"
                                },
                                {
                                    "authorId": "2813347",
                                    "name": "Andrea Gesmundo"
                                },
                                {
                                    "authorId": "2809991",
                                    "name": "Mona Attariyan"
                                },
                                {
                                    "authorId": "1802148",
                                    "name": "S. Gelly"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 4518
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lee et al., 2019)",
                        "snippets": [
                            "In natural language processing, it has been observed recently that generalization could be greatly improved by finetuning a large-scale language model pretrained on a large unlabeled corpus. Despite its recent success and wide adoption, finetuning a large pretrained language model on a downstream task is prone to degenerate performance when there are only a small number of training instances available. In this paper, we introduce a new regularization technique, to which we refer as \"mixout\", motivated by dropout. Mixout stochastically mixes the parameters of two models. We show that our mixout technique regularizes learning to minimize the deviation from one of the two models and that the strength of regularization adapts along the optimization trajectory. We empirically evaluate the proposed mixout and its variants on finetuning a pretrained language model on downstream tasks. More specifically, we demonstrate that the stability of finetuning and the average accuracy greatly increase when we use the proposed approach to regularize finetuning of BERT on downstream tasks in GLUE."
                        ],
                        "paper": {
                            "corpus_id": 202750126,
                            "title": "Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models",
                            "authors": [
                                {
                                    "authorId": "81275395",
                                    "name": "Cheolhyoung Lee"
                                },
                                {
                                    "authorId": "1979489",
                                    "name": "Kyunghyun Cho"
                                },
                                {
                                    "authorId": "1742677",
                                    "name": "Wanmo Kang"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 209
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lauscher et al., 2020)",
                        "snippets": [
                            "Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions."
                        ],
                        "paper": {
                            "corpus_id": 226262344,
                            "title": "From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers",
                            "authors": [
                                {
                                    "authorId": "29891652",
                                    "name": "Anne Lauscher"
                                },
                                {
                                    "authorId": "24881798",
                                    "name": "Vinit Ravishankar"
                                },
                                {
                                    "authorId": "1747849",
                                    "name": "Ivan Vulic"
                                },
                                {
                                    "authorId": "2472657",
                                    "name": "Goran Glavas"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 315
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2021)",
                        "snippets": [
                            "Recent research has shown that model-based confidence estimators have a significant advantage over using the output softmax probabilities. If the input data to the speech recogniser is from mismatched acoustic and linguistic conditions, the ASR performance and the corresponding confidence estimators may exhibit severe degradation",
                            "Using pseudo transcriptions to provide binary targets for training model-based confidence estimators and including additional features from an OOD LM are useful for improving the confidence scores on OOD datasets, even when pretraining uses OOD data. By exposing the CEM or R-EBM to OOD data, the calibration performance is also improved."
                        ],
                        "paper": {
                            "corpus_id": 238419596,
                            "title": "Improving Confidence Estimation on Out-of-Domain Data for End-to-End Speech Recognition",
                            "authors": [
                                {
                                    "authorId": "1522347808",
                                    "name": "Qiujia Li"
                                },
                                {
                                    "authorId": "2153632494",
                                    "name": "Yu Zhang"
                                },
                                {
                                    "authorId": "150080713",
                                    "name": "David Qiu"
                                },
                                {
                                    "authorId": "2145999837",
                                    "name": "Yanzhang He"
                                },
                                {
                                    "authorId": "48749954",
                                    "name": "Liangliang Cao"
                                },
                                {
                                    "authorId": "1716393",
                                    "name": "P. Woodland"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                            "n_citations": 11
                        },
                        "score": 0.87939453125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Multicalibration and Group-wise Calibration",
                "tldr": "Multicalibration techniques ensure language models maintain appropriate confidence levels across different subgroups or \"slices\" of data, rather than just in aggregate. These methods use techniques like clustering, self-annotation, and slice-specific recalibration to address issues where models might be well-calibrated overall but significantly miscalibrated for specific domains or groups. (6 sources)",
                "text": "\nTraditional calibration metrics often assess performance in aggregate, potentially masking significant miscalibration within specific subgroups or domains. As Li et al. note, \"while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate)\" <Paper corpusId=\"268723623\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This observation has motivated the development of group-wise and multicalibration techniques that ensure reliable confidence scores across diverse data segments.\n\nMulticalibration methods aim to simultaneously achieve calibration across various intersecting groupings of data, rather than just overall calibration. The foundational concept builds on earlier work in algorithmic fairness <Paper corpusId=\"12845273\" paperTitle=\"(Kleinberg et al., 2016)\" isShortName></Paper> and has been adapted to address the unique challenges of language models. Xie et al. explain that multicalibration \"ensures not only overall alignment but also calibration across multiple subgroups through iterative alignment\" <Paper corpusId=\"274789494\" paperTitle=\"(Xie et al._1, 2024)\" isShortName></Paper>. This approach is particularly valuable for ensuring fair calibration across different demographic, linguistic, or domain-specific groups.\n\nSeveral innovative techniques have emerged for implementing multicalibration in language models. Detommaso et al. propose forming groupings for prompt/completion pairs that correlate with correctness probability through two primary methods:\n\n1. **Clustering within embedding space**: Grouping similar examples based on their representation in the model's embedding space.\n2. **Self-annotation**: Querying the language model itself with yes-or-no questions about the prompt to derive meaningful groupings <Paper corpusId=\"269004786\" paperTitle=\"(Detommaso et al., 2024)\" isShortName></Paper>.\n\nTheir work also introduces novel variants of multicalibration algorithms designed to reduce overfitting tendencies, demonstrating substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods <Paper corpusId=\"269004786\" paperTitle=\"(Detommaso et al., 2024)\" isShortName></Paper>.\n\nFor addressing domain-specific calibration issues, Li et al. propose a few-shot slice-specific recalibration framework. This approach trains a recalibration model that uses a few unlabeled examples from any given slice to predict a curve that remaps confidence scores specifically for that slice. Their method enables identification of domain-specific confidence thresholds, above which the language model's predictions can be trusted and below which it should abstain. Experiments show their few-shot recalibrator consistently outperforming existing calibration methods, improving calibration error for PaLM2-Large on MMLU by 16% compared to temperature scaling <Paper corpusId=\"268723623\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nSeveral researchers have noted the connection between group-wise calibration and fairness concerns. Traditional group-wise calibration uses predefined groupings, but faces challenges in balancing different fairness criteria <Paper corpusId=\"75455\" paperTitle=\"(Pleiss et al., 2017)\" isShortName></Paper>. In the context of language models, Manggala et al. highlight several complementary approaches:\n\n1. Training models to approximate precision-threshold curves for given groups using few-shot samples\n2. Training auxiliary models using group-specific accuracy as targets\n3. Extending multicalibration algorithms to ensure simultaneous calibration across various intersecting data groupings <Paper corpusId=\"273228151\" paperTitle=\"(Manggala et al., 2024)\" isShortName></Paper>\n\nThese group-wise calibration approaches are particularly important for applications where trust and fairness are paramount, as they help ensure that language model confidence scores are reliable across diverse domains, topics, and demographic groups.",
                "citations": [
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain. Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling."
                        ],
                        "paper": {
                            "corpus_id": 268723623,
                            "title": "Few-Shot Recalibration of Language Models",
                            "authors": [
                                {
                                    "authorId": "2293910776",
                                    "name": "Xiang Lisa Li"
                                },
                                {
                                    "authorId": "3030219",
                                    "name": "Urvashi Khandelwal"
                                },
                                {
                                    "authorId": "2091768",
                                    "name": "Kelvin Guu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.966796875
                    },
                    {
                        "id": "(Kleinberg et al., 2016)",
                        "snippets": [
                            "Recent discussion in the public sphere about algorithmic classification has involved tension between competing notions of what it means for a probabilistic classification to be fair to different groups. We formalize three fairness conditions that lie at the heart of these debates, and we prove that except in highly constrained special cases, there is no method that can satisfy these three conditions simultaneously. Moreover, even satisfying all three conditions approximately requires that the data lie in an approximate version of one of the constrained special cases identified by our theorem. These results suggest some of the ways in which key notions of fairness are incompatible with each other, and hence provide a framework for thinking about the trade-offs between them."
                        ],
                        "paper": {
                            "corpus_id": 12845273,
                            "title": "Inherent Trade-Offs in the Fair Determination of Risk Scores",
                            "authors": [
                                {
                                    "authorId": "3371403",
                                    "name": "J. Kleinberg"
                                },
                                {
                                    "authorId": "2062143",
                                    "name": "S. Mullainathan"
                                },
                                {
                                    "authorId": "38009222",
                                    "name": "Manish Raghavan"
                                }
                            ],
                            "year": 2016,
                            "venue": "Information Technology Convergence and Services",
                            "n_citations": 1776
                        },
                        "score": 0
                    },
                    {
                        "id": "(Xie et al._1, 2024)",
                        "snippets": [
                            "Calibration aims to align confidence scores with correctness. Commonly used calibration methods include temperature scaling (Guo et al., 2017), label smoothing (Szegedy et al., 2016), and logistic calibration (Kull et al., 2017). However, methods like label smoothing and logistic calibration are unsuitable for black-box LLMs, as they require internal model access or fine-tuning, whereas temperature scaling, which operates on the logits of model outputs, is better suited for gray-box models. In black-box LLMs, common calibration methods include post-processing or transforming the model into a gray-box. Due to the near-well-calibrated nature of Confidence Estimation in black-box LLMs, calibration pressure is reduced.\n\nIn post-processing methods, popular techniques include Histogram Binning (Zadrozny and Elkan, 2001) and Isotonic Regression (Jiang et al., 2011). Histogram Binning (Zadrozny and Elkan, 2001) adjusts confidence values within different intervals by assessing the correctness of predictions in each interval, providing a simple but potentially limited calibration when confidence distributions are uneven. Isotonic Regression (Jiang et al., 2011), on the other hand, seeks an optimal monotonically increasing function f (c) to map the model's confidence c 1 , c 2 , . . . , c n to correctness. f (c) need to satisfy the condition in Formula 2, ensuring that the confidence ranking aligns with the correctness.\n\nTo address more complex scenarios, some methods introduce additional constraints. For instance, H\u00e9bert-Johnson et al. ( 2018) introduced a multicalibration method, ensuring not only overall alignment but also calibration across multiple subgroups S j through iterative alignment. The overall goal can be expressed as Formula 3."
                        ],
                        "paper": {
                            "corpus_id": 274789494,
                            "title": "A Survey of Calibration Process for Black-Box LLMs",
                            "authors": [
                                {
                                    "authorId": "2336030822",
                                    "name": "Liangru Xie"
                                },
                                {
                                    "authorId": "2326538488",
                                    "name": "Hui Liu"
                                },
                                {
                                    "authorId": "2276423058",
                                    "name": "Jingying Zeng"
                                },
                                {
                                    "authorId": "2313288320",
                                    "name": "Xianfeng Tang"
                                },
                                {
                                    "authorId": "2336079495",
                                    "name": "Yan Han"
                                },
                                {
                                    "authorId": "2305051640",
                                    "name": "Chen Luo"
                                },
                                {
                                    "authorId": "2336079074",
                                    "name": "Jing Huang"
                                },
                                {
                                    "authorId": "2313299851",
                                    "name": "Zhen Li"
                                },
                                {
                                    "authorId": "2257362558",
                                    "name": "Suhang Wang"
                                },
                                {
                                    "authorId": "2315065019",
                                    "name": "Qi He"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.93408203125
                    },
                    {
                        "id": "(Detommaso et al., 2024)",
                        "snippets": [
                            "We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and \"self-annotation\" - querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods."
                        ],
                        "paper": {
                            "corpus_id": 269004786,
                            "title": "Multicalibration for Confidence Scoring in LLMs",
                            "authors": [
                                {
                                    "authorId": "2295667267",
                                    "name": "Gianluca Detommaso"
                                },
                                {
                                    "authorId": "2295665717",
                                    "name": "Martin Bertran"
                                },
                                {
                                    "authorId": "2295664744",
                                    "name": "Riccardo Fogliato"
                                },
                                {
                                    "authorId": "2295665299",
                                    "name": "Aaron Roth"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 19
                        },
                        "score": 0.970703125
                    },
                    {
                        "id": "(Pleiss et al., 2017)",
                        "snippets": [
                            "The machine learning community has become increasingly concerned with the potential for bias and discrimination in predictive models. This has motivated a growing line of work on what it means for a classification procedure to be \"fair.\" In this paper, we investigate the tension between minimizing error disparity across different population groups while maintaining calibrated probability estimates. We show that calibration is compatible only with a single error constraint (i.e. equal false-negatives rates across groups), and show that any algorithm that satisfies this relaxation is no better than randomizing a percentage of predictions for an existing classifier. These unsettling findings, which extend and generalize existing results, are empirically confirmed on several datasets."
                        ],
                        "paper": {
                            "corpus_id": 75455,
                            "title": "On Fairness and Calibration",
                            "authors": [
                                {
                                    "authorId": "10804137",
                                    "name": "Geoff Pleiss"
                                },
                                {
                                    "authorId": "38009222",
                                    "name": "Manish Raghavan"
                                },
                                {
                                    "authorId": "24277779",
                                    "name": "Felix Wu"
                                },
                                {
                                    "authorId": "3371403",
                                    "name": "J. Kleinberg"
                                },
                                {
                                    "authorId": "7446832",
                                    "name": "Kilian Q. Weinberger"
                                }
                            ],
                            "year": 2017,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 880
                        },
                        "score": 0
                    },
                    {
                        "id": "(Manggala et al., 2024)",
                        "snippets": [
                            "Group Notions of Calibration. Previous works highlight the limitations of average-case calibration. Group-wise calibration, which uses predefined groupings (Kleinberg et al., 2016)(Pleiss et al., 2017), has been adapted for language models (LMs). Li et al. ( 2024) train a model that approximates the precision-threshold curve for a given group by using few-shot samples to predict the LM's empirical precision at various confidence thresholds. Ulmer et al. (2024) train an auxiliary model using accuracy per group as target to predict an LM's confidence based on textual input and output. Detommaso et al. (2024) achieves multicalibration -simultaneous calibration across various intersecting groupings of the data. Our work complements multicalibration, and our methods could extend to this by adapting Algorithm 3 in Detommaso et al. (2024). Luo et al. (2022) measure calibration over a set of similar predictions, quantified by a kernel function on feature space."
                        ],
                        "paper": {
                            "corpus_id": 273228151,
                            "title": "QA-Calibration of Language Model Confidence Scores",
                            "authors": [
                                {
                                    "authorId": "2799707",
                                    "name": "Putra Manggala"
                                },
                                {
                                    "authorId": "1387253571",
                                    "name": "Atalanti A. Mastakouri"
                                },
                                {
                                    "authorId": "2319413371",
                                    "name": "Elke Kirschbaum"
                                },
                                {
                                    "authorId": "7993151",
                                    "name": "S. Kasiviswanathan"
                                },
                                {
                                    "authorId": "2556942",
                                    "name": "Aaditya Ramdas"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 3
                        },
                        "score": 0.8896484375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Verbalization and Consistency-based Methods",
                "tldr": "Verbalization and consistency-based methods enable language models to express their uncertainty directly in natural language or through agreement across multiple generated responses. These approaches are becoming increasingly popular as they don't require access to model internals and can effectively improve calibration, especially when combined with other techniques like temperature scaling. (13 sources)",
                "text": "\nVerbalization and consistency-based methods represent emerging approaches to calibration that leverage language models' ability to express uncertainty in natural language and generate multiple response options. Unlike traditional calibration techniques that manipulate logits, these methods can be applied to black-box models without requiring access to internal model parameters <Paper corpusId=\"267617073\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>.\n\n## Verbalized Probabilities\n\nVerbalization methods directly prompt language models to express their confidence in natural language, such as stating \"90% confidence\" or \"high confidence\" for their answers <Paper corpusId=\"249191391\" paperTitle=\"(Lin et al., 2022)\" isShortName></Paper>. Research shows these verbalized probabilities can be well-calibrated, with models capable of generalizing calibration under distribution shifts <Paper corpusId=\"249191391\" paperTitle=\"(Lin et al., 2022)\" isShortName></Paper>. This approach is particularly valuable for commercial black-box models like OpenAI's GPT family, where internal model probabilities are inaccessible <Paper corpusId=\"252917981\" paperTitle=\"(Si et al., 2022)\" isShortName></Paper>.\n\nHowever, verbalized probabilities face challenges related to \"semantic equivalence\" \u2013 different sentences can express the same meaning, making uncertainty measurement more complex <Paper corpusId=\"257039062\" paperTitle=\"(Kuhn et al., 2023)\" isShortName></Paper>. Additionally, verbalized probabilities often require rescaling, as models may express overconfidence. Wang et al. address this through an \"invert softmax trick\" that approximates logits by inverting verbalized probabilities <Paper corpusId=\"273228940\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\n## Consistency-based Methods\n\nConsistency-based approaches assess confidence by measuring agreement across multiple responses generated for the same question <Paper corpusId=\"267617073\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>. This method leverages the principle that when a model consistently produces the same answer across different attempts, it's likely more confident in that answer.\n\nA particularly effective consistency technique is multi-hypothesis generation, where models produce several potential answers before providing a confidence score. Tian et al. demonstrate that generating and evaluating multiple hypotheses significantly improves calibration of verbalized probabilities across various language models, including ChatGPT, GPT-4, and Claude 2 <Paper corpusId=\"258865733\" paperTitle=\"(Tian et al., 2023)\" isShortName></Paper>. This approach echoes earlier findings in human psychology where considering alternative possibilities reduces bias in judgment <Paper corpusId=\"24047702\" paperTitle=\"(Lord et al., 1984)\" isShortName></Paper>.\n\nSelf-consistency, which samples a diverse set of reasoning paths rather than only using greedy decoding, has also shown impressive improvements on reasoning tasks <Paper corpusId=\"247595263\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. Kuhn et al. introduce \"semantic entropy\" to address challenges in measuring uncertainty in natural language, creating an entropy measure that incorporates linguistic invariances created by shared meanings <Paper corpusId=\"257039062\" paperTitle=\"(Kuhn et al., 2023)\" isShortName></Paper>.\n\n## Hybrid Approaches\n\nRecent research increasingly focuses on hybrid approaches that combine verbalization and consistency methods <Paper corpusId=\"267617073\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>. For instance, Sun et al. evaluate different confidence estimation methods including softmax-based approaches, raw token scores, verbalized confidences, and combinations of these methods, further enhanced with a self-probing mechanism <Paper corpusId=\"272689376\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>.\n\nAn emerging trend involves directly collecting data to train models for reasonable confidence estimations. This includes applying multi-layer perceptrons on the last hidden states of pre-trained language models or training them to verbalize their confidence levels <Paper corpusId=\"259859030\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. These approaches have been successful in significantly reducing overconfidence issues in language models <Paper corpusId=\"249191391\" paperTitle=\"(Lin et al., 2022)\" isShortName></Paper>.\n\nUnlike data augmentation and model ensemble approaches that have shown success in computer vision <Paper corpusId=\"208637407\" paperTitle=\"(Hendrycks et al., 2019)\" isShortName></Paper> <Paper corpusId=\"6294674\" paperTitle=\"(Lakshminarayanan et al., 2016)\" isShortName></Paper>, these methods haven't demonstrated the same benefits for NLP tasks according to empirical studies <Paper corpusId=\"259859030\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. However, when verbalization and consistency approaches are combined with temperature scaling <Paper corpusId=\"28671436\" paperTitle=\"(Guo et al., 2017)\" isShortName></Paper>, they can reduce expected calibration error by over 50% compared to using model probabilities alone <Paper corpusId=\"258865733\" paperTitle=\"(Tian et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Huang et al., 2024)",
                        "snippets": [
                            "Improving Calibration Traditional calibration methods focus on post-processing logits (Guo et al., 2017), but with LLMs generating unbounded text, logits could fall short. Thus, extracting better confidence scores (i.e., confidence elicitation) has become crucial for improving calibration. These include: verbalization, which directly asks the model to output its confidence (Lin et al., 2022), consistency, which uses the uniformity of multiple responses to gauge confidence (Kadavath et al., 2022, Kuhn et al., 2023, Cole et al., 2023, Chen et al., 2023c, Tian et al., 2023a, Lin et al., 2023), and the hybrid of both (Xiong et al., 2023, Tian et al., 2023b, Chen and Mueller, 2023)."
                        ],
                        "paper": {
                            "corpus_id": 267617073,
                            "title": "Calibrating Long-form Generations from Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2145952181",
                                    "name": "Yukun Huang"
                                },
                                {
                                    "authorId": "2283783652",
                                    "name": "Yixin Liu"
                                },
                                {
                                    "authorId": "2283771339",
                                    "name": "Raghuveer Thirukovalluru"
                                },
                                {
                                    "authorId": "2266838179",
                                    "name": "Arman Cohan"
                                },
                                {
                                    "authorId": "2060730422",
                                    "name": "Bhuwan Dhingra"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 12
                        },
                        "score": 0.9609375
                    },
                    {
                        "id": "(Lin et al., 2022)",
                        "snippets": [
                            "We show that a GPT-3 model can learn to express uncertainty about its own answers in natural language -- without use of model logits. When given a question, the model generates both an answer and a level of confidence (e.g.\"90% confidence\"or\"high confidence\"). These levels map to probabilities that are well calibrated. The model also remains moderately calibrated under distribution shift, and is sensitive to uncertainty in its own answers, rather than imitating human examples. To our knowledge, this is the first time a model has been shown to express calibrated uncertainty about its own answers in natural language. For testing calibration, we introduce the CalibratedMath suite of tasks. We compare the calibration of uncertainty expressed in words (\"verbalized probability\") to uncertainty extracted from model logits. Both kinds of uncertainty are capable of generalizing calibration under distribution shift. We also provide evidence that GPT-3's ability to generalize calibration depends on pre-trained latent representations that correlate with epistemic uncertainty over its answers."
                        ],
                        "paper": {
                            "corpus_id": 249191391,
                            "title": "Teaching Models to Express Their Uncertainty in Words",
                            "authors": [
                                {
                                    "authorId": "48639938",
                                    "name": "Stephanie C. Lin"
                                },
                                {
                                    "authorId": "2052366271",
                                    "name": "Jacob Hilton"
                                },
                                {
                                    "authorId": "47107786",
                                    "name": "Owain Evans"
                                }
                            ],
                            "year": 2022,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 424
                        },
                        "score": 0
                    },
                    {
                        "id": "(Si et al., 2022)",
                        "snippets": [
                            "Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3's reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM's factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions. Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3."
                        ],
                        "paper": {
                            "corpus_id": 252917981,
                            "title": "Prompting GPT-3 To Be Reliable",
                            "authors": [
                                {
                                    "authorId": "152358188",
                                    "name": "Chenglei Si"
                                },
                                {
                                    "authorId": "144702900",
                                    "name": "Zhe Gan"
                                },
                                {
                                    "authorId": "2149231840",
                                    "name": "Zhengyuan Yang"
                                },
                                {
                                    "authorId": "2992833",
                                    "name": "Shuohang Wang"
                                },
                                {
                                    "authorId": "2124948371",
                                    "name": "Jianfeng Wang"
                                },
                                {
                                    "authorId": "1389036863",
                                    "name": "Jordan L. Boyd-Graber"
                                },
                                {
                                    "authorId": "29957038",
                                    "name": "Lijuan Wang"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 302
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kuhn et al., 2023)",
                        "snippets": [
                            "We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of\"semantic equivalence\"-- different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy -- an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines."
                        ],
                        "paper": {
                            "corpus_id": 257039062,
                            "title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation",
                            "authors": [
                                {
                                    "authorId": "39879848",
                                    "name": "Lorenz Kuhn"
                                },
                                {
                                    "authorId": "2681954",
                                    "name": "Y. Gal"
                                },
                                {
                                    "authorId": "33859827",
                                    "name": "Sebastian Farquhar"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 310
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Recent methods have demonstrated improved calibration by applying techniques like Platt scaling or temperature scaling to the confidence scores generated by LLMs. In this paper, we explore the calibration of verbalized probability distributions for discriminative tasks. First, we investigate the capability of LLMs to generate probability distributions over categorical labels. We theoretically and empirically identify the issue of re-softmax arising from the scaling of verbalized probabilities, and propose using the invert softmax trick to approximate the\"logit\"by inverting verbalized probabilities."
                        ],
                        "paper": {
                            "corpus_id": 273228940,
                            "title": "Calibrating Verbalized Probabilities for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2214132154",
                                    "name": "Cheng Wang"
                                },
                                {
                                    "authorId": "72270481",
                                    "name": "Gyuri Szarvas"
                                },
                                {
                                    "authorId": "2325041176",
                                    "name": "Georges Balazs"
                                },
                                {
                                    "authorId": "70407105",
                                    "name": "Pavel Danchenko"
                                },
                                {
                                    "authorId": "2325095919",
                                    "name": "Patrick Ernst"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.923828125
                    },
                    {
                        "id": "(Tian et al., 2023)",
                        "snippets": [
                            "Among the methods for verbalizing probabilities directly, we observe that generating and evaluating multiple hypotheses improves calibration (see Figure 1), similarly to humans (Lord et al., 1984), and corroborating a similar finding in LMs (Kadavath et al., 2022)",
                            "We show that prompting a model to produce several answer choices before giving its confidence scores significantly improves calibration of verbalized probabilities. Combined with temperature scaling (Guo et al., 2017), this approach generally provides better calibration than model probabilities for ChatGPT, GPT-4, and Claude 2 across three datasets, often reducing expected calibration error (ECE) by over 50%."
                        ],
                        "paper": {
                            "corpus_id": 258865733,
                            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
                            "authors": [
                                {
                                    "authorId": "2218407338",
                                    "name": "Katherine Tian"
                                },
                                {
                                    "authorId": "49688913",
                                    "name": "E. Mitchell"
                                },
                                {
                                    "authorId": "2064472884",
                                    "name": "Allan Zhou"
                                },
                                {
                                    "authorId": "50465276",
                                    "name": "Archit Sharma"
                                },
                                {
                                    "authorId": "102801230",
                                    "name": "Rafael Rafailov"
                                },
                                {
                                    "authorId": "18307037",
                                    "name": "Huaxiu Yao"
                                },
                                {
                                    "authorId": "46881670",
                                    "name": "Chelsea Finn"
                                },
                                {
                                    "authorId": "144783904",
                                    "name": "Christopher D. Manning"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 355
                        },
                        "score": 0.89501953125
                    },
                    {
                        "id": "(Lord et al., 1984)",
                        "snippets": [
                            "It is proposed that several biases in social judgment result from a failure--first noted by Francis Bacon--to consider possibilities at odds with beliefs and perceptions of the moment. Individuals who are induced to consider the opposite, therefore, should display less bias in social judgment. In two separate but conceptually parallel experiments, this reasoning was applied to two domains--biased assimilation of new evidence on social issues and biased hypothesis testing of personality impressions. Subjects were induced to consider the opposite in two ways: through explicit instructions to do so and through stimulus materials that made opposite possibilities more salient. In both experiments the induction of a consider-the-opposite strategy had greater corrective effect than more demand-laden alternative instructions to be as fair and unbiased as possible. The results are viewed as consistent with previous research on perseverance, hindsight, and logical problem solving, and are thought to suggest an effective method of retraining social judgment."
                        ],
                        "paper": {
                            "corpus_id": 24047702,
                            "title": "Considering the opposite: a corrective strategy for social judgment.",
                            "authors": [
                                {
                                    "authorId": "6802871",
                                    "name": "C. G. Lord"
                                },
                                {
                                    "authorId": "81476483",
                                    "name": "M. Lepper"
                                },
                                {
                                    "authorId": "40299604",
                                    "name": "Elizabeth L. Preston"
                                }
                            ],
                            "year": 1984,
                            "venue": "Journal of Personality and Social Psychology",
                            "n_citations": 663
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2022)",
                        "snippets": [
                            "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%)."
                        ],
                        "paper": {
                            "corpus_id": 247595263,
                            "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
                            "authors": [
                                {
                                    "authorId": "1524732527",
                                    "name": "Xuezhi Wang"
                                },
                                {
                                    "authorId": "119640649",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "50319359",
                                    "name": "D. Schuurmans"
                                },
                                {
                                    "authorId": "1998340269",
                                    "name": "Quoc Le"
                                },
                                {
                                    "authorId": "2226805",
                                    "name": "Ed H. Chi"
                                },
                                {
                                    "authorId": "65855107",
                                    "name": "Denny Zhou"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 3734
                        },
                        "score": 0
                    },
                    {
                        "id": "(Sun et al., 2024)",
                        "snippets": [
                            "We evaluate four methods for estimating confidence scores based on softmax, raw token scores, verbalized confidences, and a combination of these methods, using the area under the curve (AUC) metric to assess calibration, with higher AUC indicating better calibration. We also enhance these with a self-probing mechanism, proposed for closed models."
                        ],
                        "paper": {
                            "corpus_id": 272689376,
                            "title": "Confidence Estimation For LLM-Based Dialogue State Tracking",
                            "authors": [
                                {
                                    "authorId": "2321493889",
                                    "name": "Yi-Jyun Sun"
                                },
                                {
                                    "authorId": "29832722",
                                    "name": "Suvodip Dey"
                                },
                                {
                                    "authorId": "2365041900",
                                    "name": "Dilek Hakkani-Tur"
                                },
                                {
                                    "authorId": "5108268",
                                    "name": "Gokhan Tur"
                                }
                            ],
                            "year": 2024,
                            "venue": "Spoken Language Technology Workshop",
                            "n_citations": 1
                        },
                        "score": 0.9814453125
                    },
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "Data augmentation (Hendrycks et al., 2019)(Wang et al., 2021) and model ensemble (Gal and Ghahramani, 2016;(Lakshminarayanan et al., 2016) have been empirically proven to be successful in computer vision. However, they cannot bring the same benefits in NLP according to the empirical study in Chen et al. (2022b)",
                            "Temperature scaling (Platt, 1999)Guo et al., 2017) readjusts the output logits in a posterior way according to the calibration performance on a held-out set. Label smoothing (Szegedy et al., 2015) imposes a confidence regularization during the training process, discouraging models from being overconfident in their predictions.\n\nRecently, there is an emergent trend in NLP, motivating to directly collect data for training models to have reasonable confidence estimations. Kadavath et al. (2022) assume that the last hidden states of PLMs contain the uncertainty information, and directly apply a multi-layer perceptron on them to perform confidence estimations. Lin et al. (2022) also show that PLMs can be directly trained to give their confidence estimations by words. These two methods are proven to be successful in significantly reducing the overconfidence issue and are further extended to exploit the potential in this kind of methods (Chen et al., 2022b)."
                        ],
                        "paper": {
                            "corpus_id": 259859030,
                            "title": "Making Pre-trained Language Models both Task-solvers and Self-calibrators",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Yangyi Chen"
                                },
                                {
                                    "authorId": "2144803999",
                                    "name": "Xingyao Wang"
                                },
                                {
                                    "authorId": "2072975661",
                                    "name": "Heng Ji"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 0
                        },
                        "score": 0.9130859375
                    },
                    {
                        "id": "(Hendrycks et al., 2019)",
                        "snippets": [
                            "Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AugMix, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AugMix significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half."
                        ],
                        "paper": {
                            "corpus_id": 208637407,
                            "title": "AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty",
                            "authors": [
                                {
                                    "authorId": "3422872",
                                    "name": "Dan Hendrycks"
                                },
                                {
                                    "authorId": "52227748",
                                    "name": "Norman Mu"
                                },
                                {
                                    "authorId": "8132903",
                                    "name": "E. D. Cubuk"
                                },
                                {
                                    "authorId": "2368067",
                                    "name": "Barret Zoph"
                                },
                                {
                                    "authorId": "2058362",
                                    "name": "J. Gilmer"
                                },
                                {
                                    "authorId": "40627523",
                                    "name": "Balaji Lakshminarayanan"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 1308
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lakshminarayanan et al., 2016)",
                        "snippets": [
                            "Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet."
                        ],
                        "paper": {
                            "corpus_id": 6294674,
                            "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles",
                            "authors": [
                                {
                                    "authorId": "40627523",
                                    "name": "Balaji Lakshminarayanan"
                                },
                                {
                                    "authorId": "1863250",
                                    "name": "A. Pritzel"
                                },
                                {
                                    "authorId": "1723876",
                                    "name": "C. Blundell"
                                }
                            ],
                            "year": 2016,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 5843
                        },
                        "score": 0
                    },
                    {
                        "id": "(Guo et al., 2017)",
                        "snippets": [
                            "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."
                        ],
                        "paper": {
                            "corpus_id": 28671436,
                            "title": "On Calibration of Modern Neural Networks",
                            "authors": [
                                {
                                    "authorId": "144993411",
                                    "name": "Chuan Guo"
                                },
                                {
                                    "authorId": "10804137",
                                    "name": "Geoff Pleiss"
                                },
                                {
                                    "authorId": "2117103358",
                                    "name": "Yu Sun"
                                },
                                {
                                    "authorId": "7446832",
                                    "name": "Kilian Q. Weinberger"
                                }
                            ],
                            "year": 2017,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 5869
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Comparative Effectiveness of Calibration Methods",
                "tldr": "Comparative studies show that different calibration methods vary in effectiveness across contexts, with multi-hypothesis approaches combined with temperature scaling reducing calibration error by over 50% in some cases. Adaptive Temperature Scaling (ATS) offers significant improvements for post-RLHF models, while verbalized confidences often outperform raw model probabilities for black-box language models. (10 sources)",
                "text": "\nWhen comparing calibration methods for language models, effectiveness varies significantly based on model architecture, task type, and whether the model is accessible or a black-box system. Several comparative studies provide insights into which methods perform best in different scenarios.\n\nFor black-box language models like ChatGPT, GPT-4, and Claude 2, verbalized confidences have been shown to outperform the models' conditional probabilities across multiple benchmarks including TriviaQA, SciQ, and TruthfulQA <Paper corpusId=\"258865733\" paperTitle=\"(Tian et al., 2023)\" isShortName></Paper>. This effectiveness is further enhanced when combining verbalization with multi-hypothesis generation, where prompting models to produce several answer choices before giving confidence scores significantly improves calibration. When this approach is combined with temperature scaling, it can reduce expected calibration error (ECE) by over 50% compared to using model probabilities alone <Paper corpusId=\"258865733\" paperTitle=\"(Tian et al., 2023)\" isShortName></Paper>.\n\nTemperature scaling remains one of the most consistently effective calibration methods across various contexts. Empirical studies demonstrate that optimal temperature values typically range from 1.5 to 4.0, with a default value of 2.0 often serving as a robust choice across different datasets <Paper corpusId=\"269449363\" paperTitle=\"(Tarride et al., 2024)\" isShortName></Paper>. This simple approach significantly improves the linear correlation between confidence scores and recognition rates without changing the underlying ranking of predictions.\n\nRecent advances include Adaptive Temperature Scaling (ATS), which predicts a temperature scaling parameter for each token prediction rather than using a single global parameter. This approach addresses the varying degrees of calibration shift that can occur after RLHF fine-tuning and has been shown to improve calibration by 10-50% across multiple natural language benchmarks compared to prior calibration methods <Paper corpusId=\"272987064\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>.\n\nWhen comparing training-based and training-free approaches, there are clear tradeoffs. Training-free methods like verbalization or self-consistency can be applied to many language models without requiring access to internal parameters, but they are limited by the models' instruction-following capabilities and can be computationally expensive during inference <Paper corpusId=\"270620078\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. Self-consistency approaches, which sample a diverse set of reasoning paths rather than only using greedy decoding, have shown impressive improvements on reasoning tasks <Paper corpusId=\"247595263\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>.\n\nTraining-based methods directly learn to output a model's uncertainty and can be more efficient at inference time. Approaches like semantic entropy incorporate linguistic invariances created by shared meanings <Paper corpusId=\"257039062\" paperTitle=\"(Kuhn et al., 2023)\" isShortName></Paper>, while other methods leverage token logits or latent representations to predict response correctness <Paper corpusId=\"264811211\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>.\n\nRecent comprehensive evaluations have explored combinations of calibration techniques. Sun et al. evaluated four methods for estimating confidence scores: softmax-based approaches, raw token scores, verbalized confidences, and combinations of these methods, further enhanced with a self-probing mechanism for closed models <Paper corpusId=\"272689376\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>. Similarly, Li et al. noted various approaches, including training calibrators to predict response correctness, using latent representations to predict correctness, and multicalibration techniques <Paper corpusId=\"273821704\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>.\n\nIn the context of confidence estimation for factual accuracy, approaches like SelfCheckGPT have demonstrated that sampling-based methods can effectively detect non-factual statements in a zero-resource fashion by leveraging the consistency of multiple samples <Paper corpusId=\"257557820\" paperTitle=\"(Manakul et al., 2023)\" isShortName></Paper>. These methods have shown higher AUC-PR scores in sentence-level hallucination detection compared to grey-box methods that require partial access to model internals.",
                "citations": [
                    {
                        "id": "(Tian et al., 2023)",
                        "snippets": [
                            "Among the methods for verbalizing probabilities directly, we observe that generating and evaluating multiple hypotheses improves calibration (see Figure 1), similarly to humans (Lord et al., 1984), and corroborating a similar finding in LMs (Kadavath et al., 2022)",
                            "We show that prompting a model to produce several answer choices before giving its confidence scores significantly improves calibration of verbalized probabilities. Combined with temperature scaling (Guo et al., 2017), this approach generally provides better calibration than model probabilities for ChatGPT, GPT-4, and Claude 2 across three datasets, often reducing expected calibration error (ECE) by over 50%."
                        ],
                        "paper": {
                            "corpus_id": 258865733,
                            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
                            "authors": [
                                {
                                    "authorId": "2218407338",
                                    "name": "Katherine Tian"
                                },
                                {
                                    "authorId": "49688913",
                                    "name": "E. Mitchell"
                                },
                                {
                                    "authorId": "2064472884",
                                    "name": "Allan Zhou"
                                },
                                {
                                    "authorId": "50465276",
                                    "name": "Archit Sharma"
                                },
                                {
                                    "authorId": "102801230",
                                    "name": "Rafael Rafailov"
                                },
                                {
                                    "authorId": "18307037",
                                    "name": "Huaxiu Yao"
                                },
                                {
                                    "authorId": "46881670",
                                    "name": "Chelsea Finn"
                                },
                                {
                                    "authorId": "144783904",
                                    "name": "Christopher D. Manning"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 355
                        },
                        "score": 0.89501953125
                    },
                    {
                        "id": "(Tarride et al., 2024)",
                        "snippets": [
                            "The results show that the most effective method is to take the softmax value for each token after the CTC decoding step and average these values for each line to obtain line-level confidence scores. This approach not only ensures computational efficiency, but also yields a strong correlation with the recognition rate. As a result of these findings, we have incorporated this calculation method into PyLaia.\n\nTable 3 provides insight into the impact of model calibration on the linear correlation between confidence scores and recognition rate on the test set of different datasets",
                            "The results show significant improvements in the linear correlation between confidence scores and recognition rates when temperature scaling is applied. Across different datasets, optimal temperatures ranging from 1.5 to 4.0 yield consistently higher correlations compared to scenarios without scaling. Since temperature scaling only scales the logits without changing the underlying ranking of the predictions, it does not affect the rank correlation.\n\nOur analysis suggests that the default value of 2.0 is a robust choice for calibration, even for datasets where the optimal temperature is not equal to 2.0."
                        ],
                        "paper": {
                            "corpus_id": 269449363,
                            "title": "Improving Automatic Text Recognition with Language Models in the PyLaia Open-Source Library",
                            "authors": [
                                {
                                    "authorId": "116700157",
                                    "name": "Sol\u00e8ne Tarride"
                                },
                                {
                                    "authorId": "2298900821",
                                    "name": "Yoann Schneider"
                                },
                                {
                                    "authorId": "2298903215",
                                    "name": "Marie Generali-Lince"
                                },
                                {
                                    "authorId": "1380222130",
                                    "name": "M\u00e9lodie Boillet"
                                },
                                {
                                    "authorId": "1491425010",
                                    "name": "Bastien Abadie"
                                },
                                {
                                    "authorId": "2285800930",
                                    "name": "Christopher Kermorvant"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE International Conference on Document Analysis and Recognition",
                            "n_citations": 4
                        },
                        "score": 0.97314453125
                    },
                    {
                        "id": "(Xie et al., 2024)",
                        "snippets": [
                            "In this work, we introduce Adaptive Temperature Scaling (ATS), a post-hoc calibration method that predicts a temperature scaling parameter for each token prediction. The predicted temperature values adapt based on token-level features and are fit over a standard supervised fine-tuning (SFT) dataset. The adaptive nature of ATS addresses the varying degrees of calibration shift that can occur after RLHF fine-tuning. ATS improves calibration by over 10-50% across three downstream natural language evaluation benchmarks compared to prior calibration methods and does not impede performance improvements from RLHF."
                        ],
                        "paper": {
                            "corpus_id": 272987064,
                            "title": "Calibrating Language Models with Adaptive Temperature Scaling",
                            "authors": [
                                {
                                    "authorId": "2285068366",
                                    "name": "Johnathan Xie"
                                },
                                {
                                    "authorId": "2111073657",
                                    "name": "Annie S. Chen"
                                },
                                {
                                    "authorId": "2260289273",
                                    "name": "Yoonho Lee"
                                },
                                {
                                    "authorId": "2260106796",
                                    "name": "Eric Mitchell"
                                },
                                {
                                    "authorId": "2279838067",
                                    "name": "Chelsea Finn"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 17
                        },
                        "score": 0.93994140625
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "Popular LM calibration methods include training-free, inference-only approaches through verbalization (Tian et al., 2023a) or selfconsistency measurements (Wang et al., 2022) and training-based methods, such as tuning temperature parameters (Liang et al., 2017) or learning uncertainty estimations from LMs' logits (Liu et al., 2023).Despite their potential compatibility with many LMs, training-free methods are limited by the models' instruction-following capabilities and can be computationally expensive during inference.\n\nTraining-based methods, on the other hand, directly learn to output model's uncertainty.For instance, (Kuhn et al., 2023) leverages the token logits predicted by the model to estimate the confidence of the entire response.Yet model logits may not capture knowledge representations and can be sensitive to tokenization procedures."
                        ],
                        "paper": {
                            "corpus_id": 270620078,
                            "title": "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding",
                            "authors": [
                                {
                                    "authorId": "2120099874",
                                    "name": "Xin Liu"
                                },
                                {
                                    "authorId": "2131675694",
                                    "name": "Farima Fatahi Bayat"
                                },
                                {
                                    "authorId": "2299208178",
                                    "name": "Lu Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 6
                        },
                        "score": 0.9462890625
                    },
                    {
                        "id": "(Wang et al., 2022)",
                        "snippets": [
                            "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%)."
                        ],
                        "paper": {
                            "corpus_id": 247595263,
                            "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
                            "authors": [
                                {
                                    "authorId": "1524732527",
                                    "name": "Xuezhi Wang"
                                },
                                {
                                    "authorId": "119640649",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "50319359",
                                    "name": "D. Schuurmans"
                                },
                                {
                                    "authorId": "1998340269",
                                    "name": "Quoc Le"
                                },
                                {
                                    "authorId": "2226805",
                                    "name": "Ed H. Chi"
                                },
                                {
                                    "authorId": "65855107",
                                    "name": "Denny Zhou"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 3734
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kuhn et al., 2023)",
                        "snippets": [
                            "We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of\"semantic equivalence\"-- different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy -- an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines."
                        ],
                        "paper": {
                            "corpus_id": 257039062,
                            "title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation",
                            "authors": [
                                {
                                    "authorId": "39879848",
                                    "name": "Lorenz Kuhn"
                                },
                                {
                                    "authorId": "2681954",
                                    "name": "Y. Gal"
                                },
                                {
                                    "authorId": "33859827",
                                    "name": "Sebastian Farquhar"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 310
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "A model is considered well-calibrated when its probability estimate aligns with the actual likelihood of the output being correct. Calibrating language models (LMs) is crucial, as it plays a vital role in detecting and mitigating hallucinations of LMs as well as building more trustworthy models. However, standard calibration techniques may not be suited for LM calibration. For instance, post-processing methods such as temperature scaling do not reorder the candidate generations. On the other hand, training-based methods require fine-tuning the entire model, which is impractical for LMs of large scale. We present LitCab, a lightweight calibration mechanism consisting of a single linear layer that takes the input text representation and predicts a bias term, which is then added to the LM output logits. LitCab improves model calibration by only adding<2% of the original model parameters. For evaluation, we construct CaT, a benchmark consisting of eight text generation tasks, covering responses ranging from short phrases to paragraphs. We test LitCab with Llama2-7B, where it improves calibration across all tasks, reducing the average ECE score by as large as 30%. We further conduct a comprehensive evaluation with multiple popular open-sourced LMs from GPT and LLaMA families, yielding the following key findings: (i) Larger models within the same family exhibit better calibration on tasks with short generation tasks, but not necessarily for longer ones. (ii) GPT-family models show superior calibration compared to LLaMA, Llama2, and Vicuna models, despite having much fewer parameters. (iii) Fine-tuning pretrained model (e.g., LLaMA) with samples of limited purpose (e.g., conversations) may lead to worse calibration, highlighting the importance of fine-tuning setups for calibrating LMs."
                        ],
                        "paper": {
                            "corpus_id": 264811211,
                            "title": "LitCab: Lightweight Language Model Calibration over Short- and Long-form Responses",
                            "authors": [
                                {
                                    "authorId": "2120099874",
                                    "name": "Xin Liu"
                                },
                                {
                                    "authorId": "2261284957",
                                    "name": "Muhammad Khalifa"
                                },
                                {
                                    "authorId": "2261364835",
                                    "name": "Lu Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 27
                        },
                        "score": 0
                    },
                    {
                        "id": "(Sun et al., 2024)",
                        "snippets": [
                            "We evaluate four methods for estimating confidence scores based on softmax, raw token scores, verbalized confidences, and a combination of these methods, using the area under the curve (AUC) metric to assess calibration, with higher AUC indicating better calibration. We also enhance these with a self-probing mechanism, proposed for closed models."
                        ],
                        "paper": {
                            "corpus_id": 272689376,
                            "title": "Confidence Estimation For LLM-Based Dialogue State Tracking",
                            "authors": [
                                {
                                    "authorId": "2321493889",
                                    "name": "Yi-Jyun Sun"
                                },
                                {
                                    "authorId": "29832722",
                                    "name": "Suvodip Dey"
                                },
                                {
                                    "authorId": "2365041900",
                                    "name": "Dilek Hakkani-Tur"
                                },
                                {
                                    "authorId": "5108268",
                                    "name": "Gokhan Tur"
                                }
                            ],
                            "year": 2024,
                            "venue": "Spoken Language Technology Workshop",
                            "n_citations": 1
                        },
                        "score": 0.9814453125
                    },
                    {
                        "id": "(Li et al._1, 2024)",
                        "snippets": [
                            "(Mielke et al., 2020) train a calibrator to predict the correctness of a response for a given question. With a similar idea, Ulmer et al. (2024) train a language model (e.g., DeBERTa) based on question-response pairs to predict the probability of responses' correctness. Based on SelfCheckGPT (Manakul et al., 2023) and JAFC (Tian et al., 2023), Chen et al. (2024) train supervised models to reduce grouping losses and improve the confidence estimation. The method by Liu et al. (2024) uses an LLM's latent representations to predict the correctness of responses. Detommaso et al. (2024) use the \"multicalibration\" technique to calibrate the probability of correctness. Fadeeva et al. (2023) offer a detailed comparative study of various confidence estimation methods, providing empirical evidence on their effectiveness across different tasks."
                        ],
                        "paper": {
                            "corpus_id": 273821704,
                            "title": "Graph-based Confidence Calibration for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2329041193",
                                    "name": "Yukun Li"
                                },
                                {
                                    "authorId": "2238971842",
                                    "name": "Sijia Wang"
                                },
                                {
                                    "authorId": "2301269508",
                                    "name": "Lifu Huang"
                                },
                                {
                                    "authorId": "2294764328",
                                    "name": "Li-Ping Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.962890625
                    },
                    {
                        "id": "(Manakul et al., 2023)",
                        "snippets": [
                            "Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods."
                        ],
                        "paper": {
                            "corpus_id": 257557820,
                            "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
                            "authors": [
                                {
                                    "authorId": "89355510",
                                    "name": "Potsawee Manakul"
                                },
                                {
                                    "authorId": "2190750613",
                                    "name": "Adian Liusie"
                                },
                                {
                                    "authorId": "1740397",
                                    "name": "M. Gales"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 444
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Adaptive and Domain-specific Calibration",
                "tldr": "Adaptive and domain-specific calibration techniques address the challenge that language models may appear well-calibrated overall while being significantly miscalibrated for specific domains or contexts. These approaches include few-shot slice-specific recalibration and adaptive temperature scaling, which dynamically adjust confidence scores based on the specific domain or token-level features. (5 sources)",
                "text": "\nLanguage models often demonstrate varying levels of calibration across different domains, tasks, or contexts. While a model might appear well-calibrated when evaluated broadly, this can mask significant miscalibration within specific slices of data - for example, \"systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate\" <Paper corpusId=\"268723623\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This observation has led to the development of adaptive and domain-specific calibration techniques that dynamically adjust confidence scores based on specific contexts.\n\nFew-shot slice-specific recalibration represents a significant advancement in this area. This approach trains a recalibration model that takes a few unlabeled examples from any given domain slice and predicts a curve that remaps confidence scores to be more accurate for that specific slice. The trained model can recalibrate for arbitrary new slices without requiring labeled data from those domains, enabling the identification of domain-specific confidence thresholds above which language model predictions can be trusted. Experimental results demonstrate that this few-shot recalibrator consistently outperforms existing calibration methods, improving calibration error for PaLM2-Large on MMLU by 16% compared to temperature scaling <Paper corpusId=\"268723623\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nAnother innovative approach is Adaptive Temperature Scaling (ATS), which addresses the limitations of traditional temperature scaling by predicting a unique temperature scaling parameter for each token prediction rather than applying a single global parameter. This method adapts based on token-level features and is fitted over a standard supervised fine-tuning dataset. The adaptive nature of ATS specifically addresses the varying degrees of calibration shift that can occur after reinforcement learning from human feedback (RLHF) fine-tuning. Studies show that ATS improves calibration by 10-50% across multiple natural language evaluation benchmarks compared to prior calibration methods, without impeding performance improvements from RLHF <Paper corpusId=\"272987064\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>.\n\nWhen implementing adaptive calibration approaches, practitioners should consider the tradeoffs between open-box and post-hoc methods. While post-hoc methods like temperature scaling <Paper corpusId=\"28671436\" paperTitle=\"(Guo et al., 2017)\" isShortName></Paper>, Platt scaling, histogram binning, and training with label smoothing <Paper corpusId=\"174802983\" paperTitle=\"(Muller et al., 2019)\" isShortName></Paper> are more straightforward to implement, open-box methods that access model internals can potentially provide more informative calibration signals. However, these open-box approaches are often more complex and difficult to apply to large language models <Paper corpusId=\"278129791\" paperTitle=\"(Abbasli et al., 2025)\" isShortName></Paper>.\n\nDomain-specific calibration is particularly important for applications requiring high reliability in specific contexts, such as medical diagnosis, legal analysis, or educational assessment. By dynamically adjusting confidence scores based on the specific domain or context, these adaptive approaches help ensure that language models provide appropriately calibrated confidence estimates across the diverse range of tasks they may encounter.",
                "citations": [
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain. Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling."
                        ],
                        "paper": {
                            "corpus_id": 268723623,
                            "title": "Few-Shot Recalibration of Language Models",
                            "authors": [
                                {
                                    "authorId": "2293910776",
                                    "name": "Xiang Lisa Li"
                                },
                                {
                                    "authorId": "3030219",
                                    "name": "Urvashi Khandelwal"
                                },
                                {
                                    "authorId": "2091768",
                                    "name": "Kelvin Guu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.966796875
                    },
                    {
                        "id": "(Xie et al., 2024)",
                        "snippets": [
                            "In this work, we introduce Adaptive Temperature Scaling (ATS), a post-hoc calibration method that predicts a temperature scaling parameter for each token prediction. The predicted temperature values adapt based on token-level features and are fit over a standard supervised fine-tuning (SFT) dataset. The adaptive nature of ATS addresses the varying degrees of calibration shift that can occur after RLHF fine-tuning. ATS improves calibration by over 10-50% across three downstream natural language evaluation benchmarks compared to prior calibration methods and does not impede performance improvements from RLHF."
                        ],
                        "paper": {
                            "corpus_id": 272987064,
                            "title": "Calibrating Language Models with Adaptive Temperature Scaling",
                            "authors": [
                                {
                                    "authorId": "2285068366",
                                    "name": "Johnathan Xie"
                                },
                                {
                                    "authorId": "2111073657",
                                    "name": "Annie S. Chen"
                                },
                                {
                                    "authorId": "2260289273",
                                    "name": "Yoonho Lee"
                                },
                                {
                                    "authorId": "2260106796",
                                    "name": "Eric Mitchell"
                                },
                                {
                                    "authorId": "2279838067",
                                    "name": "Chelsea Finn"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 17
                        },
                        "score": 0.93994140625
                    },
                    {
                        "id": "(Guo et al., 2017)",
                        "snippets": [
                            "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."
                        ],
                        "paper": {
                            "corpus_id": 28671436,
                            "title": "On Calibration of Modern Neural Networks",
                            "authors": [
                                {
                                    "authorId": "144993411",
                                    "name": "Chuan Guo"
                                },
                                {
                                    "authorId": "10804137",
                                    "name": "Geoff Pleiss"
                                },
                                {
                                    "authorId": "2117103358",
                                    "name": "Yu Sun"
                                },
                                {
                                    "authorId": "7446832",
                                    "name": "Kilian Q. Weinberger"
                                }
                            ],
                            "year": 2017,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 5869
                        },
                        "score": 0
                    },
                    {
                        "id": "(Muller et al., 2019)",
                        "snippets": [
                            "The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions."
                        ],
                        "paper": {
                            "corpus_id": 174802983,
                            "title": "When Does Label Smoothing Help?",
                            "authors": [
                                {
                                    "authorId": "2114054259",
                                    "name": "Rafael M\u00fcller"
                                },
                                {
                                    "authorId": "40464924",
                                    "name": "Simon Kornblith"
                                },
                                {
                                    "authorId": "1695689",
                                    "name": "Geoffrey E. Hinton"
                                }
                            ],
                            "year": 2019,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 1954
                        },
                        "score": 0
                    },
                    {
                        "id": "(Abbasli et al., 2025)",
                        "snippets": [
                            "Post-hoc or post-training methods, such as TS (Guo et al., 2017), Platt scaling (Platt, 1999), histogram binning (Zadrozny et al., 2001), and training with label smoothing (M\u00fcller et al., 2019), are classical methods of calibration. Mixup training with learning the complexity of samples was proposed to calibrate the pre-trained language models [100]. We note that open-box methods of calibration can be complex and difficult to apply to LLMs, but they can be more informative of inner states than post-hoc methods [101]."
                        ],
                        "paper": {
                            "corpus_id": 278129791,
                            "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review",
                            "authors": [
                                {
                                    "authorId": "2357723383",
                                    "name": "Toghrul Abbasli"
                                },
                                {
                                    "authorId": "2311579070",
                                    "name": "Kentaroh Toyoda"
                                },
                                {
                                    "authorId": "2285070153",
                                    "name": "Yuan Wang"
                                },
                                {
                                    "authorId": "2115471566",
                                    "name": "Leon Witt"
                                },
                                {
                                    "authorId": "2358256611",
                                    "name": "Muhammad Asif Ali"
                                },
                                {
                                    "authorId": "2239095099",
                                    "name": "Yukai Miao"
                                },
                                {
                                    "authorId": "2302856161",
                                    "name": "Dan Li"
                                },
                                {
                                    "authorId": "2285078682",
                                    "name": "Qingsong Wei"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.953125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Implementation Considerations",
                "tldr": "Implementing effective calibration methods requires careful consideration of model architecture, available access level, and computational efficiency. Practitioners must select appropriate techniques based on whether they're working with black-box, gray-box, or open-box language models, alongside optimizing parameters like temperature values. (6 sources)",
                "text": "\nWhen implementing calibration techniques for language models, several key considerations should guide your approach:\n\n## Model Access Level\n* **Black-box models**: For commercial LLMs with no internal access, verbalization methods, consistency-based approaches, and post-processing techniques like histogram binning and isotonic regression are most appropriate <Paper corpusId=\"274789494\" paperTitle=\"(Xie et al._1, 2024)\" isShortName></Paper>\n* **Gray-box models**: Temperature scaling, which operates on the logits of model outputs, is well-suited for scenarios with partial access to model internals <Paper corpusId=\"274789494\" paperTitle=\"(Xie et al._1, 2024)\" isShortName></Paper>\n* **Open-box models**: These allow more informative calibration through direct manipulation of model parameters, though these methods can be complex and difficult to apply to large language models <Paper corpusId=\"278129791\" paperTitle=\"(Abbasli et al., 2025)\" isShortName></Paper>\n\n## Parameter Selection\n* **Temperature values**: When implementing temperature scaling, empirical studies show optimal temperatures typically range from 1.5 to 4.0, with 2.0 serving as a robust default choice across diverse datasets <Paper corpusId=\"269449363\" paperTitle=\"(Tarride et al., 2024)\" isShortName></Paper>\n* **Confidence thresholds**: For domain-specific applications, determining appropriate confidence thresholds may require validation on domain-specific data\n\n## Computational Efficiency\n* **Token-level averaging**: For some applications, taking the softmax value for each token and averaging these values provides an efficient yet effective approach to obtaining confidence scores while maintaining strong correlation with recognition rates <Paper corpusId=\"269449363\" paperTitle=\"(Tarride et al., 2024)\" isShortName></Paper>\n* **Inference costs**: Consider the additional computational burden of methods requiring multiple forward passes or sampling-based approaches\n\n## Domain Adaptation\n* **Out-of-domain (OOD) data**: When calibrating for OOD scenarios, using pseudo transcriptions to provide binary targets for training model-based confidence estimators and including additional features from an OOD language model can significantly improve confidence scores <Paper corpusId=\"238419596\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>\n* **Pre-exposure to OOD data**: Exposing confidence estimation models to OOD data during training can improve calibration performance even when the primary model wasn't trained on such data <Paper corpusId=\"238419596\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>\n\n## Method Compatibility\n* **Post-hoc vs. training-based**: Classical post-hoc methods like temperature scaling <Paper corpusId=\"28671436\" paperTitle=\"(Guo et al., 2017)\" isShortName></Paper>, Platt scaling, and histogram binning are generally easier to implement but may provide less informative calibration than training-based approaches <Paper corpusId=\"278129791\" paperTitle=\"(Abbasli et al., 2025)\" isShortName></Paper>\n* **Label smoothing**: This approach prevents networks from becoming overconfident and has been effective in many state-of-the-art models <Paper corpusId=\"174802983\" paperTitle=\"(Muller et al., 2019)\" isShortName></Paper>, but requires model training access\n\n## Evaluation Protocol\n* **Calibration metrics**: Implement appropriate evaluation metrics like Expected Calibration Error (ECE) or linear correlation between confidence scores and recognition rates to properly assess calibration performance\n* **Dataset selection**: Ensure evaluation datasets represent the diversity of domains and tasks the model will encounter in production",
                "citations": [
                    {
                        "id": "(Xie et al._1, 2024)",
                        "snippets": [
                            "Calibration aims to align confidence scores with correctness. Commonly used calibration methods include temperature scaling (Guo et al., 2017), label smoothing (Szegedy et al., 2016), and logistic calibration (Kull et al., 2017). However, methods like label smoothing and logistic calibration are unsuitable for black-box LLMs, as they require internal model access or fine-tuning, whereas temperature scaling, which operates on the logits of model outputs, is better suited for gray-box models. In black-box LLMs, common calibration methods include post-processing or transforming the model into a gray-box. Due to the near-well-calibrated nature of Confidence Estimation in black-box LLMs, calibration pressure is reduced.\n\nIn post-processing methods, popular techniques include Histogram Binning (Zadrozny and Elkan, 2001) and Isotonic Regression (Jiang et al., 2011). Histogram Binning (Zadrozny and Elkan, 2001) adjusts confidence values within different intervals by assessing the correctness of predictions in each interval, providing a simple but potentially limited calibration when confidence distributions are uneven. Isotonic Regression (Jiang et al., 2011), on the other hand, seeks an optimal monotonically increasing function f (c) to map the model's confidence c 1 , c 2 , . . . , c n to correctness. f (c) need to satisfy the condition in Formula 2, ensuring that the confidence ranking aligns with the correctness.\n\nTo address more complex scenarios, some methods introduce additional constraints. For instance, H\u00e9bert-Johnson et al. ( 2018) introduced a multicalibration method, ensuring not only overall alignment but also calibration across multiple subgroups S j through iterative alignment. The overall goal can be expressed as Formula 3."
                        ],
                        "paper": {
                            "corpus_id": 274789494,
                            "title": "A Survey of Calibration Process for Black-Box LLMs",
                            "authors": [
                                {
                                    "authorId": "2336030822",
                                    "name": "Liangru Xie"
                                },
                                {
                                    "authorId": "2326538488",
                                    "name": "Hui Liu"
                                },
                                {
                                    "authorId": "2276423058",
                                    "name": "Jingying Zeng"
                                },
                                {
                                    "authorId": "2313288320",
                                    "name": "Xianfeng Tang"
                                },
                                {
                                    "authorId": "2336079495",
                                    "name": "Yan Han"
                                },
                                {
                                    "authorId": "2305051640",
                                    "name": "Chen Luo"
                                },
                                {
                                    "authorId": "2336079074",
                                    "name": "Jing Huang"
                                },
                                {
                                    "authorId": "2313299851",
                                    "name": "Zhen Li"
                                },
                                {
                                    "authorId": "2257362558",
                                    "name": "Suhang Wang"
                                },
                                {
                                    "authorId": "2315065019",
                                    "name": "Qi He"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.93408203125
                    },
                    {
                        "id": "(Abbasli et al., 2025)",
                        "snippets": [
                            "Post-hoc or post-training methods, such as TS (Guo et al., 2017), Platt scaling (Platt, 1999), histogram binning (Zadrozny et al., 2001), and training with label smoothing (M\u00fcller et al., 2019), are classical methods of calibration. Mixup training with learning the complexity of samples was proposed to calibrate the pre-trained language models [100]. We note that open-box methods of calibration can be complex and difficult to apply to LLMs, but they can be more informative of inner states than post-hoc methods [101]."
                        ],
                        "paper": {
                            "corpus_id": 278129791,
                            "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review",
                            "authors": [
                                {
                                    "authorId": "2357723383",
                                    "name": "Toghrul Abbasli"
                                },
                                {
                                    "authorId": "2311579070",
                                    "name": "Kentaroh Toyoda"
                                },
                                {
                                    "authorId": "2285070153",
                                    "name": "Yuan Wang"
                                },
                                {
                                    "authorId": "2115471566",
                                    "name": "Leon Witt"
                                },
                                {
                                    "authorId": "2358256611",
                                    "name": "Muhammad Asif Ali"
                                },
                                {
                                    "authorId": "2239095099",
                                    "name": "Yukai Miao"
                                },
                                {
                                    "authorId": "2302856161",
                                    "name": "Dan Li"
                                },
                                {
                                    "authorId": "2285078682",
                                    "name": "Qingsong Wei"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.953125
                    },
                    {
                        "id": "(Tarride et al., 2024)",
                        "snippets": [
                            "The results show that the most effective method is to take the softmax value for each token after the CTC decoding step and average these values for each line to obtain line-level confidence scores. This approach not only ensures computational efficiency, but also yields a strong correlation with the recognition rate. As a result of these findings, we have incorporated this calculation method into PyLaia.\n\nTable 3 provides insight into the impact of model calibration on the linear correlation between confidence scores and recognition rate on the test set of different datasets",
                            "The results show significant improvements in the linear correlation between confidence scores and recognition rates when temperature scaling is applied. Across different datasets, optimal temperatures ranging from 1.5 to 4.0 yield consistently higher correlations compared to scenarios without scaling. Since temperature scaling only scales the logits without changing the underlying ranking of the predictions, it does not affect the rank correlation.\n\nOur analysis suggests that the default value of 2.0 is a robust choice for calibration, even for datasets where the optimal temperature is not equal to 2.0."
                        ],
                        "paper": {
                            "corpus_id": 269449363,
                            "title": "Improving Automatic Text Recognition with Language Models in the PyLaia Open-Source Library",
                            "authors": [
                                {
                                    "authorId": "116700157",
                                    "name": "Sol\u00e8ne Tarride"
                                },
                                {
                                    "authorId": "2298900821",
                                    "name": "Yoann Schneider"
                                },
                                {
                                    "authorId": "2298903215",
                                    "name": "Marie Generali-Lince"
                                },
                                {
                                    "authorId": "1380222130",
                                    "name": "M\u00e9lodie Boillet"
                                },
                                {
                                    "authorId": "1491425010",
                                    "name": "Bastien Abadie"
                                },
                                {
                                    "authorId": "2285800930",
                                    "name": "Christopher Kermorvant"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE International Conference on Document Analysis and Recognition",
                            "n_citations": 4
                        },
                        "score": 0.97314453125
                    },
                    {
                        "id": "(Li et al., 2021)",
                        "snippets": [
                            "Recent research has shown that model-based confidence estimators have a significant advantage over using the output softmax probabilities. If the input data to the speech recogniser is from mismatched acoustic and linguistic conditions, the ASR performance and the corresponding confidence estimators may exhibit severe degradation",
                            "Using pseudo transcriptions to provide binary targets for training model-based confidence estimators and including additional features from an OOD LM are useful for improving the confidence scores on OOD datasets, even when pretraining uses OOD data. By exposing the CEM or R-EBM to OOD data, the calibration performance is also improved."
                        ],
                        "paper": {
                            "corpus_id": 238419596,
                            "title": "Improving Confidence Estimation on Out-of-Domain Data for End-to-End Speech Recognition",
                            "authors": [
                                {
                                    "authorId": "1522347808",
                                    "name": "Qiujia Li"
                                },
                                {
                                    "authorId": "2153632494",
                                    "name": "Yu Zhang"
                                },
                                {
                                    "authorId": "150080713",
                                    "name": "David Qiu"
                                },
                                {
                                    "authorId": "2145999837",
                                    "name": "Yanzhang He"
                                },
                                {
                                    "authorId": "48749954",
                                    "name": "Liangliang Cao"
                                },
                                {
                                    "authorId": "1716393",
                                    "name": "P. Woodland"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                            "n_citations": 11
                        },
                        "score": 0.87939453125
                    },
                    {
                        "id": "(Guo et al., 2017)",
                        "snippets": [
                            "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."
                        ],
                        "paper": {
                            "corpus_id": 28671436,
                            "title": "On Calibration of Modern Neural Networks",
                            "authors": [
                                {
                                    "authorId": "144993411",
                                    "name": "Chuan Guo"
                                },
                                {
                                    "authorId": "10804137",
                                    "name": "Geoff Pleiss"
                                },
                                {
                                    "authorId": "2117103358",
                                    "name": "Yu Sun"
                                },
                                {
                                    "authorId": "7446832",
                                    "name": "Kilian Q. Weinberger"
                                }
                            ],
                            "year": 2017,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 5869
                        },
                        "score": 0
                    },
                    {
                        "id": "(Muller et al., 2019)",
                        "snippets": [
                            "The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions."
                        ],
                        "paper": {
                            "corpus_id": 174802983,
                            "title": "When Does Label Smoothing Help?",
                            "authors": [
                                {
                                    "authorId": "2114054259",
                                    "name": "Rafael M\u00fcller"
                                },
                                {
                                    "authorId": "40464924",
                                    "name": "Simon Kornblith"
                                },
                                {
                                    "authorId": "1695689",
                                    "name": "Geoffrey E. Hinton"
                                }
                            ],
                            "year": 2019,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 1954
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.32099099999999997
    }
}