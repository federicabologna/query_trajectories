{
    "query": "detect or data from language models related works topic",
    "user_id": "lib_user",
    "task_id": "d6d07b70-d320-4510-afb2-f7fb38dbb0ba",
    "timestamp": "2025-06-23T23:14:56.531240",
    "n_retrieval": 256,
    "n_retrieved": 273,
    "n_candidates": 24,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.258882,
    "decomposed_query": {
        "rewritten_query": "Detection or data from language models.",
        "keyword_query": "detect data language models",
        "search_filters": {
            "fieldsOfStudy": "Computer Science,Linguistics"
        },
        "cost": 0.009378,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 46,
            "citation_count": 6,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.21248, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2313922585",
                    "name": "Anqi Zhang"
                },
                {
                    "authorId": "2314513929",
                    "name": "Chaofeng Wu"
                }
            ],
            "abstract": "While large language models (LLMs) are extensively used, there are raising concerns regarding privacy, security, and copyright due to their opaque training data, which brings the problem of detecting pre-training data on the table. Current solutions to this problem leverage techniques explored in machine learning privacy such as Membership Inference Attacks (MIAs), which heavily depend on LLMs' capability of verbatim memorization. However, this reliance presents challenges, especially given the vast amount of training data and the restricted number of effective training epochs. In this paper, we propose an adaptive pre-training data detection method which alleviates this reliance and effectively amplify the identification. Our method adaptively locates \\textit{surprising tokens} of the input. A token is surprising to a LLM if the prediction on the token is\"certain but wrong\", which refers to low Shannon entropy of the probability distribution and low probability of the ground truth token at the same time. By using the prediction probability of surprising tokens to measure \\textit{surprising}, the detection method is achieved based on the simple hypothesis that seeing seen data is less surprising for the model compared with seeing unseen data. The method can be applied without any access to the the pre-training data corpus or additional training like reference models. Our approach exhibits a consistent enhancement compared to existing methods in diverse experiments conducted on various benchmarks and models, achieving a maximum improvement of 29.5\\%. We also introduce a new benchmark Dolma-Book developed upon a novel framework, which employs book data collected both before and after model training to provide further evaluation.",
            "corpus_id": 271570943,
            "sentences": [
                {
                    "corpus_id": "271570943",
                    "title": "Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens",
                    "text": "We first define the terminology we use, formalize the problem, and state our assumptions about the knowledge and capabilities. We follow the standard definition of the existing pre-training data detection in LLMs: \n\nAutoregressive language models. We focus on the autoregressive language models M, which are trained to predict the next text token based on the input sequence of previous tokens (i.e., different from bidirectional masked language models such as BERT [14]). We use p(\u2022|x 1 , x 2 , ..., x i\u22121 ) to represent the probability distribution of an model M on generating next token when given an input sequence x 1 , x 2 , ..., x i\u22121 . This probability distribution is among a token vocabulary, denoted as V, and for each v j \u2208 V, denote p(v j | x 1 , .., x i\u22121 ) as the probability of next token to be v j . \n\nClarify names and definitions. We want to clarify the following names used in paper for better understanding. Given an input sequence x 1 , x 2 , . . . , x N , each of x i is called token (i.e., ground-truth token); index (i.e, 1, 2, ..N ) is used to state the location of x i . We call the possible generated tokens (i.e., \u2200 v j \u2208 V) at a specific index as token candidates. \n\nProblem definition. Given a data point x = x 1 , x 2 , .., x N and a pre-trained language model M, the goal of pre-training data detection problem is to infer whether x belongs to the training dataset D for this model M. The detection process F is achieved by computing a score f (x, M) and decided through a case-dependent threshold \u03bb. Formally, x : F(f (x, M); \u03bb) \u2192 {0, 1}. Following previous works, people care about the threshold-independent metric, area under the ROC curve, for the aggregate measure of performance. \n\nKnowledge and capabilities.",
                    "score": 0.4757768092357937,
                    "section_title": "Preliminary",
                    "char_start_offset": 8216,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 126
                        },
                        {
                            "start": 127,
                            "end": 213
                        },
                        {
                            "start": 216,
                            "end": 247
                        },
                        {
                            "start": 248,
                            "end": 472
                        },
                        {
                            "start": 473,
                            "end": 643
                        },
                        {
                            "start": 644,
                            "end": 816
                        },
                        {
                            "start": 819,
                            "end": 849
                        },
                        {
                            "start": 850,
                            "end": 928
                        },
                        {
                            "start": 929,
                            "end": 970
                        },
                        {
                            "start": 971,
                            "end": 1097
                        },
                        {
                            "start": 1098,
                            "end": 1194
                        },
                        {
                            "start": 1197,
                            "end": 1216
                        },
                        {
                            "start": 1217,
                            "end": 1533
                        },
                        {
                            "start": 1534,
                            "end": 1572
                        },
                        {
                            "start": 1573,
                            "end": 1718
                        },
                        {
                            "start": 1721,
                            "end": 1748
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 466,
                            "end": 470,
                            "matchedPaperCorpusId": "52967399"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78466796875
                }
            ],
            "relevance_judgement": 0.78466796875,
            "relevance_judgment_input_expanded": "# Title: Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens\n# Venue: arXiv.org\n# Authors: Anqi Zhang, Chaofeng Wu\n## Abstract\nWhile large language models (LLMs) are extensively used, there are raising concerns regarding privacy, security, and copyright due to their opaque training data, which brings the problem of detecting pre-training data on the table. Current solutions to this problem leverage techniques explored in machine learning privacy such as Membership Inference Attacks (MIAs), which heavily depend on LLMs' capability of verbatim memorization. However, this reliance presents challenges, especially given the vast amount of training data and the restricted number of effective training epochs. In this paper, we propose an adaptive pre-training data detection method which alleviates this reliance and effectively amplify the identification. Our method adaptively locates \\textit{surprising tokens} of the input. A token is surprising to a LLM if the prediction on the token is\"certain but wrong\", which refers to low Shannon entropy of the probability distribution and low probability of the ground truth token at the same time. By using the prediction probability of surprising tokens to measure \\textit{surprising}, the detection method is achieved based on the simple hypothesis that seeing seen data is less surprising for the model compared with seeing unseen data. The method can be applied without any access to the the pre-training data corpus or additional training like reference models. Our approach exhibits a consistent enhancement compared to existing methods in diverse experiments conducted on various benchmarks and models, achieving a maximum improvement of 29.5\\%. We also introduce a new benchmark Dolma-Book developed upon a novel framework, which employs book data collected both before and after model training to provide further evaluation.\n## Preliminary\nWe first define the terminology we use, formalize the problem, and state our assumptions about the knowledge and capabilities. We follow the standard definition of the existing pre-training data detection in LLMs: \n\nAutoregressive language models. We focus on the autoregressive language models M, which are trained to predict the next text token based on the input sequence of previous tokens (i.e., different from bidirectional masked language models such as BERT [14]). We use p(\u2022|x 1 , x 2 , ..., x i\u22121 ) to represent the probability distribution of an model M on generating next token when given an input sequence x 1 , x 2 , ..., x i\u22121 . This probability distribution is among a token vocabulary, denoted as V, and for each v j \u2208 V, denote p(v j | x 1 , .., x i\u22121 ) as the probability of next token to be v j . \n\nClarify names and definitions. We want to clarify the following names used in paper for better understanding. Given an input sequence x 1 , x 2 , . . . , x N , each of x i is called token (i.e., ground-truth token); index (i.e, 1, 2, ..N ) is used to state the location of x i . We call the possible generated tokens (i.e., \u2200 v j \u2208 V) at a specific index as token candidates. \n\nProblem definition. Given a data point x = x 1 , x 2 , .., x N and a pre-trained language model M, the goal of pre-training data detection problem is to infer whether x belongs to the training dataset D for this model M. The detection process F is achieved by computing a score f (x, M) and decided through a case-dependent threshold \u03bb. Formally, x : F(f (x, M); \u03bb) \u2192 {0, 1}. Following previous works, people care about the threshold-independent metric, area under the ROC curve, for the aggregate measure of performance. \n\nKnowledge and capabilities.",
            "reference_string": "[271570943 | Zhang et al. | 2024 | Citations: 6]"
        },
        {
            "title": "Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 50,
            "citation_count": 54,
            "influential_citation_count": 16,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.02936, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2267788653",
                    "name": "Jingyang Zhang"
                },
                {
                    "authorId": "2156016815",
                    "name": "Jingwei Sun"
                },
                {
                    "authorId": "2119236605",
                    "name": "Eric C. Yeats"
                },
                {
                    "authorId": "2294877492",
                    "name": "Ouyang Yang"
                },
                {
                    "authorId": "2211526996",
                    "name": "Martin Kuo"
                },
                {
                    "authorId": "2265652686",
                    "name": "Jianyi Zhang"
                },
                {
                    "authorId": "2297270173",
                    "name": "Hao k Yang"
                },
                {
                    "authorId": "2294901591",
                    "name": "Hai Li"
                }
            ],
            "abstract": "The problem of pre-training data detection for large language models (LLMs) has received growing attention due to its implications in critical issues like copyright violation and test data contamination. Despite improved performance, existing methods (including the state-of-the-art, Min-K%) are mostly developed upon simple heuristics and lack solid, reasonable foundations. In this work, we propose a novel and theoretically motivated methodology for pre-training data detection, named Min-K%++. Specifically, we present a key insight that training samples tend to be local maxima of the modeled distribution along each input dimension through maximum likelihood training, which in turn allow us to insightfully translate the problem into identification of local maxima. Then, we design our method accordingly that works under the discrete distribution modeled by LLMs, whose core idea is to determine whether the input forms a mode or has relatively high probability under the conditional categorical distribution. Empirically, the proposed method achieves new SOTA performance across multiple settings. On the WikiMIA benchmark, Min-K%++ outperforms the runner-up by 6.2% to 10.5% in detection AUROC averaged over five models. On the more challenging MIMIR benchmark, it consistently improves upon reference-free methods while performing on par with reference-based method that requires an extra reference model.",
            "corpus_id": 268889777,
            "sentences": [
                {
                    "corpus_id": "268889777",
                    "title": "Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models",
                    "text": "The problem of pre-training data detection for large language models (LLMs) has received growing attention due to its implications in critical issues like copyright violation and test data contamination. Despite improved performance, existing methods (including the state-of-the-art, Min-K%) are mostly developed upon simple heuristics and lack solid, reasonable foundations. In this work, we propose a novel and theoretically motivated methodology for pre-training data detection, named Min-K%++. Specifically, we present a key insight that training samples tend to be local maxima of the modeled distribution along each input dimension through maximum likelihood training, which in turn allow us to insightfully translate the problem into identification of local maxima. Then, we design our method accordingly that works under the discrete distribution modeled by LLMs, whose core idea is to determine whether the input forms a mode or has relatively high probability under the conditional categorical distribution. Empirically, the proposed method achieves new SOTA performance across multiple settings. On the WikiMIA benchmark, Min-K%++ outperforms the runner-up by 6.2% to 10.5% in detection AUROC averaged over five models. On the more challenging MIMIR benchmark, it consistently improves upon reference-free methods while performing on par with reference-based method that requires an extra reference model.",
                    "score": 0.4310919816026011,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.74365234375
                }
            ],
            "relevance_judgement": 0.74365234375,
            "relevance_judgment_input_expanded": "# Title: Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models\n# Venue: arXiv.org\n# Authors: Jingyang Zhang, Jingwei Sun, Eric C. Yeats, Ouyang Yang, Martin Kuo, Jianyi Zhang, Hao k Yang, Hai Li\n## Abstract\nThe problem of pre-training data detection for large language models (LLMs) has received growing attention due to its implications in critical issues like copyright violation and test data contamination. Despite improved performance, existing methods (including the state-of-the-art, Min-K%) are mostly developed upon simple heuristics and lack solid, reasonable foundations. In this work, we propose a novel and theoretically motivated methodology for pre-training data detection, named Min-K%++. Specifically, we present a key insight that training samples tend to be local maxima of the modeled distribution along each input dimension through maximum likelihood training, which in turn allow us to insightfully translate the problem into identification of local maxima. Then, we design our method accordingly that works under the discrete distribution modeled by LLMs, whose core idea is to determine whether the input forms a mode or has relatively high probability under the conditional categorical distribution. Empirically, the proposed method achieves new SOTA performance across multiple settings. On the WikiMIA benchmark, Min-K%++ outperforms the runner-up by 6.2% to 10.5% in detection AUROC averaged over five models. On the more challenging MIMIR benchmark, it consistently improves upon reference-free methods while performing on par with reference-based method that requires an extra reference model.\n",
            "reference_string": "[268889777 | Zhang et al. | 2024 | Citations: 54]"
        },
        {
            "title": "From Text to Source: Results in Detecting Large Language Model-Generated Content",
            "venue": "International Conference on Language Resources and Evaluation",
            "year": 2023,
            "reference_count": 54,
            "citation_count": 13,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.13322",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.13322, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51040671",
                    "name": "Wissam Antoun"
                },
                {
                    "authorId": "68990982",
                    "name": "Beno\u00eet Sagot"
                },
                {
                    "authorId": "1679170",
                    "name": "Djam\u00e9 Seddah"
                }
            ],
            "abstract": "The widespread use of Large Language Models (LLMs), celebrated for their ability to generate human-like text, has raised concerns about misinformation and ethical implications. Addressing these concerns necessitates the development of robust methods to detect and attribute text generated by LLMs. This paper investigates \u201cCross-Model Detection,\u201d by evaluating whether a classifier trained to distinguish between source LLM-generated and human-written text can also detect text from a target LLM without further training. The study comprehensively explores various LLM sizes and families and assesses the impact of conversational fine-tuning techniques, quantization, and watermarking on classifier generalization. The research also explores Model Attribution, encompassing source model identification, model family, and model size classification, in addition to quantization and watermarking detection. Our results reveal several key findings: a clear inverse relationship between classifier effectiveness and model size, with larger LLMs being more challenging to detect, especially when the classifier is trained on data from smaller models. Training on data from similarly sized LLMs can improve detection performance from larger models but may lead to decreased performance when dealing with smaller models. Additionally, model attribution experiments show promising results in identifying source models and model families, highlighting detectable signatures in LLM-generated text, with particularly remarkable outcomes in watermarking detection, while no detectable signatures of quantization were observed. Overall, our study contributes valuable insights into the interplay of model size, family, and training data in LLM detection and attribution.",
            "corpus_id": 262465111,
            "sentences": [
                {
                    "corpus_id": "262465111",
                    "title": "From Text to Source: Results in Detecting Large Language Model-Generated Content",
                    "text": "Additionally, we explore the impact of conversational fine-tuning techniques, including Chat, Instruct (Mishra et al., 2022;Wei et al., 2022), and Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017;Ziegler et al., 2020), the impact of model quantization and watermarking of the generated text on the general-ization and transferability of the classifier across this wide array of models. \n\nThis comprehensive investigation enables us to gain a deeper understanding of the generalization and transferability of the classifier across a diverse array of models, thus eliminating a potential source of bias in our results. It also allows us to identify how the factors mentioned before (e.g model size, family, conversational finetuning, quantization and watermarking) impact the detection and attribution of generated text. \n\nOur contributions in this study can be summarized as follows: \n\n\u2022 A comprehensive investigation into crossmodel detection, evaluating the classifier's ability to detect text generated by different LLMs, and in model attribution, encompassing a broad range of sizes and model families. \n\n\u2022 We highlight the role of both model size and family in the detection of text generated by Language Model Models (LLMs). We observed an inverse relationship between classifier effectiveness and LLM size. Detecting larger models can be challenging, but training on similarly sized LLMs can improve performance. Additionally, our study provides valuable insights into the impact of conversational finetuning, quantization, and watermarking on detection, indicating the robustness of LLM detection in the face of these techniques. \n\n\u2022 Our experiments in model attribution reveal the potential for identifying the source model of generated text. While human-generated text is distinguishable, confusion primarily occurs between models from the same family or with adjacent sizes. This suggests that LLMs leave distinct signatures, enabling source model identification and model family classification, further enhancing our understanding of how different LLMs generate text.",
                    "score": 0.44101224527111293,
                    "section_title": "Introduction",
                    "char_start_offset": 1795,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 411
                        },
                        {
                            "start": 414,
                            "end": 642
                        },
                        {
                            "start": 643,
                            "end": 844
                        },
                        {
                            "start": 847,
                            "end": 908
                        },
                        {
                            "start": 911,
                            "end": 1131
                        },
                        {
                            "start": 1134,
                            "end": 1255
                        },
                        {
                            "start": 1256,
                            "end": 1338
                        },
                        {
                            "start": 1339,
                            "end": 1444
                        },
                        {
                            "start": 1445,
                            "end": 1662
                        },
                        {
                            "start": 1665,
                            "end": 1776
                        },
                        {
                            "start": 1777,
                            "end": 1910
                        },
                        {
                            "start": 1911,
                            "end": 2104
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 103,
                            "end": 124,
                            "matchedPaperCorpusId": "237421373"
                        },
                        {
                            "start": 124,
                            "end": 141,
                            "matchedPaperCorpusId": "237416585"
                        },
                        {
                            "start": 197,
                            "end": 222,
                            "matchedPaperCorpusId": "4787508"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.74267578125
                },
                {
                    "corpus_id": "262465111",
                    "title": "From Text to Source: Results in Detecting Large Language Model-Generated Content",
                    "text": "The lack of a discernible pattern in the cross-model detection performance across different model families may be attributed to the extensive overlap in their pretraining data, with a predominant reliance on ThePile (Gao et al., 2020) dataset or its subsets across most models, supplemented by Common Crawl as the primary data source. Consequently, the primary distinguishing factor among these models lies in their respective data cleaning pipelines. \n\nInfluence of Conversational Finetuning Our experiments reveal a clear pattern in the cross-model detection results, as shown in Figure 4. Specifically, a classifier trained on text generated by chat models exhibits limited capability in detecting normal language models (LMs). However, it demonstrates improved performance when tasked with detecting other chat models. Notably, when trained on LLaMA-2-70b-chat data, the classifier achieves the highest scores, albeit with a slight decline in detection accuracy when tested on chat models. This observation suggests that the LLaMA-2-70b-chat model effectively follows instructions to continue input text. Surprisingly, training the classifier on vanilla LM output also yields commendable results in detecting these distinct model categories. These findings underscore the nuanced relationship between chat models and traditional language models in the context of detection.",
                    "score": 0.48404378554336486,
                    "section_title": "Cross-Model Detection Results",
                    "char_start_offset": 14826,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 334
                        },
                        {
                            "start": 335,
                            "end": 451
                        },
                        {
                            "start": 454,
                            "end": 730
                        },
                        {
                            "start": 731,
                            "end": 822
                        },
                        {
                            "start": 823,
                            "end": 993
                        },
                        {
                            "start": 994,
                            "end": 1108
                        },
                        {
                            "start": 1109,
                            "end": 1245
                        },
                        {
                            "start": 1246,
                            "end": 1377
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.54541015625
                }
            ],
            "relevance_judgement": 0.74267578125,
            "relevance_judgment_input_expanded": "# Title: From Text to Source: Results in Detecting Large Language Model-Generated Content\n# Venue: International Conference on Language Resources and Evaluation\n# Authors: Wissam Antoun, Beno\u00eet Sagot, Djam\u00e9 Seddah\n## Abstract\nThe widespread use of Large Language Models (LLMs), celebrated for their ability to generate human-like text, has raised concerns about misinformation and ethical implications. Addressing these concerns necessitates the development of robust methods to detect and attribute text generated by LLMs. This paper investigates \u201cCross-Model Detection,\u201d by evaluating whether a classifier trained to distinguish between source LLM-generated and human-written text can also detect text from a target LLM without further training. The study comprehensively explores various LLM sizes and families and assesses the impact of conversational fine-tuning techniques, quantization, and watermarking on classifier generalization. The research also explores Model Attribution, encompassing source model identification, model family, and model size classification, in addition to quantization and watermarking detection. Our results reveal several key findings: a clear inverse relationship between classifier effectiveness and model size, with larger LLMs being more challenging to detect, especially when the classifier is trained on data from smaller models. Training on data from similarly sized LLMs can improve detection performance from larger models but may lead to decreased performance when dealing with smaller models. Additionally, model attribution experiments show promising results in identifying source models and model families, highlighting detectable signatures in LLM-generated text, with particularly remarkable outcomes in watermarking detection, while no detectable signatures of quantization were observed. Overall, our study contributes valuable insights into the interplay of model size, family, and training data in LLM detection and attribution.\n## Introduction\nAdditionally, we explore the impact of conversational fine-tuning techniques, including Chat, Instruct (Mishra et al., 2022;Wei et al., 2022), and Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017;Ziegler et al., 2020), the impact of model quantization and watermarking of the generated text on the general-ization and transferability of the classifier across this wide array of models. \n\nThis comprehensive investigation enables us to gain a deeper understanding of the generalization and transferability of the classifier across a diverse array of models, thus eliminating a potential source of bias in our results. It also allows us to identify how the factors mentioned before (e.g model size, family, conversational finetuning, quantization and watermarking) impact the detection and attribution of generated text. \n\nOur contributions in this study can be summarized as follows: \n\n\u2022 A comprehensive investigation into crossmodel detection, evaluating the classifier's ability to detect text generated by different LLMs, and in model attribution, encompassing a broad range of sizes and model families. \n\n\u2022 We highlight the role of both model size and family in the detection of text generated by Language Model Models (LLMs). We observed an inverse relationship between classifier effectiveness and LLM size. Detecting larger models can be challenging, but training on similarly sized LLMs can improve performance. Additionally, our study provides valuable insights into the impact of conversational finetuning, quantization, and watermarking on detection, indicating the robustness of LLM detection in the face of these techniques. \n\n\u2022 Our experiments in model attribution reveal the potential for identifying the source model of generated text. While human-generated text is distinguishable, confusion primarily occurs between models from the same family or with adjacent sizes. This suggests that LLMs leave distinct signatures, enabling source model identification and model family classification, further enhancing our understanding of how different LLMs generate text.\n\n## Cross-Model Detection Results\nThe lack of a discernible pattern in the cross-model detection performance across different model families may be attributed to the extensive overlap in their pretraining data, with a predominant reliance on ThePile (Gao et al., 2020) dataset or its subsets across most models, supplemented by Common Crawl as the primary data source. Consequently, the primary distinguishing factor among these models lies in their respective data cleaning pipelines. \n\nInfluence of Conversational Finetuning Our experiments reveal a clear pattern in the cross-model detection results, as shown in Figure 4. Specifically, a classifier trained on text generated by chat models exhibits limited capability in detecting normal language models (LMs). However, it demonstrates improved performance when tasked with detecting other chat models. Notably, when trained on LLaMA-2-70b-chat data, the classifier achieves the highest scores, albeit with a slight decline in detection accuracy when tested on chat models. This observation suggests that the LLaMA-2-70b-chat model effectively follows instructions to continue input text. Surprisingly, training the classifier on vanilla LM output also yields commendable results in detecting these distinct model categories. These findings underscore the nuanced relationship between chat models and traditional language models in the context of detection.",
            "reference_string": "[262465111 | Antoun et al. | 2023 | Citations: 13]"
        },
        {
            "title": "Probing Language Models for Pre-training Data Detection",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 13,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.01333, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294376388",
                    "name": "Zhenhua Liu"
                },
                {
                    "authorId": "1914586128",
                    "name": "Tong Zhu"
                },
                {
                    "authorId": "2186374155",
                    "name": "Chuanyuan Tan"
                },
                {
                    "authorId": "2304460083",
                    "name": "Haonan Lu"
                },
                {
                    "authorId": "2330946427",
                    "name": "Bing Liu"
                },
                {
                    "authorId": "2265943980",
                    "name": "Wenliang Chen"
                }
            ],
            "abstract": "Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the model's internal activations. Our method is simple and effective and leads to more trustworthy pre-training data detection. Additionally, we propose ArxivMIA, a new challenging benchmark comprising arxiv abstracts from Computer Science and Mathematics categories. Our experiments demonstrate that our method outperforms all baselines, and achieves state-of-the-art performance on both WikiMIA and ArxivMIA, with additional experiments confirming its efficacy (Our code and dataset are available at https://github.com/zhliu0106/probing-lm-data).",
            "corpus_id": 270217411,
            "sentences": [],
            "relevance_judgement": 0.72412109375,
            "relevance_judgment_input_expanded": "# Title: Probing Language Models for Pre-training Data Detection\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Zhenhua Liu, Tong Zhu, Chuanyuan Tan, Haonan Lu, Bing Liu, Wenliang Chen\n## Abstract\nLarge Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the model's internal activations. Our method is simple and effective and leads to more trustworthy pre-training data detection. Additionally, we propose ArxivMIA, a new challenging benchmark comprising arxiv abstracts from Computer Science and Mathematics categories. Our experiments demonstrate that our method outperforms all baselines, and achieves state-of-the-art performance on both WikiMIA and ArxivMIA, with additional experiments confirming its efficacy (Our code and dataset are available at https://github.com/zhliu0106/probing-lm-data).\n",
            "reference_string": "[270217411 | Liu et al. | 2024 | Citations: 13]"
        },
        {
            "title": "Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 78,
            "citation_count": 8,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.18966, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294828739",
                    "name": "Yujuan Fu"
                },
                {
                    "authorId": "2298238369",
                    "name": "\u00d6zlem Uzuner"
                },
                {
                    "authorId": "1398215463",
                    "name": "Meliha Yetisgen-Yildiz"
                },
                {
                    "authorId": "2294363357",
                    "name": "Fei Xia"
                }
            ],
            "abstract": "Large language models (LLMs) have demonstrated great performance across various benchmarks, showing potential as general-purpose task solvers. However, as LLMs are typically trained on vast amounts of data, a significant concern in their evaluation is data contamination, where overlap between training data and evaluation datasets inflates performance assessments. Multiple approaches have been developed to identify data contamination. These approaches rely on specific assumptions that may not hold universally across different settings. To bridge this gap, we systematically review 50 papers on data contamination detection, categorize the underlying assumptions, and assess whether they have been rigorously validated. We identify and analyze eight categories of assumptions and test three of them as case studies. Our case studies focus on detecting direct, instance-level data contamination, which is also referred to as Membership Inference Attacks (MIA). Our analysis reveals that MIA approaches based on these three assumptions can have similar performance to random guessing, on datasets used in LLM pretraining, suggesting that current LLMs might learn data distributions rather than memorizing individual instances. Meanwhile, MIA can easily fail when there are data distribution shifts between the seen and unseen instances.",
            "corpus_id": 273549214,
            "sentences": [
                {
                    "corpus_id": "273549214",
                    "title": "Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions",
                    "text": "Large language models (LLMs) have achieved remarkable performance across various benchmarks, signaling their potential to revolutionize numerous technical domains as general-purpose problem solvers (Achiam et al., 2023;Meta AI, 2024). However, a key concern in accurately evaluating those LLMs is the possibility of data contamination, where the LLM's training data overlaps with the evaluation dataset (Balloccu et al., 2024). Evaluating LLMs on contaminated benchmarks leads to inflated performance assessments (Balloccu et al., 2024;Sainz et al., 2023a;Li and Flanigan, 2024), and creates a misleading perception of their capabilities. Therefore, multiple detection approaches have been developed to identify data contamination in LLMs, and these approaches can also be deployed to identify the use of copyrighted or sensitive content in LLM training (Xu et al., 2024;Meeus et al., 2024b). \n\nAll existing approaches for detecting data contamination in language models (LMs) rely on specific assumptions regarding the LMs and datasets, which may not be universally applicable across different settings2 . While previous surveys have focused on detection and mitigation techniques, to our best knowledge, there is currently no comprehensive analysis that surveys and validates the assumptions underlying these approaches (Xu et al., 2024;Ishihara, 2023;Hu et al., 2022). \n\nTo bridge this gap, we (1) systematically review 50 papers on data contamination detection for LMs, (2) present the formal, mathematical definitions for different levels of data contamination, (3) categorize the underlying requirements and assumptions associated with each approach and critically assess whether these assumptions have been rigorously validated, and (4) demonstrate through case studies that some unverified assumptions can be wrong in multiple scenarios.",
                    "score": 0.4439982908604859,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 234
                        },
                        {
                            "start": 235,
                            "end": 427
                        },
                        {
                            "start": 428,
                            "end": 638
                        },
                        {
                            "start": 639,
                            "end": 892
                        },
                        {
                            "start": 895,
                            "end": 1106
                        },
                        {
                            "start": 1107,
                            "end": 1371
                        },
                        {
                            "start": 1374,
                            "end": 1845
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 403,
                            "end": 426,
                            "matchedPaperCorpusId": "267499939"
                        },
                        {
                            "start": 513,
                            "end": 536,
                            "matchedPaperCorpusId": "267499939"
                        },
                        {
                            "start": 536,
                            "end": 556,
                            "matchedPaperCorpusId": "264555419"
                        },
                        {
                            "start": 1339,
                            "end": 1354,
                            "matchedPaperCorpusId": "258888114"
                        },
                        {
                            "start": 1354,
                            "end": 1370,
                            "matchedPaperCorpusId": "232233426"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.62744140625
                }
            ],
            "relevance_judgement": 0.62744140625,
            "relevance_judgment_input_expanded": "# Title: Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Yujuan Fu, \u00d6zlem Uzuner, Meliha Yetisgen-Yildiz, Fei Xia\n## Abstract\nLarge language models (LLMs) have demonstrated great performance across various benchmarks, showing potential as general-purpose task solvers. However, as LLMs are typically trained on vast amounts of data, a significant concern in their evaluation is data contamination, where overlap between training data and evaluation datasets inflates performance assessments. Multiple approaches have been developed to identify data contamination. These approaches rely on specific assumptions that may not hold universally across different settings. To bridge this gap, we systematically review 50 papers on data contamination detection, categorize the underlying assumptions, and assess whether they have been rigorously validated. We identify and analyze eight categories of assumptions and test three of them as case studies. Our case studies focus on detecting direct, instance-level data contamination, which is also referred to as Membership Inference Attacks (MIA). Our analysis reveals that MIA approaches based on these three assumptions can have similar performance to random guessing, on datasets used in LLM pretraining, suggesting that current LLMs might learn data distributions rather than memorizing individual instances. Meanwhile, MIA can easily fail when there are data distribution shifts between the seen and unseen instances.\n## Introduction\nLarge language models (LLMs) have achieved remarkable performance across various benchmarks, signaling their potential to revolutionize numerous technical domains as general-purpose problem solvers (Achiam et al., 2023;Meta AI, 2024). However, a key concern in accurately evaluating those LLMs is the possibility of data contamination, where the LLM's training data overlaps with the evaluation dataset (Balloccu et al., 2024). Evaluating LLMs on contaminated benchmarks leads to inflated performance assessments (Balloccu et al., 2024;Sainz et al., 2023a;Li and Flanigan, 2024), and creates a misleading perception of their capabilities. Therefore, multiple detection approaches have been developed to identify data contamination in LLMs, and these approaches can also be deployed to identify the use of copyrighted or sensitive content in LLM training (Xu et al., 2024;Meeus et al., 2024b). \n\nAll existing approaches for detecting data contamination in language models (LMs) rely on specific assumptions regarding the LMs and datasets, which may not be universally applicable across different settings2 . While previous surveys have focused on detection and mitigation techniques, to our best knowledge, there is currently no comprehensive analysis that surveys and validates the assumptions underlying these approaches (Xu et al., 2024;Ishihara, 2023;Hu et al., 2022). \n\nTo bridge this gap, we (1) systematically review 50 papers on data contamination detection for LMs, (2) present the formal, mathematical definitions for different levels of data contamination, (3) categorize the underlying requirements and assumptions associated with each approach and critically assess whether these assumptions have been rigorously validated, and (4) demonstrate through case studies that some unverified assumptions can be wrong in multiple scenarios.",
            "reference_string": "[273549214 | Fu et al. | 2024 | Citations: 8]"
        },
        {
            "title": "Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2025,
            "reference_count": 47,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.12072, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3023068",
                    "name": "Abhilasha Ravichander"
                },
                {
                    "authorId": "33772445",
                    "name": "Jillian R. Fisher"
                },
                {
                    "authorId": "122436831",
                    "name": "Taylor Sorensen"
                },
                {
                    "authorId": "50085131",
                    "name": "Ximing Lu"
                },
                {
                    "authorId": "2284130273",
                    "name": "Yuchen Lin"
                },
                {
                    "authorId": "2266838583",
                    "name": "Maria Antoniak"
                },
                {
                    "authorId": "2254272878",
                    "name": "Niloofar Mireshghallah"
                },
                {
                    "authorId": "1857797",
                    "name": "Chandra Bhagavatula"
                },
                {
                    "authorId": "2266363632",
                    "name": "Yejin Choi"
                }
            ],
            "abstract": "High-quality training data has proven crucial for developing performant large language models (LLMs). However, commercial LLM providers disclose few, if any, details about the data used for training. This lack of transparency creates multiple challenges: it limits external oversight and inspection of LLMs for issues such as copyright infringement, it undermines the agency of data authors, and it hinders scientific research on critical issues such as data contamination and data selection. How can we recover what training data is known to LLMs? In this work, we demonstrate a new method to identify training data known to proprietary LLMs like GPT-4 without requiring any access to model weights or token probabilities, by using information-guided probes. Our work builds on a key observation: text passages with high surprisal are good search material for memorization probes. By evaluating a model's ability to successfully reconstruct high-surprisal tokens in text, we can identify a surprising number of texts memorized by LLMs.",
            "corpus_id": 277065772,
            "sentences": [
                {
                    "corpus_id": "277065772",
                    "title": "Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models",
                    "text": "6 Discussion and Future Work \n\nOur goal is to provide a foundation for greater data transparency in the ecosystem surrounding large language models. We briefly discuss our findings, The need for multiple methods to surface training data. In our work, we find that we are able to identify text that is known to even proprietary black-box LLMs, and that the examples of memorized text that were successfully identified can differ between probing methods. This indicates that the community would benefit from a range of such approaches, and that focusing on state-of-the-art detection performance should not be the only goal. Further, recent work has investigating combining signals for various training data identification methods in order to determine if a model was trained on a given document (Maini et al., 2024). This suggests that developing diverse, complementary, probes can help us better understand how data was used to train models. \n\nAnswering questions about model generalization. While LLMs have achieved state-of-the-art performance on several benchmarks, the extent of their generalization capabilities remains an open question. This is in part due to an inability to characterize train-test overlap: that is, making sense of what data a model was trained on and how it Data transparency in the LLM ecosystem. Our work is in spirit related to studies that have called for better data documentation in machine learning (Liu et al., 2024b;Bender and Friedman, 2018;Gebru et al., 2021) or offered large-scale search and indexing mechanisms for pre-training corpora (Elazar et al., 2024;Liu et al., 2024a). While previous efforts focus on supervised, accessible datasets, this work focuses on inferring training data of models, especially in cases where there is currently no data transparency. \n\nWhy not only work with open models? In this work, our central focus is closed commercial models that do not offer access to token probabilities. The reason for this choice is that this is the dominant paradigm for many popular models today.",
                    "score": 0.45064528879703847,
                    "section_title": "Analysis",
                    "char_start_offset": 23624,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 28
                        },
                        {
                            "start": 31,
                            "end": 148
                        },
                        {
                            "start": 149,
                            "end": 237
                        },
                        {
                            "start": 238,
                            "end": 452
                        },
                        {
                            "start": 453,
                            "end": 622
                        },
                        {
                            "start": 623,
                            "end": 815
                        },
                        {
                            "start": 816,
                            "end": 941
                        },
                        {
                            "start": 944,
                            "end": 991
                        },
                        {
                            "start": 992,
                            "end": 1142
                        },
                        {
                            "start": 1143,
                            "end": 1323
                        },
                        {
                            "start": 1324,
                            "end": 1616
                        },
                        {
                            "start": 1617,
                            "end": 1804
                        },
                        {
                            "start": 1807,
                            "end": 1842
                        },
                        {
                            "start": 1843,
                            "end": 1951
                        },
                        {
                            "start": 1952,
                            "end": 2047
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 794,
                            "end": 814,
                            "matchedPaperCorpusId": "268781196"
                        },
                        {
                            "start": 1432,
                            "end": 1451,
                            "matchedPaperCorpusId": "269741151"
                        },
                        {
                            "start": 1451,
                            "end": 1477,
                            "matchedPaperCorpusId": "52255687"
                        },
                        {
                            "start": 1477,
                            "end": 1496,
                            "matchedPaperCorpusId": "4421027"
                        },
                        {
                            "start": 1576,
                            "end": 1597,
                            "matchedPaperCorpusId": "264803575"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.62548828125
                }
            ],
            "relevance_judgement": 0.62548828125,
            "relevance_judgment_input_expanded": "# Title: Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Abhilasha Ravichander, Jillian R. Fisher, Taylor Sorensen, Ximing Lu, Yuchen Lin, Maria Antoniak, Niloofar Mireshghallah, Chandra Bhagavatula, Yejin Choi\n## Abstract\nHigh-quality training data has proven crucial for developing performant large language models (LLMs). However, commercial LLM providers disclose few, if any, details about the data used for training. This lack of transparency creates multiple challenges: it limits external oversight and inspection of LLMs for issues such as copyright infringement, it undermines the agency of data authors, and it hinders scientific research on critical issues such as data contamination and data selection. How can we recover what training data is known to LLMs? In this work, we demonstrate a new method to identify training data known to proprietary LLMs like GPT-4 without requiring any access to model weights or token probabilities, by using information-guided probes. Our work builds on a key observation: text passages with high surprisal are good search material for memorization probes. By evaluating a model's ability to successfully reconstruct high-surprisal tokens in text, we can identify a surprising number of texts memorized by LLMs.\n## Analysis\n6 Discussion and Future Work \n\nOur goal is to provide a foundation for greater data transparency in the ecosystem surrounding large language models. We briefly discuss our findings, The need for multiple methods to surface training data. In our work, we find that we are able to identify text that is known to even proprietary black-box LLMs, and that the examples of memorized text that were successfully identified can differ between probing methods. This indicates that the community would benefit from a range of such approaches, and that focusing on state-of-the-art detection performance should not be the only goal. Further, recent work has investigating combining signals for various training data identification methods in order to determine if a model was trained on a given document (Maini et al., 2024). This suggests that developing diverse, complementary, probes can help us better understand how data was used to train models. \n\nAnswering questions about model generalization. While LLMs have achieved state-of-the-art performance on several benchmarks, the extent of their generalization capabilities remains an open question. This is in part due to an inability to characterize train-test overlap: that is, making sense of what data a model was trained on and how it Data transparency in the LLM ecosystem. Our work is in spirit related to studies that have called for better data documentation in machine learning (Liu et al., 2024b;Bender and Friedman, 2018;Gebru et al., 2021) or offered large-scale search and indexing mechanisms for pre-training corpora (Elazar et al., 2024;Liu et al., 2024a). While previous efforts focus on supervised, accessible datasets, this work focuses on inferring training data of models, especially in cases where there is currently no data transparency. \n\nWhy not only work with open models? In this work, our central focus is closed commercial models that do not offer access to token probabilities. The reason for this choice is that this is the dominant paradigm for many popular models today.",
            "reference_string": "[277065772 | Ravichander et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Large Language Models for Anomaly and Out-of-Distribution Detection: A Survey",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 114,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.01980, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319451284",
                    "name": "Ruiyao Xu"
                },
                {
                    "authorId": "2319332607",
                    "name": "Kaize Ding"
                }
            ],
            "abstract": "Detecting anomalies or out-of-distribution (OOD) samples is critical for maintaining the reliability and trustworthiness of machine learning systems. Recently, Large Language Models (LLMs) have demonstrated their effectiveness not only in natural language processing but also in broader applications due to their advanced comprehension and generative capabilities. The integration of LLMs into anomaly and OOD detection marks a significant shift from the traditional paradigm in the field. This survey focuses on the problem of anomaly and OOD detection under the context of LLMs. We propose a new taxonomy to categorize existing approaches into two classes based on the role played by LLMs. Following our proposed taxonomy, we further discuss the related work under each of the categories and finally discuss potential challenges and directions for future research in this field. We also provide an up-to-date reading list of relevant papers.",
            "corpus_id": 272366479,
            "sentences": [
                {
                    "corpus_id": "272366479",
                    "title": "Large Language Models for Anomaly and Out-of-Distribution Detection: A Survey",
                    "text": "Large Language Models (LLMs) and Multi-Modal LLMs (MLLMs) have transformed Out-of-Distribution (OOD) detection by leveraging pretrained models like CLIP to perform downstream detection tasks. These models are capable of detecting OOD samples in zero-shot or few-shot settings, meaning they can generalize to unseen data with little to no additional training. This represents a shift from traditional OOD detection methods, which typically rely on training classifiers using the entire in-distribution (ID) dataset. \n\nIncorporating the internal knowledge of pretrained MLLMs, the field is progressing towards even greater computational efficiency, where minimal or no training data is needed. This ability to operate with limited data while maintaining performance makes LLM-based OOD detection especially appealing for real-world applications.",
                    "score": 0.45531829082067704,
                    "section_title": "D.2 LLM-based OOD Detection",
                    "char_start_offset": 41836,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 191
                        },
                        {
                            "start": 192,
                            "end": 358
                        },
                        {
                            "start": 359,
                            "end": 514
                        },
                        {
                            "start": 517,
                            "end": 691
                        },
                        {
                            "start": 692,
                            "end": 843
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.62109375
                }
            ],
            "relevance_judgement": 0.62109375,
            "relevance_judgment_input_expanded": "# Title: Large Language Models for Anomaly and Out-of-Distribution Detection: A Survey\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Ruiyao Xu, Kaize Ding\n## Abstract\nDetecting anomalies or out-of-distribution (OOD) samples is critical for maintaining the reliability and trustworthiness of machine learning systems. Recently, Large Language Models (LLMs) have demonstrated their effectiveness not only in natural language processing but also in broader applications due to their advanced comprehension and generative capabilities. The integration of LLMs into anomaly and OOD detection marks a significant shift from the traditional paradigm in the field. This survey focuses on the problem of anomaly and OOD detection under the context of LLMs. We propose a new taxonomy to categorize existing approaches into two classes based on the role played by LLMs. Following our proposed taxonomy, we further discuss the related work under each of the categories and finally discuss potential challenges and directions for future research in this field. We also provide an up-to-date reading list of relevant papers.\n## D.2 LLM-based OOD Detection\nLarge Language Models (LLMs) and Multi-Modal LLMs (MLLMs) have transformed Out-of-Distribution (OOD) detection by leveraging pretrained models like CLIP to perform downstream detection tasks. These models are capable of detecting OOD samples in zero-shot or few-shot settings, meaning they can generalize to unseen data with little to no additional training. This represents a shift from traditional OOD detection methods, which typically rely on training classifiers using the entire in-distribution (ID) dataset. \n\nIncorporating the internal knowledge of pretrained MLLMs, the field is progressing towards even greater computational efficiency, where minimal or no training data is needed. This ability to operate with limited data while maintaining performance makes LLM-based OOD detection especially appealing for real-world applications.",
            "reference_string": "[272366479 | Xu et al. | 2024 | Citations: 7]"
        },
        {
            "title": "Two Directions for Clinical Data Generation with Large Language Models: Data-to-Label and Label-to-Data",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 91,
            "citation_count": 26,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.findings-emnlp.474.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.06774, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2237106751",
                    "name": "Rumeng Li"
                },
                {
                    "authorId": "2221230794",
                    "name": "Xun Wang"
                },
                {
                    "authorId": "2273658683",
                    "name": "Hong Yu"
                }
            ],
            "abstract": "Large language models (LLMs) can generate natural language texts for various domains and tasks, but their potential for clinical text mining, a domain with scarce, sensitive, and imbalanced medical data, is under-explored. We investigate whether LLMs can augment clinical data for detecting Alzheimer's Disease (AD)-related signs and symptoms from electronic health records (EHRs), a challenging task that requires high expertise. We create a novel pragmatic taxonomy for AD sign and symptom progression based on expert knowledge and generated three datasets: (1) a gold dataset annotated by human experts on longitudinal EHRs of AD patients; (2) a silver dataset created by the data-to-label method, which labels sentences from a public EHR collection with AD-related signs and symptoms; and (3) a bronze dataset created by the label-to-data method which generates sentences with AD-related signs and symptoms based on the label definition. We train a system to detect AD-related signs and symptoms from EHRs. We find that the silver and bronze datasets improves the system performance, outperforming the system using only the gold dataset. This shows that LLMs can generate synthetic clinical data for a complex task by incorporating expert knowledge, and our label-to-data method can produce datasets that are free of sensitive information, while maintaining acceptable quality.",
            "corpus_id": 266176903,
            "sentences": [],
            "relevance_judgement": 0.61474609375,
            "relevance_judgment_input_expanded": "# Title: Two Directions for Clinical Data Generation with Large Language Models: Data-to-Label and Label-to-Data\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Rumeng Li, Xun Wang, Hong Yu\n## Abstract\nLarge language models (LLMs) can generate natural language texts for various domains and tasks, but their potential for clinical text mining, a domain with scarce, sensitive, and imbalanced medical data, is under-explored. We investigate whether LLMs can augment clinical data for detecting Alzheimer's Disease (AD)-related signs and symptoms from electronic health records (EHRs), a challenging task that requires high expertise. We create a novel pragmatic taxonomy for AD sign and symptom progression based on expert knowledge and generated three datasets: (1) a gold dataset annotated by human experts on longitudinal EHRs of AD patients; (2) a silver dataset created by the data-to-label method, which labels sentences from a public EHR collection with AD-related signs and symptoms; and (3) a bronze dataset created by the label-to-data method which generates sentences with AD-related signs and symptoms based on the label definition. We train a system to detect AD-related signs and symptoms from EHRs. We find that the silver and bronze datasets improves the system performance, outperforming the system using only the gold dataset. This shows that LLMs can generate synthetic clinical data for a complex task by incorporating expert knowledge, and our label-to-data method can produce datasets that are free of sensitive information, while maintaining acceptable quality.\n",
            "reference_string": "[266176903 | Li et al. | 2023 | Citations: 26]"
        },
        {
            "title": "Detecting Pretraining Data from Large Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 64,
            "citation_count": 201,
            "influential_citation_count": 51,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.16789, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2254168373",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "2218438150",
                    "name": "Anirudh Ajith"
                },
                {
                    "authorId": "67284811",
                    "name": "Mengzhou Xia"
                },
                {
                    "authorId": "108053318",
                    "name": "Yangsibo Huang"
                },
                {
                    "authorId": "2261780806",
                    "name": "Daogao Liu"
                },
                {
                    "authorId": "3443287",
                    "name": "Terra Blevins"
                },
                {
                    "authorId": "50536468",
                    "name": "Danqi Chen"
                },
                {
                    "authorId": "2137813791",
                    "name": "Luke S. Zettlemoyer"
                }
            ],
            "abstract": "Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. Min-K% Prob can be applied without any knowledge about the pretraining corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous methods. We apply Min-K% Prob to two real-world scenarios, copyrighted book detection, and contaminated downstream example detection, and find it a consistently effective solution.",
            "corpus_id": 264451585,
            "sentences": [
                {
                    "corpus_id": "264451585",
                    "title": "Detecting Pretraining Data from Large Language Models",
                    "text": "Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. Min-K% Prob can be applied without any knowledge about the pretraining corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous methods. We apply Min-K% Prob to two real-world scenarios, copyrighted book detection, and contaminated downstream example detection, and find it a consistently effective solution.",
                    "score": 0.45667219544946996,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6083984375
                },
                {
                    "corpus_id": "264451585",
                    "title": "Detecting Pretraining Data from Large Language Models",
                    "text": "We introduce a pretraining data detection method MIN-K% PROB that leverages minimum token probabilities of a text for detection. MIN-K% PROB is based on the hypothesis that a non-member example is more likely to include a few outlier words with high negative log-likelihood (or low probability), while a member example is less likely to include words with high negative log-likelihood. \n\nConsider a sequence of tokens in a sentence, denoted as x = x 1 , x 2 , ..., x N , the log-likelihood of a token, x i , given its preceding tokens is calculated as log p(x i |x 1 , ..., x i\u22121 ). We then select the k% of tokens from x with the minimum token probability to form a set, Min-K%(x), and compute the average log-likelihood of the tokens in this set: \n\nwhere E is the size of the Min-K%(x) set. We can detect if a piece of text was included in pretraining data simply by thresholding this MIN-K% PROB result. We summarize our method in Algorithm 1 in Appendix B.",
                    "score": 0.4757768092357937,
                    "section_title": "MIN-K% PROB: A SIMPLE REFERENCE-FREE PRETRAINING DATA DETECTION METHOD",
                    "char_start_offset": 12383,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 128
                        },
                        {
                            "start": 129,
                            "end": 385
                        },
                        {
                            "start": 388,
                            "end": 582
                        },
                        {
                            "start": 583,
                            "end": 748
                        },
                        {
                            "start": 751,
                            "end": 792
                        },
                        {
                            "start": 793,
                            "end": 906
                        },
                        {
                            "start": 907,
                            "end": 960
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.53173828125
                },
                {
                    "corpus_id": "264451585",
                    "title": "Detecting Pretraining Data from Large Language Models",
                    "text": "To determine whether a text X is in the pretraining data of a LLM such as GPT, MIN-K% PROB first gets the probability for each token in X, selects the k% tokens with minimum probabilities and calculates their average log likelihood. If the average log likelihood is high, the text is likely in the pretraining data. \n\nreducing the potential memorization required for successful MIAs (Leino & Fredrikson, 2020;Kandpal et al., 2022). Besides, previous methods often rely on one or more reference models (Carlini et al., 2022;Watson et al., 2022) trained in the same manner as the target model (e.g., on the shadow data sampled from the same underlying pretraining data distribution) to achieve precise detection. This is not possible for large language models, as the training distribution is usually not available and training would be too expensive. \n\nOur first step towards addressing these challenges is to establish a reliable benchmark. We introduce WIKIMIA, a dynamic benchmark designed to periodically and automatically evaluate detection methods on any newly released pretrained LLMs. By leveraging the Wikipedia data timestamp and the model release date, we select old Wikipedia event data as our member data (i.e, seen data during pretraining) and recent Wikipedia event data (e.g., after 2023) as our non-member data (unseen). Our datasets thus exhibit three desirable properties: (1) Accurate: events that occur after LLM pretraining are guaranteed not to be present in the pretraining data. The temporal nature of events ensures that non-member data is indeed unseen and not mentioned in the pretraining data. (2) General: our benchmark is not confined to any specific model and can be applied to various models pretrained using Wikipedia (e.g., OPT, LLaMA, GPT-Neo) since Wikipedia is a commonly used pretraining data source. (3) Dynamic: we will continually update our benchmark by gathering newer non-member data (i.e., more recent events) from Wikipedia since our data construction pipeline is fully automated.",
                    "score": 0.5318146720691732,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 1791,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 232
                        },
                        {
                            "start": 233,
                            "end": 315
                        },
                        {
                            "start": 318,
                            "end": 431
                        },
                        {
                            "start": 432,
                            "end": 710
                        },
                        {
                            "start": 711,
                            "end": 849
                        },
                        {
                            "start": 852,
                            "end": 940
                        },
                        {
                            "start": 941,
                            "end": 1091
                        },
                        {
                            "start": 1092,
                            "end": 1222
                        },
                        {
                            "start": 1223,
                            "end": 1336
                        },
                        {
                            "start": 1337,
                            "end": 1502
                        },
                        {
                            "start": 1503,
                            "end": 1621
                        },
                        {
                            "start": 1622,
                            "end": 1838
                        },
                        {
                            "start": 1839,
                            "end": 2026
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 383,
                            "end": 409,
                            "matchedPaperCorpusId": "195699554"
                        },
                        {
                            "start": 409,
                            "end": 430,
                            "matchedPaperCorpusId": "246823128"
                        },
                        {
                            "start": 501,
                            "end": 523,
                            "matchedPaperCorpusId": "244920593"
                        },
                        {
                            "start": 523,
                            "end": 543,
                            "matchedPaperCorpusId": "244130249"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.444091796875
                }
            ],
            "relevance_judgement": 0.6083984375,
            "relevance_judgment_input_expanded": "# Title: Detecting Pretraining Data from Large Language Models\n# Venue: International Conference on Learning Representations\n# Authors: Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, Luke S. Zettlemoyer\n## Abstract\nAlthough large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. Min-K% Prob can be applied without any knowledge about the pretraining corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous methods. We apply Min-K% Prob to two real-world scenarios, copyrighted book detection, and contaminated downstream example detection, and find it a consistently effective solution.\n## INTRODUCTION\nTo determine whether a text X is in the pretraining data of a LLM such as GPT, MIN-K% PROB first gets the probability for each token in X, selects the k% tokens with minimum probabilities and calculates their average log likelihood. If the average log likelihood is high, the text is likely in the pretraining data. \n\nreducing the potential memorization required for successful MIAs (Leino & Fredrikson, 2020;Kandpal et al., 2022). Besides, previous methods often rely on one or more reference models (Carlini et al., 2022;Watson et al., 2022) trained in the same manner as the target model (e.g., on the shadow data sampled from the same underlying pretraining data distribution) to achieve precise detection. This is not possible for large language models, as the training distribution is usually not available and training would be too expensive. \n\nOur first step towards addressing these challenges is to establish a reliable benchmark. We introduce WIKIMIA, a dynamic benchmark designed to periodically and automatically evaluate detection methods on any newly released pretrained LLMs. By leveraging the Wikipedia data timestamp and the model release date, we select old Wikipedia event data as our member data (i.e, seen data during pretraining) and recent Wikipedia event data (e.g., after 2023) as our non-member data (unseen). Our datasets thus exhibit three desirable properties: (1) Accurate: events that occur after LLM pretraining are guaranteed not to be present in the pretraining data. The temporal nature of events ensures that non-member data is indeed unseen and not mentioned in the pretraining data. (2) General: our benchmark is not confined to any specific model and can be applied to various models pretrained using Wikipedia (e.g., OPT, LLaMA, GPT-Neo) since Wikipedia is a commonly used pretraining data source. (3) Dynamic: we will continually update our benchmark by gathering newer non-member data (i.e., more recent events) from Wikipedia since our data construction pipeline is fully automated.\n\n## MIN-K% PROB: A SIMPLE REFERENCE-FREE PRETRAINING DATA DETECTION METHOD\nWe introduce a pretraining data detection method MIN-K% PROB that leverages minimum token probabilities of a text for detection. MIN-K% PROB is based on the hypothesis that a non-member example is more likely to include a few outlier words with high negative log-likelihood (or low probability), while a member example is less likely to include words with high negative log-likelihood. \n\nConsider a sequence of tokens in a sentence, denoted as x = x 1 , x 2 , ..., x N , the log-likelihood of a token, x i , given its preceding tokens is calculated as log p(x i |x 1 , ..., x i\u22121 ). We then select the k% of tokens from x with the minimum token probability to form a set, Min-K%(x), and compute the average log-likelihood of the tokens in this set: \n\nwhere E is the size of the Min-K%(x) set. We can detect if a piece of text was included in pretraining data simply by thresholding this MIN-K% PROB result. We summarize our method in Algorithm 1 in Appendix B.",
            "reference_string": "[264451585 | Shi et al. | 2023 | Citations: 201]"
        },
        {
            "title": "Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 16,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.14781, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2322456143",
                    "name": "Weichao Zhang"
                },
                {
                    "authorId": "2109960367",
                    "name": "Ruqing Zhang"
                },
                {
                    "authorId": "70414094",
                    "name": "Jiafeng Guo"
                },
                {
                    "authorId": "2265490493",
                    "name": "M. D. Rijke"
                },
                {
                    "authorId": "7888704",
                    "name": "Yixing Fan"
                },
                {
                    "authorId": "2244825947",
                    "name": "Xueqi Cheng"
                }
            ],
            "abstract": "As the scale of training corpora for large language models (LLMs) grows, model developers become increasingly reluctant to disclose details on their data. This lack of transparency poses challenges to scientific evaluation and ethical deployment. Recently, pretraining data detection approaches, which infer whether a given text was part of an LLM\u2019s training data through black-box access, have been explored. The Min-K% Prob method, which has achieved state-of-the-art results, assumes that a non-training example tends to contain a few outlier words with low token probabilities. However, the effectiveness may be limited as it tends to misclassify non-training texts that contain many common words with high probabilities predicted by LLMs. To address this issue, we introduce a divergence-based calibration method, inspired by the divergence-from-randomness concept, to calibrate token probabilities for pretraining data detection. We compute the cross-entropy (i.e., the divergence) between the token probability distribution and the token frequency distribution to derive a detection score.We have developed a Chinese-language benchmark, PatentMIA, to assess the performance of detection approaches for LLMs on Chinese text. Experimental results on English-language benchmarks and PatentMIA demonstrate that our proposed method significantly outperforms existing methods. Our code and PatentMIA benchmark are available at https://github.com/zhang-wei-chao/DC-PDD.",
            "corpus_id": 272827783,
            "sentences": [
                {
                    "corpus_id": "272827783",
                    "title": "Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method",
                    "text": "Given a piece of text x = x 1 x 2 . . . x n , where x i represent the tokens after tokenizing x, and a target LLM M, we compute a detection score by measuring the divergence between the token probability distribution of x and the token frequency distribution in pretraining corpus, without any model training processes. Our method consists of four steps: (i) Token probability distribution computation, by querying M with x (Section 3.2). (ii) Token frequency distribution computation, by using a large-scale publicly available corpus D \u2032 as a reference corpus to obtain an estimation of the token frequency distribution since M's pretraining corpus is not assumed to be accessible (Section 3.3). (iii) Score calculation via comparison, by comparing the above two distributions to calibrate the token probability for each token x i in x, and derive a score for pretraining data detection based on the calibrated token probabilities (Section 3.4). (iv) binary decision, by applying a predefined threshold to the score, we predict whether x was included in M's pretraining corpus or not (Section 3.5). \n\nWe summarize our method in Algorithm 1.",
                    "score": 0.48051836594576036,
                    "section_title": "Overview",
                    "char_start_offset": 8893,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 39
                        },
                        {
                            "start": 40,
                            "end": 319
                        },
                        {
                            "start": 320,
                            "end": 438
                        },
                        {
                            "start": 439,
                            "end": 696
                        },
                        {
                            "start": 697,
                            "end": 946
                        },
                        {
                            "start": 947,
                            "end": 1099
                        },
                        {
                            "start": 1102,
                            "end": 1141
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.59521484375
                }
            ],
            "relevance_judgement": 0.59521484375,
            "relevance_judgment_input_expanded": "# Title: Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Weichao Zhang, Ruqing Zhang, Jiafeng Guo, M. D. Rijke, Yixing Fan, Xueqi Cheng\n## Abstract\nAs the scale of training corpora for large language models (LLMs) grows, model developers become increasingly reluctant to disclose details on their data. This lack of transparency poses challenges to scientific evaluation and ethical deployment. Recently, pretraining data detection approaches, which infer whether a given text was part of an LLM\u2019s training data through black-box access, have been explored. The Min-K% Prob method, which has achieved state-of-the-art results, assumes that a non-training example tends to contain a few outlier words with low token probabilities. However, the effectiveness may be limited as it tends to misclassify non-training texts that contain many common words with high probabilities predicted by LLMs. To address this issue, we introduce a divergence-based calibration method, inspired by the divergence-from-randomness concept, to calibrate token probabilities for pretraining data detection. We compute the cross-entropy (i.e., the divergence) between the token probability distribution and the token frequency distribution to derive a detection score.We have developed a Chinese-language benchmark, PatentMIA, to assess the performance of detection approaches for LLMs on Chinese text. Experimental results on English-language benchmarks and PatentMIA demonstrate that our proposed method significantly outperforms existing methods. Our code and PatentMIA benchmark are available at https://github.com/zhang-wei-chao/DC-PDD.\n## Overview\nGiven a piece of text x = x 1 x 2 . . . x n , where x i represent the tokens after tokenizing x, and a target LLM M, we compute a detection score by measuring the divergence between the token probability distribution of x and the token frequency distribution in pretraining corpus, without any model training processes. Our method consists of four steps: (i) Token probability distribution computation, by querying M with x (Section 3.2). (ii) Token frequency distribution computation, by using a large-scale publicly available corpus D \u2032 as a reference corpus to obtain an estimation of the token frequency distribution since M's pretraining corpus is not assumed to be accessible (Section 3.3). (iii) Score calculation via comparison, by comparing the above two distributions to calibrate the token probability for each token x i in x, and derive a score for pretraining data detection based on the calibrated token probabilities (Section 3.4). (iv) binary decision, by applying a predefined threshold to the score, we predict whether x was included in M's pretraining corpus or not (Section 3.5). \n\nWe summarize our method in Algorithm 1.",
            "reference_string": "[272827783 | Zhang et al. | 2024 | Citations: 16]"
        },
        {
            "title": "Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 63,
            "citation_count": 26,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.06233, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "65754049",
                    "name": "Shahriar Golchin"
                },
                {
                    "authorId": "1760868",
                    "name": "M. Surdeanu"
                }
            ],
            "abstract": "We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions, devising a quiz format wherein three perturbed versions of each instance, subsampled from a specific dataset partition, are created. These changes only include word-level perturbations. The generated perturbations, along with the original dataset instance, form the options in the DCQ, with an extra option accommodating the selection of none of the provided options. Given that the only distinguishing signal among the options is the exact wording with respect to the original dataset instance, an LLM, when tasked with identifying the original dataset instance, gravitates towards selecting the original one if it has been exposed to it. While accounting for positional biases in LLMs, the quiz performance reveals the contamination level for the tested model with the dataset partition to which the quiz pertains. Applied to various datasets and LLMs, under controlled and uncontrolled contamination, our findings, while fully lacking access to training data and model parameters, suggest that DCQ achieves state-of-the-art results and uncovers greater contamination levels through memorization compared to existing methods. Also, it proficiently bypasses more safety filters, especially those set to avoid generating copyrighted content.",
            "corpus_id": 265128736,
            "sentences": [],
            "relevance_judgement": 0.5849609375,
            "relevance_judgment_input_expanded": "# Title: Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models\n# Venue: arXiv.org\n# Authors: Shahriar Golchin, M. Surdeanu\n## Abstract\nWe propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions, devising a quiz format wherein three perturbed versions of each instance, subsampled from a specific dataset partition, are created. These changes only include word-level perturbations. The generated perturbations, along with the original dataset instance, form the options in the DCQ, with an extra option accommodating the selection of none of the provided options. Given that the only distinguishing signal among the options is the exact wording with respect to the original dataset instance, an LLM, when tasked with identifying the original dataset instance, gravitates towards selecting the original one if it has been exposed to it. While accounting for positional biases in LLMs, the quiz performance reveals the contamination level for the tested model with the dataset partition to which the quiz pertains. Applied to various datasets and LLMs, under controlled and uncontrolled contamination, our findings, while fully lacking access to training data and model parameters, suggest that DCQ achieves state-of-the-art results and uncovers greater contamination levels through memorization compared to existing methods. Also, it proficiently bypasses more safety filters, especially those set to avoid generating copyrighted content.\n",
            "reference_string": "[265128736 | Golchin et al. | 2023 | Citations: 26]"
        },
        {
            "title": "On the Reliability of Watermarks for Large Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 75,
            "citation_count": 123,
            "influential_citation_count": 16,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.04634",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.04634, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2166053502",
                    "name": "John Kirchenbauer"
                },
                {
                    "authorId": "8284185",
                    "name": "Jonas Geiping"
                },
                {
                    "authorId": "123191916",
                    "name": "Yuxin Wen"
                },
                {
                    "authorId": "1643697854",
                    "name": "Manli Shu"
                },
                {
                    "authorId": "2203810783",
                    "name": "Khalid Saifullah"
                },
                {
                    "authorId": "80253287",
                    "name": "Kezhi Kong"
                },
                {
                    "authorId": "94971447",
                    "name": "Kasun Fernando"
                },
                {
                    "authorId": "2056290221",
                    "name": "Aniruddha Saha"
                },
                {
                    "authorId": "121592562",
                    "name": "Micah Goldblum"
                },
                {
                    "authorId": "1962083",
                    "name": "T. Goldstein"
                }
            ],
            "abstract": "As LLMs become commonplace, machine-generated text has the potential to flood the internet with spam, social media bots, and valueless content. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text may be modified to suit a user's needs, or entirely rewritten to avoid detection. We study the robustness of watermarked text after it is re-written by humans, paraphrased by a non-watermarked LLM, or mixed into a longer hand-written document. We find that watermarks remain detectable even after human and machine paraphrasing. While these attacks dilute the strength of the watermark, paraphrases are statistically likely to leak n-grams or even longer fragments of the original text, resulting in high-confidence detections when enough tokens are observed. For example, after strong human paraphrasing the watermark is detectable after observing 800 tokens on average, when setting a 1e-5 false positive rate. We also consider a range of new detection schemes that are sensitive to short spans of watermarked text embedded inside a large document, and we compare the robustness of watermarking to other kinds of detectors.",
            "corpus_id": 259095643,
            "sentences": [
                {
                    "corpus_id": "259095643",
                    "title": "On the Reliability of Watermarks for Large Language Models",
                    "text": "We briefly describe the methods below and provide further details and hyperparameters in Appendix A.11.3. \n\nRetrieval: A retrieval approach to detection requires the creation and maintenance of a comprehensive database of all sequences previously generated by the language model to check against at test time and we adopt the retrieval method of (Krishna et al., 2023), utilizing the BM25 search method since it performed the best in their experiments.",
                    "score": 0.46186137106523967,
                    "section_title": "PARAPHRASING BY HUMAN WRITERS",
                    "char_start_offset": 21192,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 105
                        },
                        {
                            "start": 108,
                            "end": 452
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5712890625
                }
            ],
            "relevance_judgement": 0.5712890625,
            "relevance_judgment_input_expanded": "# Title: On the Reliability of Watermarks for Large Language Models\n# Venue: International Conference on Learning Representations\n# Authors: John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, T. Goldstein\n## Abstract\nAs LLMs become commonplace, machine-generated text has the potential to flood the internet with spam, social media bots, and valueless content. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text may be modified to suit a user's needs, or entirely rewritten to avoid detection. We study the robustness of watermarked text after it is re-written by humans, paraphrased by a non-watermarked LLM, or mixed into a longer hand-written document. We find that watermarks remain detectable even after human and machine paraphrasing. While these attacks dilute the strength of the watermark, paraphrases are statistically likely to leak n-grams or even longer fragments of the original text, resulting in high-confidence detections when enough tokens are observed. For example, after strong human paraphrasing the watermark is detectable after observing 800 tokens on average, when setting a 1e-5 false positive rate. We also consider a range of new detection schemes that are sensitive to short spans of watermarked text embedded inside a large document, and we compare the robustness of watermarking to other kinds of detectors.\n## PARAPHRASING BY HUMAN WRITERS\nWe briefly describe the methods below and provide further details and hyperparameters in Appendix A.11.3. \n\nRetrieval: A retrieval approach to detection requires the creation and maintenance of a comprehensive database of all sequences previously generated by the language model to check against at test time and we adopt the retrieval method of (Krishna et al., 2023), utilizing the BM25 search method since it performed the best in their experiments.",
            "reference_string": "[259095643 | Kirchenbauer et al. | 2023 | Citations: 123]"
        },
        {
            "title": "Enhancing Robustness of LLM-Synthetic Text Detectors for Academic Writing: A Comprehensive Analysis",
            "venue": "International Conference on Advanced Information Networking and Applications",
            "year": 2024,
            "reference_count": 9,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.08046, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2279546112",
                    "name": "Zhicheng Dou"
                },
                {
                    "authorId": "2279567205",
                    "name": "Yuchen Guo"
                },
                {
                    "authorId": "2267987768",
                    "name": "Ching-Chun Chang"
                },
                {
                    "authorId": "2110490893",
                    "name": "H. Nguyen"
                },
                {
                    "authorId": "2248262967",
                    "name": "Isao Echizen"
                }
            ],
            "abstract": "The emergence of large language models (LLMs), such as Generative Pre-trained Transformer 4 (GPT-4) used by ChatGPT, has profoundly impacted the academic and broader community. While these models offer numerous advantages in terms of revolutionizing work and study methods, they have also garnered significant attention due to their potential negative consequences. One example is generating academic reports or papers with little to no human contribution. Consequently, researchers have focused on developing detectors to address the misuse of LLMs. However, most existing methods prioritize achieving higher accuracy on restricted datasets, neglecting the crucial aspect of generalizability. This limitation hinders their practical application in real-life scenarios where reliability is paramount. In this paper, we present a comprehensive analysis of the impact of prompts on the text generated by LLMs and highlight the potential lack of robustness in one of the current state-of-the-art GPT detectors. To mitigate these issues concerning the misuse of LLMs in academic writing, we propose a reference-based Siamese detector named Synthetic-Siamese which takes a pair of texts, one as the inquiry and the other as the reference. Our method effectively addresses the lack of robustness of previous detectors (OpenAI detector and DetectGPT) and significantly improves the baseline performances in realistic academic writing scenarios by approximately 67% to 95%.",
            "corpus_id": 266999625,
            "sentences": [
                {
                    "corpus_id": "266999625",
                    "title": "Enhancing Robustness of LLM-Synthetic Text Detectors for Academic Writing: A Comprehensive Analysis",
                    "text": "Large-scale language models (LLMs), such as OpenAI's GPT-4 [8], and Google's Pathways Language Model 2 [1], have become an integral part of our lives and jobs and are often utilized unknowingly. However, while LLMs greatly facilitate daily activities, they also pose significant security risks if maliciously exploited for attacks or deception. Consequently, with the growing popularity of LLMs, the importance of AI security has come to the forefront of people's attention [3,11,4]. Among the various security concerns, academic cheating stands out as a particularly grave issue. \n\nChatGPT, in particular, has gained widespread popularity among college students worldwide. Consequently, universities urgently need robust detectors, which has driven continuous advancements in the field of detection technology. \n\nResearchers in detector development have explored strategies to optimize the training set for improved model performance. Notably, Liyanage et al. [6] pioneered an AI-generated academic dataset using GPT-2, although it is considered less effective than the more advanced ChatGPT model currently available. Yuan et al. [12] proposed BERTscore, an evaluation method for filtering high-quality generated text that closely resembles human writing. Such text can be incorporated into the training set, thereby enhancing the performance of the detectors. \n\nResearchers have also focused on optimizing the detector itself. Jawahar et al. [5] addressed the challenge of hybrid text, introducing a method to detect the boundary between machine-generated and human-written content, rather than solely distinguishing between the two. Zhao et al. [13] conducted a comprehensive survey of various LLMs, analyzing their performance across multiple dimensions, including pretraining, adaptation tuning, utilization, and capacity evaluation. They also identified potential future development directions for LLMs. Additionally, Mitchell et al. [7] proposed a model utilizing a curvature-based criterion to determine whether a given passage was generated by an LLM. \n\nStudies examining the robustness of detectors include Rodriguez et al. [10], which investigated the impact of dataset domain on detector performance, highlighting a significant decrease in performance when the training and test datasets differ in domain. Their findings emphasized how the diversity of training sets directly affects the detector's performance.",
                    "score": 0.4310919816026011,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 194
                        },
                        {
                            "start": 195,
                            "end": 344
                        },
                        {
                            "start": 345,
                            "end": 483
                        },
                        {
                            "start": 484,
                            "end": 580
                        },
                        {
                            "start": 583,
                            "end": 673
                        },
                        {
                            "start": 674,
                            "end": 811
                        },
                        {
                            "start": 814,
                            "end": 935
                        },
                        {
                            "start": 936,
                            "end": 1119
                        },
                        {
                            "start": 1120,
                            "end": 1257
                        },
                        {
                            "start": 1258,
                            "end": 1362
                        },
                        {
                            "start": 1365,
                            "end": 1429
                        },
                        {
                            "start": 1430,
                            "end": 1636
                        },
                        {
                            "start": 1637,
                            "end": 1839
                        },
                        {
                            "start": 1840,
                            "end": 1910
                        },
                        {
                            "start": 1911,
                            "end": 2061
                        },
                        {
                            "start": 2064,
                            "end": 2318
                        },
                        {
                            "start": 2319,
                            "end": 2424
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 477,
                            "end": 480,
                            "matchedPaperCorpusId": "245454767"
                        },
                        {
                            "start": 2135,
                            "end": 2139,
                            "matchedPaperCorpusId": "250390908"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.564453125
                }
            ],
            "relevance_judgement": 0.564453125,
            "relevance_judgment_input_expanded": "# Title: Enhancing Robustness of LLM-Synthetic Text Detectors for Academic Writing: A Comprehensive Analysis\n# Venue: International Conference on Advanced Information Networking and Applications\n# Authors: Zhicheng Dou, Yuchen Guo, Ching-Chun Chang, H. Nguyen, Isao Echizen\n## Abstract\nThe emergence of large language models (LLMs), such as Generative Pre-trained Transformer 4 (GPT-4) used by ChatGPT, has profoundly impacted the academic and broader community. While these models offer numerous advantages in terms of revolutionizing work and study methods, they have also garnered significant attention due to their potential negative consequences. One example is generating academic reports or papers with little to no human contribution. Consequently, researchers have focused on developing detectors to address the misuse of LLMs. However, most existing methods prioritize achieving higher accuracy on restricted datasets, neglecting the crucial aspect of generalizability. This limitation hinders their practical application in real-life scenarios where reliability is paramount. In this paper, we present a comprehensive analysis of the impact of prompts on the text generated by LLMs and highlight the potential lack of robustness in one of the current state-of-the-art GPT detectors. To mitigate these issues concerning the misuse of LLMs in academic writing, we propose a reference-based Siamese detector named Synthetic-Siamese which takes a pair of texts, one as the inquiry and the other as the reference. Our method effectively addresses the lack of robustness of previous detectors (OpenAI detector and DetectGPT) and significantly improves the baseline performances in realistic academic writing scenarios by approximately 67% to 95%.\n## Introduction\nLarge-scale language models (LLMs), such as OpenAI's GPT-4 [8], and Google's Pathways Language Model 2 [1], have become an integral part of our lives and jobs and are often utilized unknowingly. However, while LLMs greatly facilitate daily activities, they also pose significant security risks if maliciously exploited for attacks or deception. Consequently, with the growing popularity of LLMs, the importance of AI security has come to the forefront of people's attention [3,11,4]. Among the various security concerns, academic cheating stands out as a particularly grave issue. \n\nChatGPT, in particular, has gained widespread popularity among college students worldwide. Consequently, universities urgently need robust detectors, which has driven continuous advancements in the field of detection technology. \n\nResearchers in detector development have explored strategies to optimize the training set for improved model performance. Notably, Liyanage et al. [6] pioneered an AI-generated academic dataset using GPT-2, although it is considered less effective than the more advanced ChatGPT model currently available. Yuan et al. [12] proposed BERTscore, an evaluation method for filtering high-quality generated text that closely resembles human writing. Such text can be incorporated into the training set, thereby enhancing the performance of the detectors. \n\nResearchers have also focused on optimizing the detector itself. Jawahar et al. [5] addressed the challenge of hybrid text, introducing a method to detect the boundary between machine-generated and human-written content, rather than solely distinguishing between the two. Zhao et al. [13] conducted a comprehensive survey of various LLMs, analyzing their performance across multiple dimensions, including pretraining, adaptation tuning, utilization, and capacity evaluation. They also identified potential future development directions for LLMs. Additionally, Mitchell et al. [7] proposed a model utilizing a curvature-based criterion to determine whether a given passage was generated by an LLM. \n\nStudies examining the robustness of detectors include Rodriguez et al. [10], which investigated the impact of dataset domain on detector performance, highlighting a significant decrease in performance when the training and test datasets differ in domain. Their findings emphasized how the diversity of training sets directly affects the detector's performance.",
            "reference_string": "[266999625 | Dou et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 29,
            "citation_count": 60,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.15938, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "26845858",
                    "name": "Yihong Dong"
                },
                {
                    "authorId": "2199808863",
                    "name": "Xue Jiang"
                },
                {
                    "authorId": "2279435524",
                    "name": "Huanyu Liu"
                },
                {
                    "authorId": "2152843753",
                    "name": "Zhi Jin"
                },
                {
                    "authorId": "2254313986",
                    "name": "Ge Li"
                }
            ],
            "abstract": "Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks. Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination. However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges. In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for LLMs. CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM's output distribution. To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of LLM's output distribution. To facilitate this study, we introduce two benchmarks, i.e., DetCon and ComiEval, for data contamination detection and contamination mitigation evaluation tasks. Extensive experimental results show that CDD achieves the average relative improvements of 21.8\\%-30.2\\% over other contamination detection approaches in terms of Accuracy, F1 Score, and AUC metrics, and can effectively detect implicit contamination. TED substantially mitigates performance improvements up to 66.9\\% attributed to data contamination across various contamination setups. In real-world applications, we reveal that ChatGPT exhibits a high potential to suffer from data contamination on HumanEval benchmark.",
            "corpus_id": 267938621,
            "sentences": [],
            "relevance_judgement": 0.56396484375,
            "relevance_judgment_input_expanded": "# Title: Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, Ge Li\n## Abstract\nRecent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks. Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination. However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges. In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for LLMs. CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM's output distribution. To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of LLM's output distribution. To facilitate this study, we introduce two benchmarks, i.e., DetCon and ComiEval, for data contamination detection and contamination mitigation evaluation tasks. Extensive experimental results show that CDD achieves the average relative improvements of 21.8\\%-30.2\\% over other contamination detection approaches in terms of Accuracy, F1 Score, and AUC metrics, and can effectively detect implicit contamination. TED substantially mitigates performance improvements up to 66.9\\% attributed to data contamination across various contamination setups. In real-world applications, we reveal that ChatGPT exhibits a high potential to suffer from data contamination on HumanEval benchmark.\n",
            "reference_string": "[267938621 | Dong et al. | 2024 | Citations: 60]"
        },
        {
            "title": "LMs go Phishing: Adapting Pre-trained Language Models to Detect Phishing Emails",
            "venue": "2022 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)",
            "year": 2022,
            "reference_count": 38,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/WI-IAT55865.2022.00028?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/WI-IAT55865.2022.00028, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145274478",
                    "name": "Kanishka Misra"
                },
                {
                    "authorId": "10681993",
                    "name": "J. Rayz"
                }
            ],
            "abstract": "Despite decades of research, the problem of Phishing in everyday email communication is ever so prevalent. Traditionally viewed as a text-classification task, the task of phishing detection is an active defense against phishing attempts. Mean-while, progress in natural language processing has established the universal usefulness of adapting pre-trained language models to perform downstream tasks, in a paradigm known as pre-train-then-fine-tune. In this work, we build on this paradigm, and propose two language models that are adapted on 725k emails containing phishing and legitimate messages. We use these two models in two ways: 1) by performing classification-based fine-tuning, and 2) by developing a simple priming-based approach. Our approaches achieve empirical gains over a good deal of prior work, achieving near perfect performance on in-domain data, and relative improvements on out-of-domain emails.",
            "corpus_id": 258328237,
            "sentences": [],
            "relevance_judgement": 0.5556640625,
            "relevance_judgment_input_expanded": "# Title: LMs go Phishing: Adapting Pre-trained Language Models to Detect Phishing Emails\n# Venue: 2022 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)\n# Authors: Kanishka Misra, J. Rayz\n## Abstract\nDespite decades of research, the problem of Phishing in everyday email communication is ever so prevalent. Traditionally viewed as a text-classification task, the task of phishing detection is an active defense against phishing attempts. Mean-while, progress in natural language processing has established the universal usefulness of adapting pre-trained language models to perform downstream tasks, in a paradigm known as pre-train-then-fine-tune. In this work, we build on this paradigm, and propose two language models that are adapted on 725k emails containing phishing and legitimate messages. We use these two models in two ways: 1) by performing classification-based fine-tuning, and 2) by developing a simple priming-based approach. Our approaches achieve empirical gains over a good deal of prior work, achieving near perfect performance on in-domain data, and relative improvements on out-of-domain emails.\n",
            "reference_string": "[258328237 | Misra et al. | 2022 | Citations: 8]"
        },
        {
            "title": "Large Language Models for Forecasting and Anomaly Detection: A Systematic Literature Review",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 86,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.10350, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116967910",
                    "name": "Jing Su"
                },
                {
                    "authorId": "2283937122",
                    "name": "Chufeng Jiang"
                },
                {
                    "authorId": "2284759893",
                    "name": "Xin Jin"
                },
                {
                    "authorId": null,
                    "name": "Yuxin Qiao"
                },
                {
                    "authorId": "2349753157",
                    "name": "Tingsong Xiao"
                },
                {
                    "authorId": "2284731807",
                    "name": "Hongda Ma"
                },
                {
                    "authorId": "2284594383",
                    "name": "Rong Wei"
                },
                {
                    "authorId": "15217571",
                    "name": "Zhi Jing"
                },
                {
                    "authorId": "2284638322",
                    "name": "Jiajun Xu"
                },
                {
                    "authorId": "2284641943",
                    "name": "Junhong Lin"
                }
            ],
            "abstract": "This systematic literature review comprehensively examines the application of Large Language Models (LLMs) in forecasting and anomaly detection, highlighting the current state of research, inherent challenges, and prospective future directions. LLMs have demonstrated significant potential in parsing and analyzing extensive datasets to identify patterns, predict future events, and detect anomalous behavior across various domains. However, this review identifies several critical challenges that impede their broader adoption and effectiveness, including the reliance on vast historical datasets, issues with generalizability across different contexts, the phenomenon of model hallucinations, limitations within the models' knowledge boundaries, and the substantial computational resources required. Through detailed analysis, this review discusses potential solutions and strategies to overcome these obstacles, such as integrating multimodal data, advancements in learning methodologies, and emphasizing model explainability and computational efficiency. Moreover, this review outlines critical trends that are likely to shape the evolution of LLMs in these fields, including the push toward real-time processing, the importance of sustainable modeling practices, and the value of interdisciplinary collaboration. Conclusively, this review underscores the transformative impact LLMs could have on forecasting and anomaly detection while emphasizing the need for continuous innovation, ethical considerations, and practical solutions to realize their full potential.",
            "corpus_id": 267740683,
            "sentences": [],
            "relevance_judgement": 0.55126953125,
            "relevance_judgment_input_expanded": "# Title: Large Language Models for Forecasting and Anomaly Detection: A Systematic Literature Review\n# Venue: arXiv.org\n# Authors: Jing Su, Chufeng Jiang, Xin Jin, Yuxin Qiao, Tingsong Xiao, Hongda Ma, Rong Wei, Zhi Jing, Jiajun Xu, Junhong Lin\n## Abstract\nThis systematic literature review comprehensively examines the application of Large Language Models (LLMs) in forecasting and anomaly detection, highlighting the current state of research, inherent challenges, and prospective future directions. LLMs have demonstrated significant potential in parsing and analyzing extensive datasets to identify patterns, predict future events, and detect anomalous behavior across various domains. However, this review identifies several critical challenges that impede their broader adoption and effectiveness, including the reliance on vast historical datasets, issues with generalizability across different contexts, the phenomenon of model hallucinations, limitations within the models' knowledge boundaries, and the substantial computational resources required. Through detailed analysis, this review discusses potential solutions and strategies to overcome these obstacles, such as integrating multimodal data, advancements in learning methodologies, and emphasizing model explainability and computational efficiency. Moreover, this review outlines critical trends that are likely to shape the evolution of LLMs in these fields, including the push toward real-time processing, the importance of sustainable modeling practices, and the value of interdisciplinary collaboration. Conclusively, this review underscores the transformative impact LLMs could have on forecasting and anomaly detection while emphasizing the need for continuous innovation, ethical considerations, and practical solutions to realize their full potential.\n",
            "reference_string": "[267740683 | Su et al. | 2024 | Citations: 86]"
        },
        {
            "title": "Respectful or Toxic? Using Zero-Shot Learning with Language Models to Detect Hate Speech",
            "venue": "WOAH",
            "year": 2023,
            "reference_count": 55,
            "citation_count": 57,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.woah-1.6.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.woah-1.6, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1410406981",
                    "name": "F. Plaza-del-Arco"
                },
                {
                    "authorId": "2101317501",
                    "name": "Debora Nozza"
                },
                {
                    "authorId": "2022288",
                    "name": "Dirk Hovy"
                }
            ],
            "abstract": "Hate speech detection faces two significant challenges: 1) the limited availability of labeled data and 2) the high variability of hate speech across different contexts and languages. Prompting brings a ray of hope to these challenges. It allows injecting a model with task-specific knowledge without relying on labeled data. This paper explores zero-shot learning with prompting for hate speech detection. We investigate how well zero-shot learning can detect hate speech in 3 languages with limited labeled data. We experiment with various large language models and verbalizers on 8 benchmark datasets. Our findings highlight the impact of prompt selection on the results. They also suggest that prompting, specifically with recent large language models, can achieve performance comparable to and surpass fine-tuned models, making it a promising alternative for under-resourced languages. Our findings highlight the potential of prompting for hate speech detection and show how both the prompt and the model have a significant impact on achieving more accurate predictions in this task.",
            "corpus_id": 259376532,
            "sentences": [],
            "relevance_judgement": 0.54541015625,
            "relevance_judgment_input_expanded": "# Title: Respectful or Toxic? Using Zero-Shot Learning with Language Models to Detect Hate Speech\n# Venue: WOAH\n# Authors: F. Plaza-del-Arco, Debora Nozza, Dirk Hovy\n## Abstract\nHate speech detection faces two significant challenges: 1) the limited availability of labeled data and 2) the high variability of hate speech across different contexts and languages. Prompting brings a ray of hope to these challenges. It allows injecting a model with task-specific knowledge without relying on labeled data. This paper explores zero-shot learning with prompting for hate speech detection. We investigate how well zero-shot learning can detect hate speech in 3 languages with limited labeled data. We experiment with various large language models and verbalizers on 8 benchmark datasets. Our findings highlight the impact of prompt selection on the results. They also suggest that prompting, specifically with recent large language models, can achieve performance comparable to and surpass fine-tuned models, making it a promising alternative for under-resourced languages. Our findings highlight the potential of prompting for hate speech detection and show how both the prompt and the model have a significant impact on achieving more accurate predictions in this task.\n",
            "reference_string": "[259376532 | Plaza-del-Arco et al. | 2023 | Citations: 57]"
        },
        {
            "title": "A tutorial on open-source large language models for behavioral science",
            "venue": "Behavior Research Methods",
            "year": 2024,
            "reference_count": 92,
            "citation_count": 25,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.3758/s13428-024-02455-8",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11525391, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2302450822",
                    "name": "Zak Hussain"
                },
                {
                    "authorId": "32354733",
                    "name": "Marcel Binz"
                },
                {
                    "authorId": "2184033930",
                    "name": "Rui Mata"
                },
                {
                    "authorId": "2302732683",
                    "name": "Dirk U. Wulff"
                }
            ],
            "abstract": "Large language models (LLMs) have the potential to revolutionize behavioral science by accelerating and improving the research cycle, from conceptualization to data analysis. Unlike closed-source solutions, open-source frameworks for LLMs can enable transparency, reproducibility, and adherence to data protection standards, which gives them a crucial advantage for use in behavioral science. To help researchers harness the promise of LLMs, this tutorial offers a primer on the open-source Hugging Face ecosystem and demonstrates several applications that advance conceptual and empirical work in behavioral science, including feature extraction, fine-tuning of models for prediction, and generation of behavioral responses. Executable code is made available at github.com/Zak-Hussain/LLM4BeSci.git. Finally, the tutorial discusses challenges faced by research with (open-source) LLMs related to interpretability and safety and offers a perspective on future research at the intersection of language modeling and behavioral science.",
            "corpus_id": 271880412,
            "sentences": [
                {
                    "corpus_id": "271880412",
                    "title": "A tutorial on open-source large language models for behavioral science",
                    "text": "This tutorial has introduced three common LLM use cases: feature extraction, fine-tuning, and text generation. Although these are perhaps more well-known use cases, they are not exhaustive. For instance, an additional use of LLMs is to deploy them on the very tasks for which they were pretrained; namely, to assign probabilities to tokens. During training, the model is incentivized to assign high probabilities to tokens and token sequences that are common in the training data and vice versa for those that are uncommon. As a result, the probabilities produced by a trained model can be used to detect text sequences that (the model has learned) are uncommon. Measures based on token and token sequence probabilities have thus been used to, for instance, investigate how language models capture grammatical gender (An et al., 2019) and to predict human reading times (e.g., Merkx & Frank, 2020). The present example demonstrates how the log probabilities extracted from GPT-2 can be used to predict teachers' text readability ratings. Specifically, these are teachers' ratings of how difficult student readers would find certain text excerpts, obtained from the CommonLit Ease of Readability (CLEAR) corpus (Table 3; Crossley et al., 2023). \n\nA common metric to evaluate the probability of a sequence is called perplexity (Jelinek et al., 1977). Given a sequence of tokens X = (x 0 , x 1 , ..., x t ), perplexity can be defined as \n\nwith p(x i |x <i ) being the probability assigned to the i-th token in the sequence given the preceding tokens. Perplexity is thus the inverse geometric mean of sequentially produced token probabilities. As perplexity assumes sequentially produced tokens, it is not well-defined for masked language modeling. In our example, we therefore rely on GPT-2, which is a decoder model trained using causal language modeling and a direct precursor to today's GPT-4. The code begins by reading the 'Exerpt' and 'BT_easiness' columns of the CLEAR corpus into a pandas.-DataFrame.",
                    "score": 0.4439982908604859,
                    "section_title": "Token probability and perplexity extraction",
                    "char_start_offset": 71439,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 110
                        },
                        {
                            "start": 111,
                            "end": 189
                        },
                        {
                            "start": 190,
                            "end": 340
                        },
                        {
                            "start": 341,
                            "end": 523
                        },
                        {
                            "start": 524,
                            "end": 662
                        },
                        {
                            "start": 663,
                            "end": 898
                        },
                        {
                            "start": 899,
                            "end": 1037
                        },
                        {
                            "start": 1038,
                            "end": 1243
                        },
                        {
                            "start": 1246,
                            "end": 1348
                        },
                        {
                            "start": 1349,
                            "end": 1433
                        },
                        {
                            "start": 1436,
                            "end": 1547
                        },
                        {
                            "start": 1548,
                            "end": 1639
                        },
                        {
                            "start": 1640,
                            "end": 1744
                        },
                        {
                            "start": 1745,
                            "end": 1893
                        },
                        {
                            "start": 1894,
                            "end": 2005
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1220,
                            "end": 1242,
                            "matchedPaperCorpusId": "247498576"
                        },
                        {
                            "start": 1325,
                            "end": 1347,
                            "matchedPaperCorpusId": "121680873"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.541015625
                }
            ],
            "relevance_judgement": 0.541015625,
            "relevance_judgment_input_expanded": "# Title: A tutorial on open-source large language models for behavioral science\n# Venue: Behavior Research Methods\n# Authors: Zak Hussain, Marcel Binz, Rui Mata, Dirk U. Wulff\n## Abstract\nLarge language models (LLMs) have the potential to revolutionize behavioral science by accelerating and improving the research cycle, from conceptualization to data analysis. Unlike closed-source solutions, open-source frameworks for LLMs can enable transparency, reproducibility, and adherence to data protection standards, which gives them a crucial advantage for use in behavioral science. To help researchers harness the promise of LLMs, this tutorial offers a primer on the open-source Hugging Face ecosystem and demonstrates several applications that advance conceptual and empirical work in behavioral science, including feature extraction, fine-tuning of models for prediction, and generation of behavioral responses. Executable code is made available at github.com/Zak-Hussain/LLM4BeSci.git. Finally, the tutorial discusses challenges faced by research with (open-source) LLMs related to interpretability and safety and offers a perspective on future research at the intersection of language modeling and behavioral science.\n## Token probability and perplexity extraction\nThis tutorial has introduced three common LLM use cases: feature extraction, fine-tuning, and text generation. Although these are perhaps more well-known use cases, they are not exhaustive. For instance, an additional use of LLMs is to deploy them on the very tasks for which they were pretrained; namely, to assign probabilities to tokens. During training, the model is incentivized to assign high probabilities to tokens and token sequences that are common in the training data and vice versa for those that are uncommon. As a result, the probabilities produced by a trained model can be used to detect text sequences that (the model has learned) are uncommon. Measures based on token and token sequence probabilities have thus been used to, for instance, investigate how language models capture grammatical gender (An et al., 2019) and to predict human reading times (e.g., Merkx & Frank, 2020). The present example demonstrates how the log probabilities extracted from GPT-2 can be used to predict teachers' text readability ratings. Specifically, these are teachers' ratings of how difficult student readers would find certain text excerpts, obtained from the CommonLit Ease of Readability (CLEAR) corpus (Table 3; Crossley et al., 2023). \n\nA common metric to evaluate the probability of a sequence is called perplexity (Jelinek et al., 1977). Given a sequence of tokens X = (x 0 , x 1 , ..., x t ), perplexity can be defined as \n\nwith p(x i |x <i ) being the probability assigned to the i-th token in the sequence given the preceding tokens. Perplexity is thus the inverse geometric mean of sequentially produced token probabilities. As perplexity assumes sequentially produced tokens, it is not well-defined for masked language modeling. In our example, we therefore rely on GPT-2, which is a decoder model trained using causal language modeling and a direct precursor to today's GPT-4. The code begins by reading the 'Exerpt' and 'BT_easiness' columns of the CLEAR corpus into a pandas.-DataFrame.",
            "reference_string": "[271880412 | Hussain et al. | 2024 | Citations: 25]"
        },
        {
            "title": "Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 55,
            "citation_count": 20,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.16617",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.16617, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145114723",
                    "name": "Zhijie Deng"
                },
                {
                    "authorId": "2162081759",
                    "name": "Hongcheng Gao"
                },
                {
                    "authorId": "2188993538",
                    "name": "Yibo Miao"
                },
                {
                    "authorId": "46702482",
                    "name": "Hao Zhang"
                }
            ],
            "abstract": "The detection of machine-generated text, especially from large language models (LLMs), is crucial in preventing serious social problems resulting from their misuse. Some methods train dedicated detectors on specific datasets but fall short in generalizing to unseen test data, while other zero-shot ones often yield suboptimal performance. Although the recent DetectGPT has shown promising detection performance, it suffers from significant inefficiency issues, as detecting a single candidate requires querying the source LLM with hundreds of its perturbations. This paper aims to bridge this gap. Concretely, we propose to incorporate a Bayesian surrogate model, which allows us to select typical samples based on Bayesian uncertainty and interpolate scores from typical samples to other samples, to improve query efficiency. Empirical results demonstrate that our method significantly outperforms existing approaches under a low query budget. Notably, when detecting the text generated by LLaMA family models, our method with just 2 or 3 queries can outperform DetectGPT with 200 queries.",
            "corpus_id": 258947640,
            "sentences": [
                {
                    "corpus_id": "258947640",
                    "title": "Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model",
                    "text": "Large language models. LLMs (Radford et al., 2019;Brown et al., 2020;Chowdhery et al., 2022;Zhang et al., 2022;OpenAI, 2022) have revolutionized the field of natural language processing by offering several advantages over previous pre-trained models (Devlin et al., 2018;Liu et al., 2019;Lan et al., 2019), including a better characterization of complex patterns and dependencies in the text, and the appealing in-context learning ability for solving downstream tasks with minimal examples. Representative models such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), and Chat-GPT (OpenAI, 2022) have showcased their remarkable ability to generate text with high coherence, fluency, and semantic relevance. They can even effectively address complex inquiries related to science, mathematics, history, current events, and social trends. Therefore, it is increasingly important to effectively regulate the use of LLMs to prevent significant social issues. \n\nLLM-generated text detection. Previous methods can be broadly categorized into two groups. The first group of methods performs detection in a zero-shot manner (Solaiman et al., 2019;Gehrmann et al., 2019a;Mitchell et al., 2023;Yang et al., 2023), but they require access to the source model that generates the texts to derive quantities like output logits or losses for detection. For instance, Solaiman et al. (2019) suggest that a higher log probability for each token indicates that the text will likely be machine-generated. When the output logits/losses of the source model are unavailable, these methods rely on a proxy model for detection. However, there is often a substantial gap between the proxy and source models from which the text is generated. Another group of methods trains DNN-based classifiers on col-lected human-written and machine-generated texts for detection (Guo et al., 2023;Uchendu et al., 2020;Ope-nAI, 2023b).",
                    "score": 0.47108735754461806,
                    "section_title": "Related Works",
                    "char_start_offset": 4723,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 22
                        },
                        {
                            "start": 23,
                            "end": 490
                        },
                        {
                            "start": 491,
                            "end": 718
                        },
                        {
                            "start": 719,
                            "end": 847
                        },
                        {
                            "start": 848,
                            "end": 965
                        },
                        {
                            "start": 968,
                            "end": 997
                        },
                        {
                            "start": 998,
                            "end": 1058
                        },
                        {
                            "start": 1059,
                            "end": 1348
                        },
                        {
                            "start": 1349,
                            "end": 1496
                        },
                        {
                            "start": 1497,
                            "end": 1614
                        },
                        {
                            "start": 1615,
                            "end": 1726
                        },
                        {
                            "start": 1727,
                            "end": 1906
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 28,
                            "end": 50,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 50,
                            "end": 69,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 527,
                            "end": 547,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1150,
                            "end": 1173,
                            "matchedPaperCorpusId": "182952848"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.52880859375
                }
            ],
            "relevance_judgement": 0.52880859375,
            "relevance_judgment_input_expanded": "# Title: Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Zhijie Deng, Hongcheng Gao, Yibo Miao, Hao Zhang\n## Abstract\nThe detection of machine-generated text, especially from large language models (LLMs), is crucial in preventing serious social problems resulting from their misuse. Some methods train dedicated detectors on specific datasets but fall short in generalizing to unseen test data, while other zero-shot ones often yield suboptimal performance. Although the recent DetectGPT has shown promising detection performance, it suffers from significant inefficiency issues, as detecting a single candidate requires querying the source LLM with hundreds of its perturbations. This paper aims to bridge this gap. Concretely, we propose to incorporate a Bayesian surrogate model, which allows us to select typical samples based on Bayesian uncertainty and interpolate scores from typical samples to other samples, to improve query efficiency. Empirical results demonstrate that our method significantly outperforms existing approaches under a low query budget. Notably, when detecting the text generated by LLaMA family models, our method with just 2 or 3 queries can outperform DetectGPT with 200 queries.\n## Related Works\nLarge language models. LLMs (Radford et al., 2019;Brown et al., 2020;Chowdhery et al., 2022;Zhang et al., 2022;OpenAI, 2022) have revolutionized the field of natural language processing by offering several advantages over previous pre-trained models (Devlin et al., 2018;Liu et al., 2019;Lan et al., 2019), including a better characterization of complex patterns and dependencies in the text, and the appealing in-context learning ability for solving downstream tasks with minimal examples. Representative models such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), and Chat-GPT (OpenAI, 2022) have showcased their remarkable ability to generate text with high coherence, fluency, and semantic relevance. They can even effectively address complex inquiries related to science, mathematics, history, current events, and social trends. Therefore, it is increasingly important to effectively regulate the use of LLMs to prevent significant social issues. \n\nLLM-generated text detection. Previous methods can be broadly categorized into two groups. The first group of methods performs detection in a zero-shot manner (Solaiman et al., 2019;Gehrmann et al., 2019a;Mitchell et al., 2023;Yang et al., 2023), but they require access to the source model that generates the texts to derive quantities like output logits or losses for detection. For instance, Solaiman et al. (2019) suggest that a higher log probability for each token indicates that the text will likely be machine-generated. When the output logits/losses of the source model are unavailable, these methods rely on a proxy model for detection. However, there is often a substantial gap between the proxy and source models from which the text is generated. Another group of methods trains DNN-based classifiers on col-lected human-written and machine-generated texts for detection (Guo et al., 2023;Uchendu et al., 2020;Ope-nAI, 2023b).",
            "reference_string": "[258947640 | Deng et al. | 2023 | Citations: 20]"
        },
        {
            "title": "Automated Statistical Model Discovery with Language Models",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 46,
            "citation_count": 19,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.17879, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2288065395",
                    "name": "Michael Y. Li"
                },
                {
                    "authorId": "2287933421",
                    "name": "Emily B. Fox"
                },
                {
                    "authorId": "2280334415",
                    "name": "Noah D. Goodman"
                }
            ],
            "abstract": "Statistical model discovery is a challenging search over a vast space of models subject to domain-specific constraints. Efficiently searching over this space requires expertise in modeling and the problem domain. Motivated by the domain knowledge and programming capabilities of large language models (LMs), we introduce a method for language model driven automated statistical model discovery. We cast our automated procedure within the principled framework of Box's Loop: the LM iterates between proposing statistical models represented as probabilistic programs, acting as a modeler, and critiquing those models, acting as a domain expert. By leveraging LMs, we do not have to define a domain-specific language of models or design a handcrafted search procedure, which are key restrictions of previous systems. We evaluate our method in three settings in probabilistic modeling: searching within a restricted space of models, searching over an open-ended space, and improving expert models under natural language constraints (e.g., this model should be interpretable to an ecologist). Our method identifies models on par with human expert designed models and extends classic models in interpretable ways. Our results highlight the promise of LM-driven model discovery.",
            "corpus_id": 268041863,
            "sentences": [
                {
                    "corpus_id": "268041863",
                    "title": "Automated Statistical Model Discovery with Language Models",
                    "text": "Our framework is motivated by recent work in inductive reasoning with LMs (Qiu et al., 2024;Wang et al., 2024), integrating tools with LMs (Gao et al., 2023), and driving LMs via linguistic feedback (Shinn et al., 2023). \n\nAt a high level, we consider a method for learning probabilistic models from data that involves two steps: a model building step and a criticism step1 . Crucially, by learning a model, we mean searching over a space of model structures and not just learning the parameters of some fixed model class. In each step, we leverage LMs. In the proposal step, a proposal LM proposes probabilistic programs for a dataset. We then fit these probabilistic programs and evaluate them. \n\nIn the criticism step, we provide a critic LM with programs and statistics assessing model fit (e.g., model criticism statistics) and ask the critic LM to provide feedback to guide the next round of proposals. \n\nWe start with a dataset D = {x i , y i } n i=1 . Here x i \u2208 R d are fixed d-dimensional input values (e.g., features) and y i \u2208 R are the observations. Let \u03a3 be the vocabulary of the LM. For each dataset, we have an associated metadata set C \u2208 \u03a3 d+1 , which consists of natural language descriptions of D (e.g., animal ages vs length) and natural language descriptions of each feature in D (e.g., length of animal). Context informs how human modelers leverage prior knowledge; for example, if a modeler knows their dataset consists of monthly carbon dioxide measurements over a fifty-year time span, they will choose a model that can capture periodicity and a linear trend. Our goal is to find a probabilistic program z \u223c \u03a3 * that maximizes some notion of quality, which we take here to be either the log marginal likelihood or expected log predictive density (ELPD) estimated via cross validation (LOO) (Vehtari et al., 2017).",
                    "score": 0.4353487993929209,
                    "section_title": "Problem formulation",
                    "char_start_offset": 9960,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 220
                        },
                        {
                            "start": 223,
                            "end": 375
                        },
                        {
                            "start": 376,
                            "end": 522
                        },
                        {
                            "start": 523,
                            "end": 553
                        },
                        {
                            "start": 554,
                            "end": 636
                        },
                        {
                            "start": 637,
                            "end": 696
                        },
                        {
                            "start": 699,
                            "end": 908
                        },
                        {
                            "start": 911,
                            "end": 959
                        },
                        {
                            "start": 960,
                            "end": 1062
                        },
                        {
                            "start": 1063,
                            "end": 1097
                        },
                        {
                            "start": 1098,
                            "end": 1326
                        },
                        {
                            "start": 1327,
                            "end": 1584
                        },
                        {
                            "start": 1585,
                            "end": 1838
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 74,
                            "end": 92,
                            "matchedPaperCorpusId": "263909078"
                        },
                        {
                            "start": 92,
                            "end": 110,
                            "matchedPaperCorpusId": "261696510"
                        },
                        {
                            "start": 139,
                            "end": 157,
                            "matchedPaperCorpusId": "253708270"
                        },
                        {
                            "start": 199,
                            "end": 219,
                            "matchedPaperCorpusId": "258833055"
                        },
                        {
                            "start": 1815,
                            "end": 1837,
                            "matchedPaperCorpusId": "36568061"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.52783203125
                }
            ],
            "relevance_judgement": 0.52783203125,
            "relevance_judgment_input_expanded": "# Title: Automated Statistical Model Discovery with Language Models\n# Venue: International Conference on Machine Learning\n# Authors: Michael Y. Li, Emily B. Fox, Noah D. Goodman\n## Abstract\nStatistical model discovery is a challenging search over a vast space of models subject to domain-specific constraints. Efficiently searching over this space requires expertise in modeling and the problem domain. Motivated by the domain knowledge and programming capabilities of large language models (LMs), we introduce a method for language model driven automated statistical model discovery. We cast our automated procedure within the principled framework of Box's Loop: the LM iterates between proposing statistical models represented as probabilistic programs, acting as a modeler, and critiquing those models, acting as a domain expert. By leveraging LMs, we do not have to define a domain-specific language of models or design a handcrafted search procedure, which are key restrictions of previous systems. We evaluate our method in three settings in probabilistic modeling: searching within a restricted space of models, searching over an open-ended space, and improving expert models under natural language constraints (e.g., this model should be interpretable to an ecologist). Our method identifies models on par with human expert designed models and extends classic models in interpretable ways. Our results highlight the promise of LM-driven model discovery.\n## Problem formulation\nOur framework is motivated by recent work in inductive reasoning with LMs (Qiu et al., 2024;Wang et al., 2024), integrating tools with LMs (Gao et al., 2023), and driving LMs via linguistic feedback (Shinn et al., 2023). \n\nAt a high level, we consider a method for learning probabilistic models from data that involves two steps: a model building step and a criticism step1 . Crucially, by learning a model, we mean searching over a space of model structures and not just learning the parameters of some fixed model class. In each step, we leverage LMs. In the proposal step, a proposal LM proposes probabilistic programs for a dataset. We then fit these probabilistic programs and evaluate them. \n\nIn the criticism step, we provide a critic LM with programs and statistics assessing model fit (e.g., model criticism statistics) and ask the critic LM to provide feedback to guide the next round of proposals. \n\nWe start with a dataset D = {x i , y i } n i=1 . Here x i \u2208 R d are fixed d-dimensional input values (e.g., features) and y i \u2208 R are the observations. Let \u03a3 be the vocabulary of the LM. For each dataset, we have an associated metadata set C \u2208 \u03a3 d+1 , which consists of natural language descriptions of D (e.g., animal ages vs length) and natural language descriptions of each feature in D (e.g., length of animal). Context informs how human modelers leverage prior knowledge; for example, if a modeler knows their dataset consists of monthly carbon dioxide measurements over a fifty-year time span, they will choose a model that can capture periodicity and a linear trend. Our goal is to find a probabilistic program z \u223c \u03a3 * that maximizes some notion of quality, which we take here to be either the log marginal likelihood or expected log predictive density (ELPD) estimated via cross validation (LOO) (Vehtari et al., 2017).",
            "reference_string": "[268041863 | Li et al. | 2024 | Citations: 19]"
        },
        {
            "title": "Detecting out-of-distribution text using topological features of transformer-based language models",
            "venue": "AISafety@IJCAI",
            "year": 2023,
            "reference_count": 24,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.13102, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2267725677",
                    "name": "Andres Pollano"
                },
                {
                    "authorId": "2267728923",
                    "name": "Anupam Chaudhuri"
                },
                {
                    "authorId": "2267725822",
                    "name": "Anj Simmons"
                }
            ],
            "abstract": "To safeguard machine learning systems that operate on textual data against out-of-distribution (OOD) inputs that could cause unpredictable behaviour, we explore the use of topological features of self-attention maps from transformer-based language models to detect when input text is out of distribution. Self-attention forms the core of transformer-based language models, dynamically assigning vectors to words based on context, thus in theory our methodology is applicable to any transformer-based language model with multihead self-attention. We evaluate our approach on BERT and compare it to a traditional OOD approach using CLS embeddings. Our results show that our approach outperforms CLS embeddings in distinguishing in-distribution samples from far-out-of-domain samples, but struggles with near or same-domain datasets.",
            "corpus_id": 265351715,
            "sentences": [
                {
                    "corpus_id": "265351715",
                    "title": "Detecting out-of-distribution text using topological features of transformer-based language models",
                    "text": "Machine learning (ML) models perform well on the datasets they have been trained on, but can behave unreliably when tested on data that is out-of-distribution (OOD). For example, when a ML model has been trained to recognise different breeds of cats is fed an image of a dog, the results are unpredictable. OOD detection is the task of identifying that an input does not seem to be drawn from the same distribution as the training data, and thus the prediction given by the ML model should not be trusted. OOD detectors can be used to defend ML models deployed in high stakes applications from OOD data by providing a warning/error message for OOD inputs rather than processing the input and producing untrustworthy results [1]. \n\nIn this paper, we focus on OOD detection for textual inputs to safeguard ML models that perform natural language processing (NLP) tasks. For example, a sentiment classification model trained on formal restaurant reviews may not produce valid results when applied to informal posts from social media. Determining that an input is OOD requires a way to measure the distance between an input and the in-distribution data. This in turn requires a method to convert textual data into an embedding space in which we can measure distance. One approach to this is to input the text to a transformer-based language model, such as BERT [2], to extract an embedding vector for the input text (e.g., the hidden representation of the special [] token). We can then measure the distance of the embedding vector for an input text to the nearest (or k-nearest) embedding vector of a text from an in-distribution validation set. When this distance is beyond some threshold (which needs to be calibrated for the application), the input text is flagged as out of distribution. The internal state of transformer-based language models contains important information, which may be able to offer richer representations than only using the embedding obtained from the last or penultimate layer. For example, Azaria and Mitchell [3] demonstrated that it is possible to train a classifier on the activation values of the hidden layers of large language models to predict when they are generating false information rather than true information.",
                    "score": 0.4568701584118535,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 165
                        },
                        {
                            "start": 166,
                            "end": 306
                        },
                        {
                            "start": 307,
                            "end": 505
                        },
                        {
                            "start": 506,
                            "end": 728
                        },
                        {
                            "start": 731,
                            "end": 867
                        },
                        {
                            "start": 868,
                            "end": 1030
                        },
                        {
                            "start": 1031,
                            "end": 1149
                        },
                        {
                            "start": 1150,
                            "end": 1262
                        },
                        {
                            "start": 1263,
                            "end": 1470
                        },
                        {
                            "start": 1471,
                            "end": 1642
                        },
                        {
                            "start": 1643,
                            "end": 1788
                        },
                        {
                            "start": 1789,
                            "end": 2001
                        },
                        {
                            "start": 2002,
                            "end": 2248
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1357,
                            "end": 1360,
                            "matchedPaperCorpusId": "52967399"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.51904296875
                }
            ],
            "relevance_judgement": 0.51904296875,
            "relevance_judgment_input_expanded": "# Title: Detecting out-of-distribution text using topological features of transformer-based language models\n# Venue: AISafety@IJCAI\n# Authors: Andres Pollano, Anupam Chaudhuri, Anj Simmons\n## Abstract\nTo safeguard machine learning systems that operate on textual data against out-of-distribution (OOD) inputs that could cause unpredictable behaviour, we explore the use of topological features of self-attention maps from transformer-based language models to detect when input text is out of distribution. Self-attention forms the core of transformer-based language models, dynamically assigning vectors to words based on context, thus in theory our methodology is applicable to any transformer-based language model with multihead self-attention. We evaluate our approach on BERT and compare it to a traditional OOD approach using CLS embeddings. Our results show that our approach outperforms CLS embeddings in distinguishing in-distribution samples from far-out-of-domain samples, but struggles with near or same-domain datasets.\n## Introduction\nMachine learning (ML) models perform well on the datasets they have been trained on, but can behave unreliably when tested on data that is out-of-distribution (OOD). For example, when a ML model has been trained to recognise different breeds of cats is fed an image of a dog, the results are unpredictable. OOD detection is the task of identifying that an input does not seem to be drawn from the same distribution as the training data, and thus the prediction given by the ML model should not be trusted. OOD detectors can be used to defend ML models deployed in high stakes applications from OOD data by providing a warning/error message for OOD inputs rather than processing the input and producing untrustworthy results [1]. \n\nIn this paper, we focus on OOD detection for textual inputs to safeguard ML models that perform natural language processing (NLP) tasks. For example, a sentiment classification model trained on formal restaurant reviews may not produce valid results when applied to informal posts from social media. Determining that an input is OOD requires a way to measure the distance between an input and the in-distribution data. This in turn requires a method to convert textual data into an embedding space in which we can measure distance. One approach to this is to input the text to a transformer-based language model, such as BERT [2], to extract an embedding vector for the input text (e.g., the hidden representation of the special [] token). We can then measure the distance of the embedding vector for an input text to the nearest (or k-nearest) embedding vector of a text from an in-distribution validation set. When this distance is beyond some threshold (which needs to be calibrated for the application), the input text is flagged as out of distribution. The internal state of transformer-based language models contains important information, which may be able to offer richer representations than only using the embedding obtained from the last or penultimate layer. For example, Azaria and Mitchell [3] demonstrated that it is possible to train a classifier on the activation values of the hidden layers of large language models to predict when they are generating false information rather than true information.",
            "reference_string": "[265351715 | Pollano et al. | 2023 | Citations: 1]"
        },
        {
            "title": "Raidar: geneRative AI Detection viA Rewriting",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 31,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.12970, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "7700460",
                    "name": "Chengzhi Mao"
                },
                {
                    "authorId": "1856025",
                    "name": "Carl Vondrick"
                },
                {
                    "authorId": "2281126484",
                    "name": "Hao Wang"
                },
                {
                    "authorId": "2110694456",
                    "name": "Junfeng Yang"
                }
            ],
            "abstract": "We find that large language models (LLMs) are more likely to modify human-written text than AI-generated text when tasked with rewriting. This tendency arises because LLMs often perceive AI-generated text as high-quality, leading to fewer modifications. We introduce a method to detect AI-generated content by prompting LLMs to rewrite text and calculating the editing distance of the output. We dubbed our geneRative AI Detection viA Rewriting method Raidar. Raidar significantly improves the F1 detection scores of existing AI content detection models -- both academic and commercial -- across various domains, including News, creative writing, student essays, code, Yelp reviews, and arXiv papers, with gains of up to 29 points. Operating solely on word symbols without high-dimensional features, our method is compatible with black box LLMs, and is inherently robust on new content. Our results illustrate the unique imprint of machine-generated text through the lens of the machines themselves.",
            "corpus_id": 267095281,
            "sentences": [
                {
                    "corpus_id": "267095281",
                    "title": "Raidar: geneRative AI Detection viA Rewriting",
                    "text": "data, even they are generated from a different model. We also evaluate our detection efficiency on the Claude (Anthropic, 2023) generated text on student essay (Verma et al., 2023), where we achieve an F1 score of 57.80. \n\nIn the out-of-distribution experiment, we train the detector on data from two language models, assuming it is unaware that the test text will be generated from the third model. Despite a performance drop on detecting the out-of-distribution test data generated from the third model, our method remains effective in detecting content from this unseen model, underscoring our approach's robustness and adaptability, with up to 91 points on F1 score. The number shows the number of data, reflecting by the size of the dot. \n\nType of Detection Model. Mireshghallah et al. (2023) showed that model size affects performance in perturbation-based detection methods. Given the same input text generated from GPT-3.5, We explore our approach's efficacy with alternative rewriting models with different size. In addition to using the costly GPT-3.5 to rewrite, we incorporate two smaller models, Ada and Text-Davinci-002, and evaluate their detection performance when they are used to rewrite. In Table 5, while all models achieve significant detection performance, our results indicate that a larger rewriting language model enhances detection performance in our method. \n\nImpact of Different Prompts. Figure 6 displays the detection F1 score for various prompts across three datasets. While Mitchell et al. (2023) employs up to 100 perturbations to query LLM and compute curvature from loss, our approach achieves high detection performance using just a single rewriting prompt. \n\nImpact of Content Length. We assess our detection method's performance across varying input lengths using the Yelp Review dataset in Figure 5. Longer inputs, in general, achieve higher detection performance. Notably, while many algorithms fail with shorter inputs (Tian, 2023;Verma et al., 2023), our method can achieve 74 points of detection F1 score even with inputs as brief as ten words, highlighting the effectiveness of our approach.",
                    "score": 0.45062864914027756,
                    "section_title": "LLM for",
                    "char_start_offset": 29564,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 53
                        },
                        {
                            "start": 54,
                            "end": 220
                        },
                        {
                            "start": 223,
                            "end": 399
                        },
                        {
                            "start": 400,
                            "end": 670
                        },
                        {
                            "start": 671,
                            "end": 742
                        },
                        {
                            "start": 745,
                            "end": 769
                        },
                        {
                            "start": 770,
                            "end": 881
                        },
                        {
                            "start": 882,
                            "end": 931
                        },
                        {
                            "start": 932,
                            "end": 1021
                        },
                        {
                            "start": 1022,
                            "end": 1206
                        },
                        {
                            "start": 1207,
                            "end": 1384
                        },
                        {
                            "start": 1387,
                            "end": 1499
                        },
                        {
                            "start": 1500,
                            "end": 1693
                        },
                        {
                            "start": 1696,
                            "end": 1721
                        },
                        {
                            "start": 1722,
                            "end": 1903
                        },
                        {
                            "start": 1904,
                            "end": 2135
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.51318359375
                }
            ],
            "relevance_judgement": 0.51318359375,
            "relevance_judgment_input_expanded": "# Title: Raidar: geneRative AI Detection viA Rewriting\n# Venue: International Conference on Learning Representations\n# Authors: Chengzhi Mao, Carl Vondrick, Hao Wang, Junfeng Yang\n## Abstract\nWe find that large language models (LLMs) are more likely to modify human-written text than AI-generated text when tasked with rewriting. This tendency arises because LLMs often perceive AI-generated text as high-quality, leading to fewer modifications. We introduce a method to detect AI-generated content by prompting LLMs to rewrite text and calculating the editing distance of the output. We dubbed our geneRative AI Detection viA Rewriting method Raidar. Raidar significantly improves the F1 detection scores of existing AI content detection models -- both academic and commercial -- across various domains, including News, creative writing, student essays, code, Yelp reviews, and arXiv papers, with gains of up to 29 points. Operating solely on word symbols without high-dimensional features, our method is compatible with black box LLMs, and is inherently robust on new content. Our results illustrate the unique imprint of machine-generated text through the lens of the machines themselves.\n## LLM for\ndata, even they are generated from a different model. We also evaluate our detection efficiency on the Claude (Anthropic, 2023) generated text on student essay (Verma et al., 2023), where we achieve an F1 score of 57.80. \n\nIn the out-of-distribution experiment, we train the detector on data from two language models, assuming it is unaware that the test text will be generated from the third model. Despite a performance drop on detecting the out-of-distribution test data generated from the third model, our method remains effective in detecting content from this unseen model, underscoring our approach's robustness and adaptability, with up to 91 points on F1 score. The number shows the number of data, reflecting by the size of the dot. \n\nType of Detection Model. Mireshghallah et al. (2023) showed that model size affects performance in perturbation-based detection methods. Given the same input text generated from GPT-3.5, We explore our approach's efficacy with alternative rewriting models with different size. In addition to using the costly GPT-3.5 to rewrite, we incorporate two smaller models, Ada and Text-Davinci-002, and evaluate their detection performance when they are used to rewrite. In Table 5, while all models achieve significant detection performance, our results indicate that a larger rewriting language model enhances detection performance in our method. \n\nImpact of Different Prompts. Figure 6 displays the detection F1 score for various prompts across three datasets. While Mitchell et al. (2023) employs up to 100 perturbations to query LLM and compute curvature from loss, our approach achieves high detection performance using just a single rewriting prompt. \n\nImpact of Content Length. We assess our detection method's performance across varying input lengths using the Yelp Review dataset in Figure 5. Longer inputs, in general, achieve higher detection performance. Notably, while many algorithms fail with shorter inputs (Tian, 2023;Verma et al., 2023), our method can achieve 74 points of detection F1 score even with inputs as brief as ten words, highlighting the effectiveness of our approach.",
            "reference_string": "[267095281 | Mao et al. | 2024 | Citations: 31]"
        },
        {
            "title": "DoLLM: How Large Language Models Understanding Network Flow Data to Detect Carpet Bombing DDoS",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 48,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.07638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2301171718",
                    "name": "Qingyang Li"
                },
                {
                    "authorId": "2257101405",
                    "name": "Yihang Zhang"
                },
                {
                    "authorId": "2301200359",
                    "name": "Zhidong Jia"
                },
                {
                    "authorId": "2216800907",
                    "name": "Yannan Hu"
                },
                {
                    "authorId": "2301176195",
                    "name": "Lei Zhang"
                },
                {
                    "authorId": "2301169914",
                    "name": "Jianrong Zhang"
                },
                {
                    "authorId": "2301199102",
                    "name": "Yongming Xu"
                },
                {
                    "authorId": "2301175274",
                    "name": "Yong Cui"
                },
                {
                    "authorId": "2112582246",
                    "name": "Zongming Guo"
                },
                {
                    "authorId": "1714130",
                    "name": "Xinggong Zhang"
                }
            ],
            "abstract": "It is an interesting question Can and How Large Language Models (LLMs) understand non-language network data, and help us detect unknown malicious flows. This paper takes Carpet Bombing as a case study and shows how to exploit LLMs' powerful capability in the networking area. Carpet Bombing is a new DDoS attack that has dramatically increased in recent years, significantly threatening network infrastructures. It targets multiple victim IPs within subnets, causing congestion on access links and disrupting network services for a vast number of users. Characterized by low-rates, multi-vectors, these attacks challenge traditional DDoS defenses. We propose DoLLM, a DDoS detection model utilizes open-source LLMs as backbone. By reorganizing non-contextual network flows into Flow-Sequences and projecting them into LLMs semantic space as token embeddings, DoLLM leverages LLMs' contextual understanding to extract flow representations in overall network context. The representations are used to improve the DDoS detection performance. We evaluate DoLLM with public datasets CIC-DDoS2019 and real NetFlow trace from Top-3 countrywide ISP. The tests have proven that DoLLM possesses strong detection capabilities. Its F1 score increased by up to 33.3% in zero-shot scenarios and by at least 20.6% in real ISP traces.",
            "corpus_id": 269757388,
            "sentences": [],
            "relevance_judgement": 0.51123046875,
            "relevance_judgment_input_expanded": "# Title: DoLLM: How Large Language Models Understanding Network Flow Data to Detect Carpet Bombing DDoS\n# Venue: arXiv.org\n# Authors: Qingyang Li, Yihang Zhang, Zhidong Jia, Yannan Hu, Lei Zhang, Jianrong Zhang, Yongming Xu, Yong Cui, Zongming Guo, Xinggong Zhang\n## Abstract\nIt is an interesting question Can and How Large Language Models (LLMs) understand non-language network data, and help us detect unknown malicious flows. This paper takes Carpet Bombing as a case study and shows how to exploit LLMs' powerful capability in the networking area. Carpet Bombing is a new DDoS attack that has dramatically increased in recent years, significantly threatening network infrastructures. It targets multiple victim IPs within subnets, causing congestion on access links and disrupting network services for a vast number of users. Characterized by low-rates, multi-vectors, these attacks challenge traditional DDoS defenses. We propose DoLLM, a DDoS detection model utilizes open-source LLMs as backbone. By reorganizing non-contextual network flows into Flow-Sequences and projecting them into LLMs semantic space as token embeddings, DoLLM leverages LLMs' contextual understanding to extract flow representations in overall network context. The representations are used to improve the DDoS detection performance. We evaluate DoLLM with public datasets CIC-DDoS2019 and real NetFlow trace from Top-3 countrywide ISP. The tests have proven that DoLLM possesses strong detection capabilities. Its F1 score increased by up to 33.3% in zero-shot scenarios and by at least 20.6% in real ISP traces.\n",
            "reference_string": "[269757388 | Li et al. | 2024 | Citations: 8]"
        },
        {
            "title": "Your Large Language Models Are Leaving Fingerprints",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 25,
            "citation_count": 14,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.14057, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2115144895",
                    "name": "Hope McGovern"
                },
                {
                    "authorId": "2040711244",
                    "name": "Rickard Stureborg"
                },
                {
                    "authorId": "2283136281",
                    "name": "Yoshi Suhara"
                },
                {
                    "authorId": "71152801",
                    "name": "Dimitris Alikaniotis"
                }
            ],
            "abstract": "It has been shown that finetuned transformers and other supervised detectors effectively distinguish between human and machine-generated text in some situations arXiv:2305.13242, but we find that even simple classifiers on top of n-gram and part-of-speech features can achieve very robust performance on both in- and out-of-domain data. To understand how this is possible, we analyze machine-generated output text in five datasets, finding that LLMs possess unique fingerprints that manifest as slight differences in the frequency of certain lexical and morphosyntactic features. We show how to visualize such fingerprints, describe how they can be used to detect machine-generated text and find that they are even robust across textual domains. We find that fingerprints are often persistent across models in the same model family (e.g. llama-13b vs. llama-65b) and that models fine-tuned for chat are easier to detect than standard language models, indicating that LLM fingerprints may be directly induced by the training data.",
            "corpus_id": 269982474,
            "sentences": [],
            "relevance_judgement": 0.501953125,
            "relevance_judgment_input_expanded": "# Title: Your Large Language Models Are Leaving Fingerprints\n# Venue: arXiv.org\n# Authors: Hope McGovern, Rickard Stureborg, Yoshi Suhara, Dimitris Alikaniotis\n## Abstract\nIt has been shown that finetuned transformers and other supervised detectors effectively distinguish between human and machine-generated text in some situations arXiv:2305.13242, but we find that even simple classifiers on top of n-gram and part-of-speech features can achieve very robust performance on both in- and out-of-domain data. To understand how this is possible, we analyze machine-generated output text in five datasets, finding that LLMs possess unique fingerprints that manifest as slight differences in the frequency of certain lexical and morphosyntactic features. We show how to visualize such fingerprints, describe how they can be used to detect machine-generated text and find that they are even robust across textual domains. We find that fingerprints are often persistent across models in the same model family (e.g. llama-13b vs. llama-65b) and that models fine-tuned for chat are easier to detect than standard language models, indicating that LLM fingerprints may be directly induced by the training data.\n",
            "reference_string": "[269982474 | McGovern et al. | 2024 | Citations: 14]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "236477801",
            "title": "Perceptual Models of Machine-Edited Text",
            "text": "Detectability is a property of text in context, without regard for a reference. We evaluate language model scores, which measure fluency, as proxies for detectability. \n\nWe evaluate a Kneser-Ney 5-gram language model trained on a full English Wikipedia dump (Wikipedia contributors, 2020) with KenLM (Heafield, 2011). We estimate the model using modified Kneser-Ney smoothing without pruning. We also evaluate the language model score given by GPT-2, a large neural transformerbased language model trained on 8 million web pages (Radford et al., 2019). We use the technique described in Salazar et al. (2020) to obtain a BERT Masked Language Model (MLM) that accounts for the model's self-attention. We compute each language model score under two conditions: using only the edited sentence, and including one sentence before and after the edited sentence (+context). \n\nPredictions from BERT's Next Sentence Prediction (NSP) task estimate the likelihood for a sequence of sentences. This classifier is trained to discriminate sequences of two sentences found in the pretraining corpus from sequences drawn using negative sampling (Devlin et al., 2019).",
            "score": 0.5877960865087538,
            "section_title": "Measures of Detectability",
            "char_start_offset": 21204,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 167
                },
                {
                    "start": 170,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 866
                },
                {
                    "start": 869,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1151
                }
            ],
            "ref_mentions": [
                {
                    "start": 300,
                    "end": 316,
                    "matchedPaperCorpusId": "8313873"
                },
                {
                    "start": 529,
                    "end": 551,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 587,
                    "end": 608,
                    "matchedPaperCorpusId": "218628872"
                },
                {
                    "start": 1129,
                    "end": 1150,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33740234375
        },
        {
            "corpus_id": "257389458",
            "title": "Deep Study of CRF Models for Speech understanding in Limited Task",
            "text": "Spoken Language Understanding is an important component in spoken dialogue systems. It aims to extract concepts from an utterance to clarify speech meaning. Therefore, the key link in an automatic understanding process revolves around the correspondence between the set of words in the utterance and the set of semantic concepts. In order to resolve this correspondence, the first research works in this field exploited linguistic formalisms such as regular grammars and context-free grammars. Recent works have rather oriented towards the exploitation of machine learning models for concept detection, these models are widely used for the semantic annotation of speech utterances. \n\nOur overview of the literature showed that learning models constitute the dominant context for speech understanding due to the performances recorded particularly in restricted domains. These models enjoy several advantages reported by [1] [2] [3]. Indeed, the intervention of a human expert is limited to the labeling of data, which represents an easier task than the modeling of grammars or patterns. Moreover, these models offer better portability since they are domain and language independent. However, the effectiveness of these models is sensitive to the used corpus, which must be representative and large, in order to determine their parameters [4]. \n\nMachine learning models are classified into generative and discriminative models [2] and they are widely applied to speech understanding. HMM (Hidden Markov Models) is an example of generative models and they are used by [5] for speech understanding of Spanish language, using the DIHANA corpus. The DIHANA corpus task deals with requests of information about railway services. This work uses HMM in the most realistic situations where dialogues are not segmented into utterances. The results of their work are very important. They obtained 92% as F-measure. This good result is due to the large size of the corpus used for training models. \n\nIn the literature, several studies show that discriminant models perform better than generative [6] [2] [7]. CRF models (Conditional Random Fields), as an example of discriminant models, have been widely exploited in many tasks in natural language processing such as semantic annotation and syntactic analysis [8], [9] [10]. A particular distinction is reported for CRF models whose performance exceeded that of other models, [11] [2] [12].",
            "score": 0.5386300645299766,
            "section_title": "I. INTRODUCTION AND RELATED WORKS",
            "char_start_offset": 36,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 681
                },
                {
                    "start": 684,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1341
                },
                {
                    "start": 1344,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 1984
                },
                {
                    "start": 1987,
                    "end": 2095
                },
                {
                    "start": 2096,
                    "end": 2311
                },
                {
                    "start": 2312,
                    "end": 2427
                }
            ],
            "ref_mentions": [
                {
                    "start": 1337,
                    "end": 1340,
                    "matchedPaperCorpusId": "18142929"
                },
                {
                    "start": 1425,
                    "end": 1428,
                    "matchedPaperCorpusId": "9769108"
                },
                {
                    "start": 1565,
                    "end": 1568,
                    "matchedPaperCorpusId": "16376600"
                },
                {
                    "start": 2083,
                    "end": 2086,
                    "matchedPaperCorpusId": "6091797"
                },
                {
                    "start": 2091,
                    "end": 2094,
                    "matchedPaperCorpusId": "26069779"
                },
                {
                    "start": 2302,
                    "end": 2305,
                    "matchedPaperCorpusId": "17226949"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2152099609375
        },
        {
            "corpus_id": "264451585",
            "title": "Detecting Pretraining Data from Large Language Models",
            "text": "To determine whether a text X is in the pretraining data of a LLM such as GPT, MIN-K% PROB first gets the probability for each token in X, selects the k% tokens with minimum probabilities and calculates their average log likelihood. If the average log likelihood is high, the text is likely in the pretraining data. \n\nreducing the potential memorization required for successful MIAs (Leino & Fredrikson, 2020;Kandpal et al., 2022). Besides, previous methods often rely on one or more reference models (Carlini et al., 2022;Watson et al., 2022) trained in the same manner as the target model (e.g., on the shadow data sampled from the same underlying pretraining data distribution) to achieve precise detection. This is not possible for large language models, as the training distribution is usually not available and training would be too expensive. \n\nOur first step towards addressing these challenges is to establish a reliable benchmark. We introduce WIKIMIA, a dynamic benchmark designed to periodically and automatically evaluate detection methods on any newly released pretrained LLMs. By leveraging the Wikipedia data timestamp and the model release date, we select old Wikipedia event data as our member data (i.e, seen data during pretraining) and recent Wikipedia event data (e.g., after 2023) as our non-member data (unseen). Our datasets thus exhibit three desirable properties: (1) Accurate: events that occur after LLM pretraining are guaranteed not to be present in the pretraining data. The temporal nature of events ensures that non-member data is indeed unseen and not mentioned in the pretraining data. (2) General: our benchmark is not confined to any specific model and can be applied to various models pretrained using Wikipedia (e.g., OPT, LLaMA, GPT-Neo) since Wikipedia is a commonly used pretraining data source. (3) Dynamic: we will continually update our benchmark by gathering newer non-member data (i.e., more recent events) from Wikipedia since our data construction pipeline is fully automated.",
            "score": 0.5318146720691732,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1791,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 315
                },
                {
                    "start": 318,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 2026
                }
            ],
            "ref_mentions": [
                {
                    "start": 383,
                    "end": 409,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 409,
                    "end": 430,
                    "matchedPaperCorpusId": "246823128"
                },
                {
                    "start": 501,
                    "end": 523,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 523,
                    "end": 543,
                    "matchedPaperCorpusId": "244130249"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.444091796875
        },
        {
            "corpus_id": "214605571",
            "title": "TNT-KID: Transformer-based neural tagger for keyword identification",
            "text": "Our approach relies on a transfer learning technique (Howard & Ruder 2018;Devlin et al. 2018), where a neural model is first pretrained as a language model on a large corpus. This model is then fine-tuned for each specific keyword detection task on each specific manually labeled corpus by adding and training the token classification head described in the previous section. With this approach, the syntactic and semantic knowledge of the pretrained language model is transferred and leveraged in the keyword detection task, improving the detection on datasets that are too small for the successful semantic and syntactic generalization of the neural model. \n\nIn the transfer learning scenario, two distinct pretraining objectives can be considered. First is the autoregressive language modelling where the task can be formally defined as predicting a probability distribution of words from the fixed size vocabulary V , for word w t , given the historical sequence w 1:t-1 = [w 1 , ..., w t\u22121 ]. This pretraining regime was used in the GPT-2 model (Radford et al. 2019) that we modified. Since in the standard transformer architecture self-attention is applied to an entire surrounding context of a specific word (i.e., the words that appear after a specific word in each input sequence are also used in the self-attention calculation), we employ obfuscation masking to the right context of each word when the autoregressive language model objective is used, in order to restrict the information only to the prior words in the sentence (plus the word itself) and prevent target leakage (see Radford et al. (2019) for details on the masking procedure). \n\nAnother option is a masked language modelling objective, first proposed by Devlin et al. (2018). Here, a percentage of words from the input sequence is masked in advance, and the objective is to predict these masked words from an unmasked context. This allows the model to leverage both left and right context, or more formally, the token w t is also determined by sequence of tokens w t+1:n = [w t+1 , ..., w t+n ].",
            "score": 0.5204356573180369,
            "section_title": "Transfer learning",
            "char_start_offset": 25851,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 657
                },
                {
                    "start": 660,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1652
                },
                {
                    "start": 1655,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2071
                }
            ],
            "ref_mentions": [
                {
                    "start": 1049,
                    "end": 1069,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1592,
                    "end": 1613,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.322265625
        },
        {
            "corpus_id": "260379022",
            "title": "Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?",
            "text": "recent work [31] demonstrating that this challenge is only going to get exacerbated with newer, larger, more capable LLMs. There are existing detection methods, such as feature-based classifiers [20], methods that differentiate human and AI text using statistical measures [14; 26], along with methods that use fine-tuned language models to classify text as human or AI [38; 33]. In this work, however, we want to investigate the capability of ChatGPT (GPT3.5, and the more advanced  to differentiate between human and AI-generated text. ChatGPT has been shown to perform well on a variety of NLP and NLU tasks [28], with newer versions of the models being able to use human prompts better than older ones. In order to probe the performance of ChatGPT on detecting AI-generated text, we propose to investigate the following research question: Can ChatGPT identify AI-generated text from a variety of generators? The rest of the paper is organized as follows: Section 2 provides a brief overview of LLMs and how they work, Section 3 elaborates on our experiments and experimental settings. Section 4 presents and discusses our results. Section 5 grounds our work in the context of related work, and finally, Section 6 concludes with a discussion of future directions. scale corpora of data. Broadly, LLMs can be of two types: (i) autoregressive (or causal) language models, and (ii) masked language models. Autoregressive LLMs (such as the GPT family of models) are trained to predict the next word or token, given the previous token sequence in a sentence. Formally, at time step t, the model samples a token from the distribution p(xt|x1, ..., xt\u22121), based on some pre-defined sampling strategy, and forms the text sequence one token at a time. Sampling strategies may be greedy, top-k, nucleus sampling [19], etc., with the latter two being used commonly in recent LLMs. Masked language models (such as the BERT family of models [11]) are trained on cloze test tasks. Given an input text sequence, k% of the tokens are masked using a special [MASK] token, and the LLM is",
            "score": 0.5158280944374402,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1955,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.484130859375
        },
        {
            "corpus_id": "243865212",
            "title": "Semantic Novelty Detection in Natural Language Descriptions",
            "text": "For dataset details, please refer to Sec. 3. Appendix has additional information about the data and model implementation details. 5Baselines. We compare GAT-MA with three categories of baselines: (1) four language model based novelty detection models, (2) seven one-class classification models, (3) other models based on different text encoders and loss functions (see Sec. 5.2). All the results in this section are the average of five runs with different seeds. The results are statistically significant with p < 0.001. \n\nA trained language model (LM) can be intuitively used as a novelty detection model due to the following reasons: (1) When training a LM on normal scene descriptions, the model minimizes the perplexity of the training data by maximizing the likelihood of each word appearing in its context. In this way, it indirectly learns the semantic meaning of words and sentences. ( 2) Each LM trained on normal descriptions can output the probability of each word in a description appearing in its context. Thus a sentence probability can be calculated from the list of word probabilities. We have tried various ways of calculating the sentence score from the word probability list, such as arithmetic mean, geometric mean, harmonic mean, and multiplication of all word probabilities and found harmonic mean to be the best choice. We use N-gram, the bag of words LM, N \u2208 {1, 2, 3, 4, 5} (N = 1 gives the best result), LSTM (Hochreiter and Schmidhuber, 1997), BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019) as our LM baselines. The results are listed in Table 2. \n\nFor general one-class classification models, most of them only work on images. We modified the related components of the models to make them suitable for text data. More details regarding model modification and parameter setting are provided in Appendix Sec. F. The following 7 baselines are compared: (1) DSVDD (Deep SVDD) (Ruff et al., 2018): a recent one-class classifier, which is the deep learning version of SVDD (see Sec. 2).",
            "score": 0.5155176446090101,
            "section_title": "Experiment Setup",
            "char_start_offset": 26886,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 520
                },
                {
                    "start": 523,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1583
                },
                {
                    "start": 1586,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2018
                }
            ],
            "ref_mentions": [
                {
                    "start": 1435,
                    "end": 1469,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1476,
                    "end": 1497,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4296875
        },
        {
            "corpus_id": "269004895",
            "title": "Linguistic Changes in Spontaneous Speech for Detecting Parkinsons Disease Using Large Language Models",
            "text": "We have demonstrated that the state-of-the-art large language models can detect PD with up to 73% accuracy using a linguistic feature space generated with large language models.We show that the text-embedding-3 models, outperform the other models.This finding is consistent with the benchmarked performance of all of the models across a variety of tasks (Muennighoff et al., 2022).\n\nThe previous research for PD detection with large language models, specifically BERT, is only 66% accurate with our implementation and with the dataset that we used (Escobar-Grisales et al., 2023).\n\nWe demonstrate that the text-embedding-3 models surpass BERT across performance benchmarks.\n\nThe performance metrics for text-embedding-3 are largely independent of the dimensionality of the embedding output.In particular, even with the dimensionality reduced to 768 to match BERT, the performance metrics are still better.Therefore, we conclude that the better performance with the use of the state-of-the-art models is due to the intrinsic architecture of the large language models and not due to the increased dimensionality.\n\nComparison between different detection methods is difficult due to the use of proprietary datasets and insufficient implementation details.We aim to overcome these limitations by using a dataset that is in the public domain, providing implementation details, and making our source code available upon request.This enables other researchers to replicate our methods and implementation.",
            "score": 0.5101122198941723,
            "section_title": "Discussion",
            "char_start_offset": 8837,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 177,
                    "end": 247
                },
                {
                    "start": 247,
                    "end": 381
                },
                {
                    "start": 383,
                    "end": 580
                },
                {
                    "start": 582,
                    "end": 673
                },
                {
                    "start": 675,
                    "end": 790
                },
                {
                    "start": 790,
                    "end": 905
                },
                {
                    "start": 905,
                    "end": 1110
                },
                {
                    "start": 1112,
                    "end": 1251
                },
                {
                    "start": 1251,
                    "end": 1421
                },
                {
                    "start": 1421,
                    "end": 1496
                }
            ],
            "ref_mentions": [
                {
                    "start": 548,
                    "end": 579,
                    "matchedPaperCorpusId": "259729070"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40185546875
        },
        {
            "corpus_id": "263605550",
            "title": "On the Generalization of Training-based ChatGPT Detection Methods",
            "text": "In Table 3, we report the performance of trained classification models, which are based on model architectures RoBERTa-base, RoBERTa-large and T-5. We also include representative \"similarity-based\" methods DNA-GPT (Yang et al., 2023) and GPT-PAT (Chen et al., 2023), and \"score-based\" methods including GLTR (Gehrmann et al., 2019)  1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 0.99 0.95 0.97 0.94 RoBERTa-b 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 0.98 0.99 0.98 RoBERTa-l 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 T-5 1.00 1.00 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 0.99 1.00 1.00 0.98 0.98 0.96 \n\nThe training-based methods present extraordinary \"in-distribution\" detection performance. This motivates us to have a further exploration on their generalization performance under out-of-distribution scenarios. In the following, we design experiments to analyze them when the training data cannot fully cover the distribution of test data. Our analysis contains two major scenarios. In Section 4, we consider the scenario that the model trainer aims to detect the texts from their interested language tasks and topics. In this case, the possible distribution shifts can be due to the variation of prompts and lengths. In Section 5, we discuss the cases that the models encounter unforeseen tasks or topics.",
            "score": 0.5075951401104997,
            "section_title": "IN-DISTRIBUTION EVALUATION",
            "char_start_offset": 12178,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1385
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0849609375
        },
        {
            "corpus_id": "270370917",
            "title": "Improving Logits-based Detector without Logits from Black-box LLMs",
            "text": "Our task is to detect whether the given input passage x = [x 1 , ..., x L ] (L is the sequence length) is produced by an AI model f tar or a human, which can be considered as a binary classification task. Typically, there are two different task settings for LLM detection, namely white-box and black-box detection. In the black-box setting, we only have access to the generated text, treating the language model as a \"black box\" where we input text and receive output without knowing the internal workings or probabilities. In the white-box setting, we have additional information about the model, specifically the output probabilities p(x l |x [1:l\u22121] ) for each token at each position l in the text. However, in the practical scenario, it is usually difficult to get access to the source model, especially widespread but closed-source models such as ChatGPT, GPT-4 and Claude-3. Therefore, we focus on improving the black-box detection without any access to the source model logits in our setting.",
            "score": 0.5000234060481228,
            "section_title": "Task and Settings",
            "char_start_offset": 8378,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 999
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.177978515625
        },
        {
            "corpus_id": "269922223",
            "title": "RoBERTa-Augmented Synthesis for Detecting Malicious API Requests",
            "text": "Among the various NLP tasks supported by MLM, the fill-mask task allows a language model to infer and predict the masked token in a sentence using the surrounding context Kesgin and Amasyali (2023). \n\nThe proposed framework (depicted in Figure 1) comprises two modules: a datastore generator designed to augment sparse datasets by synthesizing realistic API requests, and a downstream anomaly detection module trained on the extended dataset that includes both original and generated samples. The first module leverages a RoBERTa-based architecture to generate realistic API requests from limited data, using token masking guided by structural and semantic constraints. Once the synthetic data is validated, it is used to train the second module, an anomaly detection model capable of classifying API requests as benign or malicious.",
            "score": 0.49869843999556135,
            "section_title": "Framework",
            "char_start_offset": 20055,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 201,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 833
                }
            ],
            "ref_mentions": [
                {
                    "start": 171,
                    "end": 197,
                    "matchedPaperCorpusId": "266741990"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31640625
        },
        {
            "corpus_id": "235677188",
            "title": "On the Correlation of Context-Aware Language Models With the Intelligibility of Polish Target Words to Czech Readers",
            "text": "Language models are machine learning models that are typically trained on text corpora and can predict the probability of a word given its context. As an example, an LM trained on a standard English corpus, given the start of the sentence A small, green would assign most likely the word frog a higher probability for continuing the sentence than the word cow. The probability for a target word, given its context, is obtained via a learned model that bases its predictions on occurrence statistics in the training corpus. \n\nMost commonly, an LM predicts the probability of a word given the previous (left) context. Formally, for a sentence s consisting of words or tokens w 1 , ..., w n , an LM computes the probability p(w t |w t\u22121 , ..., w 0 ). The probability of a sentence can be obtained by factorizing the joint probability as a product of conditional probabilities, i.e., by applying the product rule of probabilities: \n\nTraditionally, count-based n-gram models have been used for language modeling. In this case, the previous context is limited to n \u2212 1 words. A 3-g model, therefore, can only compute the probability of a word given its two predecessors, i.e., p(w t |w t\u22121 , w t\u22122 ). Increasing the value of n for count-based models is difficult due to factors like data sparsity (Jelinek and Mercer, 1980).",
            "score": 0.49506449754752213,
            "section_title": "Language Modeling",
            "char_start_offset": 17997,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 522
                },
                {
                    "start": 525,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 926
                },
                {
                    "start": 929,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1318
                }
            ],
            "ref_mentions": [
                {
                    "start": 1291,
                    "end": 1317,
                    "matchedPaperCorpusId": "61012010"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.163818359375
        },
        {
            "corpus_id": "16763786",
            "title": "A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser Ney Smoothing",
            "text": "Language Models are a probabilistic approach for predicting the occurrence of a sequence of words. They are used in many applications, e.g. word prediction [1], speech recognition [19], machine translation [2], or spelling correction [17]. The task language models attempt to solve is the estimation of a probability of a given sequence of words w l 1 = w 1 , . . . , w l . The probability P (w l 1 ) of this sequence can be broken down into a product of conditional probabilities: methodology and introducing the data sets in Section 4 we will present the results of our evaluation in Section 5. In Section 6 we discuss why a generalized language model performs better than a standard language model. Finally, in Section 7 we summarize our findings and conclude with an overview of further interesting research challenges in the field of generalized language models.",
            "score": 0.49506449754752213,
            "section_title": "Introduction motivation",
            "char_start_offset": 26,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 867
                }
            ],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 159,
                    "matchedPaperCorpusId": "6178017"
                },
                {
                    "start": 206,
                    "end": 209,
                    "matchedPaperCorpusId": "14386564"
                },
                {
                    "start": 234,
                    "end": 238,
                    "matchedPaperCorpusId": "39634210"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2232666015625
        },
        {
            "corpus_id": "271083368",
            "title": "Achieving Peak Performance for Large Language Models: A Systematic Review",
            "text": "Language modeling is a fundamental approach to enhancing the ability of machines to understand and process human language. It is a computational model that can learn and predict the possibilities of incoming (or missing) tokens [24]. \n\nThe development of language models can be classified as follows (see Fig. 3): \n\n\u2022 N-gram language models, like bigrams and trigrams, are basic methods that learn from the frequency of word sequences in text [36], [37]. However, their limited context window restricts their ability to capture longrange dependencies and understand the deeper semantic relationships between words. \u2022 Markov assumption language models, refers to those models that predict the next word based on the most recent in the context [24]. Both n-gram and Markov assumption language models are commonly used to improve task performance in NLP, and information retrieval (IR) [38]. \u2022 Machine learning models, these models investigate machine learning algorithms to enhance language comprehension. They are trained on extensive text corpora to discern patterns and relationships [39]. The adoption of machine learning in NLP introduced a more advanced methodology, enabling the creation of applications such as spam detection [40] and sentiment analysis [41]. \u2022 Neural language models, these models are developed based on NN for working with a sequence of data. \n\nThey have a special ability of learning effective features for words or sentences. These studies [42]- [44] have initiated the use of language models for representation learning (beyond word sequence modeling), and show Markov assumption LM: \n\nHMMs.",
            "score": 0.49365788299679103,
            "section_title": "II. LANGUAGE MODELING DEVELOPMENT",
            "char_start_offset": 18960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 233
                },
                {
                    "start": 236,
                    "end": 313
                },
                {
                    "start": 316,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1367
                },
                {
                    "start": 1370,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1611
                },
                {
                    "start": 1614,
                    "end": 1619
                }
            ],
            "ref_mentions": [
                {
                    "start": 449,
                    "end": 453,
                    "matchedPaperCorpusId": "10463701"
                },
                {
                    "start": 883,
                    "end": 887,
                    "matchedPaperCorpusId": "11338792"
                },
                {
                    "start": 1232,
                    "end": 1236,
                    "matchedPaperCorpusId": "206988459"
                },
                {
                    "start": 1467,
                    "end": 1471,
                    "matchedPaperCorpusId": "351666"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35107421875
        },
        {
            "corpus_id": "267042566",
            "title": "Development and use of machine learning algorithms in vaccine target selection",
            "text": "The use of the same small filter enables the detection of localized features and the equivariance of predictions (i.e., when input features are translated the output of the convolutional layer is translated consistently), which ensures that feature detection is robust to its exact position. Decision tree: ML algorithm generating a tree-like structure through a series of decisions based on the input features, which serve to obtain the final classification or regression prediction, see for example Fig. 4a. One of the most popular applications is within methods that train ensembles of decision trees and combine their predictions to gain robustness and generalization power, like random forests. Language models: ML architectures developed to model relationships in language data, like sentences, used for language processing tasks such as machine translation, keyword recognition and text generation. Language models are currently widely adopted in protein modeling, where protein sequences are treated in analogy to sequences of text symbols. One of the most powerful language model architectures is the transformer, a neural network which processes sequences of symbols by alternating attention-mechanism layers and non-linear transformations. The transformer is increasingly preferred to other established language models, like Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, due to its ability to effectively capture long-range dependencies between symbols and hence to produce fully contextual representations. Attention mechanism: after its introduction in the transformer architecture, it has become a key building block of language models and other deep learning architectures. The attention mechanism assigns to each input component (like a text symbol) a score quantifying its relevance to the context of the other input components, based on the statistical dependencies detected. The set of these scores for the different input components forms an attention map (see for example Fig. 4c). Generative models: models that perform density estimation, i.e., they reconstruct the probability distribution from which the data have been generated, supporting the design of synthetic data by sampling from the learnt distribution. For sequence data, generative models range from probability distributions obtained simply from the frequencies of symbols at each position (independentsite models, see for example Fig. 2a), to probability distributions specified by shallow ML architectures like RBMs or by deep generative language models based on transformers, RNNs and LSTMs.",
            "score": 0.49270911495919656,
            "section_title": "Box 3 ML architectures",
            "char_start_offset": 16958,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2267
                },
                {
                    "start": 2268,
                    "end": 2611
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2254638671875
        },
        {
            "corpus_id": "218486757",
            "title": "A Girl Has A Name: Detecting Authorship Obfuscation",
            "text": "Given a word sequence, language models are designed to predict the next word. They do this by building contextual models of word occurrences as probability distributions over the full vocabulary. \n\nThen some heuristic is used to pick the next word e.g., select the word with the highest probability. In our case, instead of word prediction, we extract the likelihood from the language model (either as a probability or as a rank) for each word in the text given its context. \n\nThe language model has a critical role. Thus, we use neural language models with deep architectures and trained on large amounts of data which are better at identifying both long-term and short-term context. In order to imitate an adversary who may not have the significant resources needed to train such models, we use off-the-shelf pre-trained neural language models. Specifically, we choose well-known context-aware neural language models GPT-2 (Radford et al., 2019) and BERT (Devlin et al., 2018). We choose both as they use different approaches. GPT-2 has been shown to perform better than BERT (Gehrmann et al., 2019) at synthetic text detection, with word rank giving higher performance than word probability. Their relative merit for obfuscation detection is unknown. \n\n1) GPT-2. GPT-2 released by Open AI in 2019 uses at its core, a variation of the \"transformer\" architecture, an attention based model (Vaswani et al., 2017) and is trained on text from 45 million outbound links on Reddit (40 GB worth of text). We use GPT-2 to compute the conditional probability for word i as p(w i |w 1...i\u22121 ). The position of w i in the sorted list (descending order of probability) of vocabulary words gives the word rank. The authors (Radford et al., 2019) trained four versions of GPT-2 differing in architecture size. Of these, we used the small and medium versions containing 117M and 345M parameters, respectively. The authors eventually also released a large version containing 762M parameters and a very large version containing 1542M parameters. 2 We did not use 2) BERT. BERT released by Google in 2018 is also based on \"Transformers\".",
            "score": 0.4904683100903239,
            "section_title": "Word likelihood extraction",
            "char_start_offset": 9590,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 195
                },
                {
                    "start": 198,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 474
                },
                {
                    "start": 477,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1253
                },
                {
                    "start": 1256,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2056
                },
                {
                    "start": 2057,
                    "end": 2121
                }
            ],
            "ref_mentions": [
                {
                    "start": 925,
                    "end": 947,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1390,
                    "end": 1412,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1712,
                    "end": 1734,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.485107421875
        },
        {
            "corpus_id": "261697402",
            "title": "Studying the impacts of pre-training using ChatGPT-generated text on downstream tasks",
            "text": "The remainder of the research proposal is structured as follows, Chapter 2 begins by providing a theoretical background on different language models and the training process involved. It also reviews existing research on the detection of artificial text generated by language models and discusses the current state of research on identifying bias in language models. Chapter 3 focuses on the methodology employed in the study.\n\nIt outlines the data generation process using ChatGPT and describes the methods used to compare the language of CNN journalists with that of ChatGPT. Additionally, it explains the process of pre-training the language model, along with the techniques used for fine-tuning and evaluating the trained models in terms of performance and bias metrics. Chapter 4 presents the results obtained from the experiments conducted in the study. Chapter 5 involves a comprehensive discussion of the results and explores the limitations encountered during the research and suggests potential directions for future research that can be pursued. The Chapter 6 addresses the ethical issues and environmental impacts associated with training language models in the study. Finally, we conclude our research questions and thesis in Chapter 7.",
            "score": 0.490161307119962,
            "section_title": "Research Outline",
            "char_start_offset": 8098,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.463623046875
        },
        {
            "corpus_id": "236253294",
            "title": "Applying NLP techniques to malware detection in a practical environment",
            "text": "Section 4 describes our NLP-based detection model. Section 5 evaluates our model with the dataset. Section 6 discusses the performance and research ethics. Finally, Section 7 concludes this study.",
            "score": 0.490161307119962,
            "section_title": "Introduction",
            "char_start_offset": 2452,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 51,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 196
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0472412109375
        },
        {
            "corpus_id": "246823897",
            "title": "What Does it Mean for a Language Model to Preserve Privacy?",
            "text": "Language models (LMs) are essential components of state-of-the-art natural language processing pipelines, and refer to systems that are mainly trained on a large corpus of text for word sequence prediction tasks. More precisely, a language model is optimized to learn the occurrence probability of tokens1 in any sequence, based on the co-occurrence of tokens in the training data. The ultimate objective is to find the relation between a token and its preceding or surrounding segments. To this end, language models extract various statistics and correlations from sequences of words, at the level of sentences or paragraphs. \n\nThe current trends of language modeling also shows that aggressive data collection and training enormous models are crucial for improving the performance of LMs. State of the art algorithms based on large neural networks enable effective extraction and encoding of a vast number of statistics about the training corpus, and have achieved unprecedented performance on a wide range of applications. The pervasive application of LMs and ever-larger datasets needed to train them pose serious privacy concerns.",
            "score": 0.490161307119962,
            "section_title": "Background on Language Models",
            "char_start_offset": 5307,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 626
                },
                {
                    "start": 629,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1135
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13623046875
        },
        {
            "corpus_id": "254220840",
            "title": "SOLD: Sinhala Offensive Language Dataset",
            "text": "For the sentence-level, we used a filtering technique to filter the unannotated instances because the benefits of data augmentation can be hampered by noise in initial model predictions. We selected the three best sentence-level offensive language detection models from Section 5; XLM-R [83], XLM-T [116], and CNN with fastText [112]. For each instance in SemiSOLD, we calculated the standard deviation of the confidences of these three models for the positive class, which corresponds to the uncertainty of the models. We used different threshold values for model uncertainty to filter the data from SemiSOLD. For the labels, we compute an aggregated single prediction based on the average predicted by each of the above-mentioned models. If the average is greater than 0.5, we label the instance as offensive, and not offensive otherwise. \n\nWe used three threshold values; 0.05, 0.1 and 0.15. For each threshold value, we filter the instances in SemiSOLD and augment it to the training set of SOLD. We train the same ML models we experimented with in Section 5 on the augmented training set. We evaluated the results on the testing set of SOLD. The results are shown in Table 8. \n\nAs shown in the results, all the models benefitted from semi-supervised data augmentation. The best result was produced by XLM-R with a 0.1 threshold. We discover two key observations from the results. (1) Models only improve with 0.05 and 0.1. Despite having more instances in the 0.15 threshold, it does not improve the results in many ML models. This is mainly because the 0.15 threshold adds a large number of uncertain noisy instances to the training set, and ML models find it difficult to learn from these instances. \n\n(2) Smaller and lightweight models such as BiLSTM and CNN show notable improvements with data augmentation compared to large transformer models. This is similar to the previous experiments in data augmentation [37] where the results do not improve when the machine learning classifier is already strong. We can assume that the transformer models are already well trained for SOLD, and adding further instances to the training process would not improve the results for the transformer models.",
            "score": 0.48675502378987434,
            "section_title": "Sentence-level Offensive Language Detection",
            "char_start_offset": 52867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 840
                },
                {
                    "start": 843,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1180
                },
                {
                    "start": 1183,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1706
                },
                {
                    "start": 1709,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2200
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 291,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 299,
                    "end": 304,
                    "matchedPaperCorpusId": "248693350"
                },
                {
                    "start": 328,
                    "end": 333,
                    "matchedPaperCorpusId": "207556454"
                },
                {
                    "start": 1919,
                    "end": 1923,
                    "matchedPaperCorpusId": "216868486"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.043853759765625
        },
        {
            "corpus_id": "269517985",
            "title": "Commit Classification into Maintenance Activities Using In-Context Learning Capabilities of Large Language Models",
            "text": "We start this section with an overview of maintenance activity categories considered in this work.Then, we introduce in-context learning and GPT, a large language model that has been illustrating revolutionary capabilities in different natural language processing (NLP) tasks, especially text classification.Finally, we explain statistical methods used in this study to evaluate classification performance.",
            "score": 0.4853129009429891,
            "section_title": "BACKGROUND",
            "char_start_offset": 6333,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 98,
                    "end": 308
                },
                {
                    "start": 308,
                    "end": 406
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.279052734375
        },
        {
            "corpus_id": "259129855",
            "title": "On the Detectability of ChatGPT Content: Benchmarking, Methodology, and Evaluation through the Lens of Academic Writing",
            "text": "Based on the design philosophy, we further categorize them into three groups: (a) Pre-trained detectors. We directly adopt the trained models: HC3-Perplexity (HC3-PPL) [41], HC3-GLTR [41], HC3-Roberta (HC3-RBT) [41], OpenAI-Roberta (OpenAI-RBT) [86,101]. (b) Statistics-based detectors. They analyze the statistical differences between LLM and human-written text: Histogram-of-Likelihood Ranks (HLR) [36], Rank [81], Log-Rank [81], Total Probability (TP) [101] Perplexity (PPL) [41], Entropy   [36,81], DetectGPT [81]. (c) Fine-tuned Language Models. They finetune the pre-trained language models for detection: BERT [53], DistillBERT [43], and RoBERTa [41,70,76,116]. We also include GPT-2, which is even larger with 355M parameters. Please refer to Appendix B.1 for more details. \n\nThe statistics-and fine-tuning-based detectors are trained/tuned with the entire training set in GPABench2. We evaluate all the detectors on GPABench2 with samples generated by Prompt 1. From the results presented in Table 2, we observe the following: (1) The pre-trained detectors work poorly on GPABench2. The failure of the OpenAI detector can be attributed to the discrepancy in the target model, as it was designed for GPT-2. For HC3 detectors, their poor performance indicates the ineffectiveness of statistics-based methods (HC3-PPL, HC3-GLTR) and the limited transferability of RoBERTa (HC3-RBT). (2) The statistics-based detectors provide satisfactory performance in Task 1, however, they become mostly ineffective in Tasks 2 and 3.",
            "score": 0.4853129009429891,
            "section_title": "Benchmarking Online and Open-source ChatGPT Detectors",
            "char_start_offset": 13760,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 781
                },
                {
                    "start": 784,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1525
                }
            ],
            "ref_mentions": [
                {
                    "start": 400,
                    "end": 404,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 494,
                    "end": 498,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 617,
                    "end": 621,
                    "matchedPaperCorpusId": "218560609"
                },
                {
                    "start": 660,
                    "end": 663,
                    "matchedPaperCorpusId": "264300609"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.278564453125
        },
        {
            "corpus_id": "258187051",
            "title": "UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining",
            "text": "We put our above findings into practice by training a suite of \"umT5\" models over a trillion tokens. For these final models, we also update the training corpus and add analysis comparing to mT5, which is the most direct point of comparison. As large language models are increasingly used for knowledge-intensive tasks (Petroni et al., 2019;Roberts et al., 2020;Petroni et al., 2021), it is important that training corpora are up-to-date. Given that the mC4 corpus is over two years old, we update the corpus (update version 3.1.0) to cover crawled documents through August 2022. \n\nBeyond adding fresh documents, we make three changes to mC4. First, as in Section \u00a74.1, we raise the language detection confidence threshold from 0.7 to 0.95, which we found to increase accuracy and reduce documents with little or no natural language. Second, we adjust the mC4 bad word filters to be soft filters, allowing a random 0.1% of documents with bad words to pass through. This ensures that models trained on our corpus will have at least a minimal exposure to any term, which is likely to help on tasks like toxicity detection. Finally, we remove from mC4's bad words lists any term that results in filtering >10% of documents from the language in question. 4 Applying these changes, our resulting training corpus consists of 28.8 trillion characters from 9.0 billion documents-a 35% increase in documents over mC4. This increase is due primarily to added documents, and persists despite the more aggressive filtering during language detection. See Table 8 for full corpus statistics. \n\nWe closely follow the training setup and evaluation tasks from the mT5 paper for a fair comparison. \n\nTable 3 shows that umT5 outperforms mT5 on most tasks, across all sizes, and particularly at the largest size. 5 See Appendix E for an additional ablation isolating the effect of the data refresh.",
            "score": 0.48436005572557767,
            "section_title": "UMT5 MODELS",
            "char_start_offset": 22482,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 578
                },
                {
                    "start": 581,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1576
                },
                {
                    "start": 1579,
                    "end": 1678
                },
                {
                    "start": 1681,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1877
                }
            ],
            "ref_mentions": [
                {
                    "start": 318,
                    "end": 340,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 340,
                    "end": 361,
                    "matchedPaperCorpusId": "211205183"
                },
                {
                    "start": 361,
                    "end": 382,
                    "matchedPaperCorpusId": "221507798"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.114990234375
        },
        {
            "corpus_id": "262465111",
            "title": "From Text to Source: Results in Detecting Large Language Model-Generated Content",
            "text": "The lack of a discernible pattern in the cross-model detection performance across different model families may be attributed to the extensive overlap in their pretraining data, with a predominant reliance on ThePile (Gao et al., 2020) dataset or its subsets across most models, supplemented by Common Crawl as the primary data source. Consequently, the primary distinguishing factor among these models lies in their respective data cleaning pipelines. \n\nInfluence of Conversational Finetuning Our experiments reveal a clear pattern in the cross-model detection results, as shown in Figure 4. Specifically, a classifier trained on text generated by chat models exhibits limited capability in detecting normal language models (LMs). However, it demonstrates improved performance when tasked with detecting other chat models. Notably, when trained on LLaMA-2-70b-chat data, the classifier achieves the highest scores, albeit with a slight decline in detection accuracy when tested on chat models. This observation suggests that the LLaMA-2-70b-chat model effectively follows instructions to continue input text. Surprisingly, training the classifier on vanilla LM output also yields commendable results in detecting these distinct model categories. These findings underscore the nuanced relationship between chat models and traditional language models in the context of detection.",
            "score": 0.48404378554336486,
            "section_title": "Cross-Model Detection Results",
            "char_start_offset": 14826,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 451
                },
                {
                    "start": 454,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1377
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54541015625
        },
        {
            "corpus_id": "275212524",
            "title": "Alzheimer's disease detection based on large language model prompt engineering",
            "text": "The data set is then fed into the back-end classifier, which achieves an 88% classification accuracy. In the literature [9] , a Google VGGish model, pre-trained through Google's AudioSet, is employed to transform audio input features into linguistically and semantically meaningful high-level 128-dimensional embeddings. Wav2Vec 2.0, as detailed in literature [4][10], is a self-supervised learning framework for speech recognition. It learns high-quality speech representations directly from raw audio waveforms through self-supervised learning, offering the advantages of lower data requirements and multilingual support. \n\nThe advent of large language models that have been pre-trained on vast quantities of data has given rise to novel approaches for the detection of AD. Pretrained language models can be classified into two main categories: masked language models and autoregressive generative language models. The most prevalent masked language model is BERT and its numerous variants, including RoBERTa and Sentence BERT. Supervised model fine-tuning can be performed directly using labeled text, or the classification task can be converted into a prediction token using methods such as prompt. Study [1]demonstrated the feasibility of this detection method by using spontaneous speech for the first time to classify text embeddings extracted from a large amount of pre-trained semantic knowledge on GPT-3. Nevertheless, the considerable number of parameters inherent to these large language models presents a challenge. For instance, GPT-3 has 175 billion parameters.",
            "score": 0.48058371538420375,
            "section_title": "Related work",
            "char_start_offset": 4642,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1576
                }
            ],
            "ref_mentions": [
                {
                    "start": 1209,
                    "end": 1212,
                    "matchedPaperCorpusId": "255029590"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.362060546875
        },
        {
            "corpus_id": "239220665",
            "title": "Incorporating Concreteness in Multi-Modal Language Models with Curriculum Learning",
            "text": "In addition to the tests mentioned above, we performed several tests for measuring the informativeness of the newly constructed dataset. \n\nThe rest of the manuscript is structured as follows: In Section 2, we give background information on the task of language modeling/representation. Model details and the new dataset are explained in Section 3. We share the experimental results in Section 4, along with the descriptions of the datasets used. In addition, finally, in Section 5, final remarks are made with possible future directions.",
            "score": 0.48051836594576036,
            "section_title": "Introduction",
            "char_start_offset": 4147,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 139,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 537
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11981201171875
        },
        {
            "corpus_id": "259203206",
            "title": "FDINet: Protecting against DNN Model Extraction via Feature Distortion Index",
            "text": "Language model. This paper primarily focuses on empirical studies conducted in the field of computer vision. However, it is crucial to recognize the significant advancements achieved in language model development. Prominent pretrained language transformers, including BERT and GPT-3, have been extensively employed in various downstream applications. Nonetheless, these models are still under the threat of model extraction attacks [7], [63]. We believe that our proposed method is able to transfer to language models. In the future, we plan to extend this research to encompass language models and devise a novel model extraction detector approach specifically designed for the NLP domain.",
            "score": 0.48051836594576036,
            "section_title": "Limitations and Future Work",
            "char_start_offset": 41031,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 15
                },
                {
                    "start": 16,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 690
                }
            ],
            "ref_mentions": [
                {
                    "start": 432,
                    "end": 435,
                    "matchedPaperCorpusId": "204907203"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.392578125
        },
        {
            "corpus_id": "272827783",
            "title": "Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method",
            "text": "Given a piece of text x = x 1 x 2 . . . x n , where x i represent the tokens after tokenizing x, and a target LLM M, we compute a detection score by measuring the divergence between the token probability distribution of x and the token frequency distribution in pretraining corpus, without any model training processes. Our method consists of four steps: (i) Token probability distribution computation, by querying M with x (Section 3.2). (ii) Token frequency distribution computation, by using a large-scale publicly available corpus D \u2032 as a reference corpus to obtain an estimation of the token frequency distribution since M's pretraining corpus is not assumed to be accessible (Section 3.3). (iii) Score calculation via comparison, by comparing the above two distributions to calibrate the token probability for each token x i in x, and derive a score for pretraining data detection based on the calibrated token probabilities (Section 3.4). (iv) binary decision, by applying a predefined threshold to the score, we predict whether x was included in M's pretraining corpus or not (Section 3.5). \n\nWe summarize our method in Algorithm 1.",
            "score": 0.48051836594576036,
            "section_title": "Overview",
            "char_start_offset": 8893,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1099
                },
                {
                    "start": 1102,
                    "end": 1141
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59521484375
        },
        {
            "corpus_id": "257496757",
            "title": "The Science of Detecting LLM-Generated Text",
            "text": "The detection task is typically approached as a binary classification problem, aiming to capture textual features that differentiate between human-authored and LLM-generated texts. This section provides an overview of the primary categories of classification models. GPT-2, GPT-3, and Grover models [15]. Similarly, Solaiman et al. achieved solid performance in identifying texts generated by GPT-2 through a combination of TF-IDF unigram and bigram features with a logistic regression model [34]. In addition, studies have also shown that using pre-trained language models to extract semantic textual features, followed by SVM for classification, can outperform the use of statistical features alone [7]. One advantage of these algorithms is their interpretability, allowing researchers to analyze the importance of input features and understand why the model classifies texts as LLM-generated or not.",
            "score": 0.48051836594576036,
            "section_title": "Classification Model",
            "char_start_offset": 19016,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 902
                }
            ],
            "ref_mentions": [
                {
                    "start": 299,
                    "end": 303,
                    "matchedPaperCorpusId": "233563022"
                },
                {
                    "start": 701,
                    "end": 704,
                    "matchedPaperCorpusId": "247451060"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.230224609375
        },
        {
            "corpus_id": "254853721",
            "title": "Optimal Transport for Unsupervised Hallucination Detection in Neural Machine Translation",
            "text": "In order to establish the broader validity of our model-based detectors, we present an analysis on their performance for other NMT models and on mid and low-resource language pairs. Overall, the detectors exhibit similar trends to those discussed in the main text (Section 6).",
            "score": 0.48051836594576036,
            "section_title": "J Experiments on the MLQE-PE dataset",
            "char_start_offset": 36829,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11859130859375
        },
        {
            "corpus_id": "241583460",
            "title": "A Large-scale Comprehensive Abusiveness Detection Dataset with Multifaceted Labels from Reddit",
            "text": "The experimental results of all baseline models for all tasks are shown in Table 5. We experimented with six baselines, with or without context for each of the three models, respectively. Generally, all baseline models achieved quite stable performance, to endorse that our dataset is adequate for abusive language detection on both tasks. \n\nThe result shows that three transformer models outperform the dictionary-based models. It suggests that our dataset is not lexically biased, requiring a more complex model to solve its problem than simple dictionary-based models. The result also shows that the models with contextual information marginally outperform the models without it. It implies that the context may have affected crowdworkers during annotation (Pavlopoulos et al., 2020). It also suggests that the contexts of our dataset provide informative clues to abusiveness, which is also explored in Section 5.2. The performance gain due to context is more striking when conducted on Task 2. The fine-grained detection task may require more abundant information such as context, suggesting that the performance gain may become more significant if more powerful detection models are engaged. \n\nWe also investigated how the model performance is increased with different percentages of the training dataset (Figures 4a and 4b). It shows that using 20% of our training data leads to a steep improvement, which gains a marginal increment afterwards. It implies that our dataset includes a sufficient number of samples for training models. In addition, even though it is a marginal increment, the performance is improved continuously, especially for Task 2 (from 0.56 to 0.62 for the BERT case). We assume that the increased lexical diversity, shown in Figure 4c, might be one of the possible reasons for such improvement 9 . \n\nIt is noted that our experiments involve just two tasks, so that if we build a model where all of the labels are combined effectively, it would help many downstream tasks on our dataset. Designing such a model, however, requires a more rigorous examination of each label. We leave it for future work.",
            "score": 0.48019659712376234,
            "section_title": "Experimental results and analysis",
            "char_start_offset": 23003,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 339
                },
                {
                    "start": 342,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1196
                },
                {
                    "start": 1199,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1825
                },
                {
                    "start": 1828,
                    "end": 2014
                },
                {
                    "start": 2015,
                    "end": 2099
                },
                {
                    "start": 2100,
                    "end": 2128
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07501220703125
        },
        {
            "corpus_id": "271097890",
            "title": "On the Universal Truthfulness Hyperplane Inside LLMs",
            "text": "First, there are several other methods to probe the language model's knowledge or hallucination, such as CCS (Burns et al., 2023) and LAT (Zou et al., 2023). In our paper, we only consider the commonly used supervised probing methods: logistic regression and mass mean. Further work can explore other methods. Second, although we strive to include a wide range of diverse datasets, there is still a gap between our curated datasets and realworld data on truthfulness. Third, we leave the intervention work as future research to verify whether the identified vector is causally related to model behavior. Fourth, although we are talking about truthfulness, the absolute detection accuracy is restricted by the knowledge of the model. The separation of correct and incorrect data within hidden representations is contingent upon the model's understanding. Consequently, our curated datasets may include noise stemming from the divergence between the model's knowledge and real-world knowledge, or from instances that exceed the model's knowledge boundaries. We hypothesis that, in most cases, the knowledge of models aligns with the knowledge in data so that the Probe trained on our data can well discern the truthful or untruthful belief of the model.",
            "score": 0.47819315498527193,
            "section_title": "Limitations",
            "char_start_offset": 24988,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1251
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.189697265625
        },
        {
            "corpus_id": "256460957",
            "title": "CamPros at CASE 2022 Task 1: Transformer-based Multilingual Protest News Detection",
            "text": "In natural language processing (NLP), Event detection is a task that detects event triggers/mentions (i.e., the key terms that drive or express an event) and categorizes them into predefined event types (Lai et al., 2021b). The early detection of ongoing and past events exploited featurebased approaches to detect events (Li et al., 2013). However, the early data-driven (Hogenboom et al., 2011), knowledge-driven, and rulebased approaches missed the semantic relationship in the data (Danilova and Popova, 2014). Other early approaches for event detection include machine learning models such as SVM and decision trees (Schrodt et al., 2014). The recent deep learning approaches proposed in the literature (Ahmad et al., 2020) improve event detection; nonetheless, they are not generalizable for low-resource languages. To address the data scarcity problem for low-resource languages, researchers have recently used the pre-trained language model GPT-2 to generate training samples (Veyseh et al., 2021a). \n\nAnother less-discovered approach in the Event detection task is Cross-Lingual event detection which proposes model creation for effective performance over different languages (Guzman-Nateras et al., 2022)",
            "score": 0.47658191045290765,
            "section_title": "Related Work",
            "char_start_offset": 3730,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 1007
                },
                {
                    "start": 1010,
                    "end": 1214
                }
            ],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 222,
                    "matchedPaperCorpusId": "235792491"
                },
                {
                    "start": 322,
                    "end": 339,
                    "matchedPaperCorpusId": "2114517"
                },
                {
                    "start": 708,
                    "end": 728,
                    "matchedPaperCorpusId": "226438141"
                },
                {
                    "start": 984,
                    "end": 1006,
                    "matchedPaperCorpusId": "236460229"
                },
                {
                    "start": 1185,
                    "end": 1214,
                    "matchedPaperCorpusId": "249061255"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18603515625
        },
        {
            "corpus_id": "269915030",
            "title": "Systematic Review on Offensive Language Detection of Multilingual Texts using NLP and Machine Learning Techniques",
            "text": "Supervised learning is a popular approach in offensive language detection, where the model is trained on labeled data, meaning the input text is paired with corresponding offensive or non-offensive labels.The model learns from these examples to make predictions on unseen data.",
            "score": 0.4761173578576093,
            "section_title": "Supervised Learning Approaches",
            "char_start_offset": 8907,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 205,
                    "end": 277
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.133544921875
        },
        {
            "corpus_id": "271570943",
            "title": "Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens",
            "text": "We first define the terminology we use, formalize the problem, and state our assumptions about the knowledge and capabilities. We follow the standard definition of the existing pre-training data detection in LLMs: \n\nAutoregressive language models. We focus on the autoregressive language models M, which are trained to predict the next text token based on the input sequence of previous tokens (i.e., different from bidirectional masked language models such as BERT [14]). We use p(\u2022|x 1 , x 2 , ..., x i\u22121 ) to represent the probability distribution of an model M on generating next token when given an input sequence x 1 , x 2 , ..., x i\u22121 . This probability distribution is among a token vocabulary, denoted as V, and for each v j \u2208 V, denote p(v j | x 1 , .., x i\u22121 ) as the probability of next token to be v j . \n\nClarify names and definitions. We want to clarify the following names used in paper for better understanding. Given an input sequence x 1 , x 2 , . . . , x N , each of x i is called token (i.e., ground-truth token); index (i.e, 1, 2, ..N ) is used to state the location of x i . We call the possible generated tokens (i.e., \u2200 v j \u2208 V) at a specific index as token candidates. \n\nProblem definition. Given a data point x = x 1 , x 2 , .., x N and a pre-trained language model M, the goal of pre-training data detection problem is to infer whether x belongs to the training dataset D for this model M. The detection process F is achieved by computing a score f (x, M) and decided through a case-dependent threshold \u03bb. Formally, x : F(f (x, M); \u03bb) \u2192 {0, 1}. Following previous works, people care about the threshold-independent metric, area under the ROC curve, for the aggregate measure of performance. \n\nKnowledge and capabilities.",
            "score": 0.4757768092357937,
            "section_title": "Preliminary",
            "char_start_offset": 8216,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 213
                },
                {
                    "start": 216,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1194
                },
                {
                    "start": 1197,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1718
                },
                {
                    "start": 1721,
                    "end": 1748
                }
            ],
            "ref_mentions": [
                {
                    "start": 466,
                    "end": 470,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78466796875
        },
        {
            "corpus_id": "264451585",
            "title": "Detecting Pretraining Data from Large Language Models",
            "text": "We introduce a pretraining data detection method MIN-K% PROB that leverages minimum token probabilities of a text for detection. MIN-K% PROB is based on the hypothesis that a non-member example is more likely to include a few outlier words with high negative log-likelihood (or low probability), while a member example is less likely to include words with high negative log-likelihood. \n\nConsider a sequence of tokens in a sentence, denoted as x = x 1 , x 2 , ..., x N , the log-likelihood of a token, x i , given its preceding tokens is calculated as log p(x i |x 1 , ..., x i\u22121 ). We then select the k% of tokens from x with the minimum token probability to form a set, Min-K%(x), and compute the average log-likelihood of the tokens in this set: \n\nwhere E is the size of the Min-K%(x) set. We can detect if a piece of text was included in pretraining data simply by thresholding this MIN-K% PROB result. We summarize our method in Algorithm 1 in Appendix B.",
            "score": 0.4757768092357937,
            "section_title": "MIN-K% PROB: A SIMPLE REFERENCE-FREE PRETRAINING DATA DETECTION METHOD",
            "char_start_offset": 12383,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 385
                },
                {
                    "start": 388,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 748
                },
                {
                    "start": 751,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 960
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53173828125
        },
        {
            "corpus_id": "26204237",
            "title": "Techniques and Challenges in Speech Synthesis",
            "text": "an n-gram model to determine part of speech within a sentence. Before continuing, we will further discuss the n-gram technique which was previously outlined.\n\nThis technique is an example of a Hidden Markov Model, or HMM. In a HMM, we assume that the underlying system is a Markov process. This is a system where the likelihood of a transition from one state to another is entirely determined by the current state. Here, our \"current state\" is the lexical classes of the last two words in order and the possible lexical classes of the following word, and our possible future states are the most recent word paired with the prediction for the next word. The probabilities of moving from one state to another are defined by our COCA corpus data. If, for example, we had a Noun and a Noun, and the following word could possibly be either a Noun or a Verb, we can look at Table 28 to see that NNN occurs 289770 times, while NNV occurs 142513 times in the corpus; together, these sum to 432283. Therefore using this method, we predict the probability of the next word being a noun is 289770/432283, or about 67%, leaving the probability of the next word being a verb as 33%.\n\nIn our HMM, we observe the probabilities of each path through our data, and then find the maximally probable path. This is the \"hidden\" component of the Hidden Markov Model: we have the complete sentence as the \"output\" of the Markov process, and wish to determine the underlying sequence of system states that it passed through. Those states then correspond to the sequence of lexical classes through the sentence. This technique is known as a Most Likely Explanation HMM, as it gives the most likely state sequence for the overall sentence. While this is similar to the technique we used in grapheme to phoneme conversion, in that system each grapheme cluster could only possibly correspond to one graphone. A HMM model would include multiple possible graphones for each grapheme cluster, and find the maximum likelihood based on adjacent clusters. While this would offer improved effectiveness, it drastically increases database size.\n\nReturning to part of speech tagging, we must consider what to do if a sentence is smaller than three words long. We know the data in MPOS gives",
            "score": 0.4757768092357937,
            "section_title": "JERK THE ROPE VtN Dv Nti",
            "char_start_offset": 204969,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0259552001953125
        },
        {
            "corpus_id": "267750173",
            "title": "Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting",
            "text": "We first apply our methods to fine-tuning pre-trained language models with LoRA on two sets of language modeling tasks: text classification and causal language modeling. The reason for this choice of task is twofold: The first is that language models can be evaluated quantitatively; a clear metric is associated with each task. The second is that it allows objective comparison with the wider literature. \n\n1) Text Classification: We select three sentence-pair classification tasks and one single-sentence classification task from the GLUE benchmark [49]. The sentence-pair tasks are: MNLI [50], a natural language inference task of predicting whether a premise entails, contradicts or is neutral to a hypothesis, QQP [51], a paraphrase detection task of predicting whether a pair of sentences are semantically equivalent, and QNLI [52], a question answering task of predicting whether a sentence answers a question. The single-sentence task is SST-2 [53], a sentiment analysis task of predicting whether a sentence has positive or negative sentiment. For all tasks, the fine-tuning performance is reflected by the accuracy on the validation set. The number of training examples in the four selected datasets are MNLI: 393k, QQP: 363k, QNLI: 105k, and SST-2: 67k. \n\n2) Causal Language Modeling: We experiment on the two subsets, WikiText-2 and WikiText-103, of the WikiText dataset [54], a collection of over 100 million tokens extracted Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply. \n\nfrom the set of verified good and featured articles on Wikipedia. The number of tokens in WikiText-2 and WikiText-103 are 2.1M and 103M, respectively. The fine-tuning performance is reflected by the perplexity on the validation set, which is shared by the two subsets.",
            "score": 0.4757768092357937,
            "section_title": "A. Tasks",
            "char_start_offset": 19711,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 405
                },
                {
                    "start": 408,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1264
                },
                {
                    "start": 1267,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1550
                },
                {
                    "start": 1553,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1821
                }
            ],
            "ref_mentions": [
                {
                    "start": 551,
                    "end": 555,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 591,
                    "end": 595,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 833,
                    "end": 837,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 952,
                    "end": 956,
                    "matchedPaperCorpusId": "990233"
                },
                {
                    "start": 1383,
                    "end": 1387,
                    "matchedPaperCorpusId": "16299141"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1669921875
        },
        {
            "corpus_id": "8409243",
            "title": "Cross-Task Knowledge-Constrained Self Training",
            "text": "In the two-sided version, we assume that we have a small amount of data labeled by f 1 (call this D 1 ), a small amount of data labeled by f 2 (call this D 2 ) and a large amount of unlabeled data (call this D unlab ). The algorithm we propose for learning hypotheses for both tasks is below: \n\nWe use 3500 examples from NER and 1000 from WSJ. We use the remaining 18447 examples as unlabeled data. The baseline HMMs achieve Fscores of 50.8 and 76.3, respectively. In step 2, we add 7512 examples to each data set. After step 3, the new models achieve F -scores of 54.6 and 79.2, respectively. The gain for NER is lower than before as it is trained against \"noisy\" syntactic labels.",
            "score": 0.4757768092357937,
            "section_title": "Two-sided Learning with Hints",
            "char_start_offset": 6955,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 292
                },
                {
                    "start": 295,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 682
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.021453857421875
        },
        {
            "corpus_id": "10959945",
            "title": "Two decades of statistical language modeling: where do we go from here?",
            "text": "Statistical language models estimate the distribution of various natural language phenomena for the purpose of speech recognition and other language technologies. Since the first significant model was proposed in 1980, many attempts have been made to improve the state of the art. We review them, point to a few promising directions, and argue for a Bayesian approach to integration of linguistic theories with data.",
            "score": 0.4757768092357937,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.171630859375
        },
        {
            "corpus_id": "268358050",
            "title": "Detectors for Safe and Reliable LLMs: Implementations, Uses, and Limitations",
            "text": "Large language models (LLMs) possess tremendous potential in numerous real-world applications, thanks to their versatility, adaptability, and ease of use, coupled with their continuously improving performance. However, their deployment, especially in critical domains such as healthcare and finance, poses significant risks (IBM AI Ethics Board 2024; IBM AI Risk Atlas). New challenges arise due to their generative and intuitive nature of these models, coupled with their often unconstrained mode of interaction through natural language (i.e., prompting). These models can produce textual responses that are convincing, but often layered with problems like toxicity, bias, hallucinations, and more. \n\nIn this paper, we describe our work at IBM Research on detecting and mitigating undesirable LLM behaviors via auxiliary classifier models, hereafter referred to as \"detectors\". We also explain how these detectors are being used in the data and model factory responsible for producing the IBM Granite series of LLMs (Building AI for business: IBM's Granite foundation models). The detectors have also been deployed as moderations in IBM Research's experimental prompt laboratory, with more than 25,000 internal users, to test them before possible inclusion into IBM's commercial foundational model platform (IBM watsonx -An AI and data platform built for business). Specifically, our goals and approaches in developing and studying these detectors are: \n\n1. Comprehensive: (Section 2) We attempt to detect harms in a variety of ways, including at the output (prejudice, social norms, safety, AI-generated content), the input (prompt injection or jail-breaking), and both input and output (unfaithfulness). 2. Efficient and reliable: (Sections 2. 1, 2.2, 2.4) We investigate ways in which the detectors can be made efficient in both data and computation. To improve reliability and robustness, we explore calibration and data augmentation through synthetic data generation. 3. Continual improvement: (Section 2.3) We practice iterative improvement of the detectors, utilizing human redteaming to obtain valuable insights into failure modes. 4. Multi-use: (Section 3) We design our detectors to be used in a variety of applications and throughout an LLM life-cycle as depicted in Figure 1.",
            "score": 0.4757768092357937,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 699
                },
                {
                    "start": 702,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1453
                },
                {
                    "start": 1456,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2140
                },
                {
                    "start": 2141,
                    "end": 2288
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47119140625
        },
        {
            "corpus_id": "17269653",
            "title": "Language Recognition using Random Indexing",
            "text": "The algorithm outlined above was implemented by (Joshi & Halseth (2014)), and used to create Language Vectors for 23 languages. Texts for the Language Vectors were taken from Project Gutenberg (Hart) where it is available in a number of languages, and from the Wortschatz Corpora (Quastoff et al. (2006)) where large numbers of sentences in selected languages can be easily downloaded. Each Language Vector was based on about 100,000 bytes of text. Computing of the Language Vectors corresponds to training the system and took about 1 second per language on a laptop computer. \n\nIntuitively, Language Vectors within a language family should be closer to each other than vectors for unrelated languages. Indeed, the hyperdimensional Language Vectors roughly cluster in this manner, as seen in Figure 1. \n\nTo get an idea of how well the actual detection algorithm works, we tested the Language Vectors' ability to identify text samples from the Europarl Parallel Corpus, described in Nakatani. This corpus includes 21 languages with 1,000 samples of each, and each sample is a single sentence. \n\nTable 1 shows the result for n-gram sizes from 1 to 5 (n = 1 is the equivalent of comparing letter histograms). With tetragrams we were able to guess the correct language with 97.8% accuracy. Even when incorrect, the system usually chose a language from the same family, as seen from  The arithmetic (algebra) of the operations with which Text Vectors are made-i.e., permutation, multiplication, and addition, and how they work together-make it possible to analyze the Language Vectors and find out, for example, what letters are most likely to follow \"the\". (In English it would be the Space, but what is the next most likely?) Notice that we don't need to contemplate such questions in advance and then design the data-gathering algorithm with that in mind. The information is in the vectors in a form that allows it to be retrieved with the arithmetic. \n\nBecause of the generality of Random Indexing on texts, any time series with a well-defined \"alphabet\" can be encoded using this scheme. In this way, we propose that our method can be used to do language detection in speech data, addressing our original problem.",
            "score": 0.47495420822518075,
            "section_title": "EXPERIMENTAL RESULTS",
            "char_start_offset": 10049,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 801
                },
                {
                    "start": 804,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1091
                },
                {
                    "start": 1094,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 1949
                },
                {
                    "start": 1952,
                    "end": 2087
                },
                {
                    "start": 2088,
                    "end": 2213
                }
            ],
            "ref_mentions": [
                {
                    "start": 280,
                    "end": 303,
                    "matchedPaperCorpusId": "17089484"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08880615234375
        },
        {
            "corpus_id": "274423012",
            "title": "Sensitive Content Classification in Social Media: A Holistic Resource and Evaluation",
            "text": "In order to assess the zero/few-shot capabilities of large language models in our dataset, we compare four models of different sizes and architectures. Llama3: The 8 and 70 billion instruct versions of Llama3 are tested. These models are designed to follow user instructions more effectively, allowing us to assess how well they adapt in settings where training data is limited or not available. chat-gpt-3.5-turbo (chat-gpt): from OpenaAI,7 an encoder/decoder model with approximately 175 billion parameters (Brown et al., 2020). gpt-4o: the currently latest model from OpenAI which significantly outperforms it's predecessor. \n\nAssessing the performance in zero-and few-shot settings, help us to exlore the capabilities and limitations of these large language models for sensitive content detection.",
            "score": 0.47332173356967966,
            "section_title": "Zero-and Few-shot",
            "char_start_offset": 20747,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 627
                },
                {
                    "start": 630,
                    "end": 801
                }
            ],
            "ref_mentions": [
                {
                    "start": 509,
                    "end": 529,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2386474609375
        },
        {
            "corpus_id": "265281337",
            "title": "Detection and Analysis of Offensive Online Content in Hausa Language",
            "text": "To determine the best detection model, we explore the following groups of machine learning models: \n\n-Linear and kernel-based models: This group consists of logistic regression, a collection of regression algorithms that convey the relationship between variables (dependent and independent) and support vector machines (SVM) that categorise information separately using hyperplane to maximise the margin between them. \n\n-Naive Bayes Naive Bayes is one of the popular models for classification tasks, especially in NLP. \n\nIt is based on Bayes' theorem and assumes that features are conditionally independent given the class label. \n\n-Ensemble models: This group comprises random forest and XGBoost models. The random forest classifier employs a set of decision trees. Closely related to the random forest is the extreme gradient boosting algorithm (XGBoost), which is based on the gradient tree boosting technique. These algorithms have been used for their fast learning and performance scalability. \n\n-Neural networks: This group consists of multilayer perceptron (MLP) and convolutional neural network (CNN) models. The MLP is made up of an input layer, at least one hidden layer of computational neurones, and an output layer. A CNN is a deep neural network design consisting of convolutional and pooling or sub-sampling layers that feed input to a fully connected network. \n\nPretrained Multilingual Models In addition to the above standard ML models discussed earlier, we leverage state-of-the-art multilingual pretrained models that are widely used in NLP downstream tasks and align well with our research objectives. The models utilised in the study include: \n\n-XLM-RoBERTa Model: we employ the Cross-lingual Language Model -Robustly Optimised BERT Pretraining Approach (XLM-RoBERTa) [14], implemented via Hugging Face7 . Trained on a large-scale multilingual dataset comprising 100 languages sourced from CommonCrawl8 , this model benefits from extensive pretraining on 2.5TB of filtered data. This enables XLM-RoBERTa to capture rich cross-lingual representations, making it highly suitable for our application.",
            "score": 0.4716748228943488,
            "section_title": "Model Building",
            "char_start_offset": 16553,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 101,
                    "end": 417
                },
                {
                    "start": 420,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 629
                },
                {
                    "start": 632,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1375
                },
                {
                    "start": 1378,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1663
                },
                {
                    "start": 1666,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2118
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.140625
        },
        {
            "corpus_id": "9044768",
            "title": "Unsupervised Learning of Field Segmentation Models for Information Extraction",
            "text": "Hidden Markov models (HMMs) are commonly used to represent a wide range of linguistic phenomena in text, including morphology, parts-ofspeech (POS), named entity mentions, and even topic changes in discourse. An HMM consists of a set of states S, a set of observations (in our case words or tokens) W , a transition model specifying P(s t |s t\u22121 ), the probability of transitioning from state s t\u22121 to state s t , and an emission model specifying P(w|s) the probability of emitting word w while in state s. For a good tutorial on general HMM techniques, see Rabiner (1989).\n\nFor all of the unsupervised learning experiments we fit an HMM with the same number of hidden states as gold labels to an unannotated training set using EM. 1 To compute hidden state expectations efficiently, we use the Forward-Backward algorithm in the standard way. Emission models are initialized to almost-uniform probability distributions, where a small amount of noise is added to break initial symmetry. Transition model initialization varies by experiment. We run the EM algorithm to convergence. Finally, we use the Viterbi algorithm with the learned parameters to label the test data.\n\nAll baselines and experiments use the same tokenization, normalization, and smoothing techniques, which were not extensively investigated. Tokenization was performed in the style of the Penn Treebank, and tokens were normalized in various ways: numbers, dates, phone numbers, URLs, and email addresses were collapsed to dedicated tokens, and all remaining tokens were converted to lowercase. Unless otherwise noted, the emission models use simple add-\u03bb smoothing, where \u03bb was 0.001 for supervised techniques, and 0.2 for unsupervised techniques.",
            "score": 0.47108735754461806,
            "section_title": "Hidden Markov Models",
            "char_start_offset": 2925,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 558,
                    "end": 572,
                    "matchedPaperCorpusId": "13618539"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1171875
        },
        {
            "corpus_id": "273963957",
            "title": "VocalTweets: Investigating Social Media Offensive Language Among Nigerian Musicians",
            "text": "Natural Language Processing (NLP) and Machine Learning (ML) techniques were employed to improve detecting accuracy, and models like Support Vector Machines (SVM) and Random Forest were tested. The SVM model outperformed others, achieving an accuracy of 86%.",
            "score": 0.47108735754461806,
            "section_title": "Introduction",
            "char_start_offset": 6714,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 257
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0850830078125
        },
        {
            "corpus_id": "59316955",
            "title": "Word Embeddings: A Survey",
            "text": "Statistical language models are probabilistic models of the distribution of words in a language. For example, they can be used to calculate the likelihood of the next word given the words immedi-ately preceding it (its context). One of their earliest uses has been in the field of speech recognition (Bahl et al. (1983)), to aid in correctly recognizing words and phrases in sound signals that have been subjected to noise and/or faulty channels. \n\nIn the realm of textual data, such models are useful in a wide range of NLP tasks, as well as other related tasks, such as information retrieval. \n\nWhile a full probabilistic model containing the likelihood of every word given all possible word contexts that may arise in a language is clearly intractable, it has been empirically observed that satisfactory results are obtained using a context size as small as 3 words (Goodman (2001)). A simple mathematical formulation of such an n-gram model with window size equal to T follows: \n\nwhere w t is the t-th word and w T i refers to the sequence of words from w i to w T , i.e. (w i , w i+1 , w i+2 ...w T ). P (w t |w t\u22121 1 ) refers to the fraction of times w t appears after the sequence w t\u22121 1 . Actual prediction of the next word given a context is done via maximum likelihood estimation (MLE), over all words in the vocabulary. \n\nSome problems reported with these models have been (Bengio et al. (2003)) the high dimensionality involved in calculating discrete joint distributions of words with vocabulary sizes in the order of 100,000 words and difficulties related to generalizing the model to word sequences not present in the training set. \n\nEarly attempts of mitigating these effects, particularly those related to generalization to unseen phrases, include the use of smoothing, e.g. pretending every new sequence has count one, rather than zero in the training set (this is referred to as add-one or Laplace smoothing. Also, backing off to increasingly shorter contexts when longer contexts aren't available (Katz (1987)). Another strategy which reduces the number of calculations needed and helps with generalization is the clustering of words in so-called classes (cf.",
            "score": 0.47108735754461806,
            "section_title": "Statistical Language Modelling",
            "char_start_offset": 6773,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 446
                },
                {
                    "start": 449,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 981
                },
                {
                    "start": 984,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1331
                },
                {
                    "start": 1334,
                    "end": 1647
                },
                {
                    "start": 1650,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2180
                }
            ],
            "ref_mentions": [
                {
                    "start": 2018,
                    "end": 2030,
                    "matchedPaperCorpusId": "6555412"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2587890625
        },
        {
            "corpus_id": "270703139",
            "title": "Detecting AI-Generated Text: Factors Influencing Detectability with Current Methods",
            "text": "Experimentally, ELECTRA is shown to be more robust to unseen data. He et al. (2024) note that some data domains are more transferable than others, observing that language model-based methods are more flexible when trained on creative writing data with many human authors rather than news data with few human authors. Despite the poor generalization capability of language modelbased detectors, Xu et al. (2023) show that transfer learning is possible, meaning that existing trained detectors can be adapted using a small amount of new domain data. \n\nIn the case of unseen generating models, we observe that statistical detectors generalize poorly while finetuned language model-based detectors generalize to some extent, according to performance on several benchmark datasets including ArguGPT (Liu et al., 2023b), MGTBench (He et al., 2024), and M4 (Wang et al., 2024b). Notably, GPTZero is not able to detect essays written by a model other than GPT-3.5 for which it is calibrated (Liu et al., 2023b), and DetectGPT does not transfer between ChatGPT and GPT-3, two closely related language models (Abdalla et al., 2023). Although language model-based detectors might generalize better than statistical detectors, their performance is still shown to be unsatisfactory in some contexts (He et al., 2024;Abdalla et al., 2023). Furthermore, their generalization ability is conditional on the input text length and size of the generating model. Liu et al. (2023b) observe that RoBERTa's accuracy drops by only 2% on full essay examples, but drops by 13% on sentence length texts, compared to in-distribution performance. Pagnoni et al. (2022) show that smaller LLMs struggle to detect text produced by larger LLMs, although this disparity becomes less pronounced among the largest models. Contradictory results across evaluation studies may speak to this conditional generalizability of language model-based detectors. \n\nFinally, out-of-distribution data may occur when the generating prompt differs from training data to test data.",
            "score": 0.47108735754461806,
            "section_title": "Out-of-Distribution Domains and Generating Models",
            "char_start_offset": 88669,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 547
                },
                {
                    "start": 550,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1915
                },
                {
                    "start": 1918,
                    "end": 2029
                }
            ],
            "ref_mentions": [
                {
                    "start": 67,
                    "end": 83,
                    "matchedPaperCorpusId": "257766697"
                },
                {
                    "start": 824,
                    "end": 841,
                    "matchedPaperCorpusId": "257766697"
                },
                {
                    "start": 850,
                    "end": 870,
                    "matchedPaperCorpusId": "258866053"
                },
                {
                    "start": 1099,
                    "end": 1121,
                    "matchedPaperCorpusId": "263169445"
                },
                {
                    "start": 1286,
                    "end": 1303,
                    "matchedPaperCorpusId": "257766697"
                },
                {
                    "start": 1303,
                    "end": 1324,
                    "matchedPaperCorpusId": "263169445"
                },
                {
                    "start": 1618,
                    "end": 1639,
                    "matchedPaperCorpusId": "252819091"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.460693359375
        },
        {
            "corpus_id": "227119057",
            "title": "Fine-Tuning BERT for Sentiment Analysis of Vietnamese Reviews",
            "text": "The language model is a probability distribution over text sets. The language model can show how much probability a sentence (or phrase) belongs to a language. Language models analyze bodies of text data to provide a basis for their word predictions.\n\nwhere x 1 ,x 2 ,. . . ,x n is the sequence of words that make up a sentence, and n is length of the sentence (n > 1).\n\nCurrently, pre-trained language models are widely used in NLP tasks. These models have been trained on a very large dataset, integrating many languages and knowledge. Users can thus use them in many different NLP tasks [5].",
            "score": 0.47108735754461806,
            "section_title": "Language Model",
            "char_start_offset": 5261,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.291259765625
        },
        {
            "corpus_id": "273901232",
            "title": "Text Fluoroscopy: Detecting LLM-Generated Text through Intrinsic Features",
            "text": "Previous methods (Guo et al., 2023;Hu et al., 2023;Guo and Yu, 2023) employ the semantic features stored in the last layer of the language model to perform detection. For example, Hello-Chatgptdetector-roberta (Guo et al., 2023) uses the Roberta model to extract semantic features and then trains a classifier to detect LLM-generated texts. The features stored in the last layer are abstract representations of semantic content, causing the method to overfit the domain of the training set and show deficiencies in generalization. Therefore, to obtain more generalizable detection methods, current researchers work on developing methods by linguistic features. For example, DNA-GPT (Yang et al., 2023) takes advantage of the divergence between multiple completions of a truncated passage. Some researchers (McGovern et al., 2024) find simple classifiers on top of n-gram and part-of-speech features can achieve very robust performance on both in-and out-of-domain data. These \"fingerprints\" retain the more primitive features of LLMs and are more useful for detecting LLM-generated text. However, these features are more susceptible to exploitation by attackers, and detection methods based on linguistic features are less robust than those based on BERT's features when facing paraphrase attacks (McGovern et al., 2024).",
            "score": 0.47108735754461806,
            "section_title": "Related Work",
            "char_start_offset": 4670,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1321
                }
            ],
            "ref_mentions": [
                {
                    "start": 35,
                    "end": 51,
                    "matchedPaperCorpusId": "259501842"
                },
                {
                    "start": 682,
                    "end": 701,
                    "matchedPaperCorpusId": "258960101"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1973876953125
        },
        {
            "corpus_id": "253455901",
            "title": "A review on abusive content automatic detection: approaches, challenges and opportunities",
            "text": "In addition to automatic detection models and approaches, many tools and programming techniques have been used to perform several tasks. For example, NLP tasks were performed using the Natural Language Toolkit (NLTK) (Rizos, Hemker & Schuller, 2019;Huang, Singh & Atrey, 2014;Pawar et al., 2018), SpaCy (Salminen et al., 2020;Ribeiro et al., 2018) and GenSim (Rizos, Hemker & Schuller, 2019;Chowdhury et al., 2019;Kamble & Joshi, 2018). Furthermore, the majority of the work has been performed using the Python programming language, and some of works done by using R language also some studies have also used the Weka tool to train their models and achieve the results (Huang, Singh & Atrey, 2014;Pericherla & Ilavarasan, 2021;Rachid, Azza & Ben Ghezala, 2020). Recently, researchers used Huggingface.co to import transformer models to perform pretraining and fine-tuning strategies. Transformer models are included in a Python-based library that has an API for using many well-known transformer architectures, such as BERT, which obtain state-of-the-art results on a variety of NLP tasks.",
            "score": 0.47108735754461806,
            "section_title": "Data and resources",
            "char_start_offset": 15852,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1089
                }
            ],
            "ref_mentions": [
                {
                    "start": 217,
                    "end": 249,
                    "matchedPaperCorpusId": "207757246"
                },
                {
                    "start": 276,
                    "end": 295,
                    "matchedPaperCorpusId": "53040876"
                },
                {
                    "start": 303,
                    "end": 326,
                    "matchedPaperCorpusId": "209528769"
                },
                {
                    "start": 326,
                    "end": 347,
                    "matchedPaperCorpusId": "4425883"
                },
                {
                    "start": 359,
                    "end": 391,
                    "matchedPaperCorpusId": "207757246"
                },
                {
                    "start": 391,
                    "end": 414,
                    "matchedPaperCorpusId": "202640206"
                },
                {
                    "start": 697,
                    "end": 727,
                    "matchedPaperCorpusId": "224920485"
                },
                {
                    "start": 727,
                    "end": 760,
                    "matchedPaperCorpusId": "221662483"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10760498046875
        },
        {
            "corpus_id": "248177764",
            "title": "METRO: Efficient Denoising Pretraining of Large Scale Autoencoding Language Models with Model Generated Signals",
            "text": "Our baseline model follows the vanilla training setup of ELECTRA (Clark et al., 2020), using a MLM language model as the auxiliary, and the Replaced Token Detection (RTD) task to train the main model. \n\nBaseline Training Objectives. Given an input sequence with 15% tokens masked, X mask = [x orig 1 , ..., [MASK] i , ..., x orig n ], the auxiliary model g aux is trained using the standard MLM task: \n\nThe auxiliary Transformer g aux first produces the contextualized representation h aux i for each token, which is used by the language model head p mlm to generate a softmax distribution over the vocabulary. The auxiliary model is pretrained to recover the masked tokens in M using the standard language modeling loss L aux . \n\nThen the auxiliary model is used to corrupt text sequences for the denoising pretraining objective for the main model training. Following ELECTRA, we construct X noise by sampling tokens from g aux at all masked positions: \n\nThe main model, Transformer g main , is then trained using the binary RTD task: \n\nIt learns to detect whether a token x noise i is being replaced (noise) or not (original), using p rtd , a binary classification head upon h i , the contextualized representation generated by g main . \n\nThe auxiliary model and the main model are pretrained side-by-side as \n\nThe auxiliary model learns to construct samples that are increasingly harder to denoise, thus forms an effective learning curriculum for the main model. The hyperparameter \u03bb balances the learning speeds of the main and auxiliary models. \n\nBaseline Configurations. We improve the vanilla ELECTRA in our baseline model with several modeling practices whose effectiveness is verified in recent studies. \n\n\u2022 Shallow auxiliary model. Following COCO-LM (Meng et al., 2021), we keep the same hidden dimension with the main model but reduce the network depths of the auxiliary model, e.g., to 1/3 in base settings. \n\n\u2022 Relative Position Embedding. Following T5 (Raffel et al., 2019), we use relative position bins for both the auxiliary and main model, with 32 bins and 128 maximum distance. \n\n\u2022 Large Vocabulary.",
            "score": 0.47108735754461806,
            "section_title": "BASELINE MODEL",
            "char_start_offset": 14827,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 203,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 400
                },
                {
                    "start": 403,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 728
                },
                {
                    "start": 731,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 953
                },
                {
                    "start": 956,
                    "end": 1035
                },
                {
                    "start": 1038,
                    "end": 1238
                },
                {
                    "start": 1241,
                    "end": 1310
                },
                {
                    "start": 1313,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1549
                },
                {
                    "start": 1552,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1712
                },
                {
                    "start": 1715,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1919
                },
                {
                    "start": 1922,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2096
                },
                {
                    "start": 2099,
                    "end": 2118
                }
            ],
            "ref_mentions": [
                {
                    "start": 65,
                    "end": 85,
                    "matchedPaperCorpusId": "208229926"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.227783203125
        },
        {
            "corpus_id": "258947640",
            "title": "Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model",
            "text": "Large language models. LLMs (Radford et al., 2019;Brown et al., 2020;Chowdhery et al., 2022;Zhang et al., 2022;OpenAI, 2022) have revolutionized the field of natural language processing by offering several advantages over previous pre-trained models (Devlin et al., 2018;Liu et al., 2019;Lan et al., 2019), including a better characterization of complex patterns and dependencies in the text, and the appealing in-context learning ability for solving downstream tasks with minimal examples. Representative models such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), and Chat-GPT (OpenAI, 2022) have showcased their remarkable ability to generate text with high coherence, fluency, and semantic relevance. They can even effectively address complex inquiries related to science, mathematics, history, current events, and social trends. Therefore, it is increasingly important to effectively regulate the use of LLMs to prevent significant social issues. \n\nLLM-generated text detection. Previous methods can be broadly categorized into two groups. The first group of methods performs detection in a zero-shot manner (Solaiman et al., 2019;Gehrmann et al., 2019a;Mitchell et al., 2023;Yang et al., 2023), but they require access to the source model that generates the texts to derive quantities like output logits or losses for detection. For instance, Solaiman et al. (2019) suggest that a higher log probability for each token indicates that the text will likely be machine-generated. When the output logits/losses of the source model are unavailable, these methods rely on a proxy model for detection. However, there is often a substantial gap between the proxy and source models from which the text is generated. Another group of methods trains DNN-based classifiers on col-lected human-written and machine-generated texts for detection (Guo et al., 2023;Uchendu et al., 2020;Ope-nAI, 2023b).",
            "score": 0.47108735754461806,
            "section_title": "Related Works",
            "char_start_offset": 4723,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 23,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1906
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 50,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 50,
                    "end": 69,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 527,
                    "end": 547,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1150,
                    "end": 1173,
                    "matchedPaperCorpusId": "182952848"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52880859375
        },
        {
            "corpus_id": "264439179",
            "title": "A Survey on Detection of LLMs-Generated Content",
            "text": "Despite those released standard datasets, we argue that static evaluation benchmarks might not be desirable for this problem with the rapid progress of LLMs trained, tuned, or aligned on large amounts of data across the whole internet. On the one hand, Aaronson (2022) mentioned that some text from Shakespeare or the Bible is often classified as AI-generated because such classic text is frequently used in the training datasets for generative language models. On the other hand, many detectors did not fully disclose their training data, especially commercial tools like GPTZero (Tian, 2023). It is natural to worry that those standard evaluation benchmarks would face a serious test data contamination problem, considering the commercial detectors would consistently improve their products for profits. So, with the rapid evolution of LLMs and detectors, the traditional paradigm of providing standard benchmarks might no longer be suitable for AI-generated text detection. We provide a unique solution to this: \n\nUtilize the most latest human-written content to reduce data contamination problem by collecting such content from the most updated open-source websites, which themselves explicitly forbid posting AI-written posts.",
            "score": 0.4692998949001201,
            "section_title": "Datasets",
            "char_start_offset": 9645,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1014
                },
                {
                    "start": 1017,
                    "end": 1231
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.323974609375
        },
        {
            "corpus_id": "225062097",
            "title": "Speech Activity Detection Based on Multilingual Speech Recognition System",
            "text": "Speech Activity Detection (SAD), a process of identifying the speech segments in an audio utterance [1], is a critical part of Automatic Speech Recognition (ASR), speaker recognition, speaker diarization, and other speech-based applications. Developing an accurate SAD system, operating in the noisy environment is one of the active research fields in speech processing [2][3][4][5][6]. \n\nThis paper explores SAD built around multi-lingual ASR systems, as we hypothesize it can offer better generalization ability by leveraging the contextual information extracted by ASR [7]. Generally, this paper employs a conventional multi-task network as a multi-lingual Acoustic Model (AM) trained using the Lattice-Free Maximum Mutual Information (LF-MMI) framework, capable to extract the language-dependent contextual information. Using a multilingual dataset for the AM training was investigated in [8]. Unlike applying a simple block-softmax loss on stacked input data with added language indicator for phoneme names, we apply LF-MMI loss on multi-task architecture which provides a scalable approach to develop multi-lingual AM. Practically, we use PKWRAP, a Py-Torch package for LF-MMI training of acoustic models [9]. The proposed multi-lingual acoustic model is trained on 18 languages of the BABEL datasets. Within each language-dependent part of AM, speech and non-speech acoustic frames are mapped to a different set of output context-dependent phones (i.e. posteriors). For each language, we use index of maximum output posterior as a frame-level speech/non-speech decision function. To fuse the decisions from different languages, conventional logistic regression and majority voting techniques are employed. \n\nTo investigate the generalization ability of the proposed SAD, experiments presented in the paper were performed on in-domain and out-of-domain data. For out-of-domain experiments, two specific conditions are considered: (i) an access to small development set is available, or (ii) no in-domain data is available at all. Logistic regression and majority voting fusion are reported for these conditions specifically.",
            "score": 0.4692201926267695,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 386
                },
                {
                    "start": 389,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1712
                },
                {
                    "start": 1715,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2130
                }
            ],
            "ref_mentions": [
                {
                    "start": 373,
                    "end": 376,
                    "matchedPaperCorpusId": "204823996"
                },
                {
                    "start": 376,
                    "end": 379,
                    "matchedPaperCorpusId": "204976473"
                },
                {
                    "start": 572,
                    "end": 575,
                    "matchedPaperCorpusId": "226203093"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1610107421875
        },
        {
            "corpus_id": "273901232",
            "title": "Text Fluoroscopy: Detecting LLM-Generated Text through Intrinsic Features",
            "text": "Large language models (LLMs) such as PaLM (Chowdhery et al., 2023), ChatGPT (Ope-nAI, 2022), LLaMA (Touvron et al., 2023), and GPT-4 (Achiam et al., 2023) demonstrate remarkable advancements in language capabilities. LLMs have significantly impacted the field of natural language processing, enabling proficient text generation for diverse tasks, including emails, news, and academic papers. With the advent of more advanced LLMs such as GPT-4, the outstanding performance of LLMs has led to the belief that they can be the artificial general intelligence (AGI) of this era (Bubeck et al., 2023). \n\nHowever, if misused, LLMs such as ChatGPT have the potential to act as a \"weapon of mass deception\" (Sison et al., 2024). For example, the advanced writing capabilities of LLMs pose a significant threat to democracy, as they facilitate the creation of automated bots on social networks that can influence political decisions during election campaigns (Solaiman et al., 2019;Goldstein et al., 2023). Moreover, the use of ChatGPT by students in educational institutions has led to instances of academic dishonesty, with essays being generated by these models, as reported by various news outlets (Mitchell, 2022;Patrick Wood, 2023). Therefore, it is crucial and urgent to detect LLMgenerated texts. \n\nPrevious methods for detecting LLM-generated text can be classified into two categories. The first category relies on the features of the last layer in the language model, e.g., BERT (Guo et al., 2023;Hu et al., 2023;Guo and Yu, 2023), which can be seen as the semantic features (Wu et al., 2023). However, semantic features in human-created and LLM-generated text can be remarkably similar, especially when the topics are more narrowly defined, affecting detection quality and generalization. The second category relies on linguistic features (Yang et al., 2023;Wu et al., 2024;McGovern et al., 2024), which are expressed as differences in the frequency of words and grammatical patterns.",
            "score": 0.46644915669159726,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 596
                },
                {
                    "start": 599,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1295
                },
                {
                    "start": 1298,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1987
                }
            ],
            "ref_mentions": [
                {
                    "start": 42,
                    "end": 66,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 1499,
                    "end": 1515,
                    "matchedPaperCorpusId": "259501842"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2493896484375
        },
        {
            "corpus_id": "270878268",
            "title": "What We Talk About When We Talk About LMs: Implicit Paradigm Shifts and the Ship of Language Models",
            "text": "We provide more details about the semi-automated dataset creation process, including the full list of model names, and the prompts and parameters for the automated part of model name identification. \n\nA.1 A note on the scope of \"LMs\" \n\nIn the rigid sense, LMs shall correspond directly to the task of next-word prediction by enlisting the probabilities of each possible next-word (adapted from the prose of Jurafsky and Martin (2023)). However, practical referents and contexts of \"language modeling\" in the real world have been broad and complex. In fact, in Jurafsky and Martin's work cited above, the definition of LMs (given as a direct statement) has already been broader than previous senses in the prose: Models that assign probabilities to upcoming words, or sequences of words in general, are called language models or LMs. \n\nTo accommodate the underspecification and the even more vague real-world usages, we thus consider the \"broadest\" definitions that are historically involved. This would include text embedding schemes that have essentially been used in the place of \"LMs\", key parts of an LM-like system where the system itself is not otherwise named, or artifacts created in the \"LM\" way but are not exactly a \"model\". (Note that this work concerns \"what we talk about when we talk about LMs\", not just \"which LM(s) we talk about when we talk about LMs\".) This is especially worth noting since the terminology of \"LM\" is not always the most prevalent. As an important instance, \"word embeddings/vectors\" had been a central topic; it is both true that (1) the terms are not equivalent to \"LMs\" strictly speaking, and that (2) it was a very common (if not the default) practice to build (e.g. add a linear layer) on top of the embeddings to obtain probabilities/likelihood of the text in question. This is essentially close to \"LM\" as defined, though we rarely referred to the entire system this way but would have often used wordings like \"word vectors + [classification/prediction/. . . ]\" back then.",
            "score": 0.46644915669159726,
            "section_title": "A The dataset construction pipeline",
            "char_start_offset": 30059,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 201,
                    "end": 233
                },
                {
                    "start": 236,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2017
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07421875
        },
        {
            "corpus_id": "253429710",
            "title": "Validity, Reliability, and Significance: Empirical Methods for NLP and Data Science",
            "text": "of successful application to problems of NLP system evaluation and interpretation. The statistical methods are presented with motivation from first principles, with deep citations into the fundamental statistical literature and formal derivations presented at the right level for an average NLP or ML practitioner to follow. The methods and theory behind them are frequentist, and represent the current best practices within that paradigm, based on linear mixed-effects models (LMEMs) and generalized additive models (GAMs), and using generalized likelihood ratio tests for hypothesis testing. The authors also discuss fundamental philosophical ideas including the theory of measurement and construct validity, providing an overall deeper-than-average view into this literature in a succinct, comprehensible form. \n\nThe book takes the form of a kind of taxonomy of problems that can occur in NLP analysis, with each kind of problem paired with statistical solutions to diagnose and treat it, along with case studies involving Natural Language Inference (NLI), biomedical NLP, and Machine Translation (MT) evaluation, among others. Problems are divided into Validity, Robustness, and Significance. \n\nValidity (discussed in Chapter 2) refers to the extent to which a model captures real generalizable predictive information based on input features. The authors divide feature validity problems into three kinds: (1) bias features, which are spurious features that are correlated with labels in the training data; (2) illegitimate features, where features contain unexpected and non-generalizable information, such as numerical patient IDs containing information about which hospital a patient was treated in, which in turn contains information about what conditions the patient likely has; and (3) circular features, which are features in the training data that deterministically define the label. Notably, these issues are not guarded against by the train-test split paradigm. Although these three kinds of invalidity may seem difficult to distinguish, they can in fact be cleanly separated by different statistical analyses, which the authors motivate after a discussion of measurement theory and construct validity. \n\nOf these, the most interesting analyses are those to diagnose illegitimate features and circular features. Illegitimate features are cast as a form of failure of transformation invariance.",
            "score": 0.46644915669159726,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 813
                },
                {
                    "start": 816,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1196
                },
                {
                    "start": 1199,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2216
                },
                {
                    "start": 2219,
                    "end": 2325
                },
                {
                    "start": 2326,
                    "end": 2407
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1214599609375
        },
        {
            "corpus_id": "264439179",
            "title": "A Survey on Detection of LLMs-Generated Content",
            "text": "The second kind of work lies in the white-box situation when the model's full or partial parameters are accessible. For example, when we have full access to the model, GLTR (Gehrmann et al., 2019a) trains a logistic regression over absolute word ranks in each decoding step. When only partial information like the model output logits are available, SeqXGPT (Wang et al., 2023c) introduce a sentence-level detection challenge by synthesizing a dataset that contains documents that are polished with LLMs and propose to detect it with logits as waves from white-box LLMs. Sniffer (Li et al., 2023a) utilizes the contrastive logits between models as a typical feature for training to perform both detection and origin tracking.",
            "score": 0.46644915669159726,
            "section_title": "White-box",
            "char_start_offset": 16203,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 724
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.336669921875
        },
        {
            "corpus_id": "52899089",
            "title": "Automatic Data Expansion for Customer-care Spoken Language Understanding",
            "text": "ASR aims to find the most probable sequence of words, \u0174 , given a sequence of acoustic observations, X: \n\nwhere X = x 1 , x 2 , ..., x T indicates the sequence of acoustic observations; P [X|W ] computes the acoustic model likelihood and P [W ] computes the language model likelihood. Since the AM and LM scores have different dynamic range, an LM scale factor L is used to balance the scores. \n\nSince in this paper, we focus on LM and NLU models, we assume that AM is unchanged from one experiment to the other. \n\nOur language model is a traditional back-off n-gram LM [3]. Thanks to its simple structure, fast training and easy implementation, n-gram LM has been widely used in the industry. In a back-off n-gram LM, the conditional probability of a word w i given the n \u2212 1 previous words w i\u22121 i\u2212n+1 is computed by: \n\nIn this formula, p bo (w i |w i\u22121 i\u2212n+1 ) is the back-off probability of observing w i ; C(w) is the frequency of w in training set; k is a threshold for the least acceptable number of appearances and d is the Good Turing discounting estimation. Other extensions such as modified Kneser-Ney also called modified shift-beta smoothing have shown very good performance [3]. \n\nOur proposed data expansion algorithm modifies the probability of the specific n-grams that contribute the most in intent prediction. This modification is done by adding more hypotheses that contain those specific n-grams.",
            "score": 0.46644915669159726,
            "section_title": "Automatic speech recognition (ASR)",
            "char_start_offset": 6255,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 106,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 393
                },
                {
                    "start": 396,
                    "end": 512
                },
                {
                    "start": 515,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 819
                },
                {
                    "start": 822,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1192
                },
                {
                    "start": 1195,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1417
                }
            ],
            "ref_mentions": [
                {
                    "start": 570,
                    "end": 573,
                    "matchedPaperCorpusId": "215842252"
                },
                {
                    "start": 1188,
                    "end": 1191,
                    "matchedPaperCorpusId": "215842252"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15283203125
        },
        {
            "corpus_id": "263605550",
            "title": "On the Generalization of Training-based ChatGPT Detection Methods",
            "text": "\u2022 From the pessimistic side, we identify one possible reason that can hurt the detection models' generalization. For the training-based methods, the trained models tend to overfit to some \"irrelevant features\" which are not principal for ChatGPT detection. This overfitting issue can be originated from the \"incautious and insufficient\" data collection process, which collects ChatGPT texts that are distinct from human texts in these \"irrelevant features\". In Section 4.3, we conduct a theoretical analysis to deeply understand this phenomenon. \u2022 From an optimistic side, we find the trained models are also capable to extract \"transferable features\", which are shared features that can help detect the ChatGPT generated texts from various topics and language tasks. For example, in Section 5, we show that the models trained on existing topics or language tasks can be leveraged as a source model to accommodate transfer learning (Pan & Yang, 2009;Hendrycks et al., 2019), when it is adapted to unforeseen topics and language tasks.",
            "score": 0.46346794876504227,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3723,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 1034
                }
            ],
            "ref_mentions": [
                {
                    "start": 932,
                    "end": 950,
                    "matchedPaperCorpusId": "740063"
                },
                {
                    "start": 950,
                    "end": 973,
                    "matchedPaperCorpusId": "59336190"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06866455078125
        },
        {
            "corpus_id": "277940370",
            "title": "STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings",
            "text": "The problem of dataset membership (Maini et al., 2021) aims to determine whether a dataset X has been included in the pretraining data D train of a language model \u03b8. We operate under a gray-box setting, where we can compute token probabilities for any sequence S but have no access to the pretraining data or model weights. Formally detecting membership of a dataset can be posed as a hypothesis test with the goal to distinguish between the following hypothesis: \n\nwhere we treat \u03b8 as a random variable whose randomness arises from the sampling of the pretraining dataset D train (which may or may not include X). Framing membership inference (Shokri et al., 2017) as hypothesis testing provides statistical guarantees on the false detection rate. \n\nOur focus is on building statistical tests that can reliably detect dataset membership in language models. We aim to develop methods that make minimal assumptions about the format or nature of data-be it machine learning benchmarks, newsletters, or books.",
            "score": 0.4627867866187167,
            "section_title": "Dataset Membership",
            "char_start_offset": 5807,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 463
                },
                {
                    "start": 466,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 748
                },
                {
                    "start": 751,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1006
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3779296875
        },
        {
            "corpus_id": "17381137",
            "title": "A hybrid formalism to parse Sign Languages",
            "text": "The presented formalism is used to describe models for recognition. The recognition itself has been implemented in a parser. We give here an outline only of the developed system. The detailed description is the subject of a dedicated article. \n\nIn addition to the formalized model, the parser needs an input to parse. This input is an annotation of a subset of model's units (as well pattern as alternatives). Units of this subset are said to be detectable. Their annotation can originate from manual annotation or third party detectors. Detectable units appear in red on the figures. The parser is able to work interactively with the detectors. In this case, it informs the detectors of the context and therefore reduces their search spaces. \n\nOur work extends the ideas of Mahanti (Mahanti et al., 2003) for the parsing. The internal representation of the model in the parser is an AND/OR graph. This representation is called the implicit graph. In this, nodes represent units and not instances. The figure 4 gives an example of graph. The parser then explore an explicit graph (without building it) to find subgraph which are solutions. The figure 5 shows an example of such solution as output by the parser. The nodes represent occurrences either externally detected or internally inferred. The arcs correspond to constituency or dependency relations of the model. \n\nThe parser is currently top-down. It builds the solution graphs starting from a set of given roots. This set can be, for example, a set of pre-detected lexical unit occurrences resulting of a first pass of lexical recognition. It is how the parser process dependency-based models. It, then, builds trees top-down from each root and merges the trees when possible. It is therefore obvious than solution graphs can have multiple connected components. This appears, for example, when a signer is interrupted by a question, answers quickly and then continues his/her speech. In the case of constituency-based models, the top-down parsing requires to introduce a detectable root. It is the function of the \"Signing\" unit in figure 4 which is detected with an activity detector.",
            "score": 0.46186137106523967,
            "section_title": "Parsing",
            "char_start_offset": 10630,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 242
                },
                {
                    "start": 245,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 742
                },
                {
                    "start": 745,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1368
                },
                {
                    "start": 1371,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1941
                },
                {
                    "start": 1942,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2143
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10247802734375
        },
        {
            "corpus_id": "259095643",
            "title": "On the Reliability of Watermarks for Large Language Models",
            "text": "We briefly describe the methods below and provide further details and hyperparameters in Appendix A.11.3. \n\nRetrieval: A retrieval approach to detection requires the creation and maintenance of a comprehensive database of all sequences previously generated by the language model to check against at test time and we adopt the retrieval method of (Krishna et al., 2023), utilizing the BM25 search method since it performed the best in their experiments.",
            "score": 0.46186137106523967,
            "section_title": "PARAPHRASING BY HUMAN WRITERS",
            "char_start_offset": 21192,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 108,
                    "end": 452
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5712890625
        },
        {
            "corpus_id": "263830440",
            "title": "GPT-who: An Information Density-based Machine-Generated Text Detector",
            "text": "Large Language Models (LLMs) such as GPT-3.5, GPT-4 (OpenAI, 2023), LLaMA (Touvron et al., 2023), Falcon (Penedo et al., 2023), have the capacity to generate human-like-quality texts, which can be easily construed as human-written (Sadasivan et al., 2023;Chakraborty et al., 2023;Zhao et al., 2023). However, while such LLMs are remarkable, it, therefore, makes them susceptible to malicious use. These include the generation of toxic and harmful content, like misinformation and terrorism recruitment (Shevlane et al., 2023;Zellers et al., 2019;Uchendu et al., 2021). Due to such potential for misuse, we must develop techniques to distinguish human-written texts from LLM-generated ones to mitigate these risks. \n\nTo mitigate this potential for misuse of LLMs, researchers have developed several types of automatic detectors. These techniques include supervised (Uchendu et al., 2021;Zellers et al., 2019;Uchendu et al., 2020;Zhong et al., 2020;Kushnareva et al., 2021;Liu et al., 2022) and unsupervised approaches (Gehrmann et al., 2019;Mitchell et al., 2023;Gall\u00e9 et al., 2021;He et al., 2023;Su et al., 2023). These supervised approaches tend to be stylometric-, deep learningand ensemble-based models while most unsupervised approaches are statistical-based detectors (Uchendu et al., 2023;Yang et al., 2023). \n\nMore recently, due to the increased ubiquity of LLMs, we need more interpretable, and less deep learning-based models. Deep learning models have been shown to be the most susceptible to adversarial perturbations than others (Pu et al., 2022).",
            "score": 0.46186137106523967,
            "section_title": "Machine-Generated Text Detection",
            "char_start_offset": 5571,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 713
                },
                {
                    "start": 716,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1315
                },
                {
                    "start": 1318,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1560
                }
            ],
            "ref_mentions": [
                {
                    "start": 525,
                    "end": 546,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 546,
                    "end": 567,
                    "matchedPaperCorpusId": "237589233"
                },
                {
                    "start": 864,
                    "end": 886,
                    "matchedPaperCorpusId": "237589233"
                },
                {
                    "start": 886,
                    "end": 907,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 928,
                    "end": 947,
                    "matchedPaperCorpusId": "222377949"
                },
                {
                    "start": 947,
                    "end": 971,
                    "matchedPaperCorpusId": "237485580"
                },
                {
                    "start": 1017,
                    "end": 1040,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 1274,
                    "end": 1296,
                    "matchedPaperCorpusId": "252992519"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.326904296875
        },
        {
            "corpus_id": "264426031",
            "title": "Toward Stronger Textual Attack Detectors",
            "text": "We use two baseline detectors. The first one is based on a language model likelihood and the second one corresponds to the Mahalanobis detector introduced in [Yoo et al., 2022]. Both of them follow the analog three consecutive steps of LAROUSSE, but do not use the same similarity score. \n\nLanguage model score. This method consists in computing the likelihood of an input with an external language model: (6) where \u03c9 i represents the individual token of the input sentence x. We compute the log-probabilities with the output of a pretrained GPT2 [Brown et al., 2020]. Notice that this baseline is also used in [Yoo et al., 2022]. \n\nMahalanobis-based detector. We follow [Yoo et al., 2022] which relies on a class-conditioned Mahalanobis distance. Following our notations, it corresponds to evaluation: \n\nwhere \u00b5 \u0177 is the empirical mean for the logits of class \u0177 and \u03a3 \u0177 is the associated empirical covariance. \n\nRemark 3 Similarly to Remark 1, for a given textual input x, we will either rely on the penultimate layer L representation f L \u03c8 (x) or on the logits predictions f L+1 \u03c8 (x) of the networks to compute s M .",
            "score": 0.46186137106523967,
            "section_title": "Baseline detectors",
            "char_start_offset": 18386,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 287
                },
                {
                    "start": 290,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 630
                },
                {
                    "start": 633,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 802
                },
                {
                    "start": 805,
                    "end": 910
                },
                {
                    "start": 913,
                    "end": 1119
                }
            ],
            "ref_mentions": [
                {
                    "start": 547,
                    "end": 567,
                    "matchedPaperCorpusId": "207599948"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.236572265625
        },
        {
            "corpus_id": "268164992",
            "title": "The Optimization of a Natural Language Processing Approach for the Automatic Detection of Alzheimer\u2019s Disease Using GPT Embeddings",
            "text": "To perform NLP, most models use word embeddings, which are N-dimensional vector representations of words (Figure 1). Embeddings allow neural networks (NNs) and other machine learning classifiers (MLCs) to process language through semantic meaning, unlike other techniques that focus on the frequency of specific words, among other aspects [8]. One of the most advanced LLMs is OpenAI's Generative Pretrained Transformer 3 (GPT-3), which is known for its use in ChatGPT. Based on the GPT-3 architecture, OpenAI offers a set of highly advanced, cost-effective embedding models [9]. First-generation versions of these models have shown promising results when it comes to the NLP-based automatic detection of AD [10]. \n\nPast research into the automatic detection of AD using speech has focused on using either acoustic features or NLP techniques [10,11]. While acoustic feature-based models have been shown to perform effectively, achieving accuracies of 63.6% in Chlasta and Wolk's work [12] using a convolutional neural network (CNN) and 65.6% in Balugopalan and Novikova's work [13] using a support vector machine (SVM) classifier, Balugopalan and Novikova showed that a word embedding or combination approach was more effective. They performed better in nearly all metrics using several machine learning classifiers, achieving an accuracy of 66.9% for embeddings and 69.2% for the combination using SVM. Cruz et al. [14] used NLP techniques, specifically sentence embeddings, using Siamese BERT-Networks (SBERT) to create embeddings and test the effectiveness of several types of ML classifiers. They found that SVMs and neural networks (NNs) were the most effective, achieving accuracies and F-1 scores (the harmonic mean of precision and recall) of 0.77 and 0.80 (SVM) and 0.78 and 0.76 (NN), respectively. Figure 1. Two-degree vector graphical interpretation of N-degree vector word embeddings to convey linguistic meaning in a numerical format.",
            "score": 0.46186137106523967,
            "section_title": "Introduction",
            "char_start_offset": 3823,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 713
                },
                {
                    "start": 716,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1948
                }
            ],
            "ref_mentions": [
                {
                    "start": 339,
                    "end": 342,
                    "matchedPaperCorpusId": "233196376"
                },
                {
                    "start": 708,
                    "end": 712,
                    "matchedPaperCorpusId": "255029590"
                },
                {
                    "start": 842,
                    "end": 846,
                    "matchedPaperCorpusId": "255029590"
                },
                {
                    "start": 846,
                    "end": 849,
                    "matchedPaperCorpusId": "239041284"
                },
                {
                    "start": 984,
                    "end": 988,
                    "matchedPaperCorpusId": "231886053"
                },
                {
                    "start": 1416,
                    "end": 1420,
                    "matchedPaperCorpusId": "246928111"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.203857421875
        },
        {
            "corpus_id": "270062554",
            "title": "ReMoDetect: Reward Models Recognize Aligned LLM's Generations",
            "text": "We suggest utilizing the human and LLM mixed dataset to further improve the detection performance.Specifically, we partially rephrase human-written texts using aligned LLMs to generate the mixed dataset, which are considered as median preference datasets between LGTs and human-written texts.Note that such a technique introduces new samples that behave like a reasonable near-decision boundary sample, which enables the detector to learn a better decision boundary.For instance, multiple out-of-distribution detection methods utilize generated samples [23] such as mixup data [46,16] as a near-decision boundary sample to improve the detector's calibration.\n\nConcretely, for a given context x and the human-written response y HU , we partially rephrase the response with a ratio of p, using LLM M rep , i.e., y MIX := M rep (y HU |x, p).We consider y MIX as a median preference response between human-written text y HU and LGT y LM which is formally described as:\n\nSince the Bradely-Terry modeling assumes binary classification, we consider dividing the triplet into three binary classification problems, i.e., y LM \u227b y HU | x, y LM \u227b y MIX | x, and y MIX \u227b y HU | x.Therefore, the final training objective of ReMoDetect additionally considers the mixed dataset's preference modeling in addition to Eq. ( 1), which is as follows:\n\nwhere \u03b2 1 and \u03b2 2 are parameters that chooses the contribution of the mixed data y MIX .\n\nDetection stage.After training ReMoDetect, we use the predicted reward score r \u03d5 (x, y) to determine whether the given text is LGT or human-written texts where a higher score indicates LGT.Unlike recent detection schemes that require multiple forwards (for perturbing the input [10,12]), ReMoDetect only requires a single forward pass, thus showing inference efficiency (in Appendix B.4).",
            "score": 0.46186137106523967,
            "section_title": "Reward Modeling of Human and LLM Mixed Dataset",
            "char_start_offset": 11296,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 98,
                    "end": 292
                },
                {
                    "start": 292,
                    "end": 466
                },
                {
                    "start": 466,
                    "end": 658
                },
                {
                    "start": 660,
                    "end": 838
                },
                {
                    "start": 838,
                    "end": 964
                },
                {
                    "start": 966,
                    "end": 1168
                },
                {
                    "start": 1168,
                    "end": 1330
                },
                {
                    "start": 1332,
                    "end": 1420
                },
                {
                    "start": 1422,
                    "end": 1438
                },
                {
                    "start": 1438,
                    "end": 1611
                },
                {
                    "start": 1611,
                    "end": 1810
                }
            ],
            "ref_mentions": [
                {
                    "start": 553,
                    "end": 557,
                    "matchedPaperCorpusId": "3464416"
                },
                {
                    "start": 577,
                    "end": 581,
                    "matchedPaperCorpusId": "3162051"
                },
                {
                    "start": 581,
                    "end": 584,
                    "matchedPaperCorpusId": "208637407"
                },
                {
                    "start": 1700,
                    "end": 1704,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 1704,
                    "end": 1707,
                    "matchedPaperCorpusId": "263831345"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05560302734375
        },
        {
            "corpus_id": "273638693",
            "title": "Humanizing the Machine: Proxy Attacks to Mislead LLM Detectors",
            "text": "To validate our approach for evading detection, we include the most advanced open-source LLMs: Llama2, Llama3, and Mixtral. Specifically, for the base large model, we use Llama2-13B (Llama2-13B-Chat), Llama3-70B (Llama3-70B-Instruct), and Mixtral-8\u00d77B (Mixtral-8\u00d77B-Instruct-v0.1). We use these source models as the base large models, attacked by small humanized model Llama2-7B (Llama2-7B-Chat), Llama3-8B (Llama3-8B-Instruct) and Mistral-7B (Mistral-7B-Instruct-v0.1), respectively. \n\nThe Detectors. We conduct experiments with a variety of strong open-source detectors from prior literature, including RoBERTa-base and RoBERTa-large (Solaiman et al., 2019), the language models trained for detection, the zero-shot detectors Likelihood and Log Rank, the perturbation-based zero-shot method DetectGPT (Mitchell et al., 2023a), DetectLLM (Su et al., 2023) and DNA-GPT (Yang et al., 2023a). We also involve Fast-DetectGPT (Bao et al., 2023) that uses a surrogate model respectively to compute the conditional probability curvature for the texts obtained from the sampling model, and Binoculars Hans et al. (2024) that computes the ratio of perplexity to crossperplexity obtained from \"observer\" and \"performer\" models. \n\nThe Settings. We evaluate the zero-shot methods in two settings, the white-box (source model is known) setting and black-box (source model is unknown) setting (Yang et al., 2023b;Bao et al., 2023). Following (Bao et al., 2023), we set the surrogate model in each detector to be identical to the source model, whereas in the black-box setting, the surrogate model differs from the source model. We utilize GPT-Neo-2.7B",
            "score": 0.46186137106523967,
            "section_title": "EXPERIMENTAL SETUPS",
            "char_start_offset": 19558,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 484
                },
                {
                    "start": 487,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1638
                }
            ],
            "ref_mentions": [
                {
                    "start": 803,
                    "end": 827,
                    "matchedPaperCorpusId": "256274849"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09844970703125
        },
        {
            "corpus_id": "226283453",
            "title": "Hybrid Emoji-Based Masked Language Models for Zero-Shot Abusive Language Detection",
            "text": "The focus of this paper is the abusive language detection task, which has been widely explored in the last years thanks to numerous datasets, approaches and shared tasks (Waseem et al., 2017;Fi\u0161er et al., 2018;Carmona et al., 2018;Wiegand et al., 2018;Bosco et al., 2018;Zampieri et al., 2019b;Roberts et al., 2019) covering different languages. An increasing number of approaches has been proposed to detect this kind of messages (for a survey on the task, see (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018)). \n\nAbusive language detection is usually framed as a supervised learning problem, built using a combination of manually crafted features such as n-grams (Wulczyn et al., 2017), syntactic features (Nobata et al., 2016), and linguistic features (Yin et al., 2009), to more recent neural networks (Park and Fung, 2017;Zhang and Tepper, 2018;Agrawal and Awekar, 2018;Corazza et al., 2018). (Lee et al., 2018) address a comparative study of various learning models on the Hate and Abusive Speech on Twitter dataset (Founta et al., 2018), while (Zampieri et al., 2019a) build the Offensive Language Identification Dataset and experiment with SVMs, BiLSTM and CNN both on the binary abusive language classification and on a more fine-grained categorization. Our work deals with the same task, addressed from a cross-lingual perspective. \n\nIn recent years, some proposals have been made to tackle abusive language detection in a cross-lingual framework (Sohn and Lee, 2019;Pamungkas and Patti, 2019;Casula et al., 2020), with some attempts at zero-shot learning (Stappen et al., 2020). Most systems, however, rely on pretrained models and do not investigate the potential of indomain data for pretraining. Additionally, as regards masked language models, we are not aware of any work in the literature modifying masking mechanisms for this task. \n\n3 Cross-Lingual Language Models",
            "score": 0.46157901742428975,
            "section_title": "Related work",
            "char_start_offset": 2114,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 521
                },
                {
                    "start": 524,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1350
                },
                {
                    "start": 1353,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1858
                },
                {
                    "start": 1861,
                    "end": 1892
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 191,
                    "matchedPaperCorpusId": "1721388"
                },
                {
                    "start": 210,
                    "end": 231,
                    "matchedPaperCorpusId": "51941810"
                },
                {
                    "start": 252,
                    "end": 271,
                    "matchedPaperCorpusId": "56483142"
                },
                {
                    "start": 271,
                    "end": 294,
                    "matchedPaperCorpusId": "84843035"
                },
                {
                    "start": 294,
                    "end": 315,
                    "matchedPaperCorpusId": "239395701"
                },
                {
                    "start": 462,
                    "end": 489,
                    "matchedPaperCorpusId": "9626793"
                },
                {
                    "start": 494,
                    "end": 519,
                    "matchedPaperCorpusId": "52184457"
                },
                {
                    "start": 674,
                    "end": 696,
                    "matchedPaperCorpusId": "6060248"
                },
                {
                    "start": 717,
                    "end": 738,
                    "matchedPaperCorpusId": "11546523"
                },
                {
                    "start": 764,
                    "end": 782,
                    "matchedPaperCorpusId": "14895410"
                },
                {
                    "start": 815,
                    "end": 836,
                    "matchedPaperCorpusId": "687037"
                },
                {
                    "start": 836,
                    "end": 859,
                    "matchedPaperCorpusId": "46939253"
                },
                {
                    "start": 859,
                    "end": 884,
                    "matchedPaperCorpusId": "4133818"
                },
                {
                    "start": 884,
                    "end": 905,
                    "matchedPaperCorpusId": "56482679"
                },
                {
                    "start": 1031,
                    "end": 1052,
                    "matchedPaperCorpusId": "3640499"
                },
                {
                    "start": 1060,
                    "end": 1084,
                    "matchedPaperCorpusId": "67856299"
                },
                {
                    "start": 1466,
                    "end": 1486,
                    "matchedPaperCorpusId": "216641657"
                },
                {
                    "start": 1486,
                    "end": 1512,
                    "matchedPaperCorpusId": "196184195"
                },
                {
                    "start": 1512,
                    "end": 1532,
                    "matchedPaperCorpusId": "227231149"
                },
                {
                    "start": 1575,
                    "end": 1597,
                    "matchedPaperCorpusId": "216641657"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.314208984375
        },
        {
            "corpus_id": "237940142",
            "title": "Mitigating Racial Biases in Toxic Language Detection with an Equity-Based Ensemble Framework",
            "text": "a general toxic language detection model, in our case a vanilla-BERT classifier trained on the original dataset. If the general model predicted a positive instance (hate or toxic), we then used the out-of-box Blodgett et al. dialect estimation model [4] to predict the probability the sample is AAE. If the sample is not AAE, we returned the predicted result from the general classifier. However, if the text is likely AAE we then passed it through the specialized AAE classifier and the ensemble model returned that prediction. The specialized classifier is created by fine-tuning the vanilla BERT on a masked language model task with Blodgett et al. 's corpus of demographic based Twitter data that belongs to African American authors [4]. We then use this BERT-AAE language model to fine-tune a classifier head on the AAE samples in the same train-validation-test splits that the general model was trained on. The final prediction for the positive AAE sample is the prediction of the specialized AAE classifier.",
            "score": 0.461567867225214,
            "section_title": "Models and Bias Remediation Techniques",
            "char_start_offset": 11517,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 250,
                    "end": 253,
                    "matchedPaperCorpusId": "1066490"
                },
                {
                    "start": 737,
                    "end": 740,
                    "matchedPaperCorpusId": "1066490"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1492919921875
        },
        {
            "corpus_id": "276575250",
            "title": "IPAD: Inverse Prompt for AI Detection - A Robust and Explainable LLM-Generated Text Detector",
            "text": "Large Language Models (LLMs), characterized by their massive scale and extensive training data (Chen et al., 2024), have achieved significant advances in natural language processing (NLP) (Ouyang et al., 2022;Veselovsky et al., 2023;Wu et al., 2025). However, with the advanced capabilities of LLMs, they are subject to frequent misused in various domains, including academic fraud, the creation of deceptive material, and the generation of fabricated information (Ji et al., 2023; * Authors contributed equally to this work. Pagnoni et al., 2022;Mirsky et al., 2023), which underscores the critical need to distinguish between human-written text (HWT) and LLM-generated text (LGT) (Pagnoni et al., 2022;Yu et al., 2025;Kirchenbauer et al., 2023). \n\nHowever, due to their sophisticated functionality, LLMs pose significant challenges in the robustness of current AI detection systems (Wu et al., 2025). The existing detection systems, including commercial ones, frequently misclassify texts as HWT (Price and Sakellarios, 2023;Walters, 2023) and generate inconsistent results when analyzing the same text using different detectors (Chaka, 2023;Weber-Wulff et al., 2023). Studies show false positive rates reaching up to 50% and false negative rates as high as 100% in different tools (Weber-Wulff et al., 2023) when dealing with out-of-distribution (OOD) datasets. \n\nAnother critical issue with the existing AI detection systems is their lack of verifiable evidence (Halaweh and Refae, 2024), as these tools typically provide only simple outputs like \"likely written by AI\" or percentage-based predictions (Weber-Wulff et al., 2023). The lack of evidence prevents users from defending themselves against false accusations (Chaka, 2023) and hinders organizations from making judgments based solely on the detection results without convincing evidences (Weber-Wulff et al., 2023).",
            "score": 0.4612836233643408,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1364
                },
                {
                    "start": 1367,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1878
                }
            ],
            "ref_mentions": [
                {
                    "start": 188,
                    "end": 209,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 233,
                    "end": 249,
                    "matchedPaperCorpusId": "264426138"
                },
                {
                    "start": 464,
                    "end": 481,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 526,
                    "end": 547,
                    "matchedPaperCorpusId": "252819091"
                },
                {
                    "start": 682,
                    "end": 704,
                    "matchedPaperCorpusId": "252819091"
                },
                {
                    "start": 704,
                    "end": 720,
                    "matchedPaperCorpusId": "258298978"
                },
                {
                    "start": 720,
                    "end": 746,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 884,
                    "end": 901,
                    "matchedPaperCorpusId": "264426138"
                },
                {
                    "start": 998,
                    "end": 1027,
                    "matchedPaperCorpusId": "265492104"
                },
                {
                    "start": 1027,
                    "end": 1041,
                    "matchedPaperCorpusId": "263672710"
                },
                {
                    "start": 1131,
                    "end": 1144,
                    "matchedPaperCorpusId": "260211177"
                },
                {
                    "start": 1144,
                    "end": 1169,
                    "matchedPaperCorpusId": "259262442"
                },
                {
                    "start": 1284,
                    "end": 1310,
                    "matchedPaperCorpusId": "259262442"
                },
                {
                    "start": 1466,
                    "end": 1491,
                    "matchedPaperCorpusId": "273995908"
                },
                {
                    "start": 1606,
                    "end": 1632,
                    "matchedPaperCorpusId": "259262442"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2447509765625
        },
        {
            "corpus_id": "90260109",
            "title": "Interpreting Black Box Models with Statistical Guarantees",
            "text": "These theoretical results, combined with the empirical results in Section 4, suggest that sufficient estimation of the conditional is tractable in practice. \n\nAn added benefit of the testing framework is that it can lever-age three distinct fields of machine learning research. First, any state-of-the-art classifiers and regressors can be used, because the framework is model-agnostic. Second, in the case of high-dimensional data like images and language, autoregressive models for joint probability estimation in these tasks can be reused as approximate null models. Third, though we manually selected image regions in Section 4.2, anomaly and object detection methods can be used to automatically find candidate features to test. Analogously, dependency and syntactic parsing could be used to detect spans of candidate word features for language models. Each of these fields has a substantial literature, and future gains in these methods will translate to improving the performance of the IRT and OSFT. \n\nThe test statistics proposed in Section 3.4 reflect traditional one-sided and two-sided tests, but the framework admits any test statistic. There may be other statistics with higher power or that better fit a specific interpretation task. Samples could also be stratified into groups so that feature importance could be analyzed at the group level. For example, a cancer patient cohort may be segmented first by the progress grade of their tumor, then model predictions from gene expression data could be analyzed for each group. We plan to explore these ideas in future work.",
            "score": 0.4607062459410798,
            "section_title": "Neg Pos",
            "char_start_offset": 30452,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 159,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1007
                },
                {
                    "start": 1010,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1586
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.257568359375
        },
        {
            "corpus_id": "250562772",
            "title": "Language Models for Code-switch Detection of te reo M\u0101ori and English in a Low-resource Setting",
            "text": "We experiment with various language models and classifiers for two main tasks: language detection (LD) and code-switch detection (CS). Our ultimate goal is to find a combination of language modelling and NLP techniques to improve the overall accuracy of LD and CS tasks. We use three databases to evaluate these tasks with details provided in Table 1. We use the Hansard database sentences as the primary data for training and testing. All three datasets were pre-processed by lower-casing and using regular expressions to remove punctuation using Python 3.9 library with Pandas data frame. All experimental results are obtained from a random seeds training-testing scheme; 70% of the shuffled data is used for training, with 10% for validation and 20% for testing, and averaged over three runs. The variation of these three independent runs is within a range of \u00b10.015.\n\nTo represent text we use both fastText pre-trained embeddings (see Table 3) and sparse vectors obtained from BOW representations. An overview of code-switch detection using trained models such as BiLSTM and CNN is presented in Figure 1. This diagram is an example to demonstrate the system we used for end-to-end code-switch detection using neural networks.\n\nStep 1 includes training and evaluating a neural network. We use the training set of the Hansard database to train the model and use validation loss as the stopping condition to avoid over-fitting. In step 2, we load the trained model and detect languages at the word level on testing data. Once the language detection is done, the points in the sentence where the language labels switch from M\u0101ori to English or from English to M\u0101ori are marked as code-switch points.\n\nNeural network models presented in this research are implemented using Keras/Tensorflow. Adam (Kingma and Ba, 2015), an adaptive learning rate optimisation algorithm, is used as the optimiser for neural networks. Softmax activation function is used in the output layer of the network. We use a combination of dropout (Srivastava et al., 2014), with a rate of 0.5, and early stopping (Zhang et al., 2017) to avoid over-fitting. We use a maximum length of 250 tokens (or words) for BiLSTM and CNN, and padding for sentences with",
            "score": 0.4579051072731272,
            "section_title": "Experimental Setup",
            "char_start_offset": 15129,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.479736328125
        },
        {
            "corpus_id": "263605550",
            "title": "On the Generalization of Training-based ChatGPT Detection Methods",
            "text": "In this part, we identify a ponteitial way to improve the ChatGPT detection in the unforeseen tasks (or topics). It is based on our finding that the models trained in each individual task can learn helpful features for other tasks. As an evidence, in Figure 8, we visualize the learned representations for various tasks rendered by the trained model in Section 5.1. From these figures, we note that the ChatGPT and human texts from unseen tasks during training are also well-separated in the representation space. It demonstrates the models can indeed learn useful features which are helpful to distinguish human and ChatGPT texts in other domains, so we call them \"Transferable Features\". We use the blue color to highlight the case that transfer learning outperforms training from scratch. \n\nTo further verify the existence of transferable features, we conduct experiments to investigate transfer learning (Hendrycks et al., 2019) for domain adaption. In reality, if the model trainer encounters test samples from the language tasks (or topics) which are not involved in the training set, it is a practical and feasible solution for them to collect several samples in the same task as the test sample by themselves. Therefore, in our study, we consider two types of transfer learning strategies: Linear Probing (LP), which refers to the strategy that only the linear classifier (based on extracted features) is optimized; and Fine Tuning (FT) which refers to the strategy that all layers are optimized. \n\nIn our experiment, we consider there are 5 and 10 more samples from both human data and ChatGPT texts are sampled for fine-tuning the models. In Table 4, we report the tuned models performance (F1 score) when tested on different targeted (downstream) tasks from various source models. For example, \"r \u2192 n\" means the model transferred from \"review\" for a downstream task \"news\". Besides, we also include the original performance (from Figure 7), which is the performance before transfer learning (denoted as \"No Transfer\" in Table 4). For comparison, we report the result if these models are tuned from scratch (on pre-trained RoBERTa-base model without training for detection). \n\nFrom the result, we can see transfer learning can benefit the detection performance in general.",
            "score": 0.4573863517092007,
            "section_title": "FINE-TUNING WITH A FEW SAMPLES HELPS CROSS-DOMAIN / TOPIC DETECTION",
            "char_start_offset": 27423,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 791
                },
                {
                    "start": 794,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1504
                },
                {
                    "start": 1507,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2184
                },
                {
                    "start": 2187,
                    "end": 2282
                }
            ],
            "ref_mentions": [
                {
                    "start": 908,
                    "end": 932,
                    "matchedPaperCorpusId": "59336190"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1707763671875
        },
        {
            "corpus_id": "8350223",
            "title": "Learning unification-based natural language grammars",
            "text": "Induction has long been a popular learning method in machine learning (for example, [129,83,81,111,98,99]). Broadly speaking, when sufficient instances of some concept have been seen, a concept description can then be constructed. The concept description can then be used to classify other instances. For example, having observed the set of instances {a, aa, aaa}, the learner might construct the concept (regular expression) a n . Clearly, this describes the instances, along with other, unseen instances, such as aaaa. Inductive approaches have also been used in linguistics.\n\nHarris [48] and others developed procedures to carry out \"distributional analysis\"; the procedure, when applied to a corpus, derived the rules of grammar from the corpus [73, p.157]. This line of research fell by the wayside. However, more recently, speech recognition researchers have re-adopted inductive grammar construction approaches [76, p.13]. These workers base their work on Shannon's Noisy Channel\n\nModel [109,25]. This can be described as follows. Suppose a sequence of objects P has been corrupted in some manner, giving the sequence of objects W. The most likely sequence of objects \u03c1 can be recovered from W by hypothesising all possible sequences of P and selecting the sequence of P that is most likely. That is, \u03c1 = argmax P P r(P )P r(W | P ) (5.1) P r(P ) is the probability that P will be present in the channel and P r(W | P ) is the probability of W given P . Argmax finds the argument with the maximal score.\n\nChannel characterisation is in reality far too difficult to achieve correctly and so channel approximations such as N-grams and Stochastic Context Free Grammars are used instead to determine the most likely sequence of events. These approximations are inductively constructed (which constitutes the learning aspect of using the Noisy Channel approach), using training material of some form.\n\nCorpus linguists (for example [40,112,46,120,6]), have, following the success of the speech recognition community's use of the Noisy Channel Model, also adopted this model. A popular reformulation of the model is to use a treebank and a matching algorithm",
            "score": 0.45732318312132936,
            "section_title": "Data-driven learning in machine learning and in linguistics",
            "char_start_offset": 88183,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 102,
                    "matchedPaperCorpusId": "13252401"
                },
                {
                    "start": 102,
                    "end": 105,
                    "matchedPaperCorpusId": "6746439"
                },
                {
                    "start": 994,
                    "end": 999,
                    "matchedPaperCorpusId": "5747983"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.049041748046875
        },
        {
            "corpus_id": "220936152",
            "title": "Trojaning Language Models for Fun and Profit",
            "text": "Language models -Central to modern NLP, language models (LMs) describe the distributions of token sequences (e.g., individual words, phrases, sentences). In the following, we mainly consider Transformer-based LMs (e.g., BERT [1] and GPT-2 [2]), which typically take as input the word embedding of individual tokens of a given sequence and generate the embedding of the whole sequence (i.e., from contextindependent embedding to context-sensitive embedding). Formally, we define an LM f as a sequence function mapping R n\u00d7d \u2192 R n\u00d7d , where n is the input sequence length and d is the embedding dimensionality. For simplicity, here we assume the input and output embedding shares the same dimensionality. \n\nPre-training and fine-tuning -Today's LMs are often pretrained over massive, unlabeled corpus (e.g., WebText) under an unsupervised setting. For instance, an LM f may be trained for the tasks including: (i) Mask language modeling -f is trained to predict the missing tokens within a given sequence (e.g., 15% tokens of each sequence are randomly masked). Let X be a token sequence and C be its context (e.g., X's surrounding tokens). This training gives f the capability of modeling the conditional probability Pr(X|C) of X appearing within the context of C. (ii) Next sentence prediction -f is trained to predict whether one token sequence C is followed by another sequence X. This training gives f the capability of modeling the conditional probability Pr(X|C) of X entailing C, where C can be considered as X's context. \n\nIn the fine-tuning stage, the LM f is further composed with a downstream model (classifier or regressor) g to form an endto-end system g\u2022f . Typically, with labeled data available from the downstream task, both f ' and g's parameters are finetuned under a supervised setting.",
            "score": 0.45732318312132936,
            "section_title": "A. Preliminaries",
            "char_start_offset": 4403,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1527
                },
                {
                    "start": 1530,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1805
                }
            ],
            "ref_mentions": [
                {
                    "start": 239,
                    "end": 242,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.045440673828125
        },
        {
            "corpus_id": "5087912",
            "title": "A Classification Approach to Word Prediction",
            "text": "The most influential problem in motivating statistical learning application in NLP tasks is that of word selection in speech recognition (Jelinek, 1998). There, word classifiers are derived from a probabilistic language model which estimates the probability of a sentence s using Bayes rule as the product of conditional probabilities, P r(s) . = P r(w 1 , w 2 , . . . w n ) = . \n\n= \u03a0 n i=1 P r(w i |w 1 , . . . w i\u22121 ) . = \u03a0 n i=1 P r(w i |h i ) \n\nwhere h i is the relevant history when predicting w i . Thus, in order to predict the most likely word in a given context, a global estimation of the sentence probability is derived which, in turn, is computed by estimating the probability of each word given its local context or history. Estimating terms of the form P r(w|h) is done by assuming some generative probabilistic model, typically using Markov or other independence assumptions, which gives rise to estimating conditional probabilities of n-grams type features (in the word or POS space). Machine learning based classifiers and maximum entropy models which, in principle, are not restricted to features of these forms have used them nevertheless, perhaps under the influence of probabilistic methods (Brill, 1995;Yarowsky, 1994;Ratnaparkhi et al., 1994). \n\nIt has been argued that the information available in the local context of each word should be augmented by global sentence information and even information external to the sentence in order to learn better classifiers and language models. Efforts in this directions consists of (1) directly adding syntactic information, as in (Chelba and Jelinek, 1998;Rosenfeld, 1996), and (2) indirectly adding syntactic and semantic information, via similarity models; in this case n-gram type features are used whenever possible, and when they cannot be used (due to data sparsity), additional information compiled into a similarity measure is used (Dagan et al., 1999).",
            "score": 0.45732318312132936,
            "section_title": "Background",
            "char_start_offset": 1460,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 378
                },
                {
                    "start": 381,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 446
                },
                {
                    "start": 449,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1266
                },
                {
                    "start": 1269,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1927
                }
            ],
            "ref_mentions": [
                {
                    "start": 1212,
                    "end": 1225,
                    "matchedPaperCorpusId": "134248"
                },
                {
                    "start": 1225,
                    "end": 1240,
                    "matchedPaperCorpusId": "1580335"
                },
                {
                    "start": 1240,
                    "end": 1265,
                    "matchedPaperCorpusId": "129886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2191162109375
        },
        {
            "corpus_id": "271769369",
            "title": "Tightly Coupled Worksheets and Homework Assignments for NLP",
            "text": "The first assignment is to build a Naive Bayes language identification model with character n-gram features. While students eventually train and test their model on eight languages,3 the worksheet focuses on just Spanish and English. Students count and smooth the character bigrams in three training instances, and then employ the model to classify a new word. The worksheet also includes a section on evaluating classifier performance by computing accuracy and creating a confusion matrix.",
            "score": 0.45732318312132936,
            "section_title": "Assignment 1: Language Identification with Naive Bayes",
            "char_start_offset": 4545,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 490
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.049774169921875
        },
        {
            "corpus_id": "7079619",
            "title": "Analyzing and Improving Statistical Language Models for Speech Recognition",
            "text": "Having introduced the key issues of language modeling, we will now review many commonly used language models. However, rather than giving the often complicated formula in terms of frequency counts, we will describe each model on a more abstract level in terms of its probability distributions. This way, we don't have to consider estimation and smoothing issues, but can focus on the following conceptually important issues: 1) We saw in section 2.3 that the task of a language model is to provide a probability distribution for a set of suitably de ned contexts. How is the context de ned or, in other words, on what does the probability distribution depend in each model? This is a crucial point of each model because it shows which linguistic regularities (e.g. those that involve only the preceding two words) it can capture. We will provide an intuitive description of the contexts as well as the formula each model uses to instantiate the general p(w i] = w l jc) with a speci c context c.\n\n2) How many probabilities have to be estimated in each model? This is important because it determines the amount of data needed to train the model. This in turn determines the situations and tasks in which each model can be used.",
            "score": 0.45732318312132936,
            "section_title": "Review of Existing Language Models",
            "char_start_offset": 49029,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3583984375
        },
        {
            "corpus_id": "2032603",
            "title": "Similarity-Based Approaches to Natural Language Processing",
            "text": "We evaluated our method by comparing its perplexity and effect on speech-recognition accuracy with the baseline bigram back-off model developed by MIT Lincoln Laboratories for the Wall Street Journal (WSJ) text and dictation corpora provided by ARPA's HLT program (Paul, 1991). 2 The baseline back-off model closely follows the Katz design discussed in section 2.2, except that for the sake of compactness all singleton bigrams are treated as unseen (recall that this omission of singletons was quite detrimental to the simple similarity-based models considered in the previous chapter). The counts used in this model and in ours were obtained from 40.5 million words of WSJ text from the years 1987-89.\n\nFor the perplexity evaluation, we tuned the similarity model parameters by minimizing perplexity via a simple grid search on an additional sample of 57.5 thousand words of WSJ text drawn from the ARPA HLT development test set. The best parameter values found were k = 60, t = 2.5, \u03b2 = 4 and \u03b3 = 0.15. For these values, the improvement in perplexity for unseen bigrams in a held-out 18 thousand word sample, in which 10.6% of the bigrams are unseen, is just over 20%. This improvement on unseen bigrams corresponds to an overall test set perplexity improvement of 2.4% (from 237.4 to 231.7). Table 5.1 shows reductions in training and test perplexity, sorted by training reduction, for different choices of k. The values of \u03b2, \u03b3 and t are the best ones found for each k.\n\nFrom equation (4.2), it is clear that the computational cost of applying the similarity model to an unseen bigram is O(k). Therefore, lower values of k (and t as well) are computationally preferable. From the table, we can see that reducing k to 30 incurs a penalty of less than 1% in the perplexity improvement, so relatively low values of k appear to be sufficient to achieve most of the benefit of the similarity model. As the table also shows, the best value of \u03b3 increases as k decreases, that is,  Table 5.1: Perplexity reduction",
            "score": 0.45732318312132936,
            "section_title": "Evaluation",
            "char_start_offset": 144792,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 276,
                    "matchedPaperCorpusId": "15782021"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.040252685546875
        },
        {
            "corpus_id": "28851985",
            "title": "Incidental Supervision: Moving beyond Supervised Learning",
            "text": "These signals are co-related to the target tasks, and can be exploited, along with appropriate algorithmic support, to provide sufficient supervision and facilitate learning. \n\nConsider, for example, the task of Named entity (NE) transliteration -the process of transcribing a NE from a source language to some target language based on phonetic similarity between the entities (e.g., determine how to write \"Obama\" in Hebrew). Identifying transliteration pairs is an important component in many linguistic applications which require identifying out-of-vocabulary words, such as machine translation and multilingual information retrieval. Naturally, to know how to write the word Hussein in Russian, say, one needs to train a model that requires as input a long list of pairs: NEs in English and their corresponding transliteration in Russian. These resources do not exists for many pairs of languages. However, (Klementiev and Roth 2006) showed that it is possible to automatically discover NEs in low resource languages, given bilingual corpora that are weakly temporally aligned. This is a much easier resource to come by, given that multiple news services gen- erate comparable news data in many languages. Moreover, this resource is independent of the task at hand and can be used to aid supervising multiple related tasks. Mentions of NEs that correspond to the same entity, in different languages, would then have similar temporal distributions across such corpora, and this similarity is a strong signal that a pair of NEs could be a transliteration of each other. Figure 1 illustrates this situation by showing the temporal histogram of the NE \"Hussein\" over the same time period in a comparable English-Russian corpus, along with the lack of a signal when considering a different NE. \n\nClearly, this signal by itself is not sufficient to supervise the learning of a transliteration model since the temporal distribution of Hussein might be quite similar to that of Iraq too. However, along with other weak signals such as phonetic similarity, it would support learning a good model. \n\nThis example provides an illustration of what we mean by an incidental supervision signal in this paper. The temporal signal is there, independently of the transliteration task at hand. It is co-related to the task at hand and, together with other signals and some inference, could be used to supervise it without the need for any significant annotation effort.",
            "score": 0.45732318312132936,
            "section_title": "Introduction",
            "char_start_offset": 2227,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 177,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1792
                },
                {
                    "start": 1795,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2091
                },
                {
                    "start": 2094,
                    "end": 2198
                },
                {
                    "start": 2199,
                    "end": 2279
                },
                {
                    "start": 2280,
                    "end": 2455
                }
            ],
            "ref_mentions": [
                {
                    "start": 911,
                    "end": 937,
                    "matchedPaperCorpusId": "669616"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0982666015625
        },
        {
            "corpus_id": "273162917",
            "title": "RAFT: Realistic Attacks to Fool Text Detectors",
            "text": "We set k to 10% across all experiments to evaluate the effectiveness of our attack with a limited number of changes. We evaluate the effectiveness of using language modeling heads for next-token generation and supervised LLM detection tasks as proxy scoring models to optimally select words to substitute in X. For next-token generation, we use the probability of the next token being X i from the language modeling head as the proxy objective. Intuitively, replacing tokens with the highest likelihood from the LLM allows us to alter the statistical properties of the machine-generated text most effectively. For LLM detection, we iteratively compute the importance of each word based on the decrease in detection score D(X) by assigning 0 to its corresponding tokens in the detector's attention mask, and ranking the score changes in descending order, where the word that yields a higher absolute change in detector score is considered to be more important for detection.",
            "score": 0.45732318312132936,
            "section_title": "Implementation Details",
            "char_start_offset": 10122,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 973
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.106689453125
        },
        {
            "corpus_id": "7570781",
            "title": "Detection of Question-Answer Pairs in Email Conversations",
            "text": "\u2022 length of the utterance \u2022 POS-bigrams from a list of 100 most discriminating POS-bigrams list. \n\nThe list of most discriminating POS-bigrams was obtained from the training data by following the same procedure that (Zechner and Lavie, 2001) used. \n\nWe then used Ripper (Cohen, 1996) to learn rules for question detection. Like many learning programs, Ripper takes as input the classes to be learned, a set of feature names and possible values, and training data specifying the class and feature values for each training example. In our case, the training examples are the speech acts extracted from the SWITCHBOARD corpus as described above. Ripper outputs a classification model for predicting the class (i.e., whether a speech act is a question or not) of future examples; the model is expressed as an ordered set of if-then rules. For testing, we manually extracted 300 questions in interrogative form and 300 statements in declarative form from the ACM corpus.2 We show our test results with recall, precision and F 1 -score3 in Table 1 on the first column. \n\nWhile the test results show that the precision was very good, the recall score could be further improved. Upon further investigation on why the recall was so low, we found that unlike the positive examples we used in our training data, most of the questions in the test data that were missed by the rules learned by Ripper started with a declarative phrase. For example, both \"I know its on 108th, but after that not sure, how exactly do we get there?\", and \"By the way, are we shutting down clic?\" begin with declarative phrases and were missed by the Ripper learned rules. Following this observation, we manually updated our question detection module to break a speech act that was not initially predicted as question into phrases separated by comma characters. Then we applied the rules on the first phrase of the speech act and if that failed on the last phrase. For example, the rules would fail on the phrase \"I know its on 108th\", but would be able to classify the phrase \"how exactly do we get there\" as a question.",
            "score": 0.45732318312132936,
            "section_title": "Automatic Question Detection",
            "char_start_offset": 9346,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 99,
                    "end": 247
                },
                {
                    "start": 250,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1062
                },
                {
                    "start": 1065,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2088
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 241,
                    "matchedPaperCorpusId": "18937832"
                },
                {
                    "start": 270,
                    "end": 283,
                    "matchedPaperCorpusId": "17832923"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0592041015625
        },
        {
            "corpus_id": "276161492",
            "title": "\"It Warned Me Just at the Right Moment\": Exploring LLM-based Real-time Detection of Phone Scams",
            "text": "For our evaluation, we selected a diverse set of LLMs to implement the detector: GPT-4 [1], GPT-4o [14], GLM4 [10], Doubao-Pro-32k [4], and ERNIE-3.5-8k [2]. We selected these models to capture a broad spectrum of strengths, particularly in natural language understanding. The diversity of these models allows us to evaluate the generalize ability of our detection method across different architectures, and benchmark state-of-the-art models to identify the most suitable candidates for real-world deployment.",
            "score": 0.45732318312132936,
            "section_title": "LLM Models",
            "char_start_offset": 11663,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 509
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1748046875
        },
        {
            "corpus_id": "265351715",
            "title": "Detecting out-of-distribution text using topological features of transformer-based language models",
            "text": "Machine learning (ML) models perform well on the datasets they have been trained on, but can behave unreliably when tested on data that is out-of-distribution (OOD). For example, when a ML model has been trained to recognise different breeds of cats is fed an image of a dog, the results are unpredictable. OOD detection is the task of identifying that an input does not seem to be drawn from the same distribution as the training data, and thus the prediction given by the ML model should not be trusted. OOD detectors can be used to defend ML models deployed in high stakes applications from OOD data by providing a warning/error message for OOD inputs rather than processing the input and producing untrustworthy results [1]. \n\nIn this paper, we focus on OOD detection for textual inputs to safeguard ML models that perform natural language processing (NLP) tasks. For example, a sentiment classification model trained on formal restaurant reviews may not produce valid results when applied to informal posts from social media. Determining that an input is OOD requires a way to measure the distance between an input and the in-distribution data. This in turn requires a method to convert textual data into an embedding space in which we can measure distance. One approach to this is to input the text to a transformer-based language model, such as BERT [2], to extract an embedding vector for the input text (e.g., the hidden representation of the special [] token). We can then measure the distance of the embedding vector for an input text to the nearest (or k-nearest) embedding vector of a text from an in-distribution validation set. When this distance is beyond some threshold (which needs to be calibrated for the application), the input text is flagged as out of distribution. The internal state of transformer-based language models contains important information, which may be able to offer richer representations than only using the embedding obtained from the last or penultimate layer. For example, Azaria and Mitchell [3] demonstrated that it is possible to train a classifier on the activation values of the hidden layers of large language models to predict when they are generating false information rather than true information.",
            "score": 0.4568701584118535,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 728
                },
                {
                    "start": 731,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2248
                }
            ],
            "ref_mentions": [
                {
                    "start": 1357,
                    "end": 1360,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51904296875
        },
        {
            "corpus_id": "264451585",
            "title": "Detecting Pretraining Data from Large Language Models",
            "text": "Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. Min-K% Prob can be applied without any knowledge about the pretraining corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous methods. We apply Min-K% Prob to two real-world scenarios, copyrighted book detection, and contaminated downstream example detection, and find it a consistently effective solution.",
            "score": 0.45667219544946996,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6083984375
        },
        {
            "corpus_id": "267068867",
            "title": "Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text",
            "text": "Language model detection may be a key technology to reduce harm, whether to monitor machine-generated text on internet platforms and social media, filter training data, or identify responses in chat applications. Nevertheless, care has to be taken so that detection mechanisms actually reduce harm, instead of proliferating or increasing it. We provide an extensive reliability investigation of the proposed Binoculars mechanisms in Section 5, and believe that this is a significant step forward in terms of reliability, for example when considering domains such as text written by non-native speakers. Yet, we note that this analysis is only a first step in the process of deploying LLM detection strategies and does not absolve developers of such applications from carefully verifying the impact on their systems. We especially caution that the existence of LLM detectors does not imply that using them is worthwhile in all scenarios. \n\nAlso, we explicitly highlight that we consider the task of detecting \"naturally\" occurring machine-generated text, as generated by LLMs in common use cases. We understand that no detector is perfect and we do not guarantee any performance in settings where a motivated adversary tries to fool our system. We also note that our detector does not provide an explanation or interpretation of its predictions for any given sample and thus is black-box in nature. We present a thorough evaluation across a wide variety of test sources, but we maintain that directed attempts to bypass our classifier might be possible, as is often the case for systems that rely on neural networks.",
            "score": 0.45653313527879874,
            "section_title": "Impact Statement",
            "char_start_offset": 28736,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1615
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30126953125
        },
        {
            "corpus_id": "266164087",
            "title": "Out-of-Distribution Generalization in Natural Language Processing: Past, Present, and Future",
            "text": "Pre-trained Language Models (PLMs) (Devlin et al., 2018;Liu et al., 2019b;Radford et al., 2018) have revolutionized natural language processing (NLP) and enabled remarkable advances in Large-scale Language Models (LLMs) (Touvron et al., 2023;Gozalo-Brizuela and Garrido-Merchan, 2023;Pichai, 2023) . Despite substantial progress in developing accurate models in several natural language understanding tasks, including sentiment analysis (Kaushik et al., 2019;Ni et al., 2019;Yang et al., 2021;Lu et al., 2022;Luo et al., 2022a,b), natural language inference (Williams et al., 2018), and machine reading comprehension (Kaushik and Lipton, 2018;Sugawara et al., 2020), a major challenge persists -out-of-distribution (OOD) generalization -which entails the ability of a model to 1 These authors contributed equally to this work. \n\n2 \"Large Language Models (LLMs)\" refers to recent generative models while \"Pre-trained Language Models refers to small-scale pre-trained models\" in this paper. \n\naccurately classify text instances from distributions different from those of the training data (Ben-David et al., 2010;Hendrycks and Gimpel, 2017;Hupkes et al., 2022). This paper aims to provide a comprehensive overview of the current state of research in OOD generalization for natural language understanding, highlighting key methodologies, advancements, and unique challenges. \n\nThe importance of OOD generalization in NLP cannot be overstated, as real-world data often exhibit diversity and unpredictability. Numerous applications, such as sentiment analysis, document categorization, and spam detection (Shen et al., 2021;Yang et al., 2022), necessitate models capable of adapting to novel and unforeseen data distributions. While machine learning models generally demonstrate strong in-distribution performance, their performance frequently deteriorates when confronted with OOD instances, underscoring the need for effective strategies that facilitate generalization beyond the training distribution.",
            "score": 0.45651514293916884,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 826
                },
                {
                    "start": 829,
                    "end": 988
                },
                {
                    "start": 991,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1371
                },
                {
                    "start": 1374,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1999
                }
            ],
            "ref_mentions": [
                {
                    "start": 459,
                    "end": 475,
                    "matchedPaperCorpusId": "202621357"
                },
                {
                    "start": 475,
                    "end": 493,
                    "matchedPaperCorpusId": "235669817"
                },
                {
                    "start": 558,
                    "end": 581,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 1087,
                    "end": 1111,
                    "matchedPaperCorpusId": "8577357"
                },
                {
                    "start": 1111,
                    "end": 1138,
                    "matchedPaperCorpusId": "13046179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.291259765625
        },
        {
            "corpus_id": "272706779",
            "title": "Unmasking the Imposters: How Censorship and Domain Adaptation Affect the Detection of Machine-Generated Tweets",
            "text": "Our work displays the dual-use nature of advanced language models. While their generative prowess can improve user experiences, it can also facilitate large-scale distribution of harmful or deceptive content. We acknowledge the risk that improved understanding of uncensored LLMs might enable malicious actors to refine their tactics. To mitigate this, our focus is on advancing detection mechanisms, raising community awareness, and encouraging preemptive safeguards rather than enabling misuse. \n\nThe potential for biases and unfair outcomes also cannot be overlooked. Although we investigate models primarily on English Twitter data, real-world deployments must consider multilingual and multicultural settings. Ensuring that detectors do not disproportionately mislabel or penalize content from marginalized communities or sensitive contexts is of vast importance. Model developers, platform operators, and regulators must collaborate to define appropriate transparency standards, audit processes, and accountability frameworks. \n\nUltimately, the evolution of LLM detection systems and moderation policies must align with human rights, privacy, and equity principles. Balancing open scientific inquiry with responsible disclosure, our goal is to guide the ethical integration of these technologies into digital ecosystems, ensuring that innovations in language generation and detection serve the public good rather than undermine it. \n\n\u2022 Word embeddings (w j , w k ) are computed using BERTweet. \n\n\u2022 Sentences containing only one word are excluded from similarity calculations, as cosine similarity requires at least two word vectors.",
            "score": 0.45596169274707926,
            "section_title": "Ethical Considerations",
            "char_start_offset": 32412,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 496
                },
                {
                    "start": 499,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1032
                },
                {
                    "start": 1035,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1437
                },
                {
                    "start": 1440,
                    "end": 1499
                },
                {
                    "start": 1502,
                    "end": 1638
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25341796875
        },
        {
            "corpus_id": "267068867",
            "title": "Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text",
            "text": "The second category of approaches comprises statistical signatures that are characteristic of machine-generated text. These approaches have the advantage of requiring none or little training data and they can easily be adapted to newer model families (Pu et al., 2022). Examples include detectors based on perplexity (Tian, 2023b;Vasilatos et al., 2023;Wang et al., 2023), perplexity curvature (Mitchell et al., 2023), log rank (Su et al., 2023), intrinsic dimensionality of generated text (Tulchinskii et al., 2023), and n-gram analysis (Yang et al., 2023a). Our coverage of the landscape is non-exhaustive, and we refer to recent surveys Ghosal et al. (2023); Tang et al. (2023); Dhaini et al. (2023); Guo et al. (2023) for additional details. \n\nFrom a theoretical perspective, Varshney et al. (2020), Helm et al. (2023), andSadasivan et al. (2023) all dis-cuss the limits of detection. These works generally agree that fully general-purpose models of language would be, by definition, impossible to detect. However, Chakraborty et al. ( 2023) note that even models that are arbitrarily close to this optimum are technically detectable given a sufficient number of samples. In practice, the relative success of detection approaches, such as the one we propose and analyze in this work, provides constructive evidence that current language models are imperfect representations of human writing -and thereby detectable. Finally, the robustness of detectors to attacks attempting to circumvent detection can provide stronger practical limits on reliability in the worst case (Bhat & Parthasarathy, 2020;Wolff & Wolff, 2022;Liyanage & Buscaldi, 2023). \n\nWith an understanding of how much work exists on LLM detection, a crucial question arises: How do we appropriately and thoroughly evaluate detectors? Many works focus on accuracy on balanced test sets and/or AUC of their proposed classifiers, but these metrics are not well-suited for the high-stakes question of detection.",
            "score": 0.4553343756032968,
            "section_title": "The LLM Detection Landscape",
            "char_start_offset": 3764,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 745
                },
                {
                    "start": 748,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1649
                },
                {
                    "start": 1652,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1975
                }
            ],
            "ref_mentions": [
                {
                    "start": 251,
                    "end": 268,
                    "matchedPaperCorpusId": "250479403"
                },
                {
                    "start": 394,
                    "end": 417,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 682,
                    "end": 702,
                    "matchedPaperCorpusId": "261822277"
                },
                {
                    "start": 780,
                    "end": 802,
                    "matchedPaperCorpusId": "211068838"
                },
                {
                    "start": 1574,
                    "end": 1602,
                    "matchedPaperCorpusId": "226283676"
                },
                {
                    "start": 1622,
                    "end": 1648,
                    "matchedPaperCorpusId": "259195404"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.327392578125
        },
        {
            "corpus_id": "272366479",
            "title": "Large Language Models for Anomaly and Out-of-Distribution Detection: A Survey",
            "text": "Large Language Models (LLMs) and Multi-Modal LLMs (MLLMs) have transformed Out-of-Distribution (OOD) detection by leveraging pretrained models like CLIP to perform downstream detection tasks. These models are capable of detecting OOD samples in zero-shot or few-shot settings, meaning they can generalize to unseen data with little to no additional training. This represents a shift from traditional OOD detection methods, which typically rely on training classifiers using the entire in-distribution (ID) dataset. \n\nIncorporating the internal knowledge of pretrained MLLMs, the field is progressing towards even greater computational efficiency, where minimal or no training data is needed. This ability to operate with limited data while maintaining performance makes LLM-based OOD detection especially appealing for real-world applications.",
            "score": 0.45531829082067704,
            "section_title": "D.2 LLM-based OOD Detection",
            "char_start_offset": 41836,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 514
                },
                {
                    "start": 517,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 843
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62109375
        },
        {
            "corpus_id": "227336081",
            "title": "Enhanced Offensive Language Detection Through Data Augmentation",
            "text": "In this way, we can obtain a more balanced and abundant dataset. Specifically, we first extract some semantic features from the target class, and then use these features to guide a conditional generator, thus generating the desired augmentation data. We implement our conditional generator by modifying a existing language model, GPT-2 (Radford et al. 2019), that is trained on 8 million web pages and has 1.5 billion parameters. \n\nThe advantages of Dager are three-fold: 1) It is powerful. Our method significantly improves the performance on all test datasets trained on 4 popular classifiers. 2) It's easy arXiv:2012.02954v1 [cs.CL] 5 Dec 2020 to deploy. Our method does not require external datasets or rely on training auxiliary tools; instead, we make use of an off-the-shelf language model (LM) and focus on mining the potentially informative features for the low-resource data classification (Liu et al. 2020). \n\n3) It has high augmentation quality. Instead of simple word-level or phrase-level replacement (Kobayashi 2018;Wei and Zou 2019), our method provides sentence-level text data augmentation, enriching semantic and syntactic features of our generated texts. \n\nThe goal of this work is to empirically analyze whether the current advances in large-scale language models can benefit low-resource data classification tasks. We take the offensive language detection task as an example and propose using LMs to generate augmentation data. The core module of our method is a conditional generator that can generate unlimited target label text. Its theoretical underpinning is described in the Approach section. In the Evaluation section, we systematically examine Dager in three perspectives, showing that our method provides substantial improvements on the offensive language classification task and that it is classifier-agnostic.",
            "score": 0.4540002340307431,
            "section_title": "Introduction",
            "char_start_offset": 2109,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 429
                },
                {
                    "start": 432,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 918
                },
                {
                    "start": 921,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1174
                },
                {
                    "start": 1177,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1842
                }
            ],
            "ref_mentions": [
                {
                    "start": 336,
                    "end": 357,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 900,
                    "end": 917,
                    "matchedPaperCorpusId": "226262374"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.427001953125
        },
        {
            "corpus_id": "266693549",
            "title": "Generating Enhanced Negatives for Training Language-Based Object Detectors",
            "text": "Language-based detection requires localization of objects by a referring free-form text descriptions. To train accurate models in a discriminative way, the training data must contain good negative samples. Starting with an existing dataset, we propose (1) novel ways to prompt LLMs for generating additional negative texts, and (2) generating negative images to complement the training signal. Based on our experimental evaluations, we conclude that such additional negative training data indeed translates into improved detection accuracy on standard benchmarks. Our analysis demonstrates the importance of diversity in the generated text, which is higher with our approach than with prior works, and the quality of the generated images, which our proposed filtering steps can significantly increase. \n\nFind objects, attributes of objects, and relationships between objects in the given text. One word or phrase should only be one of objects, attributes and relationships. \n\nProvide the answer using the JSON format as, { \"text\": <the given text>, \"objects\": <list of attributes in the given text> \"attributes\": <list of attributes in the given text> \"relationships\": <list of attributes in the given text> } Given text: {positive_text} Listing 1. Prompt to ChatGPT for extracting concepts (i.e., objects, attributes, and relationships) from a text. playing with a dog\". Such foils require candidate concepts that do not refer to the same object as the original concept. To get good candidates, we follow [30] to use WordNet, as well as other external knowledge bases, i.e., ConceptAPI, and human annotations [1].",
            "score": 0.45396640934933435,
            "section_title": "Conclusion",
            "char_start_offset": 29930,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 801
                },
                {
                    "start": 804,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 973
                },
                {
                    "start": 976,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1614
                }
            ],
            "ref_mentions": [
                {
                    "start": 1506,
                    "end": 1510,
                    "matchedPaperCorpusId": "254685851"
                },
                {
                    "start": 1610,
                    "end": 1613,
                    "matchedPaperCorpusId": "237592834"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30810546875
        },
        {
            "corpus_id": "232106216",
            "title": "Malware Classification with GMM-HMM Models",
            "text": "That is, we can train a model for a given observation sequence. \n\nIn this research, we are interested in problems 1 and 3. Specifically, we train models, then we test the resulting models by scoring observation sequences. The solution to problem 2 is of interest in various NLP applications, for example. For the sake of brevity, we omit the details of training and scoring with discrete HMMs; see (Stamp, 2018) or (Rabiner, 1989) for more information.",
            "score": 0.45283379289724596,
            "section_title": "Given an observation sequence O, determine a model \u03bb",
            "char_start_offset": 8902,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 66,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 452
                }
            ],
            "ref_mentions": [
                {
                    "start": 415,
                    "end": 429,
                    "matchedPaperCorpusId": "13618539"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06536865234375
        },
        {
            "corpus_id": "221970999",
            "title": "Reinforcement Learning-based N-ary Cross-Sentence Relation Extraction",
            "text": "As shown in Fig 1, the proposed model consists of two submodels. The first is a sentence distribution estimator (SDE), which is used to measure the probability of whether a sentence is correctly labeled by distant supervision. This model outputs a group of sentences that describe a complete fact.",
            "score": 0.45283379289724596,
            "section_title": "Proposed Model",
            "char_start_offset": 8445,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 297
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03759765625
        },
        {
            "corpus_id": "12789986",
            "title": "Model Selection for Type-Supervised Learning with Application to POS Tagging",
            "text": "We evaluate the introduced methods for model selection and training with type supervision in two type-supervised settings: when no labeled examples are available, and when a small number of labeled examples are available. \n\nWe use a feature-rich first-order HMM model (Berg-Kirkpatrick et al., 2010) with an L 2 prior on feature weights. 4 Instead of using a multinomial distribution for the local emissions and transitions, this model uses a log-linear distribution (i.e., p(x i | y i ) \u221d exp \u03bb f (x i , y i )) with a feature vector f and a weight vector \u03bb. We use the feature set described in (Li et al., 2012): transition features, word-tag features ( y i , x i ) (lowercased words with frequency greater than a threshold), whether the word contains a hyphen and/or starts with a capital letter, character suffixes, and whether the word contains a digit. We initialize the transition and emission distributions of the HMM using unambiguous words as proposed by (Zhang and DeNero, 2014). Data. We use the Danish, Dutch, German, Greek, English, Italian, Portuguese, Spanish and Swedish datasets from CoNLL-X and CoNLL-2007 shared tasks (Buchholz and Marsi, 2006;Nivre et al., 2007). We map the POS labels in the CoNLL datasets to the universal POS tagset (Petrov et al., 2012). We use the tag dictionaries provided by Li et al. (2012)  we consider M = 15 configurations of the hyperparameters which vary in the L 2 regularization strength (5 values), and the minimum word frequency for word-tag features (3 values). When a small labeled set is available we additionally choose one of 3 values for the weight of the labeled set (see Section 3). We report final performance of models selected using different criteria using token-level accuracy on an unseen test set. \n\nNo labeled examples. When no labeled examples are available, we do model training and selection using only unlabeled text T and a tagging lexicon lex.",
            "score": 0.45283379289724596,
            "section_title": "Experiments",
            "char_start_offset": 7298,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 224,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1766
                },
                {
                    "start": 1769,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1919
                }
            ],
            "ref_mentions": [
                {
                    "start": 268,
                    "end": 299,
                    "matchedPaperCorpusId": "1322232"
                },
                {
                    "start": 964,
                    "end": 988,
                    "matchedPaperCorpusId": "15112157"
                },
                {
                    "start": 1256,
                    "end": 1277,
                    "matchedPaperCorpusId": "5851561"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0615234375
        },
        {
            "corpus_id": "17337432",
            "title": "Reading Level Assessment Using Support Vector Machines and Statistical Language Models",
            "text": "Statistical LMs predict the probability that a particular word sequence will occur. The most commonly used statistical language model is the n-gram model, which assumes that the word sequence is an (n\u22121)th order Markov process. For example, for the common trigram model where n = 3, the probability of sequence w is: \n\n(1) The parameters of the model are estimated using a maximum likelihood estimate based on the observed frequency in a training corpus and smoothed using modified Kneser-Ney smoothing (Chen and Goodman, 1999). We used the SRI Language Modeling Toolkit (Stolcke, 2002) for language model training. \n\nOur first set of classifiers consists of one n-gram language model per class c in the set of possible classes C. For each text document t, we can calculate the likelihood ratio between the probability given by the model for class c and the probabilities given by the other models for the other classes: \n\nwhere we assume uniform prior probabilities P (c). \n\nThe resulting value can be compared to an empirically chosen threshold to determine if the document is in class c or not. For each class c, a language model is estimated from a corpus of training texts. \n\nIn addition to using the likelihood ratio for classification, we can use scores from language models as features in another classifier (e.g. an SVM). For example, perplexity (P P ) is an information-theoretic measure often used to assess language models: \n\nwhere H(t|c) is the entropy relative to class c of a length m word sequence t = w 1 , ..., w m , defined as \n\nLow perplexity indicates a better match between the test data and the model, corresponding to a higher probability P (t|c). Perplexity scores are used as features in the SVM model described in Section 4.3. \n\nThe likelihood ratio described above could also be used as a feature, but we achieved better results using perplexity.",
            "score": 0.45283379289724596,
            "section_title": "Statistical Language Models",
            "char_start_offset": 8897,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 316
                },
                {
                    "start": 319,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 920
                },
                {
                    "start": 923,
                    "end": 973
                },
                {
                    "start": 976,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1178
                },
                {
                    "start": 1181,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1435
                },
                {
                    "start": 1438,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1753
                },
                {
                    "start": 1756,
                    "end": 1874
                }
            ],
            "ref_mentions": [
                {
                    "start": 503,
                    "end": 527,
                    "matchedPaperCorpusId": "215842252"
                },
                {
                    "start": 571,
                    "end": 586,
                    "matchedPaperCorpusId": "1988103"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.328125
        },
        {
            "corpus_id": "263831381",
            "title": "Zero-Shot Detection of Machine-Generated Codes",
            "text": "For our detection models, we consider a range of 11 widely used open-access surrogate language models, varying in size from millions to billions of parameters: CodeParrot-110M, 1.5B (Huggingface, 2021), OPT-125M (Zhang et al., 2022), GPT-neo-125M, 1.3B, 2.7B (Gao et al., 2020), PolyCoder-125M, 0.4B, 2.7B (Xu et al., 2022), Incoder-1B, 6B (Fried et al., 2022). We obtain all of them from Huggingface 4 . Except for OPT-125M, all other models are trained on a large collection of codes.",
            "score": 0.45283379289724596,
            "section_title": "Detector Models",
            "char_start_offset": 15148,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 486
                }
            ],
            "ref_mentions": [
                {
                    "start": 306,
                    "end": 323,
                    "matchedPaperCorpusId": "247158549"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1649169921875
        },
        {
            "corpus_id": "274150397",
            "title": "SEFD: Semantic-Enhanced Framework for Detecting LLM-Generated Text",
            "text": "We define our detection task as a binary classification problem. Let X = (x i ) N i=1 be a text sequence, where X \u2208 \u2126, and \u2126 is the text space. We define: \n\n1) \n\n2) \u2126 H is the human-written text space. \n\n3) \u2126 fLM is the text space generated by an LLM f LM . 4) \u2126 H \u2229 \u2126 fLM = \u2205. Our detection task is to define a classification function D : \u2126 \u2192 {0, 1} such that for each x i : \n\nIn particular, for a text y i , \u0177i is its paraphrased version by any LLM model. Since we only want to distinguish whether a text is human-written or not, we consider \u0177i \u2208 \u2126 f LM for simplicity, which means in the detection phase, \u0177i should be classified as LLM-generated instead of human-written. Fig. 2 illustrates the structure of our framework which is a detailed version of Fig. 1. In the following sections, we detail our detection framework, starting with the three detection steps, followed by an introduction to the retrieval pool and its updating rules.",
            "score": 0.45283379289724596,
            "section_title": "III. METHOD",
            "char_start_offset": 8605,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 154
                },
                {
                    "start": 157,
                    "end": 159
                },
                {
                    "start": 162,
                    "end": 201
                },
                {
                    "start": 204,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 375
                },
                {
                    "start": 378,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 940
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06512451171875
        },
        {
            "corpus_id": "267782343",
            "title": "A temporal stochastic bias correction using a machine learning attention model",
            "text": "One component in recent successes of large language models, like GPT [6], is their Transformer decoder architecture. We will now give an abstract description of the architecture -the specifics can be found in [30], but the abstract view, we believe, makes it easier to explain the salient parts of the model and makes it clear which parts can be replaced with ease. For language modelling our dataset is comprised of  sequences, each comprised of (word, position) pairs (  ,   ) for  \u2208 1 . . .   .   is the length of sequence . Both   and   are scalars. We will take  to denote the total number of unique words in our dataset. \n\nThe goal of language modelling is to classify the next word in a sentence based on previous words.",
            "score": 0.45283379289724596,
            "section_title": "D.1. Transformer decoder for language modelling",
            "char_start_offset": 41358,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 626
                },
                {
                    "start": 629,
                    "end": 727
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1007080078125
        },
        {
            "corpus_id": "226278255",
            "title": "Machine Generation and Detection of Arabic Manipulated and Fake News",
            "text": "Table 7 provides examples where annotators disagree on either or both tasks, and Table 8 illustrates cases where annotators agree. 6 Manipulated Text and Fake News Detection   Models. For the purpose of training our manipulated text detectors, we exploit 4 large pre-trained masked language models (MLM): mBERT (Devlin et al., 2018), AraBERT (Antoun et al., 2020), XLM-R Base , and XLM-R Large (Conneau et al., 2020). 10   Training Data & Hyper-Parameters. We fine-tuned all these models on the TRAIN split of (1) ATB + , and (2) AraNews + , independently. For each model, we run for 25 epochs with a batch size of 32, maximum sequence length of 128 tokens, and a learning rate of 1e \u22125 .",
            "score": 0.45283379289724596,
            "section_title": "Annotation Data",
            "char_start_offset": 19105,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 688
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09466552734375
        },
        {
            "corpus_id": "556002",
            "title": "Language Models for Contextual Error Detection and Correction",
            "text": "The probabilities used in the language models of the generic classifiers are computed by looking at occurrences of n-grams. These occurrences are extracted from a corpus. The training instances used in the specific machine learning classifiers are also extracted from the same data set. For training purposes, we used the Reuters news corpus RCV1 (Lewis et al., 2004). The Reuters corpus contains about 810,000 categorized newswire stories as published by Reuters in 1996 and 1997. This corpus contains around 130 million tokens. \n\nFor testing purposes, we used the Wall Street Journal part of the Penn Treebank corpus (Marcus et al., 1993). This well-known corpus contains articles from the Wall Street Journal in 1987 to 1989. We extract our test-instances from this corpus in the same way as we extract our training data from the Reuters corpus. There are minor tokenization differences between the corpora. The data is corrected for these differences. \n\nBoth corpora are in the domain of English language news texts, so we expect them to have similar properties. However, they are different corpora and hence are slightly different. This means that there are also differences between the training and testing set. We have selected this division to create a more realistic setting. This should allow for a more to real-world use comparison than when both training and testing instances are extracted from the same corpus. \n\nFor the specific experiments, we selected a number of well-known confusible sets to test the different approaches. \n\nIn particular, we look at {then, than}, {its, it's}, {your, you're}, {their, there, they're}. To compare the difficulty of these problems, we also selected two words at random and used them as a confusible set. \n\nThe random category consists of two words that where randomly selected from all words in the Reuters corpus that occurred more than a thousand times. The words that where chosen, and used for all experiments here are refugees and effect . They occur around 27 thousand times in the Reuters corpus.",
            "score": 0.45283379289724596,
            "section_title": "Experimental settings",
            "char_start_offset": 16605,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 529
                },
                {
                    "start": 532,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 955
                },
                {
                    "start": 958,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1424
                },
                {
                    "start": 1427,
                    "end": 1541
                },
                {
                    "start": 1544,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1754
                },
                {
                    "start": 1757,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 1995
                },
                {
                    "start": 1996,
                    "end": 2054
                }
            ],
            "ref_mentions": [
                {
                    "start": 347,
                    "end": 367,
                    "matchedPaperCorpusId": "11027141"
                },
                {
                    "start": 619,
                    "end": 640,
                    "matchedPaperCorpusId": "252796"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.402587890625
        },
        {
            "corpus_id": "248118886",
            "title": "What do Toothbrushes do in the Kitchen? How Transformers Think our World is Structured",
            "text": "Unlike static models, the vector representations of (sub-)word (units) in contextualized models depend on the context in which they occur so that tokens of the same type may each be represented differently in different contexts. All contextual models we evaluate here are pre-trained and come from the huggingface models repository 3 . We evaluate two types of contextualized models: Masked Language Models (MLM) are trained to reconstruct randomly masked words in input sequences. We experiment with BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), ELEC-TRA (Clark et al., 2020) and ALBERT (Lan et al., 2019). The models differ in training, training data, and model size. BERT is trained using masked language modeling and next sentence prediction. RoBERTa omits the second task, but uses much more training data. Two models are trained for ELECTRA: one on masked language modeling (generator) and a second one that recognizes just these replaced tokens (discriminator). Since many of our evaluations need mask tokens, we only use the generator model for the evaluations. Finally, ALBERT is trained to predict the order of pairs of consecutive text segments in addition to masked language modeling. Causal Language Models (CLM) are trained to predict the next word for a given input text. From this class we experiment with GPT-2 (Radford et al., 2019), GPT-Neo (Gao et al., 2020;Black et al., 2021) and GPT-J (Wang and Komatsuzaki, 2021). GPT-Neo and GPT-J are re-implementations of GPT-3 (Brown et al., 2020) where GPT-J was trained on a significantly larger data set named The Pile (Gao et al., 2021) (cf. Table 7 in the appendix).",
            "score": 0.45283379289724596,
            "section_title": "Contextualized Models",
            "char_start_offset": 12087,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 506,
                    "end": 527,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11029052734375
        },
        {
            "corpus_id": "208512948",
            "title": "An analysis of observation length requirements for machine understanding of human behaviors from spoken language",
            "text": "We use a Maximum Likelihood method closely following (Frank and Hall, 2001;Rozgi\u0107 et al., 2011) where N-gram Language Models (LMs) are employed to map text onto a scalar value. N-gram LMs compute the joint probability of a sequence of words or, equivalently, the probability of a word given the context of the preceding n\u22121 words. Given a sequence of M words W = {w 1 , w 2 , . . . , w M }, its N-gram probability is given by: \n\nWe assume that the text corpus is annotated using ratings in the interval [1,K] where 1 indicates the lowest degree (\"absence of behavior\") and K indicates the highest degree (\"strong presence of behavior\"). First, we partition the corpus into A i and A i such that A i contains text whose annotations lie in the interval [1,i+1] while A i contains those in the interval (i+1,K]. Then, we train N-gram LMs on A i and A i to obtain a binary classifier, denoted as the i th pair. We repeat this for all integer-thresholded partitions \n\nTable 2. Integer-thresholded partitions for behavior ratings ranging from 1 to K \n\nFor a text sequence W , whose behavior score, x, we want to find, the ith pair provides likelihoods P A i (W ) and P \u0100i (W ): \n\nFrom these, we calculate the posterior probability of each cumulative interval using Bayes Rule, as shown in Eq. 6. For the prior probabilities, we do not use any domain-specific knowledge about the behaviors of interest, instead assuming them to be uniformly distributed i.e. X \u223c Uni f orm(1, K). \n\nWe then compute posteriors for each adjacent interval using Eq. 7, resulting in a probability mass function of the behavior score: \n\nSince the ith point in Eq. 7 denotes the interval [i,i+1], we represent it using its midpoint i+ 1 2 . Finally, the behavior score x of text sequence W is computed as simply the expected value: \n\n\u2022 Training",
            "score": 0.45283379289724596,
            "section_title": "\u2022 Model",
            "char_start_offset": 20991,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 960
                },
                {
                    "start": 963,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1171
                },
                {
                    "start": 1174,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1471
                },
                {
                    "start": 1474,
                    "end": 1604
                },
                {
                    "start": 1607,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1800
                },
                {
                    "start": 1803,
                    "end": 1813
                }
            ],
            "ref_mentions": [
                {
                    "start": 53,
                    "end": 75,
                    "matchedPaperCorpusId": "12269265"
                },
                {
                    "start": 75,
                    "end": 95,
                    "matchedPaperCorpusId": "17292640"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10723876953125
        },
        {
            "corpus_id": "7079619",
            "title": "Analyzing and Improving Statistical Language Models for Speech Recognition",
            "text": "In this chapter, we gave an overview of the di erent components of a speech recognizer, de ned the task of the component central to this thesis, the language model, and reviewed the most commonly used language models.\n\nWe began by giving an overview of the di erent components of a speech recognizer designed according to the stochastic approach (see section 1.2). We brie y explained the tasks of the signal processor, the vector quantizer, the acoustic model and the language model and how they interact to perform the mapping from the acoustic signal to a string of words. Moreover, we introduced many of the notations used throughout this thesis.\n\nAfter having described the task of the language model intuitively, we then de ned the task of the language model more formally as follows. Given a set of contexts, the task of a language model is to provide a probability distribution for each context and to provide a way of choosing a context given the words recognized so far.\n\nSince probability distribution are used frequently in this thesis, we explained how probability distributions can be estimated from frequency data using the maximum likelihood criterion. Furthermore, we brie y addressed the issue of smoothing probability distributions and presented a very simple smoothing technique, the addition of a small, constant baseline probability. The estimation and smoothing methods that have been presented here are the ones we use for our work.\n\nHaving seen the major issues in constructing language models, we reviewed many existing language models (context independent, N-gram, N-pos, decision tree based and adaptive models). For each language model presented, we focussed on two conceptually important issues: how does the model de ne the context c for its probability distributions p(w i]jc) and how many probabilities does the model have to estimate. The rst point is important because it determines which linguistic regularities (e.g. the ones involving only the two preceding words) the language model can capture. The second point is important because it determines the amount of data needed to train the model and therefore the situations in which the model can be used.",
            "score": 0.45283379289724596,
            "section_title": "Summary",
            "char_start_offset": 67574,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.444091796875
        },
        {
            "corpus_id": "237048266",
            "title": "Feature learning for efficient ASR-free keyword spotting in low-resource languages",
            "text": "When a large number of labelled spoken keywords are available, an end-toend keyword spotter can be trained to directly classify whether a keyword is present in a search utterance or not (Sainath and Parada, 2015;Palaz et al., 2016;Kamper et al., 2016). Although we have only a limited number of such labelled keywords (Section 2.2.2), we nevertheless attempt to train a convolutional neural network (CNN) classifier in this way. Assume the Ddimensional frame-level acoustic feature vector sequence for the j th template of the i th keyword type is represented by X i,j = x 1 , x 2 , . . . , x M where x m \u2208 R D for 1 \u2264 m \u2264 M and M corresponds to the number of feature vectors in X i,j . Our classifier accepts an M \u00d7 D dimensional input (M = 60, D = 39) into a number of convolutional and max pooling layers. These are followed by three dense layers terminated by a softmax layer with K outputs, one for each keyword type. This is illustrated in Figure 2 while the precise network structure and implementation are presented in Section 5.1. Let the k th softmax output in response to an input keyword template X i,j be f k (X i,j ). The CNN classifier is trained using the set of separately recorded isolated keywords in a supervised fashion using the categorical cross entropy loss, which for the single training example X i,j is given by: \n\nwhere K is the number of keyword types and J k \u2208 {0, 1} is an indicator for whether X i,j is an instance of keyword type k. The K-dimensional softmax output of the CNN f (X i,j ) \u2208 [0, 1] K can therefore be interpreted as the estimated joint probability distribution over the set of K keyword types for the particular input X i,j . \n\nAt test time, a sliding window of M consecutive feature vectors is extracted from a test utterance Y = y 1 , y 2 , . . . , y N and presented to the CNN. This results in a sequence of K-dimensional classification results for the test utterance.",
            "score": 0.45283379289724596,
            "section_title": "Convolutional neural network (CNN) keyword classifier",
            "char_start_offset": 15434,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1339
                },
                {
                    "start": 1342,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1673
                },
                {
                    "start": 1676,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1919
                }
            ],
            "ref_mentions": [
                {
                    "start": 212,
                    "end": 231,
                    "matchedPaperCorpusId": "34045138"
                },
                {
                    "start": 231,
                    "end": 251,
                    "matchedPaperCorpusId": "7880109"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.051177978515625
        },
        {
            "corpus_id": "52334848",
            "title": "Paraphrase Detection on Noisy Subtitles in Six Languages",
            "text": "We next investigate the effects of data set size and the amount of noise in the data on model performance. We are interested in finding an appropriate amount of training data to be used in training the paraphrase detection models, as well as evaluating the robustness of different models against noise in the data. \n\nFor each language, data sets containing approximately 80%, 70%, or 60% clean paraphrase pairs are created. These percentages are the proportions of assumed positive training examples; the negative examples are created using the approach outlined in Section 3.2. \n\nEstimates of the quality of the training sets exist for all languages in Opusparcus. 3 The quality estimates were used to approximate the numbers of phrase pairs corresponding to the noise levels. Because the data sets for different languages are not equal in size, the number of phrase pairs at a certain noise level differs from language to language. The different data set sizes for all noise levels and languages are shown in Table 3. \n\nTable 3 shows the results for the GRAN model. The results indicate that the GRAN model is rather robust to noise in the data. For five out of six languages, the best results are achieved using either the 70% or 60% data sets. That is, even when up to 40% of the positive examples in the training data are incorrectly labeled, the model is able to maintain or improve its performance. \n\nThe results for the WA model are very different. The last row of Table 3 shows the accuracies of the WA model at different levels of noise for English. The model's performance decreases significantly as the number of noisy pairs increases, and the results are similar for the other languages as well. We hypothesize these differences to be due to differences in model complexity. The GRAN model incorporates a sequence model and contains more parameters than the simpler WA model.",
            "score": 0.45226000790944454,
            "section_title": "Data selection",
            "char_start_offset": 17943,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 314
                },
                {
                    "start": 317,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 578
                },
                {
                    "start": 581,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1405
                },
                {
                    "start": 1408,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1888
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1007080078125
        },
        {
            "corpus_id": "207968532",
            "title": "Studying Generalisability across Abusive Language Detection Datasets",
            "text": "Work on Abusive Language Detection has tackled a wide range of subtasks and domains. As a result of this, there exists a great deal of redundancy and non-generalisability between datasets. Through experiments on cross-dataset training and testing, the paper reveals that the preconceived notion of including more non-abusive samples in a dataset (to emulate reality) may have a detrimental effect on the generalisability of a model trained on that data. Hence a hierarchical annotation model is utilised here to reveal redundancies in existing datasets and to help reduce redundancy in future efforts.",
            "score": 0.45176198510460286,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06561279296875
        },
        {
            "corpus_id": "17269653",
            "title": "Language Recognition using Random Indexing",
            "text": "As humans who communicate through language, we have the fascinating ability to recognize unknown languages in spoken or written form, using simple cues to distinguish one language from another. Some unfamiliar languages, of course, might sound very similar, especially if they come from the same language family, but we are often able to identify the language in question with very high accuracy. This is because embedded within each language are certain features that clearly distinguish one from another, whether it be accent, rhythm, or pitch patterns. The same can be said for written languages, as they all have features that are unique. Recognizing the language of a given text is the first step in all sorts of language processing, such as text analysis, categorization, translation and much more. \n\nAs popularized by Shannon (1948), most language models use distributional statistics to explain structural similarities in various specified languages. The traditional method of identifying languages consists of counting individual letters, letter bigrams, trigrams, tetragrams, etc., and comparing the frequency profiles of different text samples. As a general principle, the more accurate you want your detection method to be, the more data you have to store about the various languages. For example, Google's recently open-sourced program called Chromium Compact Language Detector uses large language profiles built from enormous corpora of data. As a result, the accuracy of their detection, as seen through large-scale testing and in practice, is near perfect (McCandless (2011)). \n\nHigh-dimensional vector models are popular in natural-language processing and are used to capture word meaning from word-use statistics. The vectors are called semantic vectors or context vectors. \n\nIdeally, words with a similar meaning are represented by semantic vectors that are close to each other in the vector space, while dissimilar meanings are represented by semantic vectors far from each other. Latent Semantic Analysis is a well-known model that is explained in detail in Landauer & Dumais (1997). It produces 300-dimensional (more or less) semantic vectors from a singular value decomposition (SVD) of a matrix of word frequencies in a large collection of documents. \n\nAn alternative to SVD, based on Random Projections, was proposed by Papadimitriou et al. and Kaski (1998).",
            "score": 0.45074189123033465,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 804
                },
                {
                    "start": 807,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1592
                },
                {
                    "start": 1595,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1791
                },
                {
                    "start": 1794,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2104
                },
                {
                    "start": 2105,
                    "end": 2274
                },
                {
                    "start": 2277,
                    "end": 2383
                }
            ],
            "ref_mentions": [
                {
                    "start": 2079,
                    "end": 2103,
                    "matchedPaperCorpusId": "1144461"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.323486328125
        },
        {
            "corpus_id": "277065772",
            "title": "Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models",
            "text": "6 Discussion and Future Work \n\nOur goal is to provide a foundation for greater data transparency in the ecosystem surrounding large language models. We briefly discuss our findings, The need for multiple methods to surface training data. In our work, we find that we are able to identify text that is known to even proprietary black-box LLMs, and that the examples of memorized text that were successfully identified can differ between probing methods. This indicates that the community would benefit from a range of such approaches, and that focusing on state-of-the-art detection performance should not be the only goal. Further, recent work has investigating combining signals for various training data identification methods in order to determine if a model was trained on a given document (Maini et al., 2024). This suggests that developing diverse, complementary, probes can help us better understand how data was used to train models. \n\nAnswering questions about model generalization. While LLMs have achieved state-of-the-art performance on several benchmarks, the extent of their generalization capabilities remains an open question. This is in part due to an inability to characterize train-test overlap: that is, making sense of what data a model was trained on and how it Data transparency in the LLM ecosystem. Our work is in spirit related to studies that have called for better data documentation in machine learning (Liu et al., 2024b;Bender and Friedman, 2018;Gebru et al., 2021) or offered large-scale search and indexing mechanisms for pre-training corpora (Elazar et al., 2024;Liu et al., 2024a). While previous efforts focus on supervised, accessible datasets, this work focuses on inferring training data of models, especially in cases where there is currently no data transparency. \n\nWhy not only work with open models? In this work, our central focus is closed commercial models that do not offer access to token probabilities. The reason for this choice is that this is the dominant paradigm for many popular models today.",
            "score": 0.45064528879703847,
            "section_title": "Analysis",
            "char_start_offset": 23624,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 31,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 941
                },
                {
                    "start": 944,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1804
                },
                {
                    "start": 1807,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2047
                }
            ],
            "ref_mentions": [
                {
                    "start": 794,
                    "end": 814,
                    "matchedPaperCorpusId": "268781196"
                },
                {
                    "start": 1432,
                    "end": 1451,
                    "matchedPaperCorpusId": "269741151"
                },
                {
                    "start": 1451,
                    "end": 1477,
                    "matchedPaperCorpusId": "52255687"
                },
                {
                    "start": 1477,
                    "end": 1496,
                    "matchedPaperCorpusId": "4421027"
                },
                {
                    "start": 1576,
                    "end": 1597,
                    "matchedPaperCorpusId": "264803575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62548828125
        },
        {
            "corpus_id": "267095281",
            "title": "Raidar: geneRative AI Detection viA Rewriting",
            "text": "data, even they are generated from a different model. We also evaluate our detection efficiency on the Claude (Anthropic, 2023) generated text on student essay (Verma et al., 2023), where we achieve an F1 score of 57.80. \n\nIn the out-of-distribution experiment, we train the detector on data from two language models, assuming it is unaware that the test text will be generated from the third model. Despite a performance drop on detecting the out-of-distribution test data generated from the third model, our method remains effective in detecting content from this unseen model, underscoring our approach's robustness and adaptability, with up to 91 points on F1 score. The number shows the number of data, reflecting by the size of the dot. \n\nType of Detection Model. Mireshghallah et al. (2023) showed that model size affects performance in perturbation-based detection methods. Given the same input text generated from GPT-3.5, We explore our approach's efficacy with alternative rewriting models with different size. In addition to using the costly GPT-3.5 to rewrite, we incorporate two smaller models, Ada and Text-Davinci-002, and evaluate their detection performance when they are used to rewrite. In Table 5, while all models achieve significant detection performance, our results indicate that a larger rewriting language model enhances detection performance in our method. \n\nImpact of Different Prompts. Figure 6 displays the detection F1 score for various prompts across three datasets. While Mitchell et al. (2023) employs up to 100 perturbations to query LLM and compute curvature from loss, our approach achieves high detection performance using just a single rewriting prompt. \n\nImpact of Content Length. We assess our detection method's performance across varying input lengths using the Yelp Review dataset in Figure 5. Longer inputs, in general, achieve higher detection performance. Notably, while many algorithms fail with shorter inputs (Tian, 2023;Verma et al., 2023), our method can achieve 74 points of detection F1 score even with inputs as brief as ten words, highlighting the effectiveness of our approach.",
            "score": 0.45062864914027756,
            "section_title": "LLM for",
            "char_start_offset": 29564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 54,
                    "end": 220
                },
                {
                    "start": 223,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 742
                },
                {
                    "start": 745,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1384
                },
                {
                    "start": 1387,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1693
                },
                {
                    "start": 1696,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 2135
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51318359375
        },
        {
            "corpus_id": "253372606",
            "title": "Play m\u00fasica alegre: A Large-Scale Empirical Analysis of Cross-Lingual Phenomena in Voice Assistant Interactions",
            "text": "assistant queries. Given the low FPR exhibited by the model, the collected utterances represent an high-quality resource which could in future be used to train generative models to produce better synthetic data, which in turn can be used to train detectors in an iterative manner.\n\nFrom an architectural prospective, models tackling word-level language identification expressly designed to solve the task of cross-lingual or codeswitched detection could benefit from the utterancelevel information about their distribution in the dataset. This could encourage the design of a multiheaded model tackling both tasks in an end-to-end approach.\n\nFinally, we aim to expand the set of considered languages to encompass other frame and mixing languages, for example considering Hinglish in India. It might be particularly interesting to compare the obtained results for Spanish with ones obtained over Spanish spoken in the United States and in Mexico, as they may involve more code-switching. Pl ac e 8% S e r v i c e 3 3 %",
            "score": 0.4494647910849146,
            "section_title": "Conclusions",
            "char_start_offset": 23044,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15576171875
        },
        {
            "corpus_id": "258291730",
            "title": "\u201cHOT\u201d ChatGPT: The Promise of ChatGPT in Detecting and Discriminating Hateful, Offensive, and Toxic Comments on Social Media",
            "text": "Models that are constructed using these two approaches are frequently employed as benchmark models (Badjatiya et al., 2017;Djuric et al., 2015). However, more recent detection models have utilized word embedding techniques, such as Word2Vec, GloVe, and Bidirectional Encoder Representations from Transformers (BERT), which represent words as vectors in a high-dimensional space based on their contextual usage (Badri et al., 2022;Mazari et al., 2023;Velankar et al., 2022). Once the text is vectorized, machine learning classifiers, such as support vector machines (SVMs), or neural networks, can be trained to detect HOT content (Khan et al., 2022;MacAvaney et al., 2019). These ML classifiers learn to recognize patterns and features associated with HOT language, such as specific combinations of words, phrases, and sentence structures commonly used in HOT language. \n\nDeveloping these supervised ML models depends on human annotations to build training datasets. While several widely-used datasets, such as Davidson Hate Speech and Offensive Language (DATASET) (Davidson et al., 2017) and OffensEval (Zampieri et al., 2019) have been made available to the public, developing new datasets or updating existing ones could potentially expose annotators to harmful content and require a significant amount of time and effort to generate. For instance, Riedl et al. (2020) found that rating \"uncivil\" comments can lead to a significant psychological toll on the annotators and reduce their accuracy on subsequent tasks. In addition, Gilardi et al. (2023) stated that while trained annotators could cause significant costs, employing crowdworkers like those on MTurk could offer a more affordable alternative, albeit with potentially insufficient quality. However, Kasthuriarachchy et al. (2021) pointed out that labeling data through MTurk can also become expensive when the target messages are small and have a lot of noise, such as toxic comments in social media data.",
            "score": 0.4491952450426386,
            "section_title": "Google Jigsaw (2017)",
            "char_start_offset": 9945,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 869
                },
                {
                    "start": 872,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1969
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 123,
                    "matchedPaperCorpusId": "2880908"
                },
                {
                    "start": 123,
                    "end": 143,
                    "matchedPaperCorpusId": "2039295"
                },
                {
                    "start": 410,
                    "end": 430,
                    "matchedPaperCorpusId": "253025766"
                },
                {
                    "start": 430,
                    "end": 450,
                    "matchedPaperCorpusId": "255669316"
                },
                {
                    "start": 630,
                    "end": 649,
                    "matchedPaperCorpusId": "248974518"
                },
                {
                    "start": 649,
                    "end": 672,
                    "matchedPaperCorpusId": "201115578"
                },
                {
                    "start": 1065,
                    "end": 1088,
                    "matchedPaperCorpusId": "1733167"
                },
                {
                    "start": 1352,
                    "end": 1371,
                    "matchedPaperCorpusId": "213842528"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09967041015625
        },
        {
            "corpus_id": "237503353",
            "title": "Types of Out-of-Distribution Texts and How to Detect Them",
            "text": "for both methods. Calibration methods completely fail when the model is over-confident due to spurious semantic features. While density estimation methods are slightly more robust, language models are easily fooled by repetitions that significantly increase the probability of a piece of text. Together, our findings suggest that better definitions of OOD and corresponding evaluation datasets are required for both model development and fair comparison of OOD detection methods. 1 We exclude task shift where the OOD examples are from a different task, e.g. textual entailment inputs for a text classification model, because it is less likely to happen in realistic settings where users are often aware of the intended use of the model.",
            "score": 0.4489290671963604,
            "section_title": "Introduction",
            "char_start_offset": 3701,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14306640625
        },
        {
            "corpus_id": "271490080",
            "title": "Advances in Human Event Modeling: From Graph Neural Networks to Language Models",
            "text": "A language model (LM) is designed to predict the probability of a sequence of words or characters in a language. It learns the patterns and structures of natural language by analyzing large corpora of text data. Language models are widely used in natural language processing tasks such as machine translation, text generation, speech recognition, and sentiment analysis. \n\nOne of the most significant advancements in language models is the development of the transformer [91], which has led to the emergence of large language models (LLMs) [117]. LLMs are transformer-based language models that contain hundreds of billions (or more) parameters, trained on massive text datasets [80], such as GPT-3 [7], PaLM [16] and LLaMA [88]. These LLMs exhibit remarkable capabilities in understanding natural language and performing complex tasks, e.g., they excel in generating coherent and contextually relevant text that closely resembles human ability. LLMs have been explored in various domains, such as in medicine [87], education [43], and programming [13].",
            "score": 0.4483924175418692,
            "section_title": "Language Models",
            "char_start_offset": 8040,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 370
                },
                {
                    "start": 373,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1053
                }
            ],
            "ref_mentions": [
                {
                    "start": 471,
                    "end": 475,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 679,
                    "end": 683,
                    "matchedPaperCorpusId": "254366666"
                },
                {
                    "start": 699,
                    "end": 702,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 709,
                    "end": 713,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 1010,
                    "end": 1014,
                    "matchedPaperCorpusId": "259947046"
                },
                {
                    "start": 1026,
                    "end": 1030,
                    "matchedPaperCorpusId": "257445349"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41455078125
        },
        {
            "corpus_id": "225039915",
            "title": "Detecting and Exorcising Statistical Demons from Language Models with Anti-Models of Negative Data",
            "text": "Our approach for detecting and attenuating undesirable signals is a process we term anti-modeling. The idea is to select a model that has a particular flaw (e.g., due to an incorrect assumption about the data, like a Markov assumption) such that the flaw makes the model vulnerable to some inherent undesirable characteristic of the data. Then, fit the flawed model to the data, to create a \"negative data distribution\" that we use to generate negative data for detecting and attenuating signals. A diagram sketching how the procedure is used for training with attenuation is in Figure 6 in the appendix. \n\nIn this work, we focus on n-grams, which are extremely useful, but ultimately imperfect accounts of language [4]. The Markov assumption in an n-gram model means it has a limited view of language but once trained we can generate negative data (e.g., from a tri-gram model of Penn Tree Bank): the organization 's desire for pork tends to lock in business long-term but cut revenue in transactions for its premium products While somewhat nonsensical, there are some real senses in which this negative sentence is reasonably language-like. Box's aphorism that \"all models are wrong, some are useful\" certainly applies to the very n-grams we propose to remove. Indeed, studies on locality effects in human sentence processing show that close by dependencies are easier to process [11,6,10]; to maximize communicative efficiency, humans are predisposed toward producing utterances that are easy for an n-gram language model to fit. Of course, n-gram language models are not perfect models of natural language, because in practice humans use both external context and longer range syntactic structures to communicate information. When applying our method to attenuating n-gram statistics, it serves as a regularizer to help the model avoid local minima that occur due to overfitting n-grams, allowing it to learn long-distance dependencies. There is a danger that training the model to ignore the n-gram statistics will cause it to poorly fit an otherwise straightforward sentence, we investigate this in Section 4.",
            "score": 0.4483924175418692,
            "section_title": "Anti-models for detecting and attenuating properties of text",
            "char_start_offset": 4796,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2115
                }
            ],
            "ref_mentions": [
                {
                    "start": 1382,
                    "end": 1386,
                    "matchedPaperCorpusId": "377292"
                },
                {
                    "start": 1386,
                    "end": 1388,
                    "matchedPaperCorpusId": "1444973"
                },
                {
                    "start": 1388,
                    "end": 1391,
                    "matchedPaperCorpusId": "5395686"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38720703125
        },
        {
            "corpus_id": "662359",
            "title": "Language and Task Independent Text Categorization with Simple Language Models",
            "text": "The dominant motivation for language modeling has traditionally come from speech recognition, but language models have recently become widely used in many other application areas. \n\nThe goal of language modeling is to predict the probability of naturally occurring word sequences, s = w 1 w 2 ...w N ; or more simply, to put high probability on word sequences that actually occur (and low probability on word sequences that never occur). Given a word sequence w 1 w 2 ...w N to be used as a test corpus, the quality of a language model can be measured by the empirical perplexity and entropy scores on this corpus \n\n(1) \n\nwhere the goal is to minimize these measures. \n\nThe simplest and most successful approach to language modeling is still based on the n-gram model. By the chain rule of probability one can write the probability of any word sequence as \n\nAn n-gram model approximates this probability by assuming that the only words relevant to predicting Pr(w i |w 1 ...w i\u22121 ) are the previous n \u2212 1 words; i.e.",
            "score": 0.4483924175418692,
            "section_title": "n-Gram Language Modeling",
            "char_start_offset": 5238,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 182,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 613
                },
                {
                    "start": 616,
                    "end": 619
                },
                {
                    "start": 622,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 1016
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.312255859375
        },
        {
            "corpus_id": "269449894",
            "title": "Temporal Scaling Law for Large Language Models",
            "text": "Language models are statistical models designed to model the probabilistic correlation in sequences of natural languages (Touvron et al., 2023). Language models have been fundamental in the field of natural language processing. Early language models relied on statistical methods, such as the ngram models (Brown et al., 1992). The transition to neural network-based models marked a significant advancement. Early works (Bengio et al., 2003) introduced the concept of distributed representations. This was followed by the development of recurrent neural networks (RNNs) (Mikolov et al., 2010) and long short-term memory (LSTM) networks (Graves, 2013), which improved the handling of sequential data and long-range dependencies. These models have accomplished promising results on various downstream tasks at the time, and have inspired progress in the computer vision (Ding et al., 2021;Wang et al., 2023) and the multimedia fields (Chen et al., 2020) in the meantime. The introduction of the Transformer architecture (Vaswani et al., 2017) lead to the development of modern language models. Built upon the scope of Transformers, representative works include BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), T5 (Raffel et al., 2020), and the GPT series (Radford et al., 2018(Radford et al., , 2019)). The advent of GPT-3 (Brown et al., 2020) marks the beginning of the large language model era, as transformer-based generative models are capable of accomplishing various tasks such as in-context learning. Further advancements in LLMs have led to popular works such as PaLM (Chowdhery et al., 2023), Pythia (Biderman et al., 2023), andLLaMA (Touvron et al., 2023). Currently, GPT-4 (OpenAI, 2023) has pushed the boundaries of LLMs further in terms of scale and capability. \n\nDespite the extensive exploration of generative language models, no prior arts has carefully looked into the topic of loss on different token positions.",
            "score": 0.4483924175418692,
            "section_title": "Language Models",
            "char_start_offset": 5949,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1806
                },
                {
                    "start": 1809,
                    "end": 1961
                }
            ],
            "ref_mentions": [
                {
                    "start": 306,
                    "end": 326,
                    "matchedPaperCorpusId": "10986188"
                },
                {
                    "start": 420,
                    "end": 441,
                    "matchedPaperCorpusId": "221275765"
                },
                {
                    "start": 570,
                    "end": 592,
                    "matchedPaperCorpusId": "17048224"
                },
                {
                    "start": 868,
                    "end": 887,
                    "matchedPaperCorpusId": "231572790"
                },
                {
                    "start": 932,
                    "end": 951,
                    "matchedPaperCorpusId": "212633580"
                },
                {
                    "start": 1018,
                    "end": 1040,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1164,
                    "end": 1185,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1193,
                    "end": 1212,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 1308,
                    "end": 1333,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1608,
                    "end": 1632,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 1641,
                    "end": 1669,
                    "matchedPaperCorpusId": "257921893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1654052734375
        },
        {
            "corpus_id": "276920734",
            "title": "Detection avoidance techniques for large language models",
            "text": "We discussed the improvement of LLMs in terms of reasoning performance, which will further complicate LLM detection in the future. In the literature review, it is noted that LLMs are no longer recognizable to humans. This makes machine-based LLM detection vital. In the experimental series, it is shown that all types of state-of-the-art classifiers can be circumvented with sufficient effort. In particular, shallow detectors (Exp. 1), transformer-based detectors (Exp. 2) and zero-shot-based detectors (Exp. 3) were attacked succesfully. For simple detectors, a small adjustment of the generative LLMs proved to be sufficient, such as adjusting the temperature (parameter  in Exp. 1). However, more sophisticated detectors, as used in practice, require a better, more elaborate adjustment of the generative LLMs. During reinforcement learning, these models tend to acquire strategies that make them unrecognizable to detectors (e.g., inserting strings of special characters). While increasing evasion, these techniques tend to impede both the syntactical and semantical quality of the altered text. To counteract these model tendencies, two options were presented. Firstly, such behaviors were penalized during reinforcement learning by introducing additional constraints (cf. Exp. 2). Secondly, paraphrasing was employed, where the algorithm filtered results based on the highest similarity to the original answer, the best linguistic acceptance, and the lowest likelihood of being generated by a language model (i.e., the lowest log loss, cf. Exp. 3). With an evasion rate of > 90%, the model trained on the resulting dataset performed not only better than comparable models, but its results were also most similar to the original response. \n\nIn the experimental series, the language used on social media, including X (formerly Twitter), was examined, and the results were generalized to internet language in general. Hence, for any current written indirect communication on the web, it is currently impossible to tell whether it is coming from a human or a machine (LLM) if the author took actions to hide the origin of the text. This everlasting battle between creating new detectors and crafting new evasion techniques will continue.",
            "score": 0.4483924175418692,
            "section_title": "Conclusions and Outlook",
            "char_start_offset": 66038,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1744
                },
                {
                    "start": 1747,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2134
                },
                {
                    "start": 2135,
                    "end": 2240
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35400390625
        },
        {
            "corpus_id": "272423645",
            "title": "LLM Detectors Still Fall Short of Real World: Case of LLM-Generated Short News-Like Posts",
            "text": "LLMs have led to a pardigm shift, from purposetraining ML models, to fine-tuning base models. Rather than re-training a new model for each application from scratch, a large model is first pretrained on a large quantity of data, and is then fine-tuned to adapt it to downstream task. This paradigm is particularly well-suited for classification tasks as demonstrated by (Devlin et al., 2019). \n\nAs we can formulate detection as a classification task, multiple papers take this approach, historically pioneered by (Solaiman et al., 2019). It is also currently considered a SotA approach for training custom LLM detectors (Ippolito et al., 2020;He et al., 2023) BERT-based detectors BERT (see (Devlin et al., 2019)) and its subsequent improved versions, RoBERTa and Electra (see (Liu et al., 2019) and (Clark et al., 2020)), have gained widespread adoption for classification tasks. Their bi-directional encoding architecture offers an advantage over the autoregressive decoder-only architecture, given the ability to account for the context of both the preceding and following context rather than just the preceding, as well as obligatory anchoring to the provided text. Moreover, their relatively small size, such as the 300M parameters for RoBERTa Large compared to the 175B parameters for GPT-3, makes them highly practical.",
            "score": 0.4483924175418692,
            "section_title": "Trained Detectors",
            "char_start_offset": 5632,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 391
                },
                {
                    "start": 394,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1325
                }
            ],
            "ref_mentions": [
                {
                    "start": 369,
                    "end": 390,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 690,
                    "end": 711,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14892578125
        },
        {
            "corpus_id": "273162261",
            "title": "MetaOOD: Automatic Selection of OOD Detection Models",
            "text": "As a meta-learning approach, we train a suite of OOD detection models offline using a variety of carefully curated datasets to gauge their performance across different scenarios during the meta-train phase (Fig.1,top). When a new dataset arrives, we apply knowledge derived from historical data to select an appropriate OOD detection model (Fig. 1,bottom). This selection is based on the similarity between the new dataset and those used in the meta-train phase. \n\nTo enhance the accuracy of this similarity assessment, we designed two versions of data embeddings via specialized OOD meta-features and language embeddings from language models. As our experiment shows, these language model-based embeddings effectively capture complex, nuanced dataset characteristics that traditional meta-features might miss. We summarize our technical contributions: \n\n\u2022 First OOD Detection Model Selection Framework. We introduce the first meta-learning-based framework for zero-shot OOD detection model selection without training and evaluation. \u2022 Specialized Embeddings for OOD Datasets and Models. We use language model-generated features to quantify the similarity among OOD detection tasks, facilitating a better understanding of OOD characteristics and enhancing OOD detection model selection. \u2022 Effectiveness. The proposed MetaOOD outperforms eleven well-known model selection methods and unsupervised meta-learners on a testbed with 24 unique test data pairs. It is superior and statistically better than all the baselines w.r.t. average rank, and efficient with small runtime. \u2022 Accessibility and Reproducibility. We release the testbed, corresponding code, and the proposed meta-learner at https://github.com/yqin43/metaood.",
            "score": 0.4483924175418692,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3524,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 462
                },
                {
                    "start": 465,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 852
                },
                {
                    "start": 855,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1721
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45361328125
        },
        {
            "corpus_id": "267657993",
            "title": "Ten Words Only Still Help: Improving Black-Box AI-Generated Text Detection via Proxy-Guided Efficient Re-Sampling",
            "text": "Settings. We continue using the datasets and LLMs introduced in Section 3. To simulate realworld situations, we have two settings: 1) Partial White-Box Setting provides true probabilities for the five open-sourced LLMs; and 2) Black-box Setting treats all models as black-box LLMs. For the former setting, we provide a variant POGER-Mixture, which uses true probabilities from whitebox LLMs and estimated ones from black-box LLMs. The estimated probability list is expanded to the same dimension as the true lists by padding with 0. We evaluate for both binary and multiclass detection tasks. \n\nBaselines. 1) GPTZero (Tian, 2023): Distinguish between human and generated text using perplexity and burstiness of text; 2) RoBERTa (Guo et al., 2023): A widely used and powerful pretraining-based detector; 3) T5-Sentinel (Chen et al., 2023) Another pretraining-based method for reframing the classification task as a nexttoken prediction task. 4) DNA-GPT (Yang et al., 2023a): Determine the source of text based on multiple re-generation, can works in two forms under black-box and white-box settings. 5) Detect-GPT (Mitchell et al., 2023): Determine whether a text is generated by comparing the probability of the original text with a large number of perturbed texts. 6) Sniffer (Li et al., 2023a): Determine the source of the text using the contrastive features of the probability lists on each candidate model. 7) SeqXGPT (Wang et al., 2023b): Also based on probability lists of the text, but reframes the classification task as a sequence labeling task. \n\nImplementation Details. For POGER and POGER-Mixture, we use GPT-2 Large as the proxy for representative word selection, with maximum error tolerance \u2206 = 1.2, representative word set size k = 10, re-sampling times N = 100, and sampling temperature t = 1.0.",
            "score": 0.4483924175418692,
            "section_title": "Experimental Settings",
            "char_start_offset": 16195,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 592
                },
                {
                    "start": 595,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1554
                },
                {
                    "start": 1557,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1812
                }
            ],
            "ref_mentions": [
                {
                    "start": 818,
                    "end": 837,
                    "matchedPaperCorpusId": "265213244"
                },
                {
                    "start": 1113,
                    "end": 1136,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 1422,
                    "end": 1442,
                    "matchedPaperCorpusId": "264128397"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.216796875
        },
        {
            "corpus_id": "231719647",
            "title": "Explaining Natural Language Processing Classifiers with Occlusion and Language Modeling",
            "text": "in NLP. We show that language modeling is especially suited to sample replacement. They excel at this task because language understanding by language models is the foundation of state-of-the-art models in NLP.\n\nWe motivate OLM psychologically and through information theory. These arguments lead us to selecting and evaluating likely alternatives to explain features of the original input. The formula for OLM is achieved by inserting sampling with a language model into the difference of probabilities formula. We analyze our method by going over the axioms that we introduced in the preceding chapter. We provide proofs of compliance. Since the formula for OLM is a weighted mean of predictions, we add the standard deviation as an additional measure OLM-S. This measure is intended as a sensitivity method, not a relevance method.\n\nWe provide experimental evaluation of our method. As alluded to in the second chapter, we do not consider a complete evaluation, like a single benchmark, possible. We try to compare our method to existing methods by a correlation experiment. We work under the assumption that sensible methods correlate, but when introducing a novel method it is also important to see that there is no perfect correlation. Both aspects are shown over three tasks. The results on one task, CoLA, point to a possible problem of OLM.\n\nThe second experiment is a deeper dive into the relevances of our method on CoLA. Linguistic acceptability is a context where it is wrong to assume that the distribution of a language model is similar to the distribution of the data. We show that this manifests in the explanations. We discuss how to interpret these explanations and how they make sense once the context is understood. The introspection with OLM is possibly deeper than with other relevance methods, as we can also provide the compared samples. Furthermore, we show that the language modeling works as it is supposed to.\n\nFinally, we experiment on a dataset that is even more specific than CoLA. It contains pairs of sentences with verb frames that are either acceptable or unacceptable. This provides the additional information that the verb has to be very important for the model, as different verbs lead to different classifications in the same structure. We show that our method is able to identify verbs as very important. Furthermore, we show that relevances of verbs in sentence pairs is weakly negatively correlated, which indicates that our method does not",
            "score": 0.4483924175418692,
            "section_title": "Summary",
            "char_start_offset": 68861,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32177734375
        },
        {
            "corpus_id": "2479254",
            "title": "Ending-based Strategies for Part-of-Speech Tagging",
            "text": "We found that the success rate increased with N up to a point and then declined. Many factors were varied, and in the best case we achieved a success rate of 97.5 per cent, the highest rate we have seen reported for a tagger that tags unseen or unknown words without any benefi t of external knowledge. \n\nThe rest of the paper is organized as follows. In the next section we describe HMMs as applied to POS tagging and related terminology and notation. In Sec tion 3, we describe our tagger and what parameters were varied in the experiments. Finally, we discuss the meaning of the results and their significance. A common approach to POS tagging is the hidden Markov model (HMM) (Jelinek, 1985;Church, 1989;Foster, 1991;Merialdo, 1990;Kuhn and De Mori, 1990) or variations thereof (DeRose, 1988;Garside et al., 1987) where language is assumed to be produced by a hidden model that cannot be observed directly but whose effects can be observed. For a good in troduction and overview, see (Charniak et a/., 1993). Given a sequence of tokens w1 \u2022.. w11, an actual stream of text of length n we abbreviate to A }'= 1 Wi, the HMM method computes the word-tag sequence (or simply tag sequence) t1 ... t11 (abbreviated A }'=1t;) that most probably generated the sequence. The sequence may be an entire sentence or, in the case of a bi-tagger (defined below), a sequence of tokens beginning and ending with an unambiguous token, a token with only one known tag assignment. Because language is inher ently ambiguous, generally there are many reasonable possibilities, even in context. A famous example is \"time flies\", but it is easy to produce others. Thus, the HMM-based approach chooses the tag sequence that maximizes \n\n(1)",
            "score": 0.4483924175418692,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3330,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 302
                },
                {
                    "start": 305,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1713
                },
                {
                    "start": 1716,
                    "end": 1719
                }
            ],
            "ref_mentions": [
                {
                    "start": 680,
                    "end": 695,
                    "matchedPaperCorpusId": "60744956"
                },
                {
                    "start": 695,
                    "end": 708,
                    "matchedPaperCorpusId": "3166885"
                },
                {
                    "start": 721,
                    "end": 736,
                    "matchedPaperCorpusId": "61014458"
                },
                {
                    "start": 988,
                    "end": 1011,
                    "matchedPaperCorpusId": "1352470"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.057708740234375
        },
        {
            "corpus_id": "238857039",
            "title": "Building Chinese Biomedical Language Models via Multi-Level Text Discrimination",
            "text": "Before diving into the details of our approach, we briefly discuss related studies on building PLMs in general and biomedical domains. \n\nGeneral Domain PLMs. Recent years have seen remarkable success of PLMs in the field of NLP. These PLMs are typically built with self-supervised learning over massive unlabeled text in the general domain, e.g., Wikipedia, newswire, or Web articles (Radford et al., 2018). Masked language modeling (MLM), which trains a model to recover the identities of a small subset of masked-out tokens (typically 15%), is the most prevailing self-supervised objective, first introduced in BERT (Devlin et al., 2019) and then widely adopted by follow-up studies (Liu et al., 2019;Lan et al., 2020;Joshi et al., 2020;Sun et al., 2020). Despite their effectiveness and popularity, MLM-based approaches can only learn from those 15% masked-out tokens per input, and therefore incur high compute costs. \n\nTo address this low efficiency issue, ELECTRA (Clark et al., 2020) uses a new pre-training framework. Specifically, it corrupts an input sequence by replacing some of the tokens with plausible alternatives sampled from an auxiliary generator, and trains a discriminator to predict for each token in that sequence whether it is original or replaced, i.e., replaced token detection (RTD). As the discriminator can learn from all input tokens rather than just 15% of them, ELECTRA enjoys better efficiency and accelerates training. \n\nWhile achieving empirical success, there are concerns about whether the over-simplified RTD task of ELECTRA, as a binary classification problem, is informative enough for language modeling (Aroca-Ouellette and Rudzicz, 2020). Xu et al. (2020) and Shen et al. (2021) thus proposed training the model via a generalization of RTD while a simplification of MLM, by recovering for each token its original identity from a few plausible candidates, rather than from the whole vocabulary. \n\nAnother limitation of ELECTRA is that it is pretrained solely at the token level but lacks semantics at the sequence level.",
            "score": 0.4483924175418692,
            "section_title": "Background",
            "char_start_offset": 4891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 137,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 921
                },
                {
                    "start": 924,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1452
                },
                {
                    "start": 1455,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1935
                },
                {
                    "start": 1938,
                    "end": 2061
                }
            ],
            "ref_mentions": [
                {
                    "start": 384,
                    "end": 406,
                    "matchedPaperCorpusId": "49313245"
                },
                {
                    "start": 618,
                    "end": 639,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 703,
                    "end": 720,
                    "matchedPaperCorpusId": "59291975"
                },
                {
                    "start": 720,
                    "end": 739,
                    "matchedPaperCorpusId": "198229624"
                },
                {
                    "start": 739,
                    "end": 756,
                    "matchedPaperCorpusId": "198968327"
                },
                {
                    "start": 970,
                    "end": 990,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 1644,
                    "end": 1679,
                    "matchedPaperCorpusId": "222133368"
                },
                {
                    "start": 1702,
                    "end": 1720,
                    "matchedPaperCorpusId": "235265630"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2303466796875
        },
        {
            "corpus_id": "267068867",
            "title": "Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text",
            "text": "We start our experiments with several datasets described in the LLM detection literature. The most recent baseline to which we compare is Ghostbuster. who propose this method, also introduce three datasets that we include in our study: Writing Prompts, News, and Student Essay. These are balanced datasets with equal numbers of human samples and machine samples. The machine samples are written by ChatGPT. 3 e also generate several datasets of our own to evaluate our capability in detecting other language models aside from ChatGPT. Drawing samples of human-written text from CCNews (Hamborg et al., 2017), PubMed (Sen et al., 2008), and CNN (Hermann et al., 2015), we generate corresponding, machine-generated completions using LLaMA-2-7B and Falcon-7B (see details in Appendix A.1.1). Further, we use the Orca dataset (Lian et al., 2023), which provides several million instruction prompts with their machine-generated completions from GPT-3 and GPT-4.",
            "score": 0.4483924175418692,
            "section_title": "Datasets",
            "char_start_offset": 15658,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 956
                }
            ],
            "ref_mentions": [
                {
                    "start": 585,
                    "end": 607,
                    "matchedPaperCorpusId": "5830937"
                },
                {
                    "start": 644,
                    "end": 666,
                    "matchedPaperCorpusId": "6203757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.265380859375
        },
        {
            "corpus_id": "8636445",
            "title": "Robust Probabilistic Predictive Syntactic Processing",
            "text": "This section will briefly introduce language modeling for statistical speech recognition, with such topics as the chain rule, n-gram language modeling, and interpolation for smoothing 1 . A speech recognition system can be thought of as a function that takes an acoustic signal as input and outputs a string of words; a \"good\" function outputs a \"good\" string of words, i.e. a string 97 of words that largely matches the string of words intended by the speaker. A statistical approach to speech recognition tries to find the string of words which is most likely given the observed speech signal, i.e. the string with the maximum a posteriori probability. Given an acoustic signal A, the system attempts to find a string of words S max in the language L such that The first component of this model, P(A|S), is the probability of the acoustic signal given the string, which is known as the acoustic model. The second component of the model, P(S), is the prior probability of the string itself, which is known as the language model. The quality of a statistical speech recognizer will jointly depend on the quality of these two models. We will put acoustic modeling to the side and consider language modeling in isolation.\n\nIn language modeling, we assign probabilities to strings of words. A string, taken as a whole, may never have been seen before, and, in fact, very likely has not been seen. To assign a probability, the chain rule is generally invoked. The chain rule states, for a string of k+1 words, w k\n\nThis corresponds to the left-to-right ordering of most speech recognition systems, in that the probability of any particular word is conditioned on the words to its left in the string. However, even if the permitted sentence length were bounded to some fixed finite length, this formulation would require an infeasibly large conditional probability estimate for the words at the end of the string. In order to estimate these conditional probabilities, the conditioning substrings (i.e. w i\u22121 0 ) are clustered into equivalence classes, and the probability of a word is conditioned on the equivalence class of the string to its left.\n\nOne interesting way in which the models that we will be discussing can and do differ is in how they define these equivalence classes. The most prevalent class of models makes the",
            "score": 0.4483924175418692,
            "section_title": "Language modeling for speech recognition",
            "char_start_offset": 249410,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1949462890625
        },
        {
            "corpus_id": "3204901",
            "title": "Aggregate and mixed-order Markov models for statistical language processing",
            "text": "These results confirm that aggregate Markov models are intermediate in accuracy between unigram (C = 1) and bigram (C = V ) models. \n\nThe aggregate Markov models were also observed to discover meaningful word classes. Table 2 shows, for the aggregate model with C = 32 classes, the most probable class assignments of the three hundred most commonly occurring words. To be precise, for each class c * , we have listed the words for which c * = arg max c P (c|w). Figure 2 shows a histogram of the winning assignment probabilities, max c P (c|w), for these words. Note that the winning assignment probabilities are distributed broadly over the interval [1 C , 1]. This demonstrates the utility of allowing \"soft\" membership classes: for most words, the maximum likelihood estimates of P (c|w) do not correspond to a winner-take-all assignment, and therefore any method that assigns each word to a single class (\"hard\" clustering), such as those used by Brown et al. (1992) or Ney, Essen, and Kneser (1994), would lose information. \n\nWe conclude this section with some final comments on overfitting. Our models were trained by thirty-two iterations of EM, allowing for nearly complete convergence in the log-likelihood. Moreover, we did not implement any flooring constraints 1 on  Table 2: Most probable assignments for the 300 most frequent words in an aggregate Markov model with C = 32 classes. Class 14 is absent because it is not the most probable class for any of the selected words.) the probabilities P (c|w 1 ) or P (w 2 |c). Nevertheless, in all our experiments, the ML aggregate Markov models assigned non-zero probability to all the bigrams in the test set. This suggests that for large vocabularies there is a useful regime 1 \u226a C \u226a V in which aggregate models do not suffer much from overfitting. In this regime, aggregate models can be relied upon to compute the probabilities of unseen word combinations. We will return to this point in Section 4, when we consider how to smooth n-gram language models.",
            "score": 0.4483924175418692,
            "section_title": "Aggregate Markov models",
            "char_start_offset": 9647,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 134,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 1028
                },
                {
                    "start": 1031,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2015
                }
            ],
            "ref_mentions": [
                {
                    "start": 951,
                    "end": 970,
                    "matchedPaperCorpusId": "10986188"
                },
                {
                    "start": 974,
                    "end": 1003,
                    "matchedPaperCorpusId": "206560877"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09332275390625
        },
        {
            "corpus_id": "12403904",
            "title": "Designing Statistical Language Learners: Experiments on Noun Compounds",
            "text": "tic models based on the meaning distributions theory.\n\nIn the experiments on syntax, a novel model, based on dependency relations between concepts, was developed and implemented. Empirical comparisons demonstrated that this model is significantly better than those previously proposed and approaches the performance of human judges on the same task. This model also correctly predicts the observed distribution of syntactic structures.\n\nIn the experiments on semantic analysis, a novel model, the first statistical model of this problem, was developed and implemented. The system uses statistics computed from prepositional phrases to predict a paraphrase with significantly better accuracy than the baseline strategy. The training data used is both sparse and noisy, and the experimental results support the need for a theory of data requirements. Without a predictive data requirements theory, statistical language learning remains an artform.  162 5.10 Numbers of training instances generated by the paraphrasing patterns . . . . 164 5.11 Relevant parameter values for hypothetical example: cask wine . . . . . . . . 166 5.12 Maximum likelihood estimate results for compound paraphrasing . . . . . . . 166 5.13 Expected likelihood estimate results for compound paraphrasing . . . . . . . 167 5.14 Paraphrasing results when restricted to three most common answers . . . . . 169 5.15 Paraphrasing results when restricted to two most common answers . . . . . . 169 vi",
            "score": 0.4483924175418692,
            "section_title": "Abstract",
            "char_start_offset": 2478,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07977294921875
        },
        {
            "corpus_id": "7079619",
            "title": "Analyzing and Improving Statistical Language Models for Speech Recognition",
            "text": "In chapter two, we give an overview of the di erent components of a speech recognizer, describe their interaction, de ne the task of the component central to this thesis, the language model more formally, and review the most commonly used language models.\n\nAfter introducing the di erent components of a speech recognizer, we de ne the task of a language model as the construction of one or more probability distributions over all the words of a vocabulary given the words that have been recognized so far. Intuitively, the speech recognizer uses this distribution to decide which words are likely to appear next, e.g. based on the probability distribution, it chooses the word \\peach\" over \\teach\" when the preceding sentence fragment is \\He ate the\". As this example illustrates, di erent words are more likely to appear in di erent contexts. Therefore, a language model usually has many di erent distributions, one for each context. In language modeling, context often means the two or three words preceding the word to predict. To show the usefulness of such a simple context, let us de ne the context as the immediately preceding word and consider what this entails for the following word. Only nouns and adjectives are likely to appear if the immediately preceding words is for example \\the\". Thus, even this simple de nition of context can severely restrict the following word.\n\nDuring speech recognition, the language model only has to choose a distribution according to the current context and to look up the probabilities of words in this distribution. The important task in constructing the language model is to determine, prior to recognition, the number of contexts it di erentiates and to construct a probability distribution for each one of them.\n\nLanguage models are usually described in terms of frequency counts of their probability distributions, but for the purposes of this thesis, it is more appropriate to describe language models at the more abstract level of probability distributions and contexts. This is because we are more interested in the conceptually important aspects, e.g. the way in which a model de nes context, rather than in the details of a particular technique of constructing probability distributions from frequency counts. Therefore, we only give a brief description of how probability distributions can be estimated from frequency data, often referred to as training data. The principle of this estimation is that of counting how often a certain event appears in",
            "score": 0.4483924175418692,
            "section_title": "Chapter 2: Language Modeling for Speech Recognition",
            "char_start_offset": 19944,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.330322265625
        },
        {
            "corpus_id": "250244107",
            "title": "Panning for gold: Lessons learned from the platform-agnostic automated detection of political content in textual data",
            "text": "We understand political content as material mentioning political actors/institutions and societally relevant issues that are tackled by political means (e.g. the economy; for more information on this definition, see Appendix A1). Because of our use case (see Appendix A2), we were particularly interested in Swiss/German actors, as well as the political issues relevant to these two countries. Although they have implications for the direct reuse of the detection models we developed, we expect that the observations generated through our model comparison will be applicable to a broad range of contexts.\n\nTo detect political content, we used three groups of techniques: SML, NN, and dictionary-based ones 4 . We were interested in the low-cost implementation of these techniques, in particular in terms of minimising the resources required to obtain data for model training and conducting the actual training (e.g. time-and processing-wise). Such an interest is attributed to our assumption that many academic projects might have limited financial/time resources for in-house NLP technique development; hence, we wanted to compare the performance of detection techniques under these unfavourable circumstances.\n\nFor SML, we used five models: logistic regression (LR), passive aggressive (PA), Bernoulli naive Bayes (BNB), multinomial naive Bayes (MNB), and stochastic gradient descent (SGD). These models differ in complexity, with some being based on simple Bayesian probabilistic modelling (e.g. BNB) and others relying on more advanced incremental learning principles (e.g. PA). All SML models were trained using the Scikit-Learn package for Python (Pedregosa et al. 2011).\n\nFor NNs, we used three models: CNN, LSTM, and BERT. CNNs have low computational costs compared with other types of NNs and focus on high-level features, while LSTMs place major emphasis on term sequences. Finally, BERT (Devlin et al. 2019) is a transformer model characterised by high computational costs but also more advanced capabilities for processing sequential data, along with an extensive awareness of contextual relationships between words attributed to it being pretrained on a large text corpus.\n\nTo train the CNN and LSTM models, we used the Python",
            "score": 0.4483924175418692,
            "section_title": "Political content detection",
            "char_start_offset": 9693,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1239013671875
        },
        {
            "corpus_id": "918796",
            "title": "Generalized Expectation Criteria for Bootstrapping Extractors using Record-Text Alignment",
            "text": "learn an accurate language model. The second approach (Sarawagi and Cohen, 2004;Michelson and Knoblock, 2005;Mansuri and Sarawagi, 2006) uses database or dictionary lookups in combination with similarity measures to add features to the text sequence. Although these features are very informative, learning algorithms still require annotated data to make use of them. The final approach heuristically labels texts using matching records and learns extractors from these annotations (Ramakrishnan and Mukherjee, 2004;Bellare and McCallum, 2007;Michelson and Knoblock, 2008). Heuris-tic labeling decisions, however, are made independently without regard for the Markov dependencies among labels in text and are sensitive to subtle changes in text.\n\nHere we propose a method that automatically induces a labeling of an input text sequence using a word alignment with a matching database record. This induced labeling is then used to train a text extractor. Our approach has several advantages over previous methods. First, we are able to model field ordering and context around fields by learning an extractor from annotations of the text itself. Second, a probabilistic model for word alignment can exploit dependencies among alignments, and is also robust to errors, formatting differences, and missing fields in text and the record. Our word alignment model is a conditional random field (CRF) (Lafferty et al., 2001) that generates alignments between tokens of a text sequence and a matching database record. The structure of the graphical model resembles IBM Model 1 (Brown et al., 1993) in which each target (record) word is assigned one or more source (text) words. The alignment is generated conditioned on both the record and text sequence, and therefore supports large sets of rich and nonindependent features of the sequence pairs. Our model is trained without the need for labeled word alignments by using generalized expectation (GE) criteria (Mann and McCallum, 2008) that penalize the divergence of specific model expectations from target expectations. Model parameters are estimated by minimizing this divergence. To limit over-fitting we include a L 2 -regularization term in the objective. The model expectations in GE criteria are taken with respect to a set of alignment latent variables that are either specific to each sequence",
            "score": 0.4483924175418692,
            "section_title": "Introduction",
            "char_start_offset": 1998,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 54,
                    "end": 80,
                    "matchedPaperCorpusId": "4108765"
                },
                {
                    "start": 80,
                    "end": 109,
                    "matchedPaperCorpusId": "10109645"
                },
                {
                    "start": 109,
                    "end": 136,
                    "matchedPaperCorpusId": "13111836"
                },
                {
                    "start": 481,
                    "end": 515,
                    "matchedPaperCorpusId": "45822290"
                },
                {
                    "start": 515,
                    "end": 542,
                    "matchedPaperCorpusId": "8597804"
                },
                {
                    "start": 542,
                    "end": 571,
                    "matchedPaperCorpusId": "1408190"
                },
                {
                    "start": 1393,
                    "end": 1415,
                    "matchedPaperCorpusId": "219683473"
                },
                {
                    "start": 1568,
                    "end": 1588,
                    "matchedPaperCorpusId": "13259913"
                },
                {
                    "start": 1952,
                    "end": 1976,
                    "matchedPaperCorpusId": "9976345"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2022705078125
        },
        {
            "corpus_id": "277741224",
            "title": "On The Landscape of Spoken Language Models: A Comprehensive Survey",
            "text": "We then calculate the percentage of instances where the SLM assigns a higher likelihood to the positive example than to the negative one. This type of evalution has also been popular in the NLP community until very recently (Zellers et al., 2019;Mostafazadeh et al., 2016;Touvron et al., 2023b;Gemma Team et al., 2024;LCM team et al., 2024). Due to significant model improvements this type of NLP benchmark has saturated and significantly more complex ones have emerged (Jimenez et al., 2023;Srivastava et al., 2023). \n\nMetrics in this category are designed to evaluate a model's capacity to represent various speech characteristics. For example, sWUGGY (Dunbar et al., 2021) evaluates the lexical capabilities of models by presenting them with pairs of utterances, one consisting of a real word and the other a similar non-word (e.g., 'brick' vs. 'blick'), and measuring the model's ability to assign a higher probability to the real word (where nonwords were obtained from the text WUGGY task (Keuleers & Brysbaert, 2010)). Similarly, sBLIMP (Dunbar et al., 2021) (which was adapted from the text-based BLIMP (Warstadt et al., 2020)) evaluates the ability of the 9 Theoretically, there is always some input, even when no one is speaking. Here, we assume that a voice activity detection (VAD) system is in place, so only the tokens representing actual user speech are sent to the SLM during speaking mode. \n\n10 Such evaluation metrics can be considered as an analogue to perplexity (PPL) for text LLMs. Since there is no standard method for speech tokenization, the speech \"vocabulary\" may change between pure SLMs. Therefore, PPL cannot be directly applied. Instead, likelihood-based metrics introduce an alternative approach of comparing sequence likelihoods. \n\nnetwork to model syntactic properties. The network is presented with a matched pair of grammatically correct and incorrect sentences. Hassid et al. (2023) expanded this method to evaluate semantic understanding of spoken text.",
            "score": 0.4483924175418692,
            "section_title": "Likelihood-Based Evaluation",
            "char_start_offset": 72253,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 517
                },
                {
                    "start": 520,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1406
                },
                {
                    "start": 1409,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1762
                },
                {
                    "start": 1765,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 1991
                }
            ],
            "ref_mentions": [
                {
                    "start": 224,
                    "end": 246,
                    "matchedPaperCorpusId": "159041722"
                },
                {
                    "start": 246,
                    "end": 272,
                    "matchedPaperCorpusId": "1726501"
                },
                {
                    "start": 492,
                    "end": 516,
                    "matchedPaperCorpusId": "263625818"
                },
                {
                    "start": 654,
                    "end": 675,
                    "matchedPaperCorpusId": "236966032"
                },
                {
                    "start": 995,
                    "end": 1023,
                    "matchedPaperCorpusId": "3671463"
                },
                {
                    "start": 1044,
                    "end": 1065,
                    "matchedPaperCorpusId": "236966032"
                },
                {
                    "start": 1111,
                    "end": 1134,
                    "matchedPaperCorpusId": "208527435"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0894775390625
        },
        {
            "corpus_id": "269188338",
            "title": "Sampling-based Pseudo-Likelihood for Membership Inference Attacks",
            "text": "Our method is applicable under severer conditions than the previous studies, which requires the loss L or token likelihood P \u03b8 of the model f \u03b8 .In other words, our method can be applied to any LLMs because it uses only generated texts without the loss or likelihood.The proposed method is formalized as follows.For a text x = (w 1 , w 2 , . . ., w T ) of length T to be detected, we divide it into a prefix x prefix = (w 1 , w 2 , . . ., w \u230aT /2\u230b ) and a reference text x ref = (w \u230aT /2\u230b+1 , w \u230aT /2\u230b+2 , . . ., w T ) based on the number of words T .The LLM then generates m text samples (candidates hereafter) x j cand (j = 1, . . ., m) that continue from the prefix x prefix .We use these candidates for MIA.\n\nSaMIA judges that a text x is included in the training data of the LLM if the candidate text x j cand generated by the LLM has a high surface-level similarity to the reference text x ref .We use ROUGE-N (Lin, 2004) as the similarity metric, which measures the recall of n-grams in the reference text.Given a candidate text x cand generated by the LLM and a reference text x ref , we calculate the ROUGE-N score (ranging from 0 to 1) with Equation 1:\n\nHere, n is the length of the n-grams.The denominator is the total number of n-grams in the reference text, and the numerator is the total number of n-grams that overlap between the candidate text and the reference text.For example, a ROUGE-1 score is high when the words generated by the LLM appear in the reference text.We expect that SPL can appropriately capture the distribution of LLM's generations by empirically sampling texts from the LLM.Let W be a random variable defined by the LLM, and let P (W = x) be the probability that W takes the text x.Let X (j) x be a random variable such that X (j)",
            "score": 0.4483924175418692,
            "section_title": "SPL",
            "char_start_offset": 4366,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 145,
                    "end": 267
                },
                {
                    "start": 267,
                    "end": 312
                },
                {
                    "start": 312,
                    "end": 345
                },
                {
                    "start": 345,
                    "end": 437
                },
                {
                    "start": 437,
                    "end": 511
                },
                {
                    "start": 511,
                    "end": 551
                },
                {
                    "start": 551,
                    "end": 634
                },
                {
                    "start": 634,
                    "end": 679
                },
                {
                    "start": 679,
                    "end": 711
                },
                {
                    "start": 713,
                    "end": 901
                },
                {
                    "start": 901,
                    "end": 1013
                },
                {
                    "start": 1013,
                    "end": 1162
                },
                {
                    "start": 1164,
                    "end": 1201
                },
                {
                    "start": 1201,
                    "end": 1383
                },
                {
                    "start": 1383,
                    "end": 1485
                },
                {
                    "start": 1485,
                    "end": 1611
                },
                {
                    "start": 1611,
                    "end": 1719
                },
                {
                    "start": 1719,
                    "end": 1767
                }
            ],
            "ref_mentions": [
                {
                    "start": 916,
                    "end": 927,
                    "matchedPaperCorpusId": "964287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00760650634765625
        },
        {
            "corpus_id": "2032603",
            "title": "Similarity-Based Approaches to Natural Language Processing",
            "text": "\"You shall know a word by the company it keeps!\" (Firth, 1957, pg. 11) We begin by considering the problem of predicting string probabilities. Suppose we are presented with two strings, 1. \"Grill doctoral candidates\", and 2. \"Grill doctoral updates\", and are asked to determine which string is more likely. Notice that this is not the same question as asking which of these strings is grammatical. In fact, both constitute legitimate English sentences. The first sentence is a command to ask a graduating Ph.D. student many difficult questions; the second might be an order to take lists of people who have just received doctorates and throw them 1 on a Hibachi.\n\nMethods for assigning probabilities to strings are called language models. In this thesis, we will abuse the term somewhat and refer to methods that assign probabilities to word associations as language models, too. That is, we will consider methods which estimate the probability of word cooccurrence relations; these methods need not be defined on sentences. For example, in chapters 3 and 4 we will be concerned with the problem of estimating the probability that a noun x and a transitive verb y appear in a sentence with x being the head noun of the direct object of y.\n\nOne important application of language modeling is error correction. Current speech recognizers do not achieve perfect recognition rates, and it is easy to imagine a situation in which a speech recognizer cannot decide whether a speaker said \"Grill doctoral candidates\" or \"Grill doctoral updates\". A language model can provide a speech recognizer with the information that the former sentence is more likely than the latter; this information would help the recognizer make the right choice. Similar situations arise in handwriting recognition, spelling correction, optical character recognition, and so on -whenever the physical evidence itself may not be enough to determine the corresponding string.\n\nMore formally, let E be some physical evidence, and suppose we wish to know whether the string W is the message conveyed or encoded by E. Using Bayes' rule, we can combine the estimate P (E|W ) given by an acoustic model with the probability P LM (W ) assigned by a language model to find the posterior probability that W is the true string given the evidence at hand: P (W |E) = P LM (W )P",
            "score": 0.4483924175418692,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1279296875
        },
        {
            "corpus_id": "268870599",
            "title": "Addressing Challenges in Hate Speech Detection using BERT-based Models: A Review",
            "text": "Typically, texts are unstructured data. However, since mathematical modeling is a fundamental component of all machine and deep learning algorithms, the unstructured character of text input must be transformed into a structured feature space. Deep learning methods are extensively used to learn the representations and not only process these representations to extract the output [42]. After cleaning the dataset, it can be transformed into a vector space using feature representation techniques [1]. Many classical text representation methods were used, such as Term Frequency-Inverse Document Frequency (TF-IDF) and Bag-of-Words. However, BERT was one of the transformer-based language models that outperformed others. \n\nBelow are models that use BERT only for feature extraction, followed by another classification method. These models treat feature extraction and classification as independent steps. Their contributions and results are illustrated. Table 2 provides a brief description of these models, including the versions used, classification algorithms, datasets, languages, and types of classification. The models giving the best results are written in bold. Each model was used to handle a certain challenge or problem from a set of open challenges in the hate speech detection task.",
            "score": 0.4467960712363391,
            "section_title": "Using BERT-Based Models for Feature Extraction",
            "char_start_offset": 17132,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 720
                },
                {
                    "start": 723,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1295
                }
            ],
            "ref_mentions": [
                {
                    "start": 496,
                    "end": 499,
                    "matchedPaperCorpusId": "235639342"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2783203125
        },
        {
            "corpus_id": "273229343",
            "title": "Training-free LLM-generated Text Detection by Mining Token Probability Sequences",
            "text": "Recent advancements in large language models (LLMs), such as GPT-4 (OpenAI, 2024b) and Gemma (Team et al., 2024), have significantly enhanced the text generation capabilities of machines. These models produce texts that are virtually indistinguishable from those written by humans, enabling broad applications across fields like journalism (Quinonez & Meij, 2024), education (M Alshater, 2022; Xiao et al., 2023), and medicine (Thirunavukarasu et al., 2023). However, the increasing sophistication of LLMs has also raised serious concerns about their potential misuse. Examples include the fabrication and dissemination of fake news (Opdahl et al., 2023;Fang et al., 2024), academic dishonesty in scientific writing (Else, 2023;Currie, 2023;Liang et al., 2024), and the risk of model collapse when trained on LLM-generated data (Shumailov et al., 2024;Wenger, 2024). \n\nPrior arts in LLM-generated text detection can be broadly categorized as training-based and trainingfree methods. Training-based methods extract discriminative features from the texts and then input them into binary classifiers. These detectors require training on labeled datasets. Typical examples include RoBERTa-based fine-tuning (Guo et al., 2023), OpenAI Text Classifier (OpenAI, 2023), and GPTZero (Tian, 2023). In contrast, training-free methods utilize global statistical features of given texts such as likelihood and rank (Solaiman et al., 2019), which can be computed from LLMs during the text inference. Typical training-free methods such as DetectGPT (Mitchell et al., 2023) and DNA-GPT (Yang et al., 2024) achieve detection by comparing or scoring these statistical features. Figure 1: Comparison of TPS fluctuations and Lastde score distributions between human-written texts and LLM-generated texts, using the first 30 tokens of human texts as a prefix to continue writing with OPT-2.7.",
            "score": 0.4463892922213719,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 866
                },
                {
                    "start": 869,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1871
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 412,
                    "matchedPaperCorpusId": "259376566"
                },
                {
                    "start": 427,
                    "end": 457,
                    "matchedPaperCorpusId": "259947046"
                },
                {
                    "start": 654,
                    "end": 672,
                    "matchedPaperCorpusId": "261898112"
                },
                {
                    "start": 716,
                    "end": 728,
                    "matchedPaperCorpusId": "255773668"
                },
                {
                    "start": 728,
                    "end": 741,
                    "matchedPaperCorpusId": "258855273"
                },
                {
                    "start": 828,
                    "end": 852,
                    "matchedPaperCorpusId": "271448069"
                },
                {
                    "start": 852,
                    "end": 865,
                    "matchedPaperCorpusId": "271455154"
                },
                {
                    "start": 1534,
                    "end": 1557,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 1570,
                    "end": 1589,
                    "matchedPaperCorpusId": "258960101"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39697265625
        },
        {
            "corpus_id": "102351017",
            "title": "Deep Learning Features for Robust Detection of Acoustic Events in Sleep-disordered Breathing",
            "text": "Breathing sounds, like language, usually follows certain patterns, e.g., a snore sound typically begins with an inspiration and is followed by a brief period of silence before an expiration. A language model (LM), such as is normally used in speech recognition, might therefore be leveraged for more accurate snore detection during the decoding process. HTK is used to compute event bigram statistics from the labelled training dataset. Given the limited amount of training data, the bigram model proves a suitable choice and is easy to incorporate. The LM is employed by both the tandem and the hybrid systems during the decoding process. In both systems, the Viterbi algorithm is employed to search for the most probable acoustic event sequence given the acoustic model and LM.",
            "score": 0.44585617351028245,
            "section_title": "Language model",
            "char_start_offset": 13407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 779
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17919921875
        },
        {
            "corpus_id": "258762215",
            "title": "Large Language Models can be Guided to Evade AI-Generated Text Detection",
            "text": "(i) DetectGPT (Mitchell et al., 2023) evaluates the variation in a language model's log probability by introducing minor perturbations to the detected text. (ii) Log-Rank (Mitchell et al., 2023) is a statistical method that employs a language model to compute the mean prediction rank of each token in a text, given its preceding context. We utilize a relatively smaller language model, GPT2-medium (Radford et al., 2019), for both methods. Because Mireshghallah et al. (2023) find that smaller language models have better detection performance than larger ones. \n\nAPIs.3 (i) GPTzero (Tian, 2023) is a widely-used commercial detector, cooperated with many academic organizations. (ii) OpenAI Detector (OpenAI-D) (OpenAI, 2023) is officially offered by OpenAI, fine-tuned from a language model.",
            "score": 0.4439982908604859,
            "section_title": "Experimental Setup",
            "char_start_offset": 12644,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 562
                },
                {
                    "start": 565,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 793
                }
            ],
            "ref_mentions": [
                {
                    "start": 399,
                    "end": 421,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3583984375
        },
        {
            "corpus_id": "272880811",
            "title": "Zero-Shot Detection of LLM-Generated Text using Token Cohesiveness",
            "text": "The increasingly powerful LLMs (Radford et al., 2019;OpenAI, 2022OpenAI, , 2023;;Chowdhery et al., 2023;Touvron et al., 2023), though demonstrating excellent performance on various language-related tasks, raise numerous ethical concerns, drawing extensive attention to automatic detection of LLM-generated text (Guo et al., 2023;Li et al., 2023b). \n\nLLM-generated text detection is typically formulated as a binary classification task, with current solutions roughly categorized into supervised classifiers and zero-shot classifiers. Supervised classifiers are those trained with statistical features (Solaiman et al., 2019;Ippolito et al., 2020;Wu et al., 2023;Verma et al., 2023) or neural representations (Uchendu et al., 2020;Bakhtin et al., 2019;Zhong et al., 2020;Bhattacharjee et al., 2023;Wang et al., 2024) to discriminate LLM-generated and humanwritten text, wherein a popular trend is to directly fine-tune a pre-trained language model like BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019) for the classification task (Zellers et al., 2019;Rodriguez et al., 2022;Mitrovi\u0107 et al., 2023;Chen et al., 2023). These supervised classifiers, though achieving excellent performance on their training domains, require periodic retraining to adapt to new LLMs, and often exhibit a tendency to overfit their training data (Pu et al., 2023). \n\nZero-shot classifiers are entirely training-free and often show better generalization ability.",
            "score": 0.4439982908604859,
            "section_title": "Related Work",
            "char_start_offset": 6246,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 347
                },
                {
                    "start": 350,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1348
                },
                {
                    "start": 1351,
                    "end": 1445
                }
            ],
            "ref_mentions": [
                {
                    "start": 31,
                    "end": 53,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 81,
                    "end": 104,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 624,
                    "end": 646,
                    "matchedPaperCorpusId": "218560609"
                },
                {
                    "start": 751,
                    "end": 770,
                    "matchedPaperCorpusId": "222377949"
                },
                {
                    "start": 770,
                    "end": 797,
                    "matchedPaperCorpusId": "261660497"
                },
                {
                    "start": 797,
                    "end": 815,
                    "matchedPaperCorpusId": "269804457"
                },
                {
                    "start": 957,
                    "end": 978,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1037,
                    "end": 1059,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 1059,
                    "end": 1082,
                    "matchedPaperCorpusId": "250390908"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.132080078125
        },
        {
            "corpus_id": "12137597",
            "title": "Sequence Models for Automatic Highlighting and Surface Information Extraction",
            "text": "In sequence analysis, a HMM is used to map the input (word) sequence into the output (class) sequence as follows. One considers that the HMM has emitted the input sequence, following a sequence of unknown (hidden) states. States map the alphabet of the output sequence, in our case the information classes, onto this input sequence. Therefore, determining a state sequence is equivalent to finding a labeling of the input sequence. \n\nA first order HMM corresponds to hypothesis (5). Term emission probabilities P w W t T are unknown, and need to be estimated. This is done by the EM algorithm with a training corpus of labeled data [9]. In the case of a continuous symbol space, probabilities are replaced by probability density functions, approximated by fitting a parametric model [6]. We used here gaussian mixture models, trained with the EM algorithm. The number of gaussians is fixed by cross-validation. \n\nNot all state transitions are necessarily allowed. A grammar is used to constrain the allowed state transitions this is simply done by setting some transition probabilities to 0. Once we have trained each concept model independently, we construct a new HMM by concatenating the states as dictated by the grammar. The grammar is not learned, but rather, it is chosen by the model designer. The choice of an appropriate grammar is difficult, and critical for the performances of the system. Although it would be possible to use a completely unconstrained grammar (i.e. each word in a sequence may belong to any concept), as we introduce knowledge into the grammar we constraint the system, limiting the computational cost of parsing and reducing the number of sequences needed for training when the labeling is unknown. \n\nAs mentioned previously, the choice of grammar determines the sequence of concepts produced by the system. For highlighting we use the approach proposed in [1,12], which consists in using a grammar of the type I-R-I, which finds the most probable single sub-sequence of relevant terms within the paragraph. While this does not correspond truly to the way relevant information is present in the MUC data, it provides a first approximation to the interest of our highlighting paradigm. Experiments with more sophisticated grammars did not led to improved performances.",
            "score": 0.4439982908604859,
            "section_title": "Hidden Markov Models",
            "char_start_offset": 9609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 431
                },
                {
                    "start": 434,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 910
                },
                {
                    "start": 913,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1730
                },
                {
                    "start": 1733,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 2039
                },
                {
                    "start": 2040,
                    "end": 2216
                },
                {
                    "start": 2217,
                    "end": 2299
                }
            ],
            "ref_mentions": [
                {
                    "start": 1892,
                    "end": 1895,
                    "matchedPaperCorpusId": "14072323"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.058135986328125
        },
        {
            "corpus_id": "273549214",
            "title": "Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions",
            "text": "Large language models (LLMs) have achieved remarkable performance across various benchmarks, signaling their potential to revolutionize numerous technical domains as general-purpose problem solvers (Achiam et al., 2023;Meta AI, 2024). However, a key concern in accurately evaluating those LLMs is the possibility of data contamination, where the LLM's training data overlaps with the evaluation dataset (Balloccu et al., 2024). Evaluating LLMs on contaminated benchmarks leads to inflated performance assessments (Balloccu et al., 2024;Sainz et al., 2023a;Li and Flanigan, 2024), and creates a misleading perception of their capabilities. Therefore, multiple detection approaches have been developed to identify data contamination in LLMs, and these approaches can also be deployed to identify the use of copyrighted or sensitive content in LLM training (Xu et al., 2024;Meeus et al., 2024b). \n\nAll existing approaches for detecting data contamination in language models (LMs) rely on specific assumptions regarding the LMs and datasets, which may not be universally applicable across different settings2 . While previous surveys have focused on detection and mitigation techniques, to our best knowledge, there is currently no comprehensive analysis that surveys and validates the assumptions underlying these approaches (Xu et al., 2024;Ishihara, 2023;Hu et al., 2022). \n\nTo bridge this gap, we (1) systematically review 50 papers on data contamination detection for LMs, (2) present the formal, mathematical definitions for different levels of data contamination, (3) categorize the underlying requirements and assumptions associated with each approach and critically assess whether these assumptions have been rigorously validated, and (4) demonstrate through case studies that some unverified assumptions can be wrong in multiple scenarios.",
            "score": 0.4439982908604859,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 892
                },
                {
                    "start": 895,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1371
                },
                {
                    "start": 1374,
                    "end": 1845
                }
            ],
            "ref_mentions": [
                {
                    "start": 403,
                    "end": 426,
                    "matchedPaperCorpusId": "267499939"
                },
                {
                    "start": 513,
                    "end": 536,
                    "matchedPaperCorpusId": "267499939"
                },
                {
                    "start": 536,
                    "end": 556,
                    "matchedPaperCorpusId": "264555419"
                },
                {
                    "start": 1339,
                    "end": 1354,
                    "matchedPaperCorpusId": "258888114"
                },
                {
                    "start": 1354,
                    "end": 1370,
                    "matchedPaperCorpusId": "232233426"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62744140625
        },
        {
            "corpus_id": "247058701",
            "title": "Short-answer scoring with ensembles of pretrained language models",
            "text": "This work is concerned with the Kaggle Short Answer Scoring (KSAS) dataset1 [24]. Each prompt consists of a passage and a prompt that asks the student to describe or explain aspects of the passage using evidence [24]. Given the semantic nature of descriptions and explanations, we expect well-trained neural networks to perform well in this task. Despite the advancements of neural networks, the current state-of-the-art for this dataset has been achieved by the application of random forest classifiers to a set of rule-based features [10]. What is remarkable from a production standpoint is that the calculations for these models can be done in a low resource setting. \n\nOur goal of this short note is to explore how some of the most popular language models do when subjected to the KSAS dataset. We were expecting that language models on their own could surpass previous results, but when it comes to single models, on average, this is not the case. We are able to show that particular ensembles are capable of exceeding this benchmark, but the computational cost would be prohibitive from a production standpoint. In this sense, this work is the antithesis of [10] in that we simply bludgeon the problem to death with computational power. Even in doing so, there remain a few prompts that seem to fall drastically short of the methods in [10]. Conversely, there are some other prompts in which we see even our most efficient models perform comparably or even exceed rule-based methods, which we believe is sufficient to show that these methods and the results of this paper are of interest. \n\nThis paper is outlined as follows: In \u00a72 we specify the way in which we have fine-tune, train, and ensemble the pretrained transformer-based language models and feature-based models, in \u00a73 we present the results of the various models produced, and in \u00a74 we discuss some corollaries of this work in terms of future directions.",
            "score": 0.4439982908604859,
            "section_title": "Introduction",
            "char_start_offset": 2099,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 670
                },
                {
                    "start": 673,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1594
                },
                {
                    "start": 1597,
                    "end": 1922
                }
            ],
            "ref_mentions": [
                {
                    "start": 76,
                    "end": 80,
                    "matchedPaperCorpusId": "143732416"
                },
                {
                    "start": 212,
                    "end": 216,
                    "matchedPaperCorpusId": "143732416"
                },
                {
                    "start": 536,
                    "end": 540,
                    "matchedPaperCorpusId": "140099680"
                },
                {
                    "start": 1164,
                    "end": 1168,
                    "matchedPaperCorpusId": "140099680"
                },
                {
                    "start": 1342,
                    "end": 1346,
                    "matchedPaperCorpusId": "140099680"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2447509765625
        },
        {
            "corpus_id": "7079619",
            "title": "Analyzing and Improving Statistical Language Models for Speech Recognition",
            "text": "of joining natural language processing and speech recognition and identify the topic of this thesis, statistical language models for speech recognition, as one of them. In the remainder of this thesis, we will use the term language model as a short hand for statistical language models for speech recognition. We conclude this chapter by giving an overview of our work. Most of the material of section 1.1, section 1.2 and of the previous three paragraphs is drawn from 100, p.10], 151, p. [1][2][3][4][5] and 104].",
            "score": 0.4439982908604859,
            "section_title": "Introduction",
            "char_start_offset": 3391,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 502,
                    "end": 505,
                    "matchedPaperCorpusId": "37867788"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1614990234375
        },
        {
            "corpus_id": "247315487",
            "title": "Towards Generalized Models for Task-oriented Dialogue Modeling on Spoken Conversations",
            "text": "Models and Data Representations Following Mi et al. ( 2021), we treat detection as a binary classification problem and make use of pre-trained language models, such as ROBERTA (Liu et al. 2019) and ELECTRA (Clark et al. 2020). We represent the source side as a concatenation of all dialogue utterances with tag labels, i.e. the dialogue history: user U 1 ... sys S i\u22121 user U i . The target side is 'True' or 'False' for binary classification. \n\nOur ensemble algorithm directly uses the error-fixing ensemble of Mi et al. (2021).",
            "score": 0.4439982908604859,
            "section_title": "Detection",
            "char_start_offset": 9860,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 443
                },
                {
                    "start": 446,
                    "end": 529
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 225,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 512,
                    "end": 528,
                    "matchedPaperCorpusId": "195792932"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.354248046875
        },
        {
            "corpus_id": "18586205",
            "title": "More or less supervised supersense tagging of Twitter",
            "text": "The HMM2 model is a second-order hidden Markov model (Mari et al., 1997;Thede and Harper, 1999) using logistic regression to estimate emission probabilities. In addition we constrain  2012) is augmented with continuous word representation features as shown in Figure 1, and our logistic regression model thus works over a combination of discrete and continuous variables when estimating emission probabilities. We do 50 passes over the data as in Li et al. (2012). \n\nWe introduce two simplifications for the HMM2 model. First, we only use the most frequent senses (k = 1) in WordNet as type constraints. The most frequent senses seem to better direct the EM search for a local optimum, and we see dramatic drops in performance on held-out data when we include more senses for the words covered by WordNet. Second, motivated by computational concerns, we only train and test on sequences of (predicted) nouns and verbs, leaving out all other word classes. Our supervised models performed slightly worse on shortened sequences, and it is an open question whether the HMM2 models would perform better if we could train them on full sentences.",
            "score": 0.4439982908604859,
            "section_title": "Weakly supervised HMMs",
            "char_start_offset": 6585,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 464
                },
                {
                    "start": 467,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1139
                }
            ],
            "ref_mentions": [
                {
                    "start": 53,
                    "end": 72,
                    "matchedPaperCorpusId": "14416823"
                },
                {
                    "start": 72,
                    "end": 95,
                    "matchedPaperCorpusId": "11338792"
                },
                {
                    "start": 447,
                    "end": 463,
                    "matchedPaperCorpusId": "8548302"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.021575927734375
        },
        {
            "corpus_id": "271880412",
            "title": "A tutorial on open-source large language models for behavioral science",
            "text": "This tutorial has introduced three common LLM use cases: feature extraction, fine-tuning, and text generation. Although these are perhaps more well-known use cases, they are not exhaustive. For instance, an additional use of LLMs is to deploy them on the very tasks for which they were pretrained; namely, to assign probabilities to tokens. During training, the model is incentivized to assign high probabilities to tokens and token sequences that are common in the training data and vice versa for those that are uncommon. As a result, the probabilities produced by a trained model can be used to detect text sequences that (the model has learned) are uncommon. Measures based on token and token sequence probabilities have thus been used to, for instance, investigate how language models capture grammatical gender (An et al., 2019) and to predict human reading times (e.g., Merkx & Frank, 2020). The present example demonstrates how the log probabilities extracted from GPT-2 can be used to predict teachers' text readability ratings. Specifically, these are teachers' ratings of how difficult student readers would find certain text excerpts, obtained from the CommonLit Ease of Readability (CLEAR) corpus (Table 3; Crossley et al., 2023). \n\nA common metric to evaluate the probability of a sequence is called perplexity (Jelinek et al., 1977). Given a sequence of tokens X = (x 0 , x 1 , ..., x t ), perplexity can be defined as \n\nwith p(x i |x <i ) being the probability assigned to the i-th token in the sequence given the preceding tokens. Perplexity is thus the inverse geometric mean of sequentially produced token probabilities. As perplexity assumes sequentially produced tokens, it is not well-defined for masked language modeling. In our example, we therefore rely on GPT-2, which is a decoder model trained using causal language modeling and a direct precursor to today's GPT-4. The code begins by reading the 'Exerpt' and 'BT_easiness' columns of the CLEAR corpus into a pandas.-DataFrame.",
            "score": 0.4439982908604859,
            "section_title": "Token probability and perplexity extraction",
            "char_start_offset": 71439,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1243
                },
                {
                    "start": 1246,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1433
                },
                {
                    "start": 1436,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2005
                }
            ],
            "ref_mentions": [
                {
                    "start": 1220,
                    "end": 1242,
                    "matchedPaperCorpusId": "247498576"
                },
                {
                    "start": 1325,
                    "end": 1347,
                    "matchedPaperCorpusId": "121680873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.541015625
        },
        {
            "corpus_id": "236486122",
            "title": "IIITT at CASE 2021 Task 1: Leveraging Pretrained Language Models for Multilingual Protest Detection",
            "text": "We used pretrained transformer-based models for identifying if a sentence talks about an event or not. The models that were used are BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and DistilBERT (Sanh et al., 2019). Even though there are 3 different languages, we used a single model for all three due to memory constraints and reduced training time. We fine-tuned these models for sequence classification. Soft Voting is done on all these models to produce the respective final outputs for the languages. In soft voting, each classifier predicts that a specific data point belongs to the particular target class. A weighted sum of the predictions is done based on the importance of the classifier (all models have equal weights). The overall prediction is chosen as the target with the greatest sum of the weighted probability, thus winning the vote (Beyeler, 2017;Hande et al., 2021).",
            "score": 0.4439982908604859,
            "section_title": "Methodology",
            "char_start_offset": 6558,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 894
                }
            ],
            "ref_mentions": [
                {
                    "start": 138,
                    "end": 159,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 874,
                    "end": 893,
                    "matchedPaperCorpusId": "233874691"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.258056640625
        },
        {
            "corpus_id": "208248333",
            "title": "Automatically Neutralizing Subjective Bias in Text",
            "text": "The detection module is a neural sequence tagger that estimates p i , the probability that each input word w s i is subjectively biased ( Figure 2). Module description. Each p i is calculated according to\n\nIt is a contextualized word vector produced by BERT, a transformer encoder that has been pre-trained as a masked language model (Devlin et al. 2019). To leverage the biastopic relationship uncovered in Section 2.1, we prepend a token indicating an article's topic category (<arts>, <sports>, etc) to s. The word vectors for these tokens are learned from scratch. \u2022 e i represents expert features of bias proposed by (Recasens, Danescu-Niculescu-Mizil, and Jurafsky 2013):\n\nW in \u2208 R f \u00d7h is a matrix of learned parameters, and f i is a vector of discrete features 4 .\n\nModule pre-training. We train this module using diffs 5 between the source and target text. A label p * i is 1 if w s i was deleted or modified as part of the neutralizing process. A label is 0 if the associated word was unchanged during editing, i.e. it occurs in both the source and target text. The loss is calculated as the average negative log likelihood of the labels:\n\nFigure 3: The MODULAR system uses join embedding v to reconcile the detector's predictions with an encoder-decoder architecture. The greater a word's probability, the more of v is mixed into that word's hidden state.\n\nEditing Module The editing module takes a subjective source sentence s and is trained to edit it into a more neutral compliment t.\n\nModule description. This module is based on a sequence-to-sequence neural machine translation model (Luong, Pham, and Manning 2015). A bi-LSTM encoder turns s into a sequence of hidden states H = (h 1 , ..., h n ) (Hochreiter and Schmidhuber 1997). Next, an LSTM decoder generates text one token at a time by repeatedly attending to H and producing probability distributions over the vocabulary. We also add two mechanisms from the summarization literature (See, Liu, and Manning 2017). The first is a copy mechanism, where the",
            "score": 0.4439982908604859,
            "section_title": "Detection Module",
            "char_start_offset": 10541,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1600,
                    "end": 1631,
                    "matchedPaperCorpusId": "1998416"
                },
                {
                    "start": 1714,
                    "end": 1747,
                    "matchedPaperCorpusId": "1915014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.025909423828125
        },
        {
            "corpus_id": "270045368",
            "title": "Learning Beyond Pattern Matching? Assaying Mathematical Understanding in LLMs",
            "text": "A language model (LM) is a statistical model of natural language.Given a sentence s with tokens w 1 , . . ., w t , the probability of the sentence in a LM parameterized by \u03b8 can be represented as a chain of conditional probability conditioned on all the previous tokens:\n\nPre-training a language model amounts to likelihood maximization for the probability of next token prediction p \u03b8 (w t |w 1 , . . ., w t\u22121 ) on a held-out dataset.\n\nIn-context learning LM is given a k-shot example of context at inference time before the context of the test question, which the LM is expected to complete.",
            "score": 0.4439982908604859,
            "section_title": "Language Model",
            "char_start_offset": 7006,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 65,
                    "end": 107
                },
                {
                    "start": 107,
                    "end": 270
                },
                {
                    "start": 272,
                    "end": 403
                },
                {
                    "start": 403,
                    "end": 435
                },
                {
                    "start": 437,
                    "end": 593
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.085693359375
        },
        {
            "corpus_id": "211076098",
            "title": "Modeling Programs Hierarchically with Stack-Augmented LSTM",
            "text": "The remainder of this paper is organized as follows.We introduce our motivation in Section 2. Section 3 highlights some work related to this paper.\n\nThen we provide background knowledge on statistical language models in Section 4 for a better understanding of our proposed model.In Section 5, we introduce our proposed model.Section 6 shows the evaluation details.In Section 7, we present the experimental results and make careful analysis of the results.Then we discuss some limitations and threats to validity of our model in Section 8. Finally, we conclude our paper in Section 9.",
            "score": 0.4439982908604859,
            "section_title": "Paper Organization",
            "char_start_offset": 6890,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 52,
                    "end": 147
                },
                {
                    "start": 149,
                    "end": 279
                },
                {
                    "start": 279,
                    "end": 325
                },
                {
                    "start": 325,
                    "end": 364
                },
                {
                    "start": 364,
                    "end": 455
                },
                {
                    "start": 455,
                    "end": 583
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03802490234375
        },
        {
            "corpus_id": "267750925",
            "title": "M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection",
            "text": "For Task 1, we simulate a practical scenario where a detector have to deal with texts generated from a new LLM while it is trained on the outputs from a limited number of other generators. A practical detector should be able to generalize over different generators and be robust on unseen inputs.",
            "score": 0.4439982908604859,
            "section_title": "Monolingual Human vs. Machine",
            "char_start_offset": 18241,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 296
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1392822265625
        },
        {
            "corpus_id": "264439054",
            "title": "Towards Possibilities & Impossibilities of AI-generated Text Detection: A Survey",
            "text": "The curvature is then calculated as: Mitchell et al. (2023) which assumes white-box access, L \u03d5 here refers to an unknown language model different from the source model. The paper evaluates a suite of 27 different detector models with varying numbers of parameters ranging from 70M to 6.7B. Based on empirical analysis, they observe two interesting trends: \n\n\u2022 For cross-model detection, smaller models show stronger performance as compared to models with larger capacities. However, they also observe that overlap in architecture family and dataset between the generator and detector model leads to improvement in detection performance. \n\n\u2022 Interestingly, empirical results highlight that partially trained models are better detectors than fully trained models. For this experiment, they save checkpoints at different steps during the training process. They find that the final checkpoint is consistently the worst one in terms of machinegenerated text detection. The authors hypothesize that this might be due to overfitting associated with a longer training process.",
            "score": 0.4439982908604859,
            "section_title": "DNA-GPT (Yang et al., 2023)",
            "char_start_offset": 45592,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 356
                },
                {
                    "start": 359,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1069
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.295654296875
        },
        {
            "corpus_id": "267617032",
            "title": "Large Language Models: A Survey",
            "text": "Language modeling is a long-standing research topic, dating back to the 1950s with Shannon's application of information theory to human language, where he measured how well simple n-gram language models predict or compress natural language text [3]. Since then, statistical language modeling became fundamental to many natural language understanding and generation tasks, ranging from speech recognition, machine translation, to information retrieval [4], [5], [6]. \n\nThe recent advances on transformer-based large language models (LMs), pretrained on Web-scale text corpora, significantly extended the capabilities of language models (LLMs). For example, OpenAI's ChatGPT and GPT-4 can be used not only for natural language processing, but also as general task solvers to power Microsoft's Co-Pilot systems, for instance, can follow human instructions of complex new tasks performing multi-step reasoning when needed. LLMs are thus becoming the basic building block for the development of general-purpose AI agents or artificial general intelligence (AGI). \n\nAs the field of LLMs is moving fast, with new findings, models and techniques being published in a matter of months or weeks [7], [8], [9], [10], [11], AI researchers and practitioners often find it challenging to figure out the best recipes to build LLM-powered AI systems for their tasks. This paper gives a timely survey of the recent advances on LLMs. We hope this survey will prove a valuable and accessible resource for students, researchers and developers. \n\nLLMs are large-scale, pre-trained, statistical language models based on neural networks. The recent success of LLMs is an accumulation of decades of research and development of language models, which can be categorized into four waves that have different starting points and velocity: statistical language models, neural language models, pre-trained language models and LLMs. \n\nStatistical language models (SLMs) view text as a sequence of words, and estimate the probability of text as the product of their word probabilities. The dominating form of SLMs are Markov chain models known as the n-gram models, which compute the probability of a word conditioned on its immediate proceeding n \u2212 1 words.",
            "score": 0.4439982908604859,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 465
                },
                {
                    "start": 468,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1057
                },
                {
                    "start": 1060,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1523
                },
                {
                    "start": 1526,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1901
                },
                {
                    "start": 1904,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2226
                }
            ],
            "ref_mentions": [
                {
                    "start": 245,
                    "end": 248,
                    "matchedPaperCorpusId": "9101213"
                },
                {
                    "start": 1190,
                    "end": 1193,
                    "matchedPaperCorpusId": "274252328"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.416259765625
        },
        {
            "corpus_id": "8904361",
            "title": "Semantic processing of out-of-vocabulary words in a spoken dialogue system",
            "text": "In [4] we presented an approach for the detection of OOV words which implicitly provides information on the word category. This involves the integration of both detection and classification of OOV words directly into the recognition process of an HMM-based word recogniser. With our approach, acoustic information as well as language model information can be used for the purpose of classifying OOV words into different word categories. Currently the same acoustic models are used for all OOV words; only language model information contributes to the assignment of a category to each. The basic idea behind our approach is to build language models for the recognition of OOV words that are based on a system of word categories. Emission probabilities of OOV words are then estimated for each word category. Even if we include in our vocabulary all words of a category that were observed in the training sample, there is still a certain probability of observing other new words of the same category in an independent test sample or in future utterances. This probability can be estimated from the training sample itself. Details on the calculation of the OOV emission probabilities were given in [4]. Figure 1 shows the principle of this estimation technique for the category city of our spontaneous train timetable inquiry sample. \n\nFor most of our linguistically-motivated word categories, the OOV probability is 0, because they describe a finite set of words. In the time model, the latter has to be combined with one or several acoustic models for OOV words. Simple 'flat' acoustic models can be used for this purpose as well as more enhanced models based on phone-or syllablegrammars.",
            "score": 0.4439982908604859,
            "section_title": "DETECTION AND CLASSIFICATION OF OOV WORDS",
            "char_start_offset": 2147,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1330
                },
                {
                    "start": 1333,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1688
                }
            ],
            "ref_mentions": [
                {
                    "start": 3,
                    "end": 6,
                    "matchedPaperCorpusId": "1536490"
                },
                {
                    "start": 1195,
                    "end": 1198,
                    "matchedPaperCorpusId": "1536490"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.265869140625
        },
        {
            "corpus_id": "265456189",
            "title": "Machine-Generated Text Detection using Deep Learning",
            "text": "In this paper, we describe our methodology, the architecture of our detection model, and the results of our experiments. We also discuss the broader implications of our findings in context with sentence features and patterns.",
            "score": 0.4439982908604859,
            "section_title": "Introduction",
            "char_start_offset": 2404,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 225
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.059417724609375
        },
        {
            "corpus_id": "8353882",
            "title": "An Empirical Comparison Between N-gram and Syntactic Language Models for Word Ordering",
            "text": "A global linear model is used to score search hypotheses. Given a hypothesis h, its score is calculated by: where \u03a6(h) is the feature vector of h, extracted by using the same feature templates as Liu et al.(2015), which are shown in Table 1 and \u03b8 is the parameter vector of the model. The feature templates essentially represents a syntactic language model. As shown in Figure 2, from the hypotheses produced in steps 2 and 4, the features \"T om 1 \u2190 likes 2 \" and \"likes 2 \u2192 potatoes 0 \" are extracted, which corresponds to P (T om 1 |likes 2 ) and P (potatoes 0 |likes 2 ) respectively in the dependency language model of Chen et al.,(2012). \n\nTraining. We apply perceptron with early-update (Collins and Roark, 2004), and iteratively tune related parameters on a set of development data. For each iteration, we measure the performance on the development data, and choose best parameters for final tests.",
            "score": 0.4439982908604859,
            "section_title": "Syntactic word ordering",
            "char_start_offset": 6208,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 57
                },
                {
                    "start": 58,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 905
                }
            ],
            "ref_mentions": [
                {
                    "start": 196,
                    "end": 212,
                    "matchedPaperCorpusId": "13911837"
                },
                {
                    "start": 623,
                    "end": 641,
                    "matchedPaperCorpusId": "5578635"
                },
                {
                    "start": 693,
                    "end": 718,
                    "matchedPaperCorpusId": "10366378"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1351318359375
        },
        {
            "corpus_id": "50512185",
            "title": "Incorporating linguistic structure into statistical language models",
            "text": "Statistical language models estimate the distribution of natural language for the purpose of improving various language technology applications. Ironically, the most successful models of this type take little advantage of the nature of language. I review the extent to which various aspects of natural language are captured in current models. I then describe a general framework, recently developed at our laboratory, for incorporating arbitrary linguistic structure into a statistical framework, and present a methodology for eliciting linguistic features currently missing from the model. Finally, I ponder our failure heretofore to integrate linguistic theories into a statistical framework, and suggest possible reasons for it.",
            "score": 0.4439982908604859,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1474609375
        },
        {
            "corpus_id": "253018848",
            "title": "Meeting Decision Tracker: Making Meeting Minutes with De-Contextualized Utterances",
            "text": "The first step of the detector is to detect decisionrelated utterances from transcription. We formulate the detection as a sequence labeling problem on the utterance level and describe the detector in two steps: input representation and classification.",
            "score": 0.4439982908604859,
            "section_title": "Decision utterance detector",
            "char_start_offset": 4131,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 252
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.048492431640625
        },
        {
            "corpus_id": "264490580",
            "title": "How well can machine-generated texts be identified and can language models be trained to avoid identification?",
            "text": "Word Distributions. By altering the probability distributions of the generating language model, statistical-based classifiers like Naive Bayes can detect changes more easily. In Figure 3, a quantile-quantile plot visualizes the statistical differences that these classifiers rely on. The plot shows the probability distributions of words present in a corpus with different temperature values, which differ significantly from the word distributions in human-written texts. \n\nThis shows how machine-based detection differs significantly from human-based detection. A classifier can detect changes in the probability distribution in both directions, while humans rely on language comprehension. The level of linguistic acceptability increases when the distribution curve is sharpened and the probability of the most likely next words increases. However, reducing the temperature to achieve this comes at the expense of reducing the information content. Sampling Schemes. Next, we compare how different sampling methods used for generating texts affect the detection rates of shallow models. The results can be seen in Figure 4. The easiest method to detect is greedy search, selecting the most likely next word based on maximum likelihood. Typical-p sampling ranks second and relies on conditional entropy to deviate from the original distribution. Top-k sampling also differs significantly from the initial distribution as it does not consider the distribution curvature. Finally, nucleus sampling, which considers the various shapes of the probability curves for the next token, is most evasive after sampling using pure randomness. \n\nSampling Size. Shallow learning models, including Naive Bayes, require small amounts of training data compared to deep learning models such as BERT. In Figure 5, we have listed how different training data sizes affect a shallow detector's accuracy in successfully identifying machine-generated texts. As can be seen, even with a training batch consisting of as little as 1000 data points, the detector can still capture some dependencies. Increasing the size of the training data improves the detector's quality. However, the improvements successively diminish above a certain threshold, from 50k to 100k. Contrary to the sample size, the parameter size of the evaluated models has a minor impact on the classification process, as seen in Figure 6. In this figure, we have grouped GPT and OPT model families in similar colors, where darker lines represent models with more internal parameters.",
            "score": 0.4439982908604859,
            "section_title": "Evaluation of Shallow Detectors",
            "char_start_offset": 20405,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 471
                },
                {
                    "start": 474,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1631
                },
                {
                    "start": 1634,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2072
                },
                {
                    "start": 2073,
                    "end": 2146
                },
                {
                    "start": 2147,
                    "end": 2239
                },
                {
                    "start": 2240,
                    "end": 2382
                },
                {
                    "start": 2383,
                    "end": 2527
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.267578125
        },
        {
            "corpus_id": "244808167",
            "title": "An Effective Conversion of Visemes to Words for High-Performance Automatic Lipreading",
            "text": "A language model is a probability distribution over sequences of words, and it can be measured on the basis of the entropy of its output from the field of information theory [8]. \n\nA language model provides context to distinguish between words and phrases that look similar when spoken. For example, the phrases \"recognize speech\" and \"wreck a nice beach\" both appear similar when uttered and consist of identical lip movements. The context of a word in a language model can be deduced by its surrounding words, and according to the linguist J. R. Firth, \"you shall know a word by the company it keeps\" [9]. \n\nThe language model will predict the most likely set of words that was spoken given the spoken visemes, and the two methods for implementing a language model include statistical language models and neural models. Statistical language models predict words based on the preceding words in the sequence according the Markov assumption, whereas neural language models use deep neural networks. \n\nAlgorithms such as Weighted Finite State Transducers(WFSTs) [10] and HMMs [11] are some examples of statistical conversion models as they implement language models based on Markov chains or N-grams, which assume that each word in a sentence depends only on its previous N \u2212 1 predecessors. \n\nStatistical models based on N-grams are limited in comparison to neural models because they are a sparse representation of language in which model sentences based on the probability of words in combination and would naturally provide a zero probability to combinations of words that have not previously appeared [12]. Furthermore, N-grams fail to accurately predict semantic and syntactic details of sentences [12]. However, one fundamental problem with N-grams is that they need a large value of N to produce an accurate language model, which requires a lot of computational overhead. \n\nAn N-gram model predicts sequences of words according to the Markov process where the probability of the next word in a sequence is predicted based on the previous (N \u2212 1) words. Equation (1) provides the ideal chain rule of probability P to apply to any language model with a sequence of K words. However, as K increases, the computation becomes impossible, so statistical language models use the Markov assumption given in Equation (2).",
            "score": 0.4439982908604859,
            "section_title": "Implementation of a Language Model",
            "char_start_offset": 6469,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 181,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 607
                },
                {
                    "start": 610,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1290
                },
                {
                    "start": 1293,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1878
                },
                {
                    "start": 1881,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2178
                },
                {
                    "start": 2179,
                    "end": 2319
                }
            ],
            "ref_mentions": [
                {
                    "start": 174,
                    "end": 177,
                    "matchedPaperCorpusId": "21062299"
                },
                {
                    "start": 1061,
                    "end": 1065,
                    "matchedPaperCorpusId": "644936"
                },
                {
                    "start": 1075,
                    "end": 1079,
                    "matchedPaperCorpusId": "61281628"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2371826171875
        },
        {
            "corpus_id": "270766723",
            "title": "NCL-UoR at SemEval-2024 Task 8: Fine-tuning Large Language Models for Multigenerator, Multidomain, and Multilingual Machine-Generated Text Detection",
            "text": "Detectors specifically designed for certain LLMs, such as the GROVER detector for the GROVER model (Zellers et al., 2019) and the RoBERTa detector using the RoBERTa model (Liu et al., 2019), also play a significant role in this field.\n\nIn summary, the combination of statistical analysis with advanced language models is being employed by researchers to more effectively differentiate between content generated by humans and machines.The continuous evolution and refinement of these techniques reflect the dynamic nature of the field and the complexities involved in distinguishing between the increasingly nuanced outputs of LLMs and human-authored texts.",
            "score": 0.4439982908604859,
            "section_title": "Related Work",
            "char_start_offset": 4110,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 236,
                    "end": 434
                },
                {
                    "start": 434,
                    "end": 656
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 121,
                    "matchedPaperCorpusId": "168169824"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.468994140625
        },
        {
            "corpus_id": "268429318",
            "title": "Knowledge Injection with Perturbation-based Constrained Attention Network for Word Sense Disambiguation",
            "text": "Our model applies (1) identifying keywords, and (2) transferring information to learn relevant contextual features for WSD. The top of Figure 1 illustrates an example of the input sequence X, i.e. \n\nFor the input sequence \n\nwe apply the so-called hard attention technique (Xu et al., 2015;Shen et al., 2018) that a model concentrates solely on learning beneficial dependencies to identify keywords in the sequence, entirely discarding the others. The middle picture of Figure 1 illustrates masked attention for a bidimensional matrix. The words aligned on the horizontal axis are heads, and those aligned on the vertical axis are dependents. As illustrated in Figure 1, we discard some segment pairs, each of which consists of a head and dependence on the sequence by masking them as these pairs are not semantically related to each other and do not include keywords that are beneficial to identify the sense of the target word. in Figure 1). The output is a sequence of words with attention weights. In Figure 1, keywords having high weight values are marked in red. From the result of keyword identification, for each candidate definition, we created a sequence for the target candidate definition starting from <def> as follows: \n\n\u2022 Context sentence segment perceived as immediately before the CD segment. ((i) in Figure 1) \n\n\u2022 <def> of the target CD has two branches. One is that it witnesses other CDs, their ES, and context definitions. ((ii) in Figure 1) Another is that its ES. Relative position of the special token <def> is set to 1. ((iii) in Figure 1) \n\nAs shown in \"(2) Transferring information\" of Figure 1, the structure provides a method to leverage the relative positions of <def> including keywords representation to learn a model more accurately.",
            "score": 0.4439982908604859,
            "section_title": "Constrained Attention Network",
            "char_start_offset": 4071,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 196
                },
                {
                    "start": 199,
                    "end": 221
                },
                {
                    "start": 224,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1231
                },
                {
                    "start": 1234,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1326
                },
                {
                    "start": 1329,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1563
                },
                {
                    "start": 1566,
                    "end": 1765
                }
            ],
            "ref_mentions": [
                {
                    "start": 272,
                    "end": 289,
                    "matchedPaperCorpusId": "1055111"
                },
                {
                    "start": 289,
                    "end": 307,
                    "matchedPaperCorpusId": "27764139"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01525115966796875
        },
        {
            "corpus_id": "271583431",
            "title": "On Measurement Validity and Language Models: Increasing Validity and Decreasing Bias with Instructions",
            "text": "We fine-tune three types of text classifiers on texts from four datasets and nine different group types under different conditions, resulting in 312 different fine-tuning runs. 1 Our results show that all types of models are susceptible to learning group-specific language patterns and that fine-tuning on biased data (from one group, e.g. one country) reduces performance on representative test sets (from all groups). 2 On average, however, these effects are surprisingly small. In particular, we show that models receiving instructions as an additional input are particularly robust against biases from the finetuning data and are more likely to produce valid measurements across different groups. A language model without instructions (BERT-base) sees its average test-set performance decrease by 1.7% F1 macro when trained on biased data compared to random data. Its probability of making an error on groups it has not seen during training increases from 26% to 31.1%. An instruction-based model (BERT-NLI) sees its performance drop by only 0.4% F1 macro when trained on biased data and its probability of making an error on groups it has not seen during training increases only from 18.5% to 19.3%.",
            "score": 0.4439982908604859,
            "section_title": "Introduction",
            "char_start_offset": 3872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1204
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3125
        },
        {
            "corpus_id": "269148425",
            "title": "Your Finetuned Large Language Model is Already a Powerful Out-of-distribution Detector",
            "text": "Considering an autoregressive language model as a distribution p, denote p \u03b8 as the pretrained large language model with parameters \u03b8. Given an in-distribution dataset D in , the model finetuned on D in is represented with parameters \u03b8 \u2032 . For an input x, we introduce the out-of-distribution detection criterion S as follows: \n\nThis criterion essentially employs the pretrained large language model as an OOD proxy introduced in Section 2. This strategy is particularly practical given the widespread availability of pretrained LLMs. Finetuning these models to adapt their distribution for specific domain contexts is a standard practice, meaning many practitioners may already possess finetuned LLMs that represent the distribution of their specific datasets. With both pretrained and finetuned models at hand, calculating the likelihood ratio becomes straightforward, eliminating the need for additional training. For example, suppose we possess a LLM that has been finetuned on legal documents. Given a new document x, we can determine whether it is a legal document by utilizing the likelihood ratio S(x).",
            "score": 0.4439982908604859,
            "section_title": "PRETRAINED LLM AS A OOD PROXY",
            "char_start_offset": 8620,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 326
                },
                {
                    "start": 329,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1110
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1763916015625
        },
        {
            "corpus_id": "235358191",
            "title": "Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis-Hastings",
            "text": "Our contribution is mostly algorithmic in that we attempt to interpret masked language models as energy-based sequence models and devise a strategy to generate samples from the probability distribution over the sequences implicitly induced by the MLM training objective. While there is no direct negative societal impact of our approach, we can envision an indirect impact of performing natural language generation with masked language models. Because MLMs like BERT are trained on vast amounts of data on the internet, they capture unsavory biases and stereotyopes in the text used to train them. So far, they have primarily been used to encode natural language sentences and fine-tuned for few-shot learning for specialized tasks. Our approach focuses on drawing samples from these models in a principled manner which would enable these models to be used for natural language generation as well. \n\nWe used the WMT datasets (2014 De-En, 2016 Ro-En) for our experiments which are public datasets that serve as standard datasets for training and evaluating the machine translation systems. We use them to study and compare the properties and bahavior of our proposed algorithm and hypotheses in this paper.",
            "score": 0.4439982908604859,
            "section_title": "D POTENTIAL IMPACT AND USAGE OF DATA",
            "char_start_offset": 34326,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 897
                },
                {
                    "start": 900,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1205
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.327880859375
        },
        {
            "corpus_id": "2032603",
            "title": "Similarity-Based Approaches to Natural Language Processing",
            "text": "Grill doctoral candidates\" and \"Grill doctoral updates\" are valid sentences, and yet we know that the first string is far more likely than the second.\n\nPerhaps the simplest statistical approach to language modeling is the maximum likelihood estimate (MLE), which simply counts the number of times that the string of interest occurs in the training sample S and normalizes by the sample size. For \"Grill doctoral candidates\", this estimate takes the form P MLE (\"Grill doctoral candidates\") = C(\"Grill doctoral candidates\") |S| , (1.2)\n\nwhere C(\"Grill doctoral candidates\") is the number of times the phrase occurred in S. The quantity |S| might be the number of word triples in S, or the number of sentences in S, or some other relevant measure.\n\nNotice that if the event of interest is unseen, that is, does not occur in S, then the maximum likelihood estimate assigns it a probability of zero. In terms of practicality, this turns out to be a fatal flaw because of the sparse data problem: even if S is quite big, a large number of possible events will not appear in S. Assigning all unseen events a probability of zero, as the MLE does, amounts to declaring many perfectly reasonable strings to have zero probability of occurring, which is clearly unsatisfactory.\n\nTo illustrate the pernicious nature of the sparse data problem, we present the following example. Consider the set S to be the text contained in all the pages indexed by AltaVista, Digital's web search engine (Digital Equipment Corporation, 1997). Currently, this set consists of 31 million web pages, which, at an extremely conservative estimate, means that S contains at least a billion words. Yet at the time of this writing, the phrase \"Grill doctoral candidates\" does not occur at all among those billion words, so the MLE would rule out this sentence as absolutely impossible.\n\nAlthough the sparse data problem affects low-frequency events, it is incorrect to infer that it therefore is not important. One might attempt to claim that if an event has such a low probability that it does not occur in a very large sample, then actually estimating its probability to be zero will not be a major error. However, the aggregate probability of unseen events can be a big percentage of the test data, which means that",
            "score": 0.4439982908604859,
            "section_title": "Introduction",
            "char_start_offset": 4070,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0239105224609375
        },
        {
            "corpus_id": "222177940",
            "title": "Is the Best Better? Bayesian Statistical Model Comparison for Natural Language Processing",
            "text": "1 Introduction Gorman and Bedrick (2019) raise concerns about standard procedures used to compare speech and language processing models. They evaluate the performance of six English part-of-speech taggers using multiple randomly-generated trainingtesting splits; in some cases, they fail to reproduce previously-published system rankings established using a single \"standard\" split. They argue that point estimates of performance derived from a single training-testing splits are insufficient to establish system rankings, even when null hypothesis significance testing is used for model comparison. \n\nIn this study, we propose a technique for system comparison based on Bayesian statistical analysis. Our approach, motivated in Section 2 and described in Section 3, allow us to infer the likelihood that one model will outperform the other, or even that both models' performance will be practically equivalent, something that is not possible with the frequentist statistical tests used by Gorman and Bedrick. Our approach can also be applied simultaneously across multiple data sets. As a proof of concept, in Sections 4-5 we apply the proposed method using the experimental setup of Gorman and Bedrick, and in Section 6, we use it to rank the six taggers, compare evaluation metrics, and interpret the results. Our failure to reproduce some of earlier reported results leads us to discuss the impact of repeating experiments, contrasting performance in multiple measures and the advantages of comparing likelihoods in Section 6. We also discuss the notion of practical equivalence for speech and language technology. \n\n2 Prior work Langley (1988) argues that machine learning should be viewed as an experimental science, and as such, machine learning technologies should be evaluated according to their performance on multiple held-out data sets. Dietterich (1998) proposes a framework for comparing two supervised classifiers using a null hypothesis tests to determine whether two classifiers have the same likelihood of predicting a correct result. This study introduces several methods, including a paired ttest for k-fold cross-validation results. However, he notes that the assumptions of normality and independence may not be satisfied in all cases. Nadeau and Bengio (2000) propose a correlationbased correction for the Dietterich t-test procedure which adjusts for the overlap between folds.",
            "score": 0.4439982908604859,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 599
                },
                {
                    "start": 602,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1618
                },
                {
                    "start": 1621,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2153
                },
                {
                    "start": 2154,
                    "end": 2257
                },
                {
                    "start": 2258,
                    "end": 2401
                }
            ],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 40,
                    "matchedPaperCorpusId": "196189229"
                },
                {
                    "start": 1634,
                    "end": 1648,
                    "matchedPaperCorpusId": "7364455"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.070556640625
        },
        {
            "corpus_id": "265609409",
            "title": "A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly",
            "text": "Large Language Models (LLMs) [347] represents an evolution from language models. Initially, language models were statistical in nature and laid the groundwork for computational linguistics. The advent of transformers has significantly increased their scale. This expansion, along with the use of extensive training corpora and advanced pretraining techniques is pivotal in areas such as AI for science, logical reasoning, and embodied AI. These models undergo extensive training on vast datasets to comprehend and produce text that closely mimics human language. Typically, LLMs are endowed with hundreds of billions, or even more, parameters, honed through the processing of massive textual data. They have spearheaded substantial advancements in the realm of Natural Language Processing (NLP) [82] and find applications in a multitude of fields (e.g., risk assessment [202], programming [26], vulnerability detection [118], medical text analysis [274], and search engine optimization [13]). \n\nBased on Yang's study [323], an LLM should have at least four key features. First, an LLM should demonstrate a deep understanding and interpretation of natural language text, enabling it to extract information and perform various language-related tasks (e.g., translation). Second, it should have the capacity to generate human-like text (e.g., completing sentences, composing paragraphs, and even writing articles) when prompted. Third, LLMs should exhibit contextual awareness by considering factors such as domain expertise, a quality referred to as \"Knowledgeintensive\". Fourth, these models should excel in problemsolving and decision-making, leveraging information within text passages to make them invaluable for tasks such as information retrieval and question-answering systems.",
            "score": 0.4438550066148003,
            "section_title": "Large Language Models (LLMs)",
            "char_start_offset": 8244,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 992
                },
                {
                    "start": 995,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1782
                }
            ],
            "ref_mentions": [
                {
                    "start": 919,
                    "end": 924,
                    "matchedPaperCorpusId": "257020533"
                },
                {
                    "start": 948,
                    "end": 953,
                    "matchedPaperCorpusId": "259947046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2568359375
        },
        {
            "corpus_id": "259311749",
            "title": "NLPashto: NLP Toolkit for Low-resource Pashto Language",
            "text": "The initial prototype of the NLPashto toolkit includes four SOTA AI models for (i) Spelling Correction, (ii) Word Segmentation, (iii) POS tagging, and (iv) Offensive Language Detection. The first three are essentially sequence tagging models that involve assigning a label or tag to each element in the sequence of input data. For sequence tagging tasks, NLP researchers use several supervised machine learning algorithms, such as HMM (Hidden Markov Model) [27], [28], RNN (Recurrent Neural Networks) [29], [30], [31], [32], [33], and CRF (Conditional Random Fields) [34], [35], [36], [37]. For various sequence tagging tasks, such as Word Segmentation and POS tagging, the CRF usually outperforms the other models. We have also incorporated CRF for training the three sequence tagging models. On the other hand, the model for offensive language detection is essentially a (binary) sequence classifier, which discriminates the input sequences into two categories, offensive and not-offensive. For that, we fine-tuned our pre-trained Pashto BERT model.",
            "score": 0.44341876695226307,
            "section_title": "VI. MODELS",
            "char_start_offset": 22813,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1051
                }
            ],
            "ref_mentions": [
                {
                    "start": 457,
                    "end": 461,
                    "matchedPaperCorpusId": "1970577"
                },
                {
                    "start": 463,
                    "end": 467,
                    "matchedPaperCorpusId": "237621304"
                },
                {
                    "start": 501,
                    "end": 505,
                    "matchedPaperCorpusId": "224770634"
                },
                {
                    "start": 507,
                    "end": 511,
                    "matchedPaperCorpusId": "5816303"
                },
                {
                    "start": 513,
                    "end": 517,
                    "matchedPaperCorpusId": "254639898"
                },
                {
                    "start": 519,
                    "end": 523,
                    "matchedPaperCorpusId": "226247188"
                },
                {
                    "start": 525,
                    "end": 529,
                    "matchedPaperCorpusId": "59553793"
                },
                {
                    "start": 567,
                    "end": 571,
                    "matchedPaperCorpusId": "59554214"
                },
                {
                    "start": 573,
                    "end": 577,
                    "matchedPaperCorpusId": "59555193"
                },
                {
                    "start": 579,
                    "end": 583,
                    "matchedPaperCorpusId": "15923494"
                },
                {
                    "start": 585,
                    "end": 589,
                    "matchedPaperCorpusId": "49212226"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.087890625
        },
        {
            "corpus_id": "273345915",
            "title": "\u2018Quis custodiet ipsos custodes?\u2019 Who will watch the watchmen? On Detecting AI-generated peer-reviews",
            "text": "D.1 RADAR (Hu et al., 2023) The way RADAR works is as follows -First, an AI-text corpus is generated from a target (frozen) language model from a human-text corpus. The next step is followed by introduction of a paraphraser (a tunable language model) and a detector (a separate tunable language model). In the training stage, the detector's objective is to distinguish between human-generated text and AI-generated text, whereas the paraphraser's goal is to rephrase AI-generated text to avoid detection. The model parameters of the paraphraser and detector are updated in an adversarial learning manner. During the evaluation (testing) phase, the deployed detector utilizes its training to assess the probability of content being AI-generated for any given input instance. \n\nD.2 LLMDET (Wu et al., 2023): \n\nThe overall framework of the system consists of two main components -1) Dictionary creation and 2) Text detection. The main idea was to make use of the perplexity as a measurement of identifying the generated text from different LLMs. So the dictionary had n-grams as keys and the next to-ken probablities as values. The dictionary serves as prior information during the detection process. \n\nSince the dictionary of n-grams and their probabilities was obtained, it enabled the utilization of the corresponding dictionary of each model as prior information for third-party detection, facilitating the calculation of the proxy perplexity of the text being detected on each model. Proxy perplexity was then used as a feature into a trained text classifier, the corresponding detection results were obtained. \n\nD.3 DEEP-FAKE (Li et al., 2023) To determine whether machine-generated text can be discerned from human-written content, the collected data was categorized into six settings based on the sources used for model training and evaluation. These settings progressively increased the difficulty of detection. The classifier then assigned a probability to each text, indicating the likelihood of it being authored by humans or generated by language model models (LLMs).",
            "score": 0.44321234538051635,
            "section_title": "D Baseline Comparison",
            "char_start_offset": 31756,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 805
                },
                {
                    "start": 808,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1197
                },
                {
                    "start": 1200,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1612
                },
                {
                    "start": 1615,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2077
                }
            ],
            "ref_mentions": [
                {
                    "start": 10,
                    "end": 27,
                    "matchedPaperCorpusId": "259501842"
                },
                {
                    "start": 787,
                    "end": 804,
                    "matchedPaperCorpusId": "258865367"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3505859375
        },
        {
            "corpus_id": "201236795",
            "title": "Information Extraction System for Transforming Unstructured Text Data in Fire Reports into Structured Forms: A Polish Case Study",
            "text": "The text data pre-processing process receives plain text as an input and returns the semi-structured report as an outcome. The process includes two main components: sentence detection and text classification. \n\nThe sentences/ segment boundary detection problem [3,19,35,39] is a fundamental problem in natural language processing (NLP), which is related to determining the location where a sentence begins and ends. The problem is non-trivial because while some written languages have specific word boundary markers, in other languages the same punctuation marks are often ambiguous. For example, a segmentation tool uses input received as plain text (see Fig. 1) and returns a list of sentences (see Table 1, column Sentence). Further, text classification or categorisation is a problem of learning a classification model from training documents labelled by pre-defined classes. Such a model is used to classify new documents [7,9,22,28]. For example, a classification model receives a sentence (see Table 1, column Sentence) as an input and returns a label/ class name (see Table 1, column Class name)as an outcome. \n\nContributions of this part of the article include a presentation of the constructed solution for sentence detection, and this solution outperforms alternative methods. In addition, this demonstrates that we can achieve satisfactory classification results using the sentences.",
            "score": 0.44160669645077133,
            "section_title": "General Overview",
            "char_start_offset": 27643,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 208
                },
                {
                    "start": 211,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1117
                },
                {
                    "start": 1120,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1395
                }
            ],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 264,
                    "matchedPaperCorpusId": "1597015"
                },
                {
                    "start": 264,
                    "end": 267,
                    "matchedPaperCorpusId": "4435044"
                },
                {
                    "start": 267,
                    "end": 270,
                    "matchedPaperCorpusId": "7590632"
                },
                {
                    "start": 270,
                    "end": 273,
                    "matchedPaperCorpusId": "11079685"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1260986328125
        },
        {
            "corpus_id": "270379488",
            "title": "Silent Signals, Loud Impact: LLMs for Word-Sense Disambiguation of Coded Dog Whistles",
            "text": "Prior work has also highlighted that the communication of different messages to different groups makes inferring policy mandates once a candidate assumes office incredibly problematic (Goodin and Saward, 2005).\n\nTo this end, longitudinal dog whistle datasets could facilitate the study of political parties' co-evolution with political, social, and economical events, and improved dog whistle detection could deter ongoing adverse political manipulation.\n\nWord Sense Disambiguation Modern Word Sense Disambiguation (WSD) systems can outperform humans (Maru et al., 2022;Bevilacqua and Navigli, 2020;Barba et al., 2021a;Conia and Navigli, 2021;Kohli, 2021).WSD tasks are typically treated as multi-label classification problems for resolving the semantic interpretation of target words in context (Bevilacqua et al., 2021;Barba et al., 2021b).A large body of research has focused on designing systems in supervised settings, leveraging pre-trained language models as foundational frameworks (Maru et al., 2022;Barba et al., 2021a;Scarlini et al., 2020;Blevins and Zettlemoyer, 2020).Notably, recent work has explored the use of LLMs for WSD, with findings pointing to strong performance on benchmark evaluations, but still short of levels attained by state-of-the-art models (Koco\u0144 et al., 2023).The detection of pejorative or abusive uses of taboo lexemes has been framed as a WSD task (Dinu et al., 2021;Pamungkas et al., 2020Pamungkas et al., , 2023)), but in these cases, the taboos were overt, and these prior works did not evaluate LLMs.\n\nOur study extends the evaluation of LLMs for WSD to contexts where word senses can be deliberately obfuscated or coded.",
            "score": 0.4411786974049671,
            "section_title": "Dog Whistles",
            "char_start_offset": 6808,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 212,
                    "end": 454
                },
                {
                    "start": 456,
                    "end": 656
                },
                {
                    "start": 656,
                    "end": 842
                },
                {
                    "start": 842,
                    "end": 1082
                },
                {
                    "start": 1082,
                    "end": 1295
                },
                {
                    "start": 1295,
                    "end": 1542
                },
                {
                    "start": 1544,
                    "end": 1663
                }
            ],
            "ref_mentions": [
                {
                    "start": 551,
                    "end": 570,
                    "matchedPaperCorpusId": "248780174"
                },
                {
                    "start": 570,
                    "end": 599,
                    "matchedPaperCorpusId": "220045824"
                },
                {
                    "start": 599,
                    "end": 619,
                    "matchedPaperCorpusId": "235097236"
                },
                {
                    "start": 619,
                    "end": 643,
                    "matchedPaperCorpusId": "233189612"
                },
                {
                    "start": 643,
                    "end": 655,
                    "matchedPaperCorpusId": "235125701"
                },
                {
                    "start": 796,
                    "end": 821,
                    "matchedPaperCorpusId": "237100274"
                },
                {
                    "start": 821,
                    "end": 841,
                    "matchedPaperCorpusId": "243865358"
                },
                {
                    "start": 990,
                    "end": 1009,
                    "matchedPaperCorpusId": "248780174"
                },
                {
                    "start": 1009,
                    "end": 1029,
                    "matchedPaperCorpusId": "235097236"
                },
                {
                    "start": 1029,
                    "end": 1051,
                    "matchedPaperCorpusId": "226262325"
                },
                {
                    "start": 1051,
                    "end": 1081,
                    "matchedPaperCorpusId": "218517044"
                },
                {
                    "start": 1386,
                    "end": 1405,
                    "matchedPaperCorpusId": "244119802"
                },
                {
                    "start": 1405,
                    "end": 1427,
                    "matchedPaperCorpusId": "218974299"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1412353515625
        },
        {
            "corpus_id": "262465111",
            "title": "From Text to Source: Results in Detecting Large Language Model-Generated Content",
            "text": "Additionally, we explore the impact of conversational fine-tuning techniques, including Chat, Instruct (Mishra et al., 2022;Wei et al., 2022), and Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017;Ziegler et al., 2020), the impact of model quantization and watermarking of the generated text on the general-ization and transferability of the classifier across this wide array of models. \n\nThis comprehensive investigation enables us to gain a deeper understanding of the generalization and transferability of the classifier across a diverse array of models, thus eliminating a potential source of bias in our results. It also allows us to identify how the factors mentioned before (e.g model size, family, conversational finetuning, quantization and watermarking) impact the detection and attribution of generated text. \n\nOur contributions in this study can be summarized as follows: \n\n\u2022 A comprehensive investigation into crossmodel detection, evaluating the classifier's ability to detect text generated by different LLMs, and in model attribution, encompassing a broad range of sizes and model families. \n\n\u2022 We highlight the role of both model size and family in the detection of text generated by Language Model Models (LLMs). We observed an inverse relationship between classifier effectiveness and LLM size. Detecting larger models can be challenging, but training on similarly sized LLMs can improve performance. Additionally, our study provides valuable insights into the impact of conversational finetuning, quantization, and watermarking on detection, indicating the robustness of LLM detection in the face of these techniques. \n\n\u2022 Our experiments in model attribution reveal the potential for identifying the source model of generated text. While human-generated text is distinguishable, confusion primarily occurs between models from the same family or with adjacent sizes. This suggests that LLMs leave distinct signatures, enabling source model identification and model family classification, further enhancing our understanding of how different LLMs generate text.",
            "score": 0.44101224527111293,
            "section_title": "Introduction",
            "char_start_offset": 1795,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 411
                },
                {
                    "start": 414,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 908
                },
                {
                    "start": 911,
                    "end": 1131
                },
                {
                    "start": 1134,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1662
                },
                {
                    "start": 1665,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2104
                }
            ],
            "ref_mentions": [
                {
                    "start": 103,
                    "end": 124,
                    "matchedPaperCorpusId": "237421373"
                },
                {
                    "start": 124,
                    "end": 141,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 197,
                    "end": 222,
                    "matchedPaperCorpusId": "4787508"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74267578125
        },
        {
            "corpus_id": "12794839",
            "title": "Using story topics for language model adaptation",
            "text": "In the speech recognition paradigm, each time a new story is decoded an initial hypothesis transcription is produced. We then feed the hypothesis transcription to the classifier, which chooses the most similar leaf clusters. Individual language models are built from the chosen clusters (or from nodes farther up in the tree when a topic tree is being used), and the models are interpolated together at the word level. The hypothesis is then reevaluated according to the language scores of the interpolated language models. Even when the word error rate of the decoder hypothesis is significant, topic detection will still perform reasonably well [1]. As long as the word errors in the hypothesis are not significantly topic-correlated, the correct content words in the hypothesis will provide enough weight for the selection of appropriate clusters.",
            "score": 0.44064268884650887,
            "section_title": "Model Interpolation",
            "char_start_offset": 5340,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 850
                }
            ],
            "ref_mentions": [
                {
                    "start": 647,
                    "end": 650,
                    "matchedPaperCorpusId": "1755585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26123046875
        },
        {
            "corpus_id": "264439179",
            "title": "A Survey on Detection of LLMs-Generated Content",
            "text": "In this section, we delve into further details about the detection algorithms. Based on their distinguishing characteristics, existing detection methods can be categorized into three classes: 1) Trainingbased classifiers, which typically fine-tune a pretrained language model on collected binary data -both human and AI-generated text distributions. \n\n2) Zero-shot detectors leverage the intrinsic properties of typical LLMs, such as probability curves or representation spaces, to perform self-detection. \n\n3) Watermarking involves hiding identifying information within the generated text that can later be used to determine if the text came from a specific language model, rather than detecting AI-generated text in general. We summarize the representative approaches in Figure 3 as classified by the scenarios listed in Section 3.",
            "score": 0.43992477788569667,
            "section_title": "Detection Methodologies",
            "char_start_offset": 14768,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 349
                },
                {
                    "start": 352,
                    "end": 505
                },
                {
                    "start": 508,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 833
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.443359375
        },
        {
            "corpus_id": "10019043",
            "title": "Multinomial Loss on Held-out Data for the Sparse Non-negative Matrix Language Model",
            "text": "A statistical language model estimates probability values P (W ) for strings of words W in a vocabulary V whose size is in the tens, hundreds of thousands and sometimes even millions. Typically the string W is broken into sentences, or other segments such as utterances in automatic speech recognition, which are often assumed to be conditionally independent; we will assume that W is such a segment, or sentence. \n\nSince the parameter space of P (w k |w 1 , w 2 , . . . , w k\u22121 ) is too large, the language model is forced to put the context W k\u22121 = w 1 , w 2 , . . . , w k\u22121 into an equivalence class determined by a function \u03a6(W k\u22121 ). As a result, \n\nResearch in language modeling consists of finding appropriate equivalence classifiers \u03a6 and methods to estimate P (w k |\u03a6(W k\u22121 )). Once the form \u03a6(W k\u22121 ) is specified, only the problem of estimating P (w k |\u03a6(W k\u22121 )) from training data remains.",
            "score": 0.4396506628741437,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 901
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1329345703125
        },
        {
            "corpus_id": "270562210",
            "title": "QueerBench: Quantifying Discrimination in Language Models Toward Queer Identities",
            "text": "Masked Language Modelling (MLM) consists of giving as input a string s to a language model (where s is then converted into tokens that represent the contextual meaning c).Some words in the sentence are then randomly masked with the token [MASK].The model is then trained to predict those words through the sentence's context finding the most likely prediction p(m|c) of masked words m giving the context c.",
            "score": 0.4396506628741437,
            "section_title": "Task",
            "char_start_offset": 6457,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 171,
                    "end": 245
                },
                {
                    "start": 245,
                    "end": 406
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0728759765625
        },
        {
            "corpus_id": "276775997",
            "title": "From Small to Large Language Models: Revisiting the Federalist Papers",
            "text": "The fundamental objective of language models is characterizing the probabilistic mechanism for W that captures how often tokens (in a particular order) occur in the language. This can be captured with a likelihood function \n\nwhich explains how likely w is under different values of the model parameters \u03b8. In LLMs, \u03b8 is entirely uninterpretable and has massive dimensionality, parametrizing layers and layers of encoders and decoders. In small language models, \u03b8 has moderate dimensionality and serves to provide insight into the language structure. Training a language model on real data D involves estimating a parameter \u03b8 for which the likelihood of observing aspects (sequences) of D is the largest. For LLMs, the training data D is a vast corpus of text (inaccessible to the user) while for small language models, D is a collection of observed text documents (such as our Federalist Papers). Training the language model (1) can be facilitated by making some simplifying structural assumptions about \u03c0 \u03b8 (w). We will roughly group language models according to the assumption made. Bag-of-words Models utilize the simplifying assumption that the order of the words in the sequence W does not matter. In statistics, this corresponds to the notion of exchangeability where \u03c0 \u03b8 (w) = \u03c0 \u03b8 (w \u03c3(1,...,T ) ) for any permutation \u03c3(1, . . . , T ). Such language models very often operate on a specific summary statistic (word counts) as opposed to raw data (word sequences). For example, our analysis of the Federalist Papers involves training data documents D = {D i } n i=1 where \n\nj=1 . The word count matrix X = {x ij } \u2208 R n\u00d7N can be then constructed by counting the occurrence of each word in each document x ij = n i=1 N j=1 I{w j \u2208 D i }, where N = |W| denotes the size of the vocabulary. We talk about bag-of-words models later in Section 3.2.1 in the context of Latent Dirichlet Allocation and Latent Semantic Analysis. \n\nAutoregressive models characterize the joint distribution using the chain rule of probability as follows:",
            "score": 0.4396506628741437,
            "section_title": "Probabilistic Language Models",
            "char_start_offset": 15227,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 222
                },
                {
                    "start": 225,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1576
                },
                {
                    "start": 1579,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1924
                },
                {
                    "start": 1927,
                    "end": 2032
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1295166015625
        },
        {
            "corpus_id": "257767013",
            "title": "Evaluating self-attention interpretability through human-grounded experimental protocol",
            "text": "Here we define the methods used to analyze the data produced by the experimental protocol presented above. Each experiment produces n \u00d7 100 answers, with n the number of participants, and 100 the number of plotted text samples during each experiment. The indicators of interest are the labeling time, which we call \"reaction time\", and whether or not the participant is wrong, which we call \"accuracy\". These variables of interest are then analyzed through their relationship with other characteristics such as features about the text (sequence length, probabiltiy score, trial number, relative position of impacting word) and the interpretability method used to color it. \n\nData description. The descriptive analysis is first performed by calculating the average reaction time and the average accuracy. The one-tail t-test is then used to compare the distributions of reaction times between interpretability methods and the random baseline in order to have statistically significant comparisons. The one-tailed t-test is a statistical hypothesis test used to determine whether the mean of a first sample is lower the mean of a second one. This test is applied here to the average difference between each interpretability method and the random baseline, per participant, per experiment. \n\nLinear modeling. The impact of interpretability methods on reaction time is estimated with a linear regression by incorporating the effect of other explanatory variables. The random baseline is used for reference. Thus, the coefficients of the linear regression associated with the method used to color the text are expressed with respect to this baseline. \n\nFor each experiment, a linear model is built per participant to explain the reaction time to the labeling task. The explanatory variables of the models for an experiment are the same for all participants. The mean value of the regression parameters and their distribution are then analyzed using the one-tail t-test presented above. \n\nNon-linear modeling. Decision tree boosting algorithms allow to model complex and non-linear phenomena. We apply this type of algorithm to model the participant accuracy. This binary classification problem is addressed via Explainable Boosting Machine (EBM) [33]. EBM obtain performance levels equivalent to other boosting approaches based on decision trees, while decomposing its prediction into contributions of the explanatory variables. EBM is a generalized additive model (GAM) of the form: \n\nWhere:",
            "score": 0.4396506628741437,
            "section_title": "Data analyses",
            "char_start_offset": 18138,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 672
                },
                {
                    "start": 675,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1286
                },
                {
                    "start": 1289,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1645
                },
                {
                    "start": 1648,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1980
                },
                {
                    "start": 1983,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2086
                },
                {
                    "start": 2087,
                    "end": 2153
                },
                {
                    "start": 2154,
                    "end": 2246
                },
                {
                    "start": 2247,
                    "end": 2423
                },
                {
                    "start": 2424,
                    "end": 2478
                },
                {
                    "start": 2481,
                    "end": 2487
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00653839111328125
        },
        {
            "corpus_id": "234302201",
            "title": "Frequency Management Method Based on Cloud Computing, Big Data and Artificial Intelligence",
            "text": "The model combines machine learning and deep learning to build corresponding acoustic and language models. The operation process of the model is as follows: First, the keywords of monitoring are set, and the model is called when the monitoring task is executed; Input voice data for speech recognition and establish keyword index; Find the corresponding keyword list and conduct confidence evaluation; Return keyword search results.",
            "score": 0.4396506628741437,
            "section_title": "Speech keyword retrieval model",
            "char_start_offset": 11784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 432
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.062103271484375
        },
        {
            "corpus_id": "256416337",
            "title": "ChatGPT or Human? Detect and Explain. Explaining Decisions of Machine Learning Model for Detecting Short ChatGPT-generated Text",
            "text": "We frame the problem at hand as a supervised classification task, where the objective is to learn a mapping between a representation of the text and a binary variable, which is 1 if the text is generated by ChatGPT, and 0 otherwise. More formally, by means of ML strategies, we learn a function f that, given an input text t i , represented as a set of features [f i 1 , ..., f i k ], outputs an estimated label li \u2208 {0, 1}, i.e., li =f (t i ). Furthermore, we are interested in explaining ML model's decisions and extracting insights that allow us to understand which features (e.g., words) contribute positively to detection of ChatGPT-generated text. To systematically discuss our findings, we pose the following research questions (RQs): RQ1) To what extent can ML-based approach and perplexity-based approach detect AI-generated text of relatively short length? And if AI-generated text is a rephrase of the original, human-generated, text? RQ2) What are the characteristics of text that contribute to detecting an AI-generated text? Can we extract insights by examining explanations of ML model decisions?",
            "score": 0.4396506628741437,
            "section_title": "Problem Statement and Research Questions",
            "char_start_offset": 5747,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1111
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02056884765625
        },
        {
            "corpus_id": "269626179",
            "title": "Learning Phonotactics from Linguistic Informants",
            "text": "Preliminaries We aim to learn a language L comprising a set of strings x, each of which is a concatenation of symbols from some inventory \u03a3 (so L \u2286 \u03a3 + ).(In phonotactics, for example, \u03a3 might be the set of phonemes, and L the set of word forms that speakers judge phonotactically acceptable.)A learned model of a language is a discriminative function that maps from elements x \u2208 \u03a3 + to values in {0, 1} where 1 indicates that x \u2208 L and 0 indicates that x / \u2208 L. In this paper, we will generalize this to graded models of language membership f : \u03a3 + \u2192 [0, 1], in which higher values assigned to strings x \u2208 \u03a3 + correspond to greater confidence that x \u2208 L (cf.Albright, 2009, for data and argumentation in favor of a gradient model of phonotactic acceptability in humans).\n\nWe may then characterize the language learning problem as one of acquiring a collection of pairs (x 1 , y 1 ), (x 2 , y 2 ), . . ., (x n , y n ) where x i \u2208 \u03a3 + , and y i \u2208 {0, 1} correspond to acceptability judgments about whether x i \u2208 L. Given this data, a learner's job is to identify a language consistent with these pairs.Importantly, in this setting, learners may have access to both positive and negative evidence.\n\nApproach In our problem characterization, the data acquisition process takes place over a series of time steps.At each time step t, the learner uses a policy \u03c0 according to which a new string x t \u2208 X is selected; here X is some set of possible strings, with L \u2282 X \u2282 \u03a3 + .",
            "score": 0.4396506628741437,
            "section_title": "Problem Formulation and Method",
            "char_start_offset": 3234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 154,
                    "end": 293
                },
                {
                    "start": 293,
                    "end": 659
                },
                {
                    "start": 659,
                    "end": 771
                },
                {
                    "start": 773,
                    "end": 903
                },
                {
                    "start": 903,
                    "end": 1101
                },
                {
                    "start": 1101,
                    "end": 1195
                },
                {
                    "start": 1197,
                    "end": 1308
                },
                {
                    "start": 1308,
                    "end": 1468
                }
            ],
            "ref_mentions": [
                {
                    "start": 659,
                    "end": 673,
                    "matchedPaperCorpusId": "61171009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02423095703125
        },
        {
            "corpus_id": "964089",
            "title": "Domain Specific Author Attribution based on Feedforward Neural Network Language Models",
            "text": "Statistical language models provide a tool to compute the probability of the target word W t given N \u2212 1 context words W 1 , W 2 , . . . , W i , . . . , W N , i \u2208 N, i = t. Normally, the target word is the Nth word and the context words are the preceding N \u2212 1 words. Denote W n 1 as a word sequence (W 1 , W 2 , . . . , W n ). Using the chain rule of probability, the probability of sequence W n 1 can be formulated as \n\n). \n\nUsing a Markov chain, which approximates the probability of a word sequence with arbitrary length n to the probability of a sequence with the closest N words, the shortened probabilities can be provided by the LM with context size N, i.e. N-gram language model. Eq. ( 20) can then be simplified to \n\nPerplexity is an intrinsic measurement to evaluate the fitness of the LM to the test word sequence W N 1 , which is defined as \n\nIn practical use, it normally converts the probability multiplication to the summation of log probabilities. Therefore, using Eq. ( 21), Eq. ( 22) can be reformulated as \n\nIn this project, the classification is performed by measuring the perplexity of the test word sequences in terms of sentences, using the trained NNLM of each course. Denote C as the candidate courses/instructors and C * as the selected one from the classifier. C * can then be expressed as \n\nThe classification performance with NNLM is also compared with baselines from an SRI N-gram backoff model with Kneser-Ney Smoothing. The perplexities are computed without insertions of start-ofsentence and end-of-sentence tokens in both SRILM and NNLM. To evaluate the LM fitness with different training methods, Table 2 lists the training-to-test perplexities for each of the 16 courses, averaged from 10 different database segmentations. Each line in Table 255.9",
            "score": 0.4396506628741437,
            "section_title": "Classification with Perplexity Measurement",
            "char_start_offset": 13161,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 419
                },
                {
                    "start": 422,
                    "end": 424
                },
                {
                    "start": 427,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 724
                },
                {
                    "start": 727,
                    "end": 853
                },
                {
                    "start": 856,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1025
                },
                {
                    "start": 1028,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1317
                },
                {
                    "start": 1320,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1784
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09332275390625
        },
        {
            "corpus_id": "269922444",
            "title": "MAGE: Machine-generated Text Detection in the Wild",
            "text": "A line of work explores the linguistic patterns to achieve automatic machine-writing detection, which has gone through n-gram frequencies (Badaskar et al., 2008), entropy (Lavergne et al., 2008;Gehrmann et al., 2019), perplexity (Beresneva, 2016), and negative curvature regions of the model's log probability (Mitchell et al., 2023;Bao et al., 2023).One limitation of these statistics-based methods is the white-box assumption that we can access the model prediction distributions, hindering wider applications on models behind APIs, such as ChatGPT.Another alternative paradigm is training neural-based detectors (Bakhtin et al., 2019;Fagni et al., 2021;Uchendu et al., 2020;OpenAI, 2023a).Some works (Meral et al., 2009;Krishna et al., 2023;Zhao et al., 2023;Kirchenbauer et al., 2023) explore the potential of watermarks in language models, making model-generated texts easier to detect.Liang et al. (2023) indicate that texts by non-native speakers are more likely to be incorrectly identified as AI-generated.Our work does not assume language models are enhanced with watermarks, instead considering a more common detection setting where we do not know the sources of detected texts.\n\nCurrent AI text detection has not achieved significant success, as evidenced by the successful exploits of paraphrasers that expose weaknesses in existing detectors (Sadasivan et al., 2023;Krishna et al., 2023), raising concerns about the robustness of current detection methods.On the other hand, most of the detectors focus on specific domains, such as news (Zellers et al., 2019b;Zhong et al., 2020) and reviews (Chakraborty et al., 2023), or specific models (Pu et al., 2022;Rodriguez et al., 2022;Mitchell et al., 2023).The transferability of detection capabilities to out-of-distribution scenarios, involving texts from unseen domains or models, remains uncertain and represents a crucial practi-cal challenge.",
            "score": 0.4396506628741437,
            "section_title": "Related Work",
            "char_start_offset": 4850,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 351
                },
                {
                    "start": 351,
                    "end": 551
                },
                {
                    "start": 551,
                    "end": 692
                },
                {
                    "start": 692,
                    "end": 891
                },
                {
                    "start": 891,
                    "end": 1015
                },
                {
                    "start": 1015,
                    "end": 1189
                },
                {
                    "start": 1191,
                    "end": 1470
                },
                {
                    "start": 1470,
                    "end": 1716
                },
                {
                    "start": 1716,
                    "end": 1907
                }
            ],
            "ref_mentions": [
                {
                    "start": 138,
                    "end": 161,
                    "matchedPaperCorpusId": "4324753"
                },
                {
                    "start": 171,
                    "end": 194,
                    "matchedPaperCorpusId": "12098535"
                },
                {
                    "start": 194,
                    "end": 216,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 229,
                    "end": 246,
                    "matchedPaperCorpusId": "1175726"
                },
                {
                    "start": 310,
                    "end": 333,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 615,
                    "end": 637,
                    "matchedPaperCorpusId": "182952342"
                },
                {
                    "start": 637,
                    "end": 656,
                    "matchedPaperCorpusId": "220936111"
                },
                {
                    "start": 656,
                    "end": 677,
                    "matchedPaperCorpusId": "221835708"
                },
                {
                    "start": 703,
                    "end": 723,
                    "matchedPaperCorpusId": "1192689"
                },
                {
                    "start": 723,
                    "end": 744,
                    "matchedPaperCorpusId": "257687440"
                },
                {
                    "start": 891,
                    "end": 910,
                    "matchedPaperCorpusId": "257985499"
                },
                {
                    "start": 1380,
                    "end": 1401,
                    "matchedPaperCorpusId": "257687440"
                },
                {
                    "start": 1551,
                    "end": 1574,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 1574,
                    "end": 1593,
                    "matchedPaperCorpusId": "222377949"
                },
                {
                    "start": 1606,
                    "end": 1632,
                    "matchedPaperCorpusId": "258048481"
                },
                {
                    "start": 1653,
                    "end": 1670,
                    "matchedPaperCorpusId": "252182306"
                },
                {
                    "start": 1670,
                    "end": 1693,
                    "matchedPaperCorpusId": "250390908"
                },
                {
                    "start": 1693,
                    "end": 1715,
                    "matchedPaperCorpusId": "256274849"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.303466796875
        },
        {
            "corpus_id": "15690223",
            "title": "Unsupervised POS Induction with Word Embeddings",
            "text": "The hidden Markov model with multinomial emissions is a classic model for POS induction. This model makes the assumption that a latent Markov process with discrete states representing POS categories emits individual words in the vocabulary according to state (i.e., tag) specific emission distributions. An HMM thus defines the following joint distribution over sequences of observations and tags: \n\nwhere distributions p(t i | t i\u22121 ) represents the transition probability and p(w i | t i ) is the emission probability, the probability of a particular tag generating the word at position i.5 \n\nWe consider two variants of the HMM as baselines: \n\n\u2022 p(w i | t i ) is parameterized as a \"na\u00efve multinomial\" distribution with one distinct parameter for each word type. \n\n\u2022 p(w i | t i ) is parameterized as a multinomial logistic regression model with hand-engineered features as detailed in (Berg-Kirkpatrick et al., 2010). \n\nGaussian Emissions. We now consider incorporating word embeddings in the HMM. Given a tag t \u2208 T , instead of generating the observed word w \u2208 V , we generate the (pre-trained) embedding v w \u2208 R d of that word. The conditional probability density assigned to v w | t follows a multivariate Gaussian distribution with mean \u00b5 t and covariance matrix \u03a3 t : \n\nThis parameterization makes the assumption that embeddings of words which are often tagged as t are concentrated around some point \u00b5 t \u2208 R d , and the concentration decays according to the covariance matrix \u03a3 t .6 Now, the joint distribution over a sequence of observations v = v w 1 , v w 2 . . . , v w (which corresponds to word sequence w = w 1 , w 2 , . . . , w , ) and a tag sequence t = t 1 , t 2 . . . , t becomes: \n\nWe use the Baum-Welch algorithm to fit the \u00b5 t and \u03a3 t i parameters. In every iteration, we update \u00b5 t * as follows:",
            "score": 0.4396506628741437,
            "section_title": "Hidden Markov Models",
            "char_start_offset": 3079,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 397
                },
                {
                    "start": 400,
                    "end": 592
                },
                {
                    "start": 595,
                    "end": 644
                },
                {
                    "start": 647,
                    "end": 765
                },
                {
                    "start": 768,
                    "end": 921
                },
                {
                    "start": 924,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1276
                },
                {
                    "start": 1279,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1700
                },
                {
                    "start": 1703,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1819
                }
            ],
            "ref_mentions": [
                {
                    "start": 889,
                    "end": 920,
                    "matchedPaperCorpusId": "10986188"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.035003662109375
        },
        {
            "corpus_id": "226283796",
            "title": "Context Analysis for Pre-trained Masked Language Models",
            "text": "STM language models to learn how much context is used and how nearby and long-range context is represented differently.\n\nOur work complements prior efforts by analyzing how models pre-trained by MLM make use of context and provides insights that different architectures can have different patterns to capture context. Distinct from previous works, we leverage no specific model architecture nor intermediate representations while performing the context analysis.\n\nAnother related topic is generic model interpretations including LIME (Ribeiro et al., 2016), SHAP (Lundberg andLee, 2017), andAncona et al. (2017). Despite the procedural similarity, our work focuses on analyzing how pre-trained MLMs behave when encoding contexts and our methodology is both model-agnostic and training-free. For context analysis, we perform the masking and predictions at the word level. Given a target word w t , all its subwords are masked X \\t = (... Following Devlin et al. (2019), the conditional probability of w t can be computed from outputs of MLMs with the independence assumption between subwords:\n\n(1)\n\nTo investigate how MLMs use context, we propose procedures to perturb the input sentence from X \\t to X \\t and monitor the change in the target word probability P (w t |X \\t ).",
            "score": 0.4396506628741437,
            "section_title": "Related Work",
            "char_start_offset": 6598,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 534,
                    "end": 556,
                    "matchedPaperCorpusId": "13029170"
                },
                {
                    "start": 576,
                    "end": 591,
                    "matchedPaperCorpusId": "21889700"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.356689453125
        },
        {
            "corpus_id": "238354171",
            "title": "ContractNLI: A Dataset for Document-level Natural Language Inference for Contracts",
            "text": "of the same type, we fixed the hypotheses throughout all the contracts including the test dataset. Given the closed set of hypotheses, this problem could also be addressed by building a text classifier for each hypothesis. However, given the modest available data for a task requiring natural language understanding, we believe more power can be achieved by viewing this as an NLI problem. Indeed, you can think of the NLI approach as building a multi-task text classifier with the hypothesis serving as a \"prompt\" to the model. We will discuss whether introducing hypotheses is helpful to the model or not in Section 5.1.",
            "score": 0.4396506628741437,
            "section_title": "Task Formulation",
            "char_start_offset": 6617,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0537109375
        },
        {
            "corpus_id": "229340369",
            "title": "A hybrid deep-learning approach for complex biochemical named entity recognition",
            "text": "The architecture of the proposed BBMC model is shown in Figure 1.The model consists of four parts: the BERT module, the BILSTM module, the MHATT module and the CRF module.The BERT module is first used to process the input data to obtain the underlying features of the input text, such as part-of-speech and characters, and primary sentence features.Next, the hidden layer of BILSTM stores two values that include forward and backward calculations.In this way, the target word can use the information at both the beginning and the end of the training process.Then, we apply the attention module to ensure that it can extract information of interest at the level of sentences.This module can address the problem of inconsistency, identify abbreviations and full-text labels (to a certain extent), and reduce the difficulty in training the model.Finally, the CRF layer decodes the output of the previous module into an optimal sequence and outputs the annotation information.The functions and underlying principles of each module are described in subsequent sections.",
            "score": 0.4396506628741437,
            "section_title": "Architecture",
            "char_start_offset": 9392,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 65,
                    "end": 171
                },
                {
                    "start": 171,
                    "end": 349
                },
                {
                    "start": 349,
                    "end": 447
                },
                {
                    "start": 447,
                    "end": 558
                },
                {
                    "start": 558,
                    "end": 674
                },
                {
                    "start": 674,
                    "end": 843
                },
                {
                    "start": 843,
                    "end": 972
                },
                {
                    "start": 972,
                    "end": 1064
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0163421630859375
        },
        {
            "corpus_id": "267294743",
            "title": "Beyond Lexical Boundaries: LLM-Generated Text Detection for Romanian Digital Libraries",
            "text": "It requires, however, access to the language model while generating the text. \n\nAnother detection approach based on text classification was proposed by OpenAI (https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text, accessed on 8 January 2024) and consisted in fine-tuning a GPT model with data from Wikipedia, WebText [55], and human input data to create an interface for a discrimination task using outputs produced by 34 language models. Their approach combined the classifier-based method with human evaluation to determine if a text was artificially generated. Nonetheless, this approach has some limitations. The text must have at least 1000 characters, and it was primarily trained on English corpora, making it inappropriate for multilingual use cases. Its authors recommend using the classifier only for English text since it performs significantly worse in other languages. Based on the preliminary evaluations of a set of English texts, the model correctly identifies 26% of AI-written text (true positives) and incorrectly labels human-authored text as AI text for 9% of the texts (false positives). \n\nSimpler classifier methods involve ML models such as XGBoost [56]. In their approach, the input features are based on the TF-IDF score and hand-crafted linguistic features based on characters and punctuation. The authors achieved an F1-score of 99% for detecting ChatGPT text. However, as a limitation, it can easily perform overfitting due to sample bias, requiring a large training dataset to overcome this drawback. \n\nAmong the various detection methods, we can also include fine-tuning language models for binary classification [2]. Solaiman et al. [2] used a sequence classifier model based on RoBERTa-base and RoBERTa-large, which achieved 95% accuracy on a GPT2 dataset detection. The advantage of this method is the bidirectionality, which allows discriminative classification models to be more powerful for detection than generative classification models. \n\nHowever, despite the numerous studies targeting MGT detection, Krishna et al. [57] highlight that paraphrased text escapes the existent detectors, including watermarking, DetectGPT, or GPTZero, with an important drop in performance.",
            "score": 0.4371173287879357,
            "section_title": "Detection Mechanisms",
            "char_start_offset": 26336,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 80,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1129
                },
                {
                    "start": 1132,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1550
                },
                {
                    "start": 1553,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1996
                },
                {
                    "start": 1999,
                    "end": 2231
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.430908203125
        },
        {
            "corpus_id": "270923838",
            "title": "LLMs Plagiarize: Ensuring Responsible Sourcing of Large Language Model Training Data Through Knowledge Graph Comparison",
            "text": "There exists a large range of literature that addresses identifying large language model training data in a \"black-box\" environment where the training corpus is unknown. For instance, LLM training data sourcing has been assessed through min-k% prob, which is a detection method based on the assumption that a member of the training data is less likely to include words that have high negative log-likelihood (and are thus outlier words) compared to a non-member of the training data (Shi et al., 2024), therefore considering \"anomalous\" vocabulary within a text. \n\nSuch an approach, based on the principles of Membership Interference Attacks (MIAs), an adversarial technique that seeks to determine whether a knowledge source is part of a model's training data, is the most common method to identify LLM training data in \"black-box\" environments. Substantial literature also exists about utilizing MIA principles to identify corpora used to fine-tune LLMs, addressing word embeddings (Mahloujifar et al., 2021), addressing NLP classification models for members of training corpora (Shejwalkar et al., 2021), and addressing source text memorization (Song and Shmatikov, 2019). \n\nHowever, such approaches based on MIA principles take a statistical and probabilistic approach to identifying LLM training data, ignoring other \"signs\" of sourcing that extend beyond simple copying or paraphrasing. Statistical measures such as only considering the likelihood of \"anomalous\" words ignore the broad relationships between ideas that exist in sentences of a source corpora that may manifest themselves in an LLM's generated answer. \n\nAdditionally, traditional plagiarism detection systems (systems that compare the similarity of multiple corpora) often rely on simple matching techniques. For instance, these systems may search direct token (word, sentence, unique phrase, paragraph, etc.) matches between a document and others, using a threshold for matches as an indicator of plagiarism/similarity. Other systems narrow down at the individual word/phrase level, analyzing semantic relationships through simple synonym/antonym detection or more complex Semantic Role Labeling techniques between words in target and source sentences (Osman et al., 2012).",
            "score": 0.4362587694583109,
            "section_title": "Literature Review",
            "char_start_offset": 3486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 562
                },
                {
                    "start": 565,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1175
                },
                {
                    "start": 1178,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1622
                },
                {
                    "start": 1625,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2245
                }
            ],
            "ref_mentions": [
                {
                    "start": 1081,
                    "end": 1106,
                    "matchedPaperCorpusId": "245222525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1937255859375
        },
        {
            "corpus_id": "12769731",
            "title": "A Provably Correct Learning Algorithm for Latent-Variable PCFGs",
            "text": "We now describe a second set of experiments, on the Saul and Pereira (1997) model for language modeling. Define V to be the set of words in the vocabulary. For any w 1 , w 2 \u2208 V , the Saul and Pereira (1997) model then defines p(w 2 | w 1 ) = m h=1 r(h | w 1 )s(w 2 | h) where r(h | w 1 ) and  (Kneser and Ney, 1995), using the SRILM toolkit.\n\ns(w 2 | h) are parameters of the approach. The conventional approach to estimation of the parameters r(h | w 1 ) and s(w 2 | h) from a corpus is to use the EM algorithm. In this section we compare the EM algorithm to a pivot-based method.\n\nIt is straightforward to represent this model as an L-PCFG, and hence to use our implementation for estimation.\n\nIn this special case, the L-PCFG learning algorithm is equivalent to a simple algorithm, with the following steps: 1) define the matrix Q with entries Q w 1 ,w 2 = count(w 1 , w 2 )/N where count(w 1 , w 2 ) is the number of times that bigram (w 1 , w 2 ) is seen in the data, and N = w 1 ,w 2 count(w 1 , w 2 ). Run the algorithm of section 5.2 on Q to recover estimates\u015d(w 2 | h); 2) estimater(h | w 1 ) using the EM algorithm to optimize the function w 1 ,w 2 Q w 1 ,w 2 log hr (h | w 1 )\u015d(w 2 | h) with respect to ther parameters; this function is concave in these parameters.\n\nWe performed the language modeling experiments for a number of reasons. First, because in this case the L-PCFG algorithm reduces to a simple algorithm, it allows us to evaluate the core ideas in the method very directly. Second, it allows us to test the pivot method on the very large datasets that are available for language modeling.\n\nWe use two corpora for our experiments. The first is the Brown corpus, as used by Bengio et al. (2006) in language modeling experiments. Following Ben",
            "score": 0.4353487993929209,
            "section_title": "Experiments on the Saul and Pereira (1997) Model for Language Modeling",
            "char_start_offset": 24224,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 294,
                    "end": 316,
                    "matchedPaperCorpusId": "9685476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08447265625
        },
        {
            "corpus_id": "2901242",
            "title": "Mention Detection Crossing the Language Barrier",
            "text": "Now take x N 1 = (x 1 , x 2 , . . . x N ), a sequence of contiguous tokens (i.e., a sentence or a document) in the source language. The goal of mention detection system is to find the most likely sequence of labels y N 1 = (y 1 , y 2 . . . y N ) that best matches the input x N 1 . In the mention detection case, each token x i in x N 1 is tagged with a label y i as follows:5 \n\n\u2022 if it's not part of any entity, y i = O (O for \"outside any mentions\") \n\n\u2022 if it is part of an entity, it is composed of a subtag specifying whether it starts a mention (B-) or is inside a mention (I-), and a sub-type corresponding to mention type (e.g. B-PERSON). \n\nIn ACE, there are seven possible types: person, organization, location, facility, geopolitical entity (GPE), weapon, and vehicle. \n\nTo compute the best sequence y N 1 , we use \n\nwhere P \u0177j |x N 1 , y j\u22121 j\u2212k has an exponential form of the type (2). We also used the standard Markov assumption that the probability P \u0177j |x N 1 , \u0177j\u22121 1 only depends on the previous k classifications. This model is similar to the MEMM model (McCallum et al., 2000), but it does not separate the probability into generation probabilities and transition probabilities, and, crucially, has access to \"future\" observed features (i.e. it can examine the entire x N 1 sequence, though in practice it will only examine some small part of it) -which is one way of eliminating label bias observed by (Lafferty et al., 2001). 6  The experiments are run on four languages, part of the ACE-2007 evaluation (NIST, 2007): Arabic, Chinese, English and Spanish. 7",
            "score": 0.4353487993929209,
            "section_title": "Mention Detection",
            "char_start_offset": 9242,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 36,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 376
                },
                {
                    "start": 379,
                    "end": 451
                },
                {
                    "start": 454,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 777
                },
                {
                    "start": 780,
                    "end": 823
                },
                {
                    "start": 826,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1577
                }
            ],
            "ref_mentions": [
                {
                    "start": 1071,
                    "end": 1094,
                    "matchedPaperCorpusId": "775373"
                },
                {
                    "start": 1421,
                    "end": 1444,
                    "matchedPaperCorpusId": "219683473"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0310821533203125
        },
        {
            "corpus_id": "266996526",
            "title": "The Optimization of a Natural Language Processing Approach for the Automatic Detection of Alzheimer\u2019s Disease Using GPT Embeddings",
            "text": "Based on the GPT 3 architecture, OpenAI offers a set of highly advanced, cost-effective set of embedding models [9]. First-generation versions of these models have shown promising results when it comes to the NLP-based automatic detection of AD [10]. \n\nPast research into the automatic detection of AD using speech has focused on either using acoustic features or NLP techniques [10,11]. While acoustic feature-based models have been shown to perform effectively, achieving accuracies of 63.6% in Chlasta and Wolk using a convolutional neural network (CNN) or 65.6% in Balugopalan and Novikova using a support vector machine (SVM) classifier, Balugopalan and Novikova showed that a word embedding or combination approach was more effective. They performed better in nearly all metrics using several machine learning classifiers, achieving an accuracy of 66.9% for embeddings and 69.2% for combination using SVM [12,13]. \n\nFigure 1: 2-Degree Vector Graphical Interpretation of N-Degree Vector Word Embeddings to Convey Linguistic Meaning in a Numerical Format. The difference in meaning between \"Brother\" and \"Sister,\" and \"Son\" and \"Daughter\" is identical and refers to the genders to which words in both groups of words apply; this equal difference can be seen through the identical vectors between them. Through these numerical interpretations of meanings, ML classifiers can be trained to detect patterns in text. Made with Bio Render. \n\nfound that SVM and neural networks (NN) were the most effective, achieving accuracies and F-1 Scores (the harmonic mean of precision and recall) of 0.77 and 0.80 (SVM) and 0.78 and 0.76 (NN), respectively [14]. \n\nAgbavor and Liang built upon the research of both Balugopalan and Novikov and Cruz et al. \n\nUsing audio files from the ADReSSO dataset, they extracted acoustic features, and they converted audio to text automatically using a transcription program, extracting embeddings using OpenAI first-generation embedding models. Using these acoustic features and embeddings, they trained multiple models using different combinations of NLP methods and ML classifiers.",
            "score": 0.4353487993929209,
            "section_title": "Introduction",
            "char_start_offset": 4109,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 250
                },
                {
                    "start": 253,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 919
                },
                {
                    "start": 922,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1438
                },
                {
                    "start": 1441,
                    "end": 1651
                },
                {
                    "start": 1654,
                    "end": 1743
                },
                {
                    "start": 1746,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2110
                }
            ],
            "ref_mentions": [
                {
                    "start": 245,
                    "end": 249,
                    "matchedPaperCorpusId": "255029590"
                },
                {
                    "start": 379,
                    "end": 383,
                    "matchedPaperCorpusId": "255029590"
                },
                {
                    "start": 383,
                    "end": 386,
                    "matchedPaperCorpusId": "239041284"
                },
                {
                    "start": 911,
                    "end": 915,
                    "matchedPaperCorpusId": "231886053"
                },
                {
                    "start": 1646,
                    "end": 1650,
                    "matchedPaperCorpusId": "246928111"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1859130859375
        },
        {
            "corpus_id": "267402678",
            "title": "The evolution, applications, and future prospects of large language models: An in-depth overview",
            "text": "A century ago, mathematician Andrei Markov found inspiration in the poetic prose of \"Eugene Onegin,\" penned by Alexander Pushkin. This encounter led to the inception of the \"language modeling\" realm. Through analyzing sequences of words and phrases in the text, Markov discerned patterns, predicting subsequent words or phrases, and essentially mimicking Pushkin's linguistic style using probabilistic methods. This technique came to be known as the Markov Chain [1,2], and its influence reverberated across diverse scientific and technological frontiers. \n\nIn the modern era, large-scale language models stand at the forefront of Natural Language Processing (NLP), reshaping the dynamics of our interaction with digital systems. Bolstered by advanced artificial intelligence methodologies, these models showcase unparalleled prowess in interpreting and reproducing human language nuances. The ChatGPT series, in particular, heralds a transformative phase in NLP. It manifests an adeptness in orchestrating nuanced, context-aware conversations, a feat made possible due to rigorous training on expansive datasets. With its profound grasp of grammar, context, and semantic nuances, ChatGPT finds resonance across multifarious sectors. This detailed research exposition offers a panoramic view of the progression, utility, and prospective avenues of large-scale language models. It meticulously traces their evolutionary arc, elucidating the foundational technological tenets. This study also encapsulates the salient application spheres of these models, casting a spotlight especially on the financial sector, elucidating the myriad opportunities they present. In addition, the research delves into the inherent challenges that contemporary large language models present, offering potential mitigations from five distinct vantage points-intending to navigate these intricacies responsibly and sustainably. This exploration, encompassing technological intricacies, application spectra, and forward-looking insights, equips readers to fathom the monumental role this technology is poised to play in steering future humandigital dialogues. As NLP continues its metamorphic journey, this document stands as an indispensable compass for scholars, professionals, and aficionados venturing into the expansive realm of large-scale language models.",
            "score": 0.4353487993929209,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 555
                },
                {
                    "start": 558,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2135
                },
                {
                    "start": 2136,
                    "end": 2338
                }
            ],
            "ref_mentions": [
                {
                    "start": 466,
                    "end": 468,
                    "matchedPaperCorpusId": "144854176"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.147216796875
        },
        {
            "corpus_id": "2234445",
            "title": "Attention Shifting for Parsing Speech",
            "text": "The noisy channel model for speech is presented in Equation 1, where A represents the acoustic data extracted from a speech signal, and W represents a word string. The acoustic model P (A|W ) assigns probability mass to the acoustic data given a word string and the language model P (W ) defines a distribution over word strings. Typically the acoustic model is broken into a series of distributions conditioned on individual words (though these are based on false independence assumptions). \n\nThe result of the acoustic modeling process is a set of string hypotheses; each word of each hypothesis is assigned a probability by the acoustic model. Word-lattices are a compact representation of output of the acoustic recognizer; an example is presented in Figure 2. The word-lattice is a weighted directed acyclic graph where a path in the graph corresponds to a string predicted by the acoustic recognizer. The (sum) product of the (log) weights on the graph (the acoustic probabilities) is the probability of the acoustic data given the string. Typically we want to know the most likely string given the acoustic data. \n\nIn Equation 3 we use Bayes' rule to find the optimal string given P (A|W ), the acoustic model, and P (W ), the language model. Although the language model can be used to rescore1 the word-lattice, it is typically used to select a single hypothesis. \n\nWe focus our attention in this paper to syntactic language modeling techniques that perform complete parsing, meaning that parse trees are built upon the strings in the word-lattice.",
            "score": 0.4353487993929209,
            "section_title": "Parsing speech word-lattices",
            "char_start_offset": 3886,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 491
                },
                {
                    "start": 494,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1119
                },
                {
                    "start": 1122,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1371
                },
                {
                    "start": 1374,
                    "end": 1556
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18115234375
        },
        {
            "corpus_id": "264439179",
            "title": "A Survey on Detection of LLMs-Generated Content",
            "text": "In this section, we discuss the common datasets used for this task. The corpus is usually adopted from previous NLP tasks, and reconstructed by prompting LLMs to generate new outputs as candidate machine-generated text. Usually, there are two prompting methods: 1). prompting LLMs with the questions in some question-answering datasets. 2). prompting LLMs with the first 20 to 30 tokens to continue writing in datasets without specific questions. Specifically, several datasets have been compiled and utilized in the field. Some noteworthy datasets include TURINGBENCH (Uchendu et al., 2021), HC3 (Guo et al., 2023), CHEAT (Yu et al., 2023a), Ghostbuster (Verma et al., 2023), OpenGPTText (Chen et al., 2023), M4 (Wang et al., 2023d), MGTBench (He et al., 2023), and MULTI-TuDE (Macko et al., 2023) and some other datasets not explicitly built for detection have also been used, such as C4 (Raffel et al., 2019), shareGPT2 , and alpaca (Taori et al., 2023), as summarized in Table 1. For text detection, we only list datasets explicitly built for detection, while some general datasets like C4 (Raffel et al., 2019) or alpaca (Taori et al., 2023) can also be used. For code detection, we only list datasets that have been used in previous code detection work (Lee et al., 2023;Yang et al., 2023e). And other codegeneration corpora can also be adopted. The detailed description is included in Appendix A.3. \n\nData Contamination. Despite those released standard datasets, we argue that static evaluation benchmarks might not be desirable for this problem with the rapid progress of LLMs trained, tuned, or aligned on large amounts of data across the whole internet.",
            "score": 0.4353487993929209,
            "section_title": "Datasets",
            "char_start_offset": 8217,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1405
                },
                {
                    "start": 1408,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1663
                }
            ],
            "ref_mentions": [
                {
                    "start": 569,
                    "end": 591,
                    "matchedPaperCorpusId": "237589233"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1744384765625
        },
        {
            "corpus_id": "271516421",
            "title": "Several questions of visual generation in 2024",
            "text": "The success of large language models is largely attributed to the effective decomposition of textual signals. Consider the task of modeling a text sequence A = x 0 , x 1 , x 2 , . . ., where x t denotes the token at the t-th position from front to back. This can be segmented into a sequence of conditional modeling subtasks based on the position: p(x 0 ), p(x 1 |x 0 ), p(x 2 |x 1 , x 0 ), . . .. Large language models employ autoregressive models to approximate these conditional distribution mappings p(x t |x i,i<t ). A critical aspect of this approach is that, for natural language, the decomposed subtasks are intrinsically interrelated. For example, a phrase is \"I love swimming,\" it might appear at the beginning or in the middle of a sentence. In other words, it can appear in any subtask. With a sufficiently large dataset, p(x 2 = \"swimming\"|x i<2 = \"I love\") and p(x 7 = \"swimming\"|x i<7 = \"you love playing basketball, I love\") represent two tasks that are very interrelated and can serve as a form of data augmentation to one another. In other words, different subtasks are \"equivariant\". Let's give \"equivariant\" a rigorous mathematical definition. \n\nDefinition 1 Assume the target distribution is P t (x), we split the signal into multiple subtasks: p(x 0 ), p(x 1 |x 0 ), p(x 2 |x 1 ) . . . p(x t+1 |x t ) . . . For the t-th conditional probability fitting task p(x t+1 |x t ), we adopt a network \u03b8 t to fit it. For any two tasks t and k and two state samples s and \u015d, if \n\nor p \u03b8t (x t+1 = \u015d|x t = s) = p \u03b8t (x k+1 = \u015d|x k = s) \n\n(2) we refer to this signal decomposition as equivariant.",
            "score": 0.4353487993929209,
            "section_title": "Language decomposition",
            "char_start_offset": 247,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1163
                },
                {
                    "start": 1166,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1488
                },
                {
                    "start": 1491,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1605
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05126953125
        },
        {
            "corpus_id": "8958619",
            "title": "Decision Tree Models Applied to the Labeling of Text with Parts-of-Speech",
            "text": "This technique can be used both to discover parts-of-speech for words which do not occur in the hand-labeled text, as well as to discover additional parts-of-speech for those that do. In both experiments multiword expressions, such as \"nineteenthcentury\" and \"stream-of-consciousness,\" which were assigned a single tag in the hand-labelled text, were broken up into single words in the training text, with each word receiving no tag. \n\nThe parameters of the HMM model were estimated from the training section of the hand-labeled text, without any use of the forward-backward algorithm. Subsequently, we used the smoothing section of the data to construct an interpolated model as described by Merialdo [4,6]. \n\nWe evaluated the performance of the interpolated hidden Markov model by tagging the 2000 sentences which make up the testing portion of the data. We then compared the resultant tags with those produced by the Lancaster team, and found the error rate to be 3.03%. \n\nWe then grew and smoothed a decision tree using the same division of training and smoothing data, and combined the resulting marginals for predicting tags from the word context with the marginals for predicting tags from the tag context derived from the HMM model. The resulting error rate was 2.61%, a 14% reduction from the HMM model figure.",
            "score": 0.4353487993929209,
            "section_title": "Experimental Results",
            "char_start_offset": 11126,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 433
                },
                {
                    "start": 436,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 708
                },
                {
                    "start": 711,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 973
                },
                {
                    "start": 976,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1319
                }
            ],
            "ref_mentions": [
                {
                    "start": 702,
                    "end": 705,
                    "matchedPaperCorpusId": "61014458"
                },
                {
                    "start": 705,
                    "end": 707,
                    "matchedPaperCorpusId": "6555412"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.072509765625
        },
        {
            "corpus_id": "262465968",
            "title": "Substituting Data Annotation with Balanced Updates and Collective Loss in Multi-label Text Classification",
            "text": "4. Label observation probabilities: For each label, a priori probability of inclusion of that label in a subset \u03bb l = p(l \u2208 S) is provided. 5. Annotated data: There is a set of training data {\u03d1 i , S i } i\u2208DA such that D A \u2282 D train and |D A | \u226a |D train | with provided ground truth label subsets. \n\n-External resources 1. Tokenizer: We are given access to a pre-trained tokenization function with vocabulary V which is able to convert the input text content and label descriptions into a sequence of tokens, i.e., given an input text where \u03c4 \u2208 \u0398 \u222a \u03a6, f tokenizer (\u03c4 ) = (t 1 , . . . , t s ) such that t i \u2208 V and s is the length of the input sequence. 2. Language model: We are given access to a pre-trained natural language inference model f NLI (H, P) with vocabulary V that calculates true (entailment) q, undetermined (neutral) q or false (contradiction) q probabilities of a hypothesis sequence H = (h 1 , . . . , h s h ) where h i \u2208 V, given a premise sequence P = (p 1 , . . . , p sp ) where p j \u2208 V such that q + q + q = 1. 3. Word embeddings: We are given access to a set of pre-trained d-dimensional word embeddings for the (tokens composing) label descriptions, i.e., f WE (t) = e where e \u2208 R d denotes embeddings of token t \u2208 V. \n\nWe consider three different scenarios of supervision. In all scenarios, we are given training data, label descriptions and external resources. The inputs of each scenario are summarized in Table 1. We use a test set {j, \u03d1 j , S j } j\u2208DTest such that D \u2229 D Test = \u2205 to evaluate the performance in all cases. \n\n-Annotation-Free: In this scenario, we do not use any annotated data to learn the classifier but require supervision on average subset cardinality and label observation probabilities.",
            "score": 0.4353487993929209,
            "section_title": "PROBLEM STATEMENT",
            "char_start_offset": 7562,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 298
                },
                {
                    "start": 301,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1242
                },
                {
                    "start": 1245,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1551
                },
                {
                    "start": 1554,
                    "end": 1737
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.058349609375
        },
        {
            "corpus_id": "5116525",
            "title": "The Measure of a Model",
            "text": "This paper describes measures that can be used to examine the appropriateness, for the test set, of the features used in a model, the parametric form of the model, and the parameter estimates. Figures 1-12 plot model complexity against a number of model performance measures. The gaps between the overall classi cation performance of a model (indicated as \\Overall Model\" in the gures) and the other measures is variously due to error introduced by the three factors under study. We rst de ne all of the performance measures shown in the gures, and then discuss what can be concluded from the relationships among measures. \n\nBelow, a completed model is a model in which the features have been speci ed; the parametric form has been speci ed; and the parameters have been estimated. \n\n1. Overall Model Performance. Given a completed model in which the parameters have been estimated from the training data: the overall model performance is the percentage of the test set tagged correctly by a classi er using that model to tag the test set. \n\nComments: Other widely-used loss functions are entropy, cross-entropy, and squared error. \n\n2. Lower Bound. Let FT be the most frequently-occurring (correct) tag for a word in the test set. The lower bound for that word is the percentage of the test set assigned tag FT. \n\nComments: The classi cation performance of a probabilistic model should not be worse than that of the simplest model, the model for independence: 8 tag; f 1 ; f 2 ; : : :; f n P(tag; f 1 ; f 2 ; : : :; f n ) = P(tag) P(f 1 ) P(f 2 ) P(f n ) (6) Because the probability of seeing each value of the classi cation variable (i.e., each tag) is independent of the context, this model assigns every object the most frequently occurring tag: c tag = argmax tag P(tagjf 1 ; f 2 ; f 3 ; :::; f n ) = argmax tag P(tag) (7) Therefore, the proportion of the test set belonging to the most frequently occurring tag establishes the lower bound on model performance.",
            "score": 0.4353487993929209,
            "section_title": "Description of Evaluation Measures",
            "char_start_offset": 15766,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 622
                },
                {
                    "start": 625,
                    "end": 781
                },
                {
                    "start": 784,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 1039
                },
                {
                    "start": 1042,
                    "end": 1131
                },
                {
                    "start": 1134,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1312
                },
                {
                    "start": 1315,
                    "end": 1966
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01337432861328125
        },
        {
            "corpus_id": "258048481",
            "title": "On the Possibilities of AI-Generated Text Detection",
            "text": "IMDb Dataset Experiments. To validate our claims on the possibilities of detection, we run experiments on the IMDb dataset (Maas et al., 2011), which is a widely-used benchmark dataset in the field of natural language processing. The dataset consists of 50, 000 movie reviews from the internet movie database that have been labeled as positive or negative based on their sentiment. The goal is to classify the reviews accordingly based on their text content. The experiments are done to validate our hypothesis on a more general class of language tasks including classification and detection. We specifically focus on the representation space of the inputs for both the human and machine distributions and try to validate our hypothesis by comparing the input space of words to the input space of a group of sentences. The objective is to analyze the variations in performance of the detector when detecting at word-level versus paragraphs. Hence, there are two scenarios to consider. The first is where we're given a word and we have to determine whether it came from positive or negative class. The second, and more practical case, is where we're given a paragraph i.e a group of sentences and we have to detect whether it came from positive or negative class. So, we first compute the total variation distance between the positive and negative classes at the word level. This is done by computing the divergence between the distribution over the space of words between the two classes. Figure 7(a) shows that the best possible AUROC achieved by the detector is 0.585 at the word level. From these results, it seems almost impossible to distinguish the two classes. However when we perform the detection at a paragraph level using a real detector (standard ML models, including random forest, logistic regression, and a vanilla multi-layer perception), we see a remarkable improvement in the detection performance. As shown in Figure 7(a), all the real detectors achieve a train AUROC of greater than 0.85 (\u2265 0.93 for random forest and MLP), and a test AUROC of greater than 0.8, which surpasses the upper-bounds of the best detector at a word level, validating our theory and intuition.",
            "score": 0.4353487993929209,
            "section_title": "C.1 Additional Experimental Details",
            "char_start_offset": 44516,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 26,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2189
                }
            ],
            "ref_mentions": [
                {
                    "start": 123,
                    "end": 142,
                    "matchedPaperCorpusId": "1428702"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1917724609375
        },
        {
            "corpus_id": "265149884",
            "title": "Large Language Models for Robotics: A Survey",
            "text": "We first provide an overview of LLM, starting with an introduction to some fundamental concepts. We then delve into the history of LLM's development, followed by a brief discussion of its growing popularity in recent years. A language model is a computational model that utilizes statistical methods to analyze and predict the probability of word sequences in a given language. It is designed to capture the patterns, grammar, and semantic meaning of natural language [92]. \n\n\u2022 N-gram models are a simple form of language models that calculate the probability of a word based on the preceding (n-1) words. They are widely used due to their simplicity and efficiency. The accuracy of the N-gram model is directly related to the length of the context used, with larger 'n' values leading to higher accuracy [13]. \n\n\u2022 Unigram models [147] are often employed for various language processing tasks including information retrieval. It evaluates each word or term independently. It is calculated without considering any conditional context, only the probability of the current word itself appearing. \n\n\u2022 Bidirectional models differ from unidirectional models, it analyzes text in both directions: backward and forward. This dual approach is commonly employed in various machine learning models and speech generation applications. Bidirectional models harness the power of contextual information from both directions, providing a deeper understanding of the text [6]. \n\n\u2022 Exponential models [28] employ an equation that combines feature functions and n-grams to evaluate text. Unlike n-grams, this type of model allows for more flexibility in analyzing parameters and does not mandate the specification of individual gram sizes. Essentially, exponential models define features and parameters based on the desired outcomes, providing a more open-ended approach to text analysis. \n\n\u2022 Neural language models, including recurrent neural networks (RNNs) [150] and transformers [131], have gained popularity in recent years. These models use deep learning techniques to capture complex language patterns and dependencies. \n\n\u2022 Transformer architecture's development revolutionized language modeling. Transformers use self-attention mechanisms to capture relationships between words in a sentence. This is currently the most popular architecture [131].",
            "score": 0.4353487993929209,
            "section_title": "Language Model Basics",
            "char_start_offset": 5842,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 810
                },
                {
                    "start": 813,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1092
                },
                {
                    "start": 1095,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1459
                },
                {
                    "start": 1462,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1869
                },
                {
                    "start": 1872,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2107
                },
                {
                    "start": 2110,
                    "end": 2184
                },
                {
                    "start": 2185,
                    "end": 2281
                },
                {
                    "start": 2282,
                    "end": 2336
                }
            ],
            "ref_mentions": [
                {
                    "start": 805,
                    "end": 809,
                    "matchedPaperCorpusId": "10986188"
                },
                {
                    "start": 830,
                    "end": 835,
                    "matchedPaperCorpusId": "13490401"
                },
                {
                    "start": 1455,
                    "end": 1458,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 1964,
                    "end": 1969,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 2330,
                    "end": 2335,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1436767578125
        },
        {
            "corpus_id": "17916864",
            "title": "Pipelined language model construction for Polish speech recognition",
            "text": "We will be considering here the languages being sets of sequences of words coming from the finite dictionary D. A stochastic n-gram language model is the set of data that makes it possible to estimate the probability of appearance of the n-th word w i provided that the sequence of preceding n \u2212 1 words w i\u2212n+1 , w i\u2212n+2 , . . . , w i\u22121 is known. In other words, the LM provides the method to compute the estimation of \n\nA sequence of n consecutive words is called the n-gram. \n\nThe conditional probabilities can be given explicitly in the LM or they can be defined procedurally. The language model is usually constructed from the language corpus which is a sufficiently large set of sample phrases in the language being modeled. The most obvious way to find out the probability estimates is to count the occurrences of n-grams in the model and to apply Maximum Likelihood (ML) estimation: \n\nwhere c(w 1 , w 2 , . . . , w n ) is the number of n-gram occurrences w 1 , w 2 , . . . , w n in the corpus. Due to a limited size of the corpus, nonzero ML estimates can be obtained only for a very limited set of n-grams. For this reason, in practice, low n-gram orders are used-in most cases n does not exceed 3. \n\nIn our experiments we use a bigram LM, which corresponds to setting n = 2. Limiting the n-gram order still does not solve the data sparseness problem completely in the case of languages consisting of thousands of words. It is still very likely that many n-grams that may appear in typical language use are missing in the corpus or the number of their occurrences is not big enough to allow reliable ML estimation of related probabilities. To prevent underestimation of probabilities of missing n-grams, the concept of a backoff is applied. Backing off consists in using lower order n-gram probabilities when the number of occurrences of an n-gram in the corpus is not sufficient. In such a case the probability (2) is approximated using the lower order n-gram probability p(w i |w i\u2212n+2 , . . . , w i\u22121 ).",
            "score": 0.4353487993929209,
            "section_title": "Backoff LM smoothing",
            "char_start_offset": 17168,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 419
                },
                {
                    "start": 422,
                    "end": 477
                },
                {
                    "start": 480,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1207
                },
                {
                    "start": 1210,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2015
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15380859375
        },
        {
            "corpus_id": "16362127",
            "title": "A Hybrid Environment for Syntax-Semantic Tagging",
            "text": "are the works by [Voutilainen 94,Oflazer & T\u00fcr 97], where a large hand-written constraint grammar is used to perform part-of-speech tagging.\n\nSecond, we find the statistical family, where the language model is left to a data-collection process which stores thousands of occurrences of some kind of linguistic phenomenon and tries to derive a statistical law from them. This model acquisition is known as training. Two main points of view are used in this family, the Bayesian point of view and the Information Theory point of view. Both of them rely on the estimation of occurrence probabilities for each relevant event, but while the former tries to obtain them computing the number of event occurrences (Maximum Likelihood Estimation) -which may cause problems when an event is infrequent or data are scarce-the later is based on assuming maximum ignorance and trying to minimize the model entropy, thus unobserved events will only keep maximum uncertainty.\n\nStatistical methods constitute a very large family, and the one that has reported most successful results to NLP field in recent years. Some examples are the works by [Rabiner 90] who presents a tutorial on Hidden Markov Models and their application to speech recognition, or [Kupiec 91,Briscoe 94] who apply statistical methods to grammar development and parsing. Other works on NLP using statistical models are that of [Matsukawa 93,McKeown & Hatzivassiloglou 93], who learn to cluster similar words, and [Brants et al. 97] who identify the grammatical function of each word in a sentence. Statistical methods have been specially successful -since the 1970s to nowadays-when applied to speech recognition tasks [Rabiner 90,Huang et al. 93,Heeman & Allen 97]. This success caused that they were also used in other NLP areas, such as optical character recognition, spelling correction, POS tagging, parsing, translation, lexicography, text compression and information retrieval.\n\nFinally, the machine-learning family, where the model acquisition is also automatic, but the knowledge acquired belongs to a higher level than simple occurrence frequencies. For instance, [Yarowsky 94] learns decision lists to properly restore accents in Spanish and French texts, the system described [Daelemans et al. 96b] learns morphological",
            "score": 0.4353487993929209,
            "section_title": "The Disambiguation Problem",
            "char_start_offset": 29137,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.302978515625
        },
        {
            "corpus_id": "7079619",
            "title": "Analyzing and Improving Statistical Language Models for Speech Recognition",
            "text": "Given the notations introduced in the previous section, we can now derive the task of a language model more formally.\n\nA good speech recognizer should choose the most likely string W , given the acoustic data A. This is expressed by the following formula W = argmax W p(WjA): ( \n\nThis decomposition is appropriate for speech recognition for the following reason. It allows us to evaluate the probability of a pre x w 1 : k]; 1 k n of W as the product of probabilities of the k words it contains:\n\nThis is very useful when we try to perform the maximization in equation 2.4. Rather than having to construct a W covering the entire signal before we can evaluate it with the language model, we can now evaluate partial strings covering only parts of the signal as they are constructed. We can thus prune the search space by never expanding or evaluating unlikely partial strings 3 . Using equation 2.6, we can now precisely de ne the task of a language model. De nition 1 Given a set of contexts C = fc 1 ; :::; c p g, the task of a language model is to provide a probability distribution p(w i]jc k ) for each context c k ; 1 k p and a way of choosing a context given the words recognized so far.\n\nDuring recognition, all the language model has to do is to determine which is the current context and to look up the probabilities of words in the distribution for this context. This is fairly straight forward once the model has been constructed. The important issue, however, is to construct the language model prior to recognition. This requires the de nition of the set of contexts and the estimation of a probability distribution for each context. These contexts can capture any information about the words spoken so far. However, the language model must be able to extract this information e ciently during recognition. An example of such information is whether the subject of the current sentence is animate or not. The language model must be able to decide e ciently whether the subject of the current sentence hypothesis is animate or not in order to determine the current context and therefore the distribution it is going to use.\n\nAs an example of a language model, consider a very simplistic model that constructs only one distribution, independent of context. The word \\teach\" for example will",
            "score": 0.4353487993929209,
            "section_title": "The Task of a Language Model",
            "char_start_offset": 38844,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.266845703125
        },
        {
            "corpus_id": "270703171",
            "title": "Investigating the Influence of Prompt-Specific Shortcuts in AI Generated Text Detection",
            "text": "Given a text sequence g written by either human or an LLM, AIGT detectors predict its score f (g), which represents the likelihood of g to be an AIGT.Based on the score, we assign a classification label as y = 1(f (g) \u2265 \u03c4 ), where \u03c4 is the predetermined detection threshold.Note that the inputs for LLM, i.e. t, a, x, are not available to detectors.A reliable detector should be able to find the correct label independent of the input choices.",
            "score": 0.4353487993929209,
            "section_title": "AIGT Detection",
            "char_start_offset": 7762,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 150,
                    "end": 274
                },
                {
                    "start": 274,
                    "end": 349
                },
                {
                    "start": 349,
                    "end": 443
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0260009765625
        },
        {
            "corpus_id": "258588444",
            "title": "WikiSQE: A Large-Scale Dataset for Sentence Quality Estimation in Wikipedia",
            "text": "The models used for detection are competitive large-scale pre-training models DeBERTaV3 (He, Gao, and Chen 2021) and BERT (Devlin et al. 2019), RoBERTa (Liu et al. 2019). DeBERTa and RoBERTa both used base models, and BERT used a base uncased model. Fine-tuning was performed with the method normally used for classification tasks in NLP. The multi-layer perceptron is used as the final layer and trained to classify in two classes, positive and negative, whether a sentence belongs to a target quality label or not. The maximum number of training epochs is 20, and the model that records the highest F1 in the development set is used as the best model to predict the test set. The learning rate is determined by searching among 1-e6, 5-e6, 1-e5, and 5-e5. The maximum input sequence length is 256 and the batch size is 64. In all setups, we report the average F1 values of the experiments with three different seeds.",
            "score": 0.4353487993929209,
            "section_title": "Setup",
            "char_start_offset": 16986,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 917
                }
            ],
            "ref_mentions": [
                {
                    "start": 122,
                    "end": 142,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10394287109375
        },
        {
            "corpus_id": "261277205",
            "title": "Large language models converge toward human-like concept organization",
            "text": "We evaluate the vector spaces induced by four well-known families of language models, conducting experiments with a total of 20 different transformer-based models, as well as a baseline static word vector space. The four families are OPT [33], GPT-2 [20], and Pythia [2] (non-deduplicated version at model checkpoint step 143000) and BERT [9]. 4 We also evaluated the vector space of GPT-3 [5], i.e., text-embedding-ada-002. 5 Transformer-based LLMs use multiple layers of self-attention [28] and can model complex interactions across large context windows. Both left and right context can be considered. GPT, OPT and Pythia are decoder-only autoregressive LLMs, however, and thus only consider left context, i.e., the words preceding the next token. BERT is an encoder-only non-autoregressive LLM and considers both left and right context of the masked token to be predicted. Transformers are in general considered state-of-the-art for most NLP tasks [31]. For each of the language model families, we consider variants with increasing size in terms of the number of model parameters. See Table 1 for a model overview.",
            "score": 0.4353487993929209,
            "section_title": "Language models",
            "char_start_offset": 2723,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1118
                }
            ],
            "ref_mentions": [
                {
                    "start": 339,
                    "end": 342,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 390,
                    "end": 393,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 488,
                    "end": 492,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 952,
                    "end": 956,
                    "matchedPaperCorpusId": "269498086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11474609375
        },
        {
            "corpus_id": "268681103",
            "title": "ToXCL: A Unified Framework for Toxic Speech Detection and Explanation",
            "text": "To optimize its performance, the decoder is finetuned using a Causal Language Modeling (CLM) loss:\n\nin which E = {e 1 , e 2 , ..., e n } is the set of tokens of the explanation, and k is the size of the window.Finally, we train the encoder-decoder model for the task of toxic speech detection and explanation by joining the two losses:\n\nin which \u03b1, \u03b2 are the contribution weights.",
            "score": 0.4353487993929209,
            "section_title": "Encoder-Decoder Model",
            "char_start_offset": 11710,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 100,
                    "end": 210
                },
                {
                    "start": 210,
                    "end": 335
                },
                {
                    "start": 337,
                    "end": 380
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0574951171875
        },
        {
            "corpus_id": "216914626",
            "title": "Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT",
            "text": "Recent prevalent pre-trained language models such as ELMo (Peters et al., 2018b), BERT (Devlin et al., 2018), and XLNet (Yang et al., 2019) achieve state-of-the-art performance for a diverse array of downstream NLP tasks. An interesting area of research is to investigate the interpretability of these pre-trained models (i.e., the linguistic properties they capture). Most recent approaches are built upon the idea of probing classifiers (Shi et al., 2016;Adi et al., 2017;Conneau et al., 2018;Peters et al., 2018a;Hewitt and Manning, 2019;Clark et al., 2019;Tenney et al., 2019b;Jawahar et al., 2019). A probe is a simple neural network (with a small additional set of parameters) that uses the feature representations generated by a pre-trained model (e.g., hidden state activations, attention weights) and is trained to perform a supervised task (e.g., dependency labeling). The performance of a probe is used to measure the quality of the generated representations with the assumption that the measured quality is mostly attributable to the pre-trained language model. \n\nOne downside of such approach, as pointed out in (Hewitt and Liang, 2019), is that a probe introduces a new set of additional parameters, which makes the results difficult to interpret. Is it the pretrained model that captures the linguistic information, or is it the probe that learns the downstream task itself and thus encodes the information in its additional parameter space? \n\nIn this paper we propose a parameter-free probing technique called Perturbed Masking to analyze and interpret pre-trained models. The main idea is to introduce the Perturbed Masking technique into the masked language modeling (MLM) objective to measure the impact a word x j has on predicting another word x i (Sec 2.2) and then induce the global linguistic properties (e.g., dependency trees) from this inter-word information. \n\nOur contributions are threefold: \u2022 We introduce a new parameter-free probing technique, Perturbed Masking, to estimate inter-word correlations. Our technique enables global syntactic information extraction.",
            "score": 0.4353487993929209,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1073
                },
                {
                    "start": 1076,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1456
                },
                {
                    "start": 1459,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1886
                },
                {
                    "start": 1889,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2095
                }
            ],
            "ref_mentions": [
                {
                    "start": 439,
                    "end": 457,
                    "matchedPaperCorpusId": "7197724"
                },
                {
                    "start": 474,
                    "end": 495,
                    "matchedPaperCorpusId": "24461982"
                },
                {
                    "start": 495,
                    "end": 516,
                    "matchedPaperCorpusId": "52098907"
                },
                {
                    "start": 516,
                    "end": 541,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 541,
                    "end": 560,
                    "matchedPaperCorpusId": "184486746"
                },
                {
                    "start": 560,
                    "end": 581,
                    "matchedPaperCorpusId": "108300988"
                },
                {
                    "start": 1125,
                    "end": 1149,
                    "matchedPaperCorpusId": "202538609"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.344482421875
        },
        {
            "corpus_id": "254327817",
            "title": "The Augmented Social Scientist: Using Sequential Transfer Learning to Annotate Millions of Texts with Human-Level Accuracy",
            "text": "Translating Indicators to Supervised Learning Tasks \n\nThese two tasks can be presented as two classic natural language processing operations. The policy/politics task is a classic NLP operation called text classification. In this case, it is performed at the level of each sentence. Every article in the training set is first split in sentences with a sentence tokenizer, before it is manually labeled either as \"Policy\", \"Politics\" or \"Other\" by an annotator (Figure 1a). The second task, the detection of prompts introducing \"off-the-record speech\", is another classic NLP task called sequence labeling, a technique which consists in classifying every token 10 of an article according to a binary code (in this case, \"off-the-record\" or \"not off-the-record\"). Sequence labeling enables us to get a fine-grained detection of an indicator in precise spans of text -generally below the sentence level. For this reason, it is sometimes called \"token classification\" (Figure 1b). In our case, each token of the sentence has exactly one label: 1 if the token is part of a segment introducing \"off-the-record\", 0 if not. 11  With a small, carefully hand coded sample, we aim to create a classifier that will replicate human behavior, and hopefully allow an automatic annotation of our vast corpus. By doing so, we resort to a supervised approach. Unlike many classic text analysis methods in the social sciences (such as clustering, semantic graphs or topic modeling), supervised learning consists in using human-annotated data. This small portion of a corpus serves as a supervisor for a model. \n\nIn mathematical terms, a model is a function f :\u0398 \u00d7 X Y, where \u0398 is the parameter space, X the input space, and Y the output space. In the first task, X is the set of the sentences in the corpus, and Y = {Policy, Politics, Other}. In a supervised setting, we have a human-annotated training corpus (X train , Y train ) and the test set (X test , Y test ). The supervised training process is the following. Let \u03b8 0 \u2208 \u0398 be the initial parameters.",
            "score": 0.4353487993929209,
            "section_title": "Methods",
            "char_start_offset": 14321,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 54,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1590
                },
                {
                    "start": 1593,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2037
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.036163330078125
        },
        {
            "corpus_id": "268041863",
            "title": "Automated Statistical Model Discovery with Language Models",
            "text": "Our framework is motivated by recent work in inductive reasoning with LMs (Qiu et al., 2024;Wang et al., 2024), integrating tools with LMs (Gao et al., 2023), and driving LMs via linguistic feedback (Shinn et al., 2023). \n\nAt a high level, we consider a method for learning probabilistic models from data that involves two steps: a model building step and a criticism step1 . Crucially, by learning a model, we mean searching over a space of model structures and not just learning the parameters of some fixed model class. In each step, we leverage LMs. In the proposal step, a proposal LM proposes probabilistic programs for a dataset. We then fit these probabilistic programs and evaluate them. \n\nIn the criticism step, we provide a critic LM with programs and statistics assessing model fit (e.g., model criticism statistics) and ask the critic LM to provide feedback to guide the next round of proposals. \n\nWe start with a dataset D = {x i , y i } n i=1 . Here x i \u2208 R d are fixed d-dimensional input values (e.g., features) and y i \u2208 R are the observations. Let \u03a3 be the vocabulary of the LM. For each dataset, we have an associated metadata set C \u2208 \u03a3 d+1 , which consists of natural language descriptions of D (e.g., animal ages vs length) and natural language descriptions of each feature in D (e.g., length of animal). Context informs how human modelers leverage prior knowledge; for example, if a modeler knows their dataset consists of monthly carbon dioxide measurements over a fifty-year time span, they will choose a model that can capture periodicity and a linear trend. Our goal is to find a probabilistic program z \u223c \u03a3 * that maximizes some notion of quality, which we take here to be either the log marginal likelihood or expected log predictive density (ELPD) estimated via cross validation (LOO) (Vehtari et al., 2017).",
            "score": 0.4353487993929209,
            "section_title": "Problem formulation",
            "char_start_offset": 9960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 223,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 696
                },
                {
                    "start": 699,
                    "end": 908
                },
                {
                    "start": 911,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1838
                }
            ],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 92,
                    "matchedPaperCorpusId": "263909078"
                },
                {
                    "start": 92,
                    "end": 110,
                    "matchedPaperCorpusId": "261696510"
                },
                {
                    "start": 139,
                    "end": 157,
                    "matchedPaperCorpusId": "253708270"
                },
                {
                    "start": 199,
                    "end": 219,
                    "matchedPaperCorpusId": "258833055"
                },
                {
                    "start": 1815,
                    "end": 1837,
                    "matchedPaperCorpusId": "36568061"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52783203125
        },
        {
            "corpus_id": "233024725",
            "title": "Improving Pretrained Models for Zero-shot Multi-label Text Classification through Reinforced Label Hierarchy Reasoning",
            "text": "Our base model adopts pretrained models M, e.g., BERT (Devlin et al., 2018), which have proven to be effective in matching modelling. Given the input text T and the label l, we follow Yin et al. (2019) by transforming the text-label pair into textual entailment representation as \"[CLS] T [SEP] hypothesis of l\". The hidden vector v cls of [CLS] is regarded as the aggregate representation and will be used in the classification layer to calculate the matching score ms. The overall calculation process of ms is abbreviated as:\n\nIf ms \u2265 \u03b3 where \u03b3 is a threshold, we then say T belongs to label l. In experiments \u03b3 is set to be 0.5.",
            "score": 0.4353487993929209,
            "section_title": "Base Model",
            "char_start_offset": 9382,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 201,
                    "matchedPaperCorpusId": "202540839"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02447509765625
        },
        {
            "corpus_id": "275800654",
            "title": "Hate Speech Detection Using Large Language Models: A Comprehensive Review",
            "text": "Large language models, such as BERT and GPT-3, have significantly advanced the field of NLP. These models are particularly well-suited for tasks that require deep contextual understanding, such as hate speech detection. The ability of LLMs to process and interpret vast amounts of text data enables them to identify potentially harmful content with a higher degree of accuracy compared to earlier machine learning models. In hate speech detection, LLMs are employed to scan text from diverse sources-including social media platforms like Twitter, Reddit, etc., as well as news websites-and identify language that could be classified as hate speech. Their capacity to understand context, syntax, and semantics allows these models to detect not only explicit hate speech but also more subtle forms of harmful language.",
            "score": 0.4353487993929209,
            "section_title": "A. THE ROLE OF LLMS IN HATE SPEECH DETECTION",
            "char_start_offset": 27428,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 816
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.424072265625
        },
        {
            "corpus_id": "261101101",
            "title": "An Information-Theoretic Method for Detecting Edits in AI-Generated Text",
            "text": "We conducted extensive simulations using publicly available datasets and new datasets that we created. The new datasets and the code for obtaining all the results are available in the link at the end of this paper. \n\nWe tried several publicly available LMs for the detection model P in Algorithm 1, including GPT2 1.5B parameters [RWC + 19], Falcon 7B parameters [AAA + 23], Llama 7B parameters [TLI + 23], and Phi2 2.7B parameters [JBA + 23]. We only report on results with GPT2 1.5B parameters (aka. GPT2xl) and Phi2 since these models attained the highest area under the ROC curve in the binary detection problem of individual sentences for all datasets we considered. We discuss in Section 5 the open challenge of selecting or crafting P with optimal detection properties. \n\nWe experimented with data created by the GLMs GPT3-curie and GPT3.5-turbo (ChatGPT) arranged in 5 datasets as we explain in detail below. We tried to generate data using publicly available GLMs not in the GPT family, but they did not produce articles of satisfactory quality. \n\nIn the sections below we report the method's performance under different settings and data.",
            "score": 0.4353487993929209,
            "section_title": "Empirical Results",
            "char_start_offset": 16480,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 214
                },
                {
                    "start": 217,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 776
                },
                {
                    "start": 779,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1054
                },
                {
                    "start": 1057,
                    "end": 1148
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1929931640625
        },
        {
            "corpus_id": "276248915",
            "title": "Induced Model Matching: Restricted Models Help Train Full-Featured Models",
            "text": "Consider the logistic regression example from the introduction. The main results are given in Figure 1 and the full experimental details can be found in Appendix D.1. Here we highlight how the problem fits the IMM framework and where it deviates from language modeling. First, note that the context decomposition is  =  1 and  = ( 2 ,  3 ). \n\nSetting We sample features uniformly over a cube and assume we have ample data points of the form ( 1 , ). This allows us to build an excellent restricted model to predict the label based on just  1 , call it  (| 1 ), nearly close to the (restricted) Bayes predictor or true conditional probability. Just like in language modeling, to induce a model we need to draw 's from its conditional distribution given .  (| 1 ), the induced model of  (| 1 ,  2 ,  3 ), can then be interpreted as the average of 's predictions, when   = ( 2 ,  3 ) is drawn from its conditional distribution given   =  1 . Since we typically don't have access to this distribution, we approximate it empirically. In language modeling, we could just sample from the empirical distribution of  for a given . In logistic regression, this is not viable since  1 is continuous and does not repeat. We rely instead on density estimation. We use a soft nearest-neighbor density estimate f \n\n, where 1/ is the bandwidth of the Laplace kernel. (With cross-validation, we determine  = 1 to be a good choice.) If we let   ( 1 ) = e \u2212 | 1, \u2212 1 | , the resulting induced model by marginalization is: \n\nThese equations are respectively equivalent to Eqs. ( 4) and ( 6). The IMM risk and the corresponding overall objective remain the same: \n\n, Cross-Entropy() +  IMM().",
            "score": 0.4353487993929209,
            "section_title": "Starting Toy Example: Logistic Regression",
            "char_start_offset": 24177,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 340
                },
                {
                    "start": 343,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1297
                },
                {
                    "start": 1300,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1502
                },
                {
                    "start": 1505,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1641
                },
                {
                    "start": 1644,
                    "end": 1671
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.023773193359375
        },
        {
            "corpus_id": "264439148",
            "title": "Do Stochastic Parrots have Feelings Too? Improving Neural Detection of Synthetic Text via Emotion Recognition",
            "text": "All experiments so far have involved PLMs pretrained with the self-supervised objective of predicting the next token or a masked token. We conduct a final experiment with ChatGPT, a more human- aligned Large Language Model (LLM) which has undergone a second training or \"alignment\" phase using Reinforcement Learning from Human Feedback on top of an underlying LLM (GPT 3.5 in our case) (OpenAI, 2022;Ouyang et al., 2022). We create a custom dataset comprising human articles and ChatGPT synthetic text from multiple non-news domains, and use it to compare our BERTsynth and emoBERTsynth models against ChatGPT (in a zeroshot setting) on the task of detecting ChatGPT's own synthetic text.7 \n\nChatGPT100 We create and release Chat-GPT100 -a dataset comprising human articles and synthetic articles generated by ChatGPT. Following Clark et al. (2021) who collected 50 human articles and generated 50 articles using GPT2 and GPT3, we also collect 50 human articles, and we then use ChatGPT to generate 50 synthetic ones. The human written articles are from 5 different domains: Science, Entertainment, Sport, Business, and Philosophy. We used reputable websites for the human text which was gathered manually, see Table 8 in Appendix B.3. The synthetic text was generated by providing ChatGPT with a prompt such as \"In less than 400 words, tell me about moral philosophy.\" where human text on the same topic, moral philosophy in this case, had already been found online. The data generated by ChatGPT is semantically correct and was checked manually. Subject areas in which the authors are knowledgeable were chosen so that the correctness of the synthetic text could be checked. To be comparable with the detectors presented in our earlier experiments, the articles were limited to a maximum of 384 words (\u2248 512 tokens) and truncated at a natural sentence boundary. The two articles were then made to be approximately the same length. Detection task Each article was appended to the following prompt to ChatGPT: \"Was the following written by a human or a computer, choose human or computer only?\"",
            "score": 0.4351404394316018,
            "section_title": "ChatGPT Experiments",
            "char_start_offset": 22755,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 690
                },
                {
                    "start": 693,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2095
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18408203125
        },
        {
            "corpus_id": "254854418",
            "title": "Rainproof: An Umbrella To Shield Text Generators From Out-Of-Distribution Data",
            "text": "Significant progress has been made in Natural Language Generation (NLG) in recent years with the development of powerful generic (e.g., GPT (Radford et al., 2018;Brown et al., 2020;Bahrini et al., 2023), LLAMA (Touvron et al., 2023) and its variants) and task-specific (e.g., Grover (Zellers et al., 2019), Pegasus (Zhang et al., 2020) and Dialog-GPT (Zhang et al., 2019b)) text generators. They power machine translation (MT) systems or chatbots that are exposed to the public, and their reliability is a prerequisite for adoption. Text generators are trained in the context of a so-called closed * maxime.darrin@mila.quebec \u2020 ILLS -International Laboratory on Learning Systems \u2021 Math\u00e9matiques et Informatique Centralesupelec world (Fei and Liu, 2016), where training and test data are assumed to be drawn i.i.d. from a single distribution, known as the in-distribution. However, when deployed, these models operate in an open world (Parmar et al., 2021;Zhou, 2022) where the i.i.d. assumption is often violated. Changes in data distribution are detrimental and induce a drop in performance. It is necessary to develop tools to protect models from harmful distribution shifts as it is a clearly unresolved practical problem (Arora et al., 2021). For example, a trained translation model is not expected to be reliable when presented with another language (e.g. a Spanish model exposed to Catalan, or a Dutch model exposed to Afrikaans) or unexpected technical language (e.g., a colloquial translation model exposed to rare technical terms from the medical field). They also tend to be released behind API (OpenAI, 2023) ruling out many usual features-based OOD detection methods. \n\nMost work on Out-Of-Distribution (OOD) detection focus on classification, leaving OOD detection in (conditional) text generation settings mainly unexplored, even though it is among the most exposed applications. Existing solutions fall into two categories.",
            "score": 0.4340852210841446,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1680
                },
                {
                    "start": 1683,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 1939
                }
            ],
            "ref_mentions": [
                {
                    "start": 162,
                    "end": 181,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 283,
                    "end": 305,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 315,
                    "end": 335,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 733,
                    "end": 752,
                    "matchedPaperCorpusId": "16956133"
                },
                {
                    "start": 955,
                    "end": 966,
                    "matchedPaperCorpusId": "249240421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1826171875
        },
        {
            "corpus_id": "249375501",
            "title": "Findings of the The RuATD Shared Task 2022 on Artificial Text Detection in Russian",
            "text": "1. foundation model misuse, including both purposeful generated text misuse and the unconditional reliance on automatic text classification results that can be false negative; 2. development of legal grounds to mitigate generative model misuse and detection model misuse; 3. widespread deployment of automatic text detection systems: the presented models can lead to an \"arms race\" between malicious content generators and detectors. Although the improvement of language modeling is undoubtedly a fundamental task of machine learning, we are of the position that a thorough study of models that classify automatic texts is necessary. As practice shows, the percentage of their errors in the Russian language is non-zero. \n\nMany research efforts are related to natural language generation (NLG) models. These works can be characterized into two broad categories -(i) training LMs on large-scale data and (ii) learning to distinguish between machine-authored and human-written content. (Jawahar et al., 2020) provides a good survey on the automatic detection of machine-generated text for English. \n\nPrior work has focused on training classifiers on samples from a model (Brown et al., 2020) and directly using a model distribution (Gehrmann et al., 2019). (Gehrmann et al., 2019) propose a visual and statistical tool named GLTR for the detection of generation artifacts across different sampling schemes. (Ippolito et al., 2020) compare human raters and automatic classifiers depending on the decoding strategy. They observe that classifiers can detect statistical artifacts of generated sequences while humans quickly notice semantic errors. Classifier accuracy ranges between 70% and 90% depending on the decoding strategy for short texts (64 tokens). (Dugan et al., 2020) propose a RoFT (Real or Fake Text) tool to detect the boundary between a human-written text passage and machine-generated sentences showing NLG models are capable of fooling humans by one or two sentences. A recent study of (Gall\u00e9 et al., 2021) focuses on the unsupervised detection of machine-generated documents leveraging repeated higher-order n-grams. They show that specific well-formed phrases over-appear in machine-generated texts as compared to human ones.",
            "score": 0.43360108588329976,
            "section_title": "Discussion",
            "char_start_offset": 20149,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 720
                },
                {
                    "start": 723,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1095
                },
                {
                    "start": 1098,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2130
                },
                {
                    "start": 2131,
                    "end": 2240
                }
            ],
            "ref_mentions": [
                {
                    "start": 984,
                    "end": 1006,
                    "matchedPaperCorpusId": "226237099"
                },
                {
                    "start": 1169,
                    "end": 1189,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1230,
                    "end": 1253,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 1255,
                    "end": 1278,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 1405,
                    "end": 1428,
                    "matchedPaperCorpusId": "218560609"
                },
                {
                    "start": 1754,
                    "end": 1774,
                    "matchedPaperCorpusId": "221854248"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.431884765625
        },
        {
            "corpus_id": "273501874",
            "title": "Guardians of Discourse: Evaluating LLMs on Multilingual Offensive Language Detection",
            "text": "Figure 3 illustrates the comparison between the previous SOTA methods and the best results during the entire evaluation process. Generally, LLMs achieved comparable or better performance in the task. The detailed results and findings will be explained in the following sections. \n\nA. Detection with Monolingual Data a) Model Performance Comparison: Table II illustrates the experimental results obtained from the LLMs fine-tuned from monolingual data. Generally, the performance of finetuned LLMs was comparable but could not surpass the SOTA methods except for the German dataset. From the results, we had the following observations: \n\nFirst, Mistral achieved the best performance in English and Spanish offensive language detection; however, GPT-3.5 outperformed Mistral in the German context. In the detection of English and Spanish offensive languages, LLMs achieved a lower precision, a higher recall, and a higher accuracy in offensive content, while a reversed case occurred in the German context. This phenomenon was attributed to the distribution of training data-the ratio between \"Offensive\" and \"Not offensive\" was 1 : 1 in English and Spanish training data during data sampling, but the entire training set of the German dataset was used, whose ratio was around 1 : 2.",
            "score": 0.43321461712743287,
            "section_title": "IV. RESULTS AND ANALYSIS",
            "char_start_offset": 8447,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 278
                },
                {
                    "start": 281,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1281
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06536865234375
        },
        {
            "corpus_id": "4995613",
            "title": "Using web text to improve keyword spotting in speech",
            "text": "For low resource languages, collecting sufficient training data to build acoustic and language models is time consuming and often expensive. But large amounts of text data, such as online newspapers, web forums or online encyclopedias, usually exist for languages that have a large population of native speakers. This text data can be easily collected from the web and then used to both expand the recognizer's vocabulary and improve the language model. One challenge, however, is normalizing and filtering the web data for a specific task. In this paper, we investigate the use of online text resources to improve the performance of speech recognition specifically for the task of keyword spotting. For the five languages provided in the base period of the IARPA BABEL project, we automatically collected text data from the web using only Limited LP resources. We then compared two methods for filtering the web data, one based on perplexity ranking and the other based on out-of-vocabulary (OOV) word detection. By integrating the web text into our systems, we observed significant improvements in keyword spotting accuracy for four out of the five languages. The best approach obtained an improvement in actual term weighted value (ATWV) of 0.0424 compared to a baseline system trained only on LimitedLP resources. On average, ATWV was improved by 0.0243 across five languages.",
            "score": 0.43287001985631623,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.181640625
        },
        {
            "corpus_id": "273963898",
            "title": "Toward Transdisciplinary Approaches to Audio Deepfake Discernment",
            "text": "Issues surrounding validity are further aggravated by methods that rely upon replacing data from the real world with information derived from generative models-not only generated data but also generated code, all of which carry the risk of unintended consequences. Although models may be following a mathematical or probabilistic method of generative data from sample distributions, they cannot account for the inherent complexities in situations where data cannot be modeled based only on a few samples. Such models also suffer from lack of verifiability and interpretability. Put simply, we cannot accurately model the complexity of the dynamics of the ocean based on a few sampled waves. These insights inform audio deepfake detection as a use 2 case, since most detection algorithms do not perform well because they are unable to fully capture the uniqueness, variability, and complexity of human language. \n\nThe Importance of Understanding Language in AI-Based Language Models AI-based language models (such as Natural Language Processing (NLP), Large Language Models (LLM)), are facing a crisis of validity along multiple dimensions. First, existing models often incorrectly generalize, based on reductive training data derived primarily from one language, English. As computational linguist Emily Bender notes, \"natural language is not a synonym for English\" (12). Most training data and major research in NLP focuses so overwhelmingly on English to the point that it is viewed as the default. Yet, English should not be viewed, or used, as a stand-in for the world's approximately 7,000 other languages. Among other particularities, English cannot generalize to non-written languages, to signed languages, to languages that use logographic systems such as Chinese or Japanese kanji, or to languages that use non-Latin scripts such as Arabic, Cyrillic, and Hebrew; there is also a vast amount of training data available for English but not for other languages, which skews methods and results (12). As with many other social dimensions, a lack of linguistic diversity in inputs leads to a homogeneity of outputs-including one-dimensional AI detection models with flawed underlying distributions. \n\nExisting AI-based language models also do not capture the inherent variability of language (13).",
            "score": 0.4326797460395944,
            "section_title": "Generative AI: Boon and Bane",
            "char_start_offset": 5425,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 910
                },
                {
                    "start": 913,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2202
                },
                {
                    "start": 2205,
                    "end": 2301
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27783203125
        },
        {
            "corpus_id": "271516194",
            "title": "Vulnerability Detection in Ethereum Smart Contracts via Machine Learning: A Qualitative Analysis",
            "text": "Sequential data are also the core of Natural Language Processing (NLP) [56], which broadens ML's reach, enabling computers to understand and generate human language. NLP has a wide range of applications, like language recognition, text analysis, and translation. A particular type of NPL is Large Language Models (LLM), large networks trained with vast amounts of text that can learn statistical distributions and can be used for many tasks. One of the first examples is BERT (Bidirectional Encoder Representations from Transformers), which has transformed language understanding by capturing word context bidirectionally [26]. LLMs are also closely related to generative models, like Generative Adversarial Networks (GANs) and autoencoders, which create realistic data samples [72]. The most notable case is OpenAI ChatGPT, which can generate not only text but also audio and images in real-time. \n\nFinally, another type of architecture is gaining popularity. It is the case of Graph Neural Networks (GNNs), which excel in tasks involving graph-structured data, making them valuable for social network analysis and recommendation systems [105]. In the context of vulnerability detection in Ethereum smart contracts, many of these models have been utilized. Detection tasks often involve supervised classification, where models are trained on labeled contracts for binary or multi-class prediction.",
            "score": 0.4322117119818752,
            "section_title": "Machine Learning Techniques",
            "char_start_offset": 34042,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 897
                },
                {
                    "start": 900,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1398
                }
            ],
            "ref_mentions": [
                {
                    "start": 1139,
                    "end": 1144,
                    "matchedPaperCorpusId": "226246289"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4052734375
        },
        {
            "corpus_id": "232290720",
            "title": "Addressing Hate Speech with Data Science: An Overview from Computer Science Perspective",
            "text": "Model is an artefact created during machine learning that is able to perform a task, in our case hate speech detection. A hate speech detection model takes feature representation of a content as input, and it outputs a prediction about the hate speech contained within the sample. Models are trained iteratively by being exposed to annotated samples. Model tries to learn from these samples by updating its inner working so it can correctly predict whether the samples contain hate speech or not. Model trained this way can be consequently applied to previously unseen samples to obtain expected predictions. \n\nLinear models are models that are trying to establish a so-called linear boundary between samples that contain and do not contain hate speech. This boundary is a hyperplane that lies in the feature space. Training of models like these is the process of moving this boundary in this space so that samples with hate speech lie on one side of this boundary while the samples without hate speech lie on the other side. Support vector machines are a popular extension that can work even with linearly inseparable feature spaces by adding additional dimensions. \n\nDecision trees are machine learning models that learn to create a sequence of decisions that lead to a prediction. The decisions are iteratively formed and linked to create a logical sequence. \n\nNeural models are the recently popular machine learning models based on an idea of abstracting the computational model of biological neurons. Multiple (up to millions) of artificial neurons form a so-called neural networka trainable model that make predictions from features and learn from the data using gradient-based methods of optimization. They also support pre-training of parameters; a transfer learning is a technique where models trained on other tasks (most notably language modelling) are later repurposed for other tasks, such as hate speech detection. Neural models are notoriously uninterpretable, i.e. we cannot really say why the neural network decided to flag a text as hate speech. However, various approaches improving the interpretability were proposed already (\u0160vec et al., 2018). \n\nAll the models we mentioned before are used for hate speech detection as well.",
            "score": 0.43157554736016335,
            "section_title": "Models",
            "char_start_offset": 24883,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1166
                },
                {
                    "start": 1169,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1361
                },
                {
                    "start": 1364,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2063
                },
                {
                    "start": 2064,
                    "end": 2165
                },
                {
                    "start": 2168,
                    "end": 2246
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06573486328125
        },
        {
            "corpus_id": "261312230",
            "title": "Question Answering Versus Named Entity Recognition for Extracting Unknown Datasets",
            "text": "The dataset detection step from Figure 1 is materialized as a binary classifier. We fine-tune the base version of the language models BERT, RoBERTa, SciBERT, and DeBERTa (cf. Section II-B) in binary classification mode. After that, the effect of tokenization is tested on the best-performing model by extending its vocabulary and replacing its tokenizer. In addition, the model is trained using BFL (see Section III-A). Furthermore, the SimCSE version of that model is used to test the impact of the contrastive loss effect. \n\nWe experiment with various text representations like TF-IDF, BERT-mean, PCA [38], and t-SNE [39] for the input contexts with different models like SVM [13] and MLP-2. MLP-2 is a two-layer perceptron with dropout and ReLU activation function as expressed in Eq. (1). It is motivated by Galke et al. [40], which showed that a wide MLP with a large hidden layer size is a strong baseline model and even 92778 VOLUME 11, 2023 Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply. outperforms text classification models like TextGCN [41]. \n\nHere, X is the input embedding of n contexts each represented by a vector of d dimensions. We set d = 768 to match the size of the BERT embeddings, since BERT-mean is used as input. The matrices W 1 and W2 are the weights for the first and second layers, respectively. We aim to improve generalizability by over-parametrization, so we choose g = 1, 024. The expanded vectors are passed through the ReLU activation function before it is dropped out with a probability of 0.5. After that, the second layer's weight W 2 transforms the input to feed it into the Sigmoid function, which converts the logits into class probabilities, to produce the output y. \n\nDuring training, Balanced Focal Loss is used to counter the imbalance nature of the data.",
            "score": 0.4313128542359022,
            "section_title": "C. MODEL FOR DATASET DETECTION",
            "char_start_offset": 18010,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 524
                },
                {
                    "start": 527,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1118
                },
                {
                    "start": 1121,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1773
                },
                {
                    "start": 1776,
                    "end": 1865
                }
            ],
            "ref_mentions": [
                {
                    "start": 603,
                    "end": 607,
                    "matchedPaperCorpusId": "2636133"
                },
                {
                    "start": 619,
                    "end": 623,
                    "matchedPaperCorpusId": "5855042"
                },
                {
                    "start": 788,
                    "end": 791,
                    "matchedPaperCorpusId": "8755162"
                },
                {
                    "start": 1113,
                    "end": 1117,
                    "matchedPaperCorpusId": "252389548"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.135498046875
        },
        {
            "corpus_id": "272063022",
            "title": "A Proposed Model for Distinguishing Between Human-Based and ChatGPT Content in Scientific Articles",
            "text": "The final strategy involves the process of fine-tuning an existing language model. For instance, in the study conducted by Zeller et al., [6] they fine-tuned a linear layer to discern whether the input was generated by the GROVE\\R model or by a human, utilizing the hidden states in the encoder of GROVER. Similarly, Solaiman et al. [1], [3] and their team fine-tuned a pre-trained RoBERTa model with a labeled dataset, developing a content detection model that achieved a remarkable state-of-the-art performance, achieving an accuracy rate of 90% in identifying text generated by GPT-2. Nevertheless, it's worth noting that the supervised learning approach necessitates a substantial quantity of labeled data, which distinguishes it from the methods discussed earlier. The fine-tuned RoBERTa model by Solaiman and collaborators required a dataset of 200,000 labeled instances for training. Additionally, Yutian et al. [7] introduced a novel approach for distinguishing between text generated by ChatGPT and human-authored content, using T5 and RoBERTa models. This approach demonstrated an accuracy rate exceeding 97% on a sizable dataset that contained content generated through ChatGPT but had been rephrased. The study conducted by Jawahar et al. has delineated five fundamental attributes that a cutting-edge detector designed for evaluating content generated by Large Language Models (LLMs) should encompass. These attributes encompass accuracy, data efficiency, generalizability, and interpretability [1]. Accuracy denotes the capacity of the model to effectively differentiate between text produced by LLMs and text composed by humans, while concurrently achieving a suitable balance between precision and recall rates. Data efficiency implies that the detector should be proficient in operating with minimal instances drawn from the LLM. Generalizability implies that the detector should consistently perform across different variations in LLM architecture, prompt length, and training datasets. Interpretability signifies the detector's ability to furnish lucid explanations for the rationale underlying its decisions. These five principles serve as a guiding framework in the development of GPT-Sentinel as shown in Table 1.",
            "score": 0.4312266221601373,
            "section_title": "C. FINE-TUNING LANGUAGE MODEL",
            "char_start_offset": 4827,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2128
                },
                {
                    "start": 2129,
                    "end": 2235
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.484619140625
        },
        {
            "corpus_id": "243865331",
            "title": "TADPOLE: Task ADapted Pre-Training via AnOmaLy DEtection",
            "text": "Language model and Downstream Tasks. A generic approach for using state-of-the-art language models such as ELMo, GPT, BERT, and XLNet is to pre-train them on an extremely large general domain corpus and then finetune the pretrained model on the downstream labeled task data. \n\nThere is evident correlation between model's pretraining loss and its performance on the downstream task after finetuning (Devlin et al., 2018). Our design is motivated by an observation, backed by empirical evidence, that the correlation is even stronger if we consider the pre-training loss not on the pre-training data but the downstream task data. \n\nTo make this distinction formal, let D, D in be the pre-training and task data. Let \u0398 denote the parameters of the language model and LM denote the language model loss function. The pre-training loss on pre-training data (L LM (\u0398)) and target data (L in LM (\u0398)) are defined as follows: \n\nis better correlated with the performance of the downstream task we consider several BERT language models pre-trained on random combinations of datasets from different domains mentioned in Section 4. Among these we select the BERT models M 1 , . . . , M k such that selected models have similar Masked Language Model (MLM) loss on the general domain corpus; | LM (\u0398 i ) \u2212 LM (\u0398 j )| <= 0.02. MLM loss of these models on the text from task domain ( in LM (\u0398 i )) is different. For each M i , we contrast it with the accuracy/f1 score of the finetuned model on the task data. \n\nWe observe in Figure 3 that if we have two equally good language models (similar language model loss on general domain), the language model which is better (tailored) for the task domain has significantly better downstream performance. We \n\nwhere \u03bb(x, D in ) are relevance weights of instance \n\nand (close to) 0 otherwise. We compute these weights using an anomaly detection model fitted on D in . Note that concept of weighted loss to handle noisy or irrelevant data is well known (Moore and Lewis, 2010;Wang et al., 2017a).",
            "score": 0.43118325799668844,
            "section_title": "TADPOLE: Task ADapted Pre-training via anOmaLy dEtection",
            "char_start_offset": 11934,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 37,
                    "end": 274
                },
                {
                    "start": 277,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 628
                },
                {
                    "start": 631,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 916
                },
                {
                    "start": 919,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1492
                },
                {
                    "start": 1495,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1733
                },
                {
                    "start": 1736,
                    "end": 1787
                },
                {
                    "start": 1790,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2020
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.303955078125
        },
        {
            "corpus_id": "247957851",
            "title": "The COVMis-Stance dataset: Stance Detection on Twitter for COVID-19 Misinformation",
            "text": "The NLI task is similar to ours but still in different domains, so we further fine-tune the models on two stance detection datasets, i.e. RumourEval and COVIDLies. Both datasets target rumors and have social media text. However, their class distributions are quite different from our dataset. Therefore, we adopt the following strategies for this problem: \n\nThe COVMis-Stance dataset  \u2022 Rescaling class weights [1]: This method takes the cost of prediction error into account. Specifically, the misclassification of the minority class will be penalized heavier than the majority class. The class weight w k is given by w k = max(x)/x k , where x is the vector with the class counts. \n\n\u2022 Undersampling: we remove some examples from the majority classes of both datasets. Specifically, we keep all Against examples and randomly select examples from Favor and Neither with a fixed probability, which is equal to the expected sample number by the class frequency. The expected sample number is 400 for COVIDLies and 550 for RumourEval. After undersampling, a combination of both balanced datasets is also used in our experiment. Table 10 shows the statistics of the balanced datasets after undersampling.",
            "score": 0.4310919816026011,
            "section_title": "Target domain",
            "char_start_offset": 37465,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 355
                },
                {
                    "start": 358,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 682
                },
                {
                    "start": 685,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1200
                }
            ],
            "ref_mentions": [
                {
                    "start": 411,
                    "end": 414,
                    "matchedPaperCorpusId": "26538892"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.010406494140625
        },
        {
            "corpus_id": "2717698",
            "title": "Catching the Drift: Probabilistic Content Models, with Applications to Generation and Summarization",
            "text": "Distributional models of content have appeared with some frequency in research on text segmentation and topic-based language modeling (Hearst, 1994;Beeferman et al., 1997;Chen et al., 1998;Florian and Yarowsky, 1999;Gildea and Hofmann, 1999;Iyer and Ostendorf, 1996;Wu and Khudanpur, 2002). In fact, the methods we employ for learning content models are quite closely related to techniques proposed in that literature (see Section 3 for more details). \n\nHowever, language-modeling research -whose goal is to predict text probabilities -tends to treat topic as a useful auxiliary variable rather than a central concern; for example, topic-based distributional information is generally interpolated with standard, non-topic-based n-gram models to improve probability estimates. Our work, in contrast, treats content as a primary entity. In particular, our induction algorithms are designed with the explicit goal of modeling document content, which is why they differ from the standard Baum-Welch (or EM) algorithm for learning Hidden Markov Models even though content models are instances of HMMs.",
            "score": 0.4310919816026011,
            "section_title": "Knowledge-lean approaches",
            "char_start_offset": 5554,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 451
                },
                {
                    "start": 454,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1096
                }
            ],
            "ref_mentions": [
                {
                    "start": 134,
                    "end": 148,
                    "matchedPaperCorpusId": "796701"
                },
                {
                    "start": 148,
                    "end": 171,
                    "matchedPaperCorpusId": "6387310"
                },
                {
                    "start": 171,
                    "end": 189,
                    "matchedPaperCorpusId": "2317833"
                },
                {
                    "start": 189,
                    "end": 216,
                    "matchedPaperCorpusId": "9938621"
                },
                {
                    "start": 216,
                    "end": 241,
                    "matchedPaperCorpusId": "5703318"
                },
                {
                    "start": 241,
                    "end": 266,
                    "matchedPaperCorpusId": "16217683"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2093505859375
        },
        {
            "corpus_id": "275212480",
            "title": "U-GIFT: Uncertainty-Guided Firewall for Toxic Speech in Few-Shot Scenario",
            "text": "The elements that play a role throughout the detection process can be broadly divided into three main parts: the pre-trained language model(PLM), the semi-supervised learning framework(SSF), and the U-GIFT method integrated within them. To validate the shared methods presented in this paper, we will respectively investigate the roles of these three components.",
            "score": 0.4310919816026011,
            "section_title": "Ablation expirements",
            "char_start_offset": 37488,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 362
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1868896484375
        },
        {
            "corpus_id": "268889777",
            "title": "Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models",
            "text": "The problem of pre-training data detection for large language models (LLMs) has received growing attention due to its implications in critical issues like copyright violation and test data contamination. Despite improved performance, existing methods (including the state-of-the-art, Min-K%) are mostly developed upon simple heuristics and lack solid, reasonable foundations. In this work, we propose a novel and theoretically motivated methodology for pre-training data detection, named Min-K%++. Specifically, we present a key insight that training samples tend to be local maxima of the modeled distribution along each input dimension through maximum likelihood training, which in turn allow us to insightfully translate the problem into identification of local maxima. Then, we design our method accordingly that works under the discrete distribution modeled by LLMs, whose core idea is to determine whether the input forms a mode or has relatively high probability under the conditional categorical distribution. Empirically, the proposed method achieves new SOTA performance across multiple settings. On the WikiMIA benchmark, Min-K%++ outperforms the runner-up by 6.2% to 10.5% in detection AUROC averaged over five models. On the more challenging MIMIR benchmark, it consistently improves upon reference-free methods while performing on par with reference-based method that requires an extra reference model.",
            "score": 0.4310919816026011,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74365234375
        },
        {
            "corpus_id": "237532173",
            "title": "Language Models are Few-shot Multilingual Learners",
            "text": "Let us define D as the distribution over the dataset and P as the prompt that we use as the input of the LM \u03b8. The prompt P = [D pos , D neg , Q] is a concatenation of few-shot samples: positive samples D pos , negative samples D neg , and the query Q, where D pos , D neg \u223c D. D pos is a sample with a label that is the same as the query, and D neg is a sample that is taken from the dataset D with a label other than the query. \u03b8 takes P as the input of the model, and the LM generates a word y. We define the task T s\u2192t , where s is the source language and t is the target language. \n\nIn this paper, we focus on the intent detection task in the monolingual and cross-lingual settings. In the monolingual setting, the source language is the same as the target language, and in the crosslingual setting, we take the source language as different from the target language (s = t). We design our task as a multiple-choice problem, in which each sample has a label l \u2208 L, where L is the set of possible labels. We predict the boolean (true or false) for each sample and take the highest prediction confidence.",
            "score": 0.4310919816026011,
            "section_title": "Notation and Tasks",
            "char_start_offset": 6682,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 585
                },
                {
                    "start": 588,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1106
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1395263671875
        },
        {
            "corpus_id": "265261266",
            "title": "Black Box Warning: Large Language Models and the Future of Infectious Diseases Consultation",
            "text": "Large language models are machine-learning algorithms trained to recognize and predict linguistic patterns (see Table 1 for a glossary of relevant terms). Most of this learning is selfsupervised and gleaned from internet texts, including websites, social media forums, and open-access code, books, and articles. Many LLMs subsequently undergo supervised training (eg, reinforcement learning with human feedback [RLHF]), during which the model learns from human ratings of its responses. \n\nGeneral-purpose LLMs like OpenAI's GPT-3 and GPT-4, Google's BERT, and Meta's LLaMA-sometimes called foundation models-have been trained on massive quantities of text without specific domain (eg, biomedical) training [3].",
            "score": 0.4310919816026011,
            "section_title": "HOW LARGE LANGUAGE MODELS WORK",
            "char_start_offset": 1758,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 486
                },
                {
                    "start": 489,
                    "end": 710
                }
            ],
            "ref_mentions": [
                {
                    "start": 706,
                    "end": 709,
                    "matchedPaperCorpusId": "257687466"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.128173828125
        },
        {
            "corpus_id": "272969030",
            "title": "LML-DAP: LANGUAGE MODEL LEARNING A DATASET FOR DATA-AUGMENTED PREDICTION",
            "text": "The system proposed in this paper, \"Language Model Learning (LML),\" powered by \"Data-Augmented Prediction (DAP),\" has demonstrated promising results in generating classifications during the experiment across different datasets using different Large Language Models (LLMs). The system classification accuracies exceeded 80% or sometimes even 90% in multiple test cases, highlighting the potential of outperforming conventional Machine Learning (ML) models. The accuracy is due to LML, which involves the use of LLMs to summarize the datasets, and DAP, which involves augmenting the prediction process with relevant rows from the training data and the data summary. This method offers a new approach to enable interpretability of the classification process with an accuracy that resembles or exceeds that of ML classification across multiple test cases. \n\nThe LML system introduces a new paradigm in data-driven classification by combining an LLM's summarization abilities with DAP, improving the way for further research in the intersection of Natural Language Processing and ML. Although intended for classification tasks, the potential of LML extends beyond current applications. In future research, the system can be adapted for numerical prediction tasks, such as time-series forecasting. Ongoing research addresses the current limitations of the resources and time required to obtain responses from LLMs.",
            "score": 0.4310919816026011,
            "section_title": "Conclusions",
            "char_start_offset": 16541,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 851
                },
                {
                    "start": 854,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1408
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.405517578125
        },
        {
            "corpus_id": "15059903",
            "title": "Detecting concept mentions in biomedical text using hidden Markov model: multiple concept types at once or one at a time?",
            "text": "In this strategy, a single detection task was assumed for all the target concept types. For example, given the three concept types in the i2b2/VA corpus, one HMM model was built using the seven labels, B_{Problem, Treatment, Test}, I_{Problem, Test, Treatment}, and O.",
            "score": 0.4310919816026011,
            "section_title": "All concept types at once",
            "char_start_offset": 5644,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 268
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.033660888671875
        },
        {
            "corpus_id": "1473",
            "title": "Similarity-Based Models of Word Cooccurrence Probabilities",
            "text": "Our main contributions are a general scheme for using word similarity to improve the probability estimates of back-off models, and a comparative analysis of several similarity measures and parameter settings in two important language processing tasks, language modeling and disambiguation, showing that similarity-based estimates are indeed useful. \n\nIn our initial study, a language-model evaluation, we used a similarity-based model to estimate unseen bigram probabilities for Wall Street Journal text and compared it to a standard back-off model (Katz, 1987). Testing on a held-out sample, the similarity model achieved a 20% perplexity reduction over back-off for unseen bigrams. These constituted 10.6% of the test sample, leading to an overall reduction in test-set perplexity of 2.4%. The similarity-based model was also tested in a speech-recognition task, where it yielded a statistically significant reduction (32 versus 64 mistakes in cases where there was disagreement with the back-off model) in recognition error. \n\nIn the disambiguation evaluation, we compared several variants of our initial method and the cooccurrence smoothing method of Essen and Steinbiss (1992) against the estimation method of Katz in a decision task involving unseen pairs of direct objects and verbs. We found that all the similaritybased models performed almost 40% better than back-off, which yielded about 49% accuracy in our experimental setting. Furthermore, a scheme based on the Jensen-Shannon divergence (Rao, 1982;Lin, 1991) 1 yielded statistically significant improvement in error rate over cooccurrence smoothing. \n\nWe also investigated the effect of removing extremely low-frequency events from the training set. We found that, in contrast to back-off smoothing, where such events are often discarded from training with little discernible effect, similarity-based smoothing methods suffer noticeable performance degradation when singletons (events that occur exactly once) are omitted. \n\nThe paper is organized as follows. Section 2 describes the general similarity-based framework; in particular, Section 2.3 presents the functions we use as measures of similarity. Section 3 details our initial language modeling experiments. Section 4 describes our comparison experiments on a pseudo-word disambiguation task. Section 5 discusses related work. Finally, Section 6 summarizes our contributions and outlines future directions.",
            "score": 0.4310919816026011,
            "section_title": "Main Contributions",
            "char_start_offset": 5276,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 348
                },
                {
                    "start": 351,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 1027
                },
                {
                    "start": 1030,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1615
                },
                {
                    "start": 1618,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1988
                },
                {
                    "start": 1991,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2169
                },
                {
                    "start": 2170,
                    "end": 2230
                },
                {
                    "start": 2231,
                    "end": 2315
                },
                {
                    "start": 2316,
                    "end": 2349
                },
                {
                    "start": 2350,
                    "end": 2429
                }
            ],
            "ref_mentions": [
                {
                    "start": 549,
                    "end": 561,
                    "matchedPaperCorpusId": "6555412"
                },
                {
                    "start": 1156,
                    "end": 1182,
                    "matchedPaperCorpusId": "62555344"
                },
                {
                    "start": 1503,
                    "end": 1514,
                    "matchedPaperCorpusId": "124998305"
                },
                {
                    "start": 1514,
                    "end": 1524,
                    "matchedPaperCorpusId": "12121632"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2462158203125
        },
        {
            "corpus_id": "17906443",
            "title": "A Phonotactic Language Model for Spoken Language Identification",
            "text": "With the sequence of tokens, we are able to estimate an n-gram language model (LM) from the statistics. It is generally agreed that phonotactics, i.e. the rules governing the phone/phonemes sequences admissible in a language, carry more language discriminative information than the phonemes themselves. An n-gram LM over the tokens describes well n-local phonotactics among neighboring tokens. While some systems model the phonotactics at the frame level (Torres-Carrasquillo et al., 2002), others have proposed P-PRLM. The latter has become one of the most promising solutions so far (Zissman, 1996).\n\nA variety of cues can be used by humans and machines to distinguish one language from another. These cues include phonology, prosody, morphology, and syntax in the context of an utterance. However, global phonotactic cues at the level of utterance or spoken document remains unexplored in previous work. In this paper, we pay special attention to it. A spoken language always contains a set of high frequency function words, prefixes, and suffixes, which are realized as phonetic token substrings in the spoken document. Individually, those substrings may be shared across languages. However, the pattern of their co-occurrences discriminates one language from another. Perceptual experiments have shown (Muthusamy, 1994) that with adequate training, human listeners' language identification ability increases when given longer excerpts of speech. Experiments have also shown that increased exposure to each language and longer training sessions improve listeners' language identification performance. Although it is not entirely clear how human listeners make use of the high-order phonotactic/prosodic cues present in longer spans of a spoken document, strong evidence shows that phonotactics over larger context provides valuable LID cues beyond n-gram, which will be further attested by our experiments in Section 4.",
            "score": 0.4310919816026011,
            "section_title": "n-gram Language Model",
            "char_start_offset": 4783,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 455,
                    "end": 489,
                    "matchedPaperCorpusId": "7785308"
                },
                {
                    "start": 585,
                    "end": 600,
                    "matchedPaperCorpusId": "6594896"
                },
                {
                    "start": 1307,
                    "end": 1323,
                    "matchedPaperCorpusId": "1598125"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1380615234375
        },
        {
            "corpus_id": "275758364",
            "title": "GenAI Content Detection Task 1: English and Multilingual Machine-Generated Text Detection: AI vs. Human",
            "text": "Table 11 presents the detection accuracy on the multilingual subtask across 15 languages, including seen and unseen languages during the training process. The top-performing languages in terms of detection accuracy are generally those seen during training, with the highest accuracy observed on Chinese (94.2), followed by Russian (89.6) and Spanish (89.5). For Arabic (AR), Italian (IT), and Dutch (NL), the performance is slightly lower but still competitive, demonstrating the model's steady generalization to seen languages. \n\nFor unseen languages, such as Hindi (HI) and Hebrew (HE), there is a noticeable drop in performance compared to seen languages. For example, the top-performing team achieved only 51.8 on Hindi. It is challenging for models to generalize to unseen languages, due to the limited exposure to linguistic patterns, structures, and features during training. It is worth noting that some unseen languages perform relatively well, such as Kazakh (KK) and Vietnamese (VI), achieving relatively high scores. This may result from knowledge transfer from similar languages to the unseen, like Russian to Kazakh, and Chinese to Vietnamese. Overall, the models perform well on seen languages, and scores decline significantly on unseen languages.The dataset size and the nature of a language (e.g., script, structure, and linguistic features) play an important role in the model's ability to generalize.",
            "score": 0.4310919816026011,
            "section_title": "Seen Languages vs. Unseen Languages",
            "char_start_offset": 25287,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1420
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.092529296875
        },
        {
            "corpus_id": "277066574",
            "title": "Bridging the LLM Accessibility Divide? Performance, Fairness, and Cost of Closed versus Open LLMs for Automated Essay Scoring",
            "text": "The rapid development of machine learning (ML) technologies, particularly large language models (LLMs), has led to major advancements in natural language processing (NLP, Abbasi et al. 2023). While much of this advancement happened under the umbrella of the common task framework which espouses transparency and openness (Abbasi et al. 2023), in recent years, closed LLMs such as GPT-3 and GPT-4 have set new performance standards in tasks ranging from text generation to question answering, demonstrating unprecedented capabilities in zero-shot and few-shot learning scenarios (Brown et al. 2020, OpenAI 2023). Given the strong performance of closed LLMs such as GPT-4, many studies within the LLM-as-a-judge paradigm rely on their scores as ground truth benchmarks for evaluating both open and closed LLMs (Chiang and Lee 2023), further entrenching the dominance of SOTA closed LLMs (Vergho et al. 2024). Along with closed LLMs, there are also LLMs where the pre-trained models (i.e., training weights) and inference code are publicly available (\"open LLMs\") such as Llama (Touvron et al. 2023, Dubey et al. 2024) as well as LLMs where the full training data and training code are also available (\"open-source LLMs\") such as OLMo (Groeneveld et al. 2024). Open and open-source LLMs provide varying levels of transparency for developers and researchers (Liu et al. 2023). \n\nAccess to model weights, training data, and inference code enables several benefits for the user-developer-researcher community, including lower costs per input/output token through third-party API services, support for local/offline pre-training and fine-tuning, and deeper analysis of model biases and debiasing strategies. However, the dominance of closed LLMs raises a number of concerns, including accessibility and fairness (Strubell et al. 2020, Bender 2021, Irugalbandara et al. 2024).",
            "score": 0.4310919816026011,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1868
                }
            ],
            "ref_mentions": [
                {
                    "start": 171,
                    "end": 190,
                    "matchedPaperCorpusId": "241978126"
                },
                {
                    "start": 321,
                    "end": 341,
                    "matchedPaperCorpusId": "241978126"
                },
                {
                    "start": 578,
                    "end": 596,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05792236328125
        },
        {
            "corpus_id": "258832386",
            "title": "A Pilot Study on Dialogue-Level Dependency Parsing for Chinese",
            "text": "Besides, there is no predicted relationship between utterances. We find that most interutterance arcs are emitted from the root node above to the root node below. Thus, we link the predicted roots from above to below as the inter-EDU arcs between utterances. MLM-based Signal Detection. Given those inter-EDU arcs, we propose a signal-based transform approach to assign inter-EDU labels to them. First, we introduce the signal detection method, which is based on a masked language modeling (MLM) paradigm. We find that certain words reflect the semantic role of EDUs. For instance, an EDU that contains \"if\" is commonly the \"condition\" role (Figure 1). We can emit an arc \"cond\" from its head EDU to it. Thus, we pre-define a word-signal dictionary, mapping words to the corresponding inter-EDU relation signals. A word that reflects the signal is called \"signal word\". \n\nSubsequently, we apply the MLM on the largescale unlabeled dialogue data D to learn inter-EDU signals. During the training stage, the signal word v is randomly dropped in a segmented EDU e, and a masked language model is to recover it. Like prompt-based learning (Liu et al., 2021), we modify e by a slotted template into a prompt e \u2032 . The slot is a placeholder with \"[mask]\" tokens. Next, a model with PLM and MLP decoder outputs the word distribution in masked positions,4 as distribution of signal words: \n\nwhere the MLP is to project the hidden vector to vocabulary space. \n\nAt the inference stage, the model outputs the distribution of signal words. The probabilities of signal words are grouped and averaged by their corresponding signals. The grouped probabilities form the distribution of inter-EDU signals. \n\nThe signal s can be obtained by argmax P (s|e). \n\nWe expand the predicted signal to the whole e. By executing the above procedure in batch with e in x, we end up with the signal sequence, which is denoted in bold s. \n\nSignal-based Transformation. Given detected signals s, Algorithm 1 show how partial syntactic labels are transformed into inter-EDU labels.",
            "score": 0.4310919816026011,
            "section_title": "Signal-based Dependency Transformation",
            "char_start_offset": 11123,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 869
                },
                {
                    "start": 872,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1380
                },
                {
                    "start": 1383,
                    "end": 1449
                },
                {
                    "start": 1452,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1688
                },
                {
                    "start": 1691,
                    "end": 1738
                },
                {
                    "start": 1741,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1906
                },
                {
                    "start": 1909,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2048
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20703125
        },
        {
            "corpus_id": "276575930",
            "title": "Synthetic vs. Gold: The Role of LLM-Generated Labels and Data in Cyberbullying Detection",
            "text": "We investigate the role of LLMs in CB detection, focusing on their utility under varying data availability conditions and under the assumption that direct use of LLMs as a classifier is too expensive due to the high volume of messages to be checked. As a baseline for comparison, we evaluate a scenario in which a lightweight, BERT-based classifier is trained exclusively on gold-standard, manually labeled authentic data without LLM involvement. We then define three additional scenarios with different data availability and that use LLMs in different ways. \n\nScenario 1: Baseline This scenario represents the ideal situation where sufficient gold-standard data is available for fine-tuning BERT. It serves as the benchmark for evaluating the effectiveness of other approaches. In this setup, no synthetic data or LLMs are involved. This scenario is feasible if resources such as time, budget and expert annotators are abundant. However, it often proves impractical due to the challenges of manual labeling. \n\nScenario 2: LLM as Classifier This scenario applies when labeled authentic data is unavailable, and there is no intention to train a separate classifier for CB detection. Instead, an instruction-tuned LLM is used directly as a classifier, leveraging its pre-trained knowledge and its ability to follow instructions to identify CB instances. The primary advantage of this method is its elimination of the need for labeled data and training time. However, there are trade-offs. While an LLM can handle nuanced language patterns, it may be less efficient and incur higher computational costs compared to simpler BERT-based classifiers with a classification head and fine-tuned on a labeled dataset. We explore two prompting strategies for generating synthetic labels: (a) guideline-enhanced (GE) prompts, guiding the LLM with detailed labeling instructions and (b) guidelne-free (GF) prompts, allowing the LLM to generate labels without such guidelines. \n\nScenario 3: Fully Synthetic Data In this scenario, only a small set of manually labeled gold data is available for testing, with no access to authentic data for training or validation. We use an LLM to generate a fully synthetic dataset, consisting of both synthetic messages and corresponding labels, for training and validation. This approach is particularly valuable in low-resource domains or emerging tasks where authentic data is scarce or difficult to collect.",
            "score": 0.4310919816026011,
            "section_title": "Overview of Scenarios",
            "char_start_offset": 6293,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 558
                },
                {
                    "start": 561,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1008
                },
                {
                    "start": 1011,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1961
                },
                {
                    "start": 1964,
                    "end": 2148
                },
                {
                    "start": 2149,
                    "end": 2294
                },
                {
                    "start": 2295,
                    "end": 2431
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09185791015625
        },
        {
            "corpus_id": "266755678",
            "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
            "text": "Large Language Models (LLMs) typically learn rich language representations through a pre-training process. During pre-training, these models leverage extensive corpora, such as text data from the internet, and undergo training through self-supervised learning methods. Language modeling is one common form of self-supervised learning task in which the model is tasked with predicting the next word in a given context. Through this task, the model acquires the ability to capture information related to vocabulary, grammar, semantics, and text structure. \n\nIn language modeling [18; 7; 8; 36], the model is required to predict the next word in a given context. This task enables the model to develop a nuanced understanding of language. Specifically, the model observes large amounts of textual data and attempts to predict the next word at each position in the text. This gradual learning process allows the model to capture the patterns and information inherent in language, encoding a vast amount of linguistic knowledge into its parameters. Once pre-training is complete, these model parameters can be fine-tuned for various natural language processing tasks to adapt to specific task requirements. The objective of language modeling is to train a model to maximize the likelihood of textual data. For a given text sequence, denoted as  1 ,  2 , ...,   , where   represents the token at position ,  (  | 1 ,  2 , ...,  \u22121 ) is the probability of predicting   given the preceding context  1 ,  2 , ...,  \u22121 , the objective function for language modeling can be expressed using cross-entropy loss. Here, we define the objective as maximizing the conditional probability of the given text sequence: \n\nLanguage modeling serves as a prevalent pretraining objective for most LLMs. In addition to language modeling, there are other pretraining tasks within the realm of language modeling. For instance, some models [68; 37] use text with certain portions randomly replaced, and then employ autoregressive methods to recover the replaced tokens. The primary training approach involves the autoregressive recovery of the replaced intervals.",
            "score": 0.4310919816026011,
            "section_title": "Pre-training Tasks",
            "char_start_offset": 37384,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 553
                },
                {
                    "start": 556,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1698
                },
                {
                    "start": 1701,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2134
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2117919921875
        },
        {
            "corpus_id": "258865367",
            "title": "LLMDet: A Third Party Large Language Models Generated Text Detection Tool",
            "text": "Drawing from previous detection works, such as DetectGPT (Mitchell et al., 2023) and GPT-Zero5 , perplexity has shown promising results in detecting machine-generated text. Therefore, we consider utilizing perplexity as a measurement of identifying the generated text from different LLMs. However, calculating the actual perplexity requires access to LLMs, which goes against the safety and efficiency characteristics of the practical LLMs detection method. \n\nPerplexity is a measure used to evaluate the performance of language models. Specifically, it is the exponential average of the negative log-likelihood of a sequence generated by the model. The perplexity score is calculated based on the probability of generating the next word, given all the previous words in the sequence, e.g. p(x i , x <i ). In order to calculate the perplexity of text without accessing the model, we need approximate p(x i , x <i ) by replacing x <i with a n-gram , thus a dictionary should be constructed, with n-gram as keys and the next token probabilities as values. This dictionary serves as prior information during the detection process, allowing us to compute the proxy perplexity of the text instead of the true perplexity. The construction process can be divided into three steps:  1) Generated Text Sampling: Due to the absence of readily available model-generated text data, it is necessary to collect a sufficient number of corresponding generated texts for each model. We provide a prompt dataset and, for each model, randomly sample an equal number of prompts. We use these prompts to generate corresponding texts and collect the required text data. \n\n2) Word Frequency Statistics: In this phase, we first utilize the generated texts collected in the previous step to perform n-gram word frequency statistics (Pang et al., 2016). The n-gram range from 2-gram to n-gram. Subsequently, we select the top-k n-gram based on their frequency. \n\n3) Next Token Probability Sampling: In this phase, we use each n-gram s obtained from word frequency statistics as samples. We input the first n \u2212 1 token s [1:n\u22121] into the corresponding generative models for predicting next-token probabilities \n\nwhere |W| is the size of vocabulary.",
            "score": 0.4310919816026011,
            "section_title": "Dictionary Construction",
            "char_start_offset": 10179,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 457
                },
                {
                    "start": 460,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1647
                },
                {
                    "start": 1650,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 1934
                },
                {
                    "start": 1937,
                    "end": 2060
                },
                {
                    "start": 2061,
                    "end": 2182
                },
                {
                    "start": 2185,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 1807,
                    "end": 1826,
                    "matchedPaperCorpusId": "3993933"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46533203125
        },
        {
            "corpus_id": "268691593",
            "title": "ELLEN: Extremely Lightly Supervised Learning for Efficient Named Entity Recognition",
            "text": "In order to obtain more robust annotations from the Masked Language Model (MLM), we only consider the entity span x i labeled by the MLM where the difference between the score of the class that is predicted with the highest probability and the score of the class that is predicted with the second highest probability is greater than some threshold t class :\n\nx i | P (y i = l 1 |x i ) \u2212 P (y i = l 2 |x i ) > class\n\nHere l 1 is the most likely class label and l 2 is the second most likely class label, according to the MLM.This is motivated by the Breaking Ties active learning method of Scheffer et al. (2001); Luo et al. (2005), which aims to select token samples where the difference between the top two predictions is the smallest, in order to increase the likelihood of confident classifications.However, for the MLM, we adopt the inverse of breaking ties, where we maximize the difference between the top two predictions, based on a threshold.We use different thresholds for each class as shown in Table 12.We empirically observe that we obtain a slightly higher F1 score with the MLM as an unsupervised NER on the CoNLL-03 development set when using different thresholds for each class instead of a single threshold value for all classes.\n\nIn our experiments, we also empirically observe that the MLM tends to produce more robust probabilities when the lexicon entities filling the [MASK] slot(s) are segmented into fewer subwords by the model's tokenizer.This is supported by the findings of Kauf and Ivanova (2023), who observe that methods that estimate the psuedo-log-likelihood of a sentence yield inflated scores for out-of-vocabulary words.Hence we employ an additional heuristic where we filter the lexicon entities to only single subword entities.We believe that better methods for estimating and aggregating probabilities for sentences that contain out-of-vocabulary words can be explored in future work.Kauf and Ivanova (2023) introduce one such method, which has been shown to address the issue of attributing uneven likelihoods to multi-token words.",
            "score": 0.4310919816026011,
            "section_title": "C. Masked Language Model (MLM): Inverse Breaking Ties",
            "char_start_offset": 35177,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 357
                },
                {
                    "start": 359,
                    "end": 414
                },
                {
                    "start": 416,
                    "end": 524
                },
                {
                    "start": 524,
                    "end": 802
                },
                {
                    "start": 802,
                    "end": 950
                },
                {
                    "start": 950,
                    "end": 1014
                },
                {
                    "start": 1014,
                    "end": 1246
                },
                {
                    "start": 1248,
                    "end": 1464
                },
                {
                    "start": 1464,
                    "end": 1655
                },
                {
                    "start": 1655,
                    "end": 1764
                },
                {
                    "start": 1764,
                    "end": 1922
                },
                {
                    "start": 1922,
                    "end": 2070
                }
            ],
            "ref_mentions": [
                {
                    "start": 589,
                    "end": 611,
                    "matchedPaperCorpusId": "38833768"
                },
                {
                    "start": 613,
                    "end": 630,
                    "matchedPaperCorpusId": "5535864"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09039306640625
        },
        {
            "corpus_id": "1314992",
            "title": "Semi-Supervised Learning of Sequence Models with Method of Moments",
            "text": "Sequence models with log-linear emissions have been considered by Smith and Eisner (2005), in a discriminative setting, and by Berg-Kirkpatrick et al. (2010), as generative models for POS induction. Feature-based HMMs (FHMMs) define a feature function for words, \u03c6(X) \u2208 R W , which can be discrete or continuous. This allows, for example, to indicate whether an observation, corresponding to a word, starts with an uppercase letter, contains digits or has specific affixes. More generally, it helps with the treatment of out-of-vocabulary words. The emission probabilities are modeled as K conditional distributions parametrized by a log-linear model, where the \u03b8 h \u2208 R W represent feature weights:\n\n) is a normalization factor. We will show in \u00a74 how our moment-based semi-supervised method can also be used to learn the feature weights \u03b8 h .",
            "score": 0.4310919816026011,
            "section_title": "Feature-Based Hidden Markov Models",
            "char_start_offset": 4473,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 89,
                    "matchedPaperCorpusId": "259144"
                },
                {
                    "start": 127,
                    "end": 157,
                    "matchedPaperCorpusId": "1322232"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07586669921875
        },
        {
            "corpus_id": "276741727",
            "title": "Detecting LLM-Generated Korean Text through Linguistic Feature Analysis",
            "text": "We perform out-of-distribution (OOD) evaluation to assess how well the detection methods generalize. Specifically, we test whether the model can accurately distinguish between human-written and LLM-generated Korean text even when faced with texts from an unseen LLM. This evaluation design is essential given the frequent emergence of LLMs with distinct text generation patterns. If a detection system relies only on data from familiar LLMs, it may struggle to maintain performance when confronted with a previously unseen model. By testing on LLMs not encountered during training, we can better approximate real-world conditions and gain deeper insights into how the detection system adapts without retraining for every new LLM. \n\nWe split the human-written text into an 8:2 ratio, using 80% of it along with text generated by GPT-4o-one of the most representative LLMs-to create the training dataset. For evaluation, we use text generated by Solar, Qwen2, and Llama3.1. Specifically, we construct three separate test sets by combining the text from each of these LLMs with the remaining 20% of human-written text. \n\nThe detection methods requiring training include our proposed approach along with the fine-tuning baseline. These methods undergo training on the training dataset and are evaluated using the test sets. Despite the training process, the models are always tested on text generated by unseen LLMs. This ensures that all detection methods operate in a zero-shot classification setting.",
            "score": 0.4310919816026011,
            "section_title": "OOD Evaluation: Unseen LLMs",
            "char_start_offset": 17882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1115
                },
                {
                    "start": 1118,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1499
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.151123046875
        },
        {
            "corpus_id": "3204901",
            "title": "Aggregate and mixed-order Markov models for statistical language processing",
            "text": "The purpose of a statistical language model is to assign high probabilities to likely word sequences and low probabilities to unlikely ones. The challenge here arises from the combinatorially large number of possibilities, only a fraction of which can ever be observed. In general, language models must learn to recognize word sequences that are functionally similar but lexically distinct. The learning problem, one of generalizing from sparse data, is particularly acute for large-sized vocabularies (Jelinek, Mercer, and Roukos, 1992). \n\nThe simplest models of natural language are ngram Markov models. In these models, the probability of each word depends on the n \u2212 1 words that precede it. The problems in estimating robust models of this form are well-documented. The number of parameters-or transition probabilitiesscales as V n , where V is the vocabulary size. For typical models (e.g., n = 3, V = 10 4 ), this number exceeds by many orders of magnitude the total number of words in any feasible training corpus. \n\nThe transition probabilities in n-gram models are estimated from the counts of word combinations in the training corpus. Maximum likelihood (ML) estimation leads to zero-valued probabilities for unseen n-grams. In practice, one adjusts or smoothes (Chen and Goodman, 1996) the ML estimates so that the language model can generalize to new phrases. Smoothing can be done in many ways-for example, by introducing artificial counts, backing off to lowerorder models (Katz, 1987), or combining models by interpolation (Jelinek and Mercer, 1980). \n\nOften a great deal of information is lost in the smoothing procedure. This is due to the great discrepancy between n-gram models of different order. The goal of this paper is to investigate models that are intermediate, in both size and accuracy, between different order n-gram models. We show that such models can \"intervene\" between different order ngrams in the smoothing procedure. Experimentally, we find that this significantly reduces the perplexity of unseen word combinations. \n\nThe language models in this paper were evaluated on the ARPA North American Business News (NAB) corpus.",
            "score": 0.4310919816026011,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 538
                },
                {
                    "start": 541,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1022
                },
                {
                    "start": 1025,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1566
                },
                {
                    "start": 1569,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2054
                },
                {
                    "start": 2057,
                    "end": 2160
                }
            ],
            "ref_mentions": [
                {
                    "start": 1273,
                    "end": 1297,
                    "matchedPaperCorpusId": "215842252"
                },
                {
                    "start": 1488,
                    "end": 1500,
                    "matchedPaperCorpusId": "6555412"
                },
                {
                    "start": 1539,
                    "end": 1565,
                    "matchedPaperCorpusId": "61012010"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.178466796875
        },
        {
            "corpus_id": "266999625",
            "title": "Enhancing Robustness of LLM-Synthetic Text Detectors for Academic Writing: A Comprehensive Analysis",
            "text": "Large-scale language models (LLMs), such as OpenAI's GPT-4 [8], and Google's Pathways Language Model 2 [1], have become an integral part of our lives and jobs and are often utilized unknowingly. However, while LLMs greatly facilitate daily activities, they also pose significant security risks if maliciously exploited for attacks or deception. Consequently, with the growing popularity of LLMs, the importance of AI security has come to the forefront of people's attention [3,11,4]. Among the various security concerns, academic cheating stands out as a particularly grave issue. \n\nChatGPT, in particular, has gained widespread popularity among college students worldwide. Consequently, universities urgently need robust detectors, which has driven continuous advancements in the field of detection technology. \n\nResearchers in detector development have explored strategies to optimize the training set for improved model performance. Notably, Liyanage et al. [6] pioneered an AI-generated academic dataset using GPT-2, although it is considered less effective than the more advanced ChatGPT model currently available. Yuan et al. [12] proposed BERTscore, an evaluation method for filtering high-quality generated text that closely resembles human writing. Such text can be incorporated into the training set, thereby enhancing the performance of the detectors. \n\nResearchers have also focused on optimizing the detector itself. Jawahar et al. [5] addressed the challenge of hybrid text, introducing a method to detect the boundary between machine-generated and human-written content, rather than solely distinguishing between the two. Zhao et al. [13] conducted a comprehensive survey of various LLMs, analyzing their performance across multiple dimensions, including pretraining, adaptation tuning, utilization, and capacity evaluation. They also identified potential future development directions for LLMs. Additionally, Mitchell et al. [7] proposed a model utilizing a curvature-based criterion to determine whether a given passage was generated by an LLM. \n\nStudies examining the robustness of detectors include Rodriguez et al. [10], which investigated the impact of dataset domain on detector performance, highlighting a significant decrease in performance when the training and test datasets differ in domain. Their findings emphasized how the diversity of training sets directly affects the detector's performance.",
            "score": 0.4310919816026011,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 580
                },
                {
                    "start": 583,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 811
                },
                {
                    "start": 814,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1362
                },
                {
                    "start": 1365,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2061
                },
                {
                    "start": 2064,
                    "end": 2318
                },
                {
                    "start": 2319,
                    "end": 2424
                }
            ],
            "ref_mentions": [
                {
                    "start": 477,
                    "end": 480,
                    "matchedPaperCorpusId": "245454767"
                },
                {
                    "start": 2135,
                    "end": 2139,
                    "matchedPaperCorpusId": "250390908"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.564453125
        },
        {
            "corpus_id": "276937673",
            "title": "An Evaluation of LLMs for Detecting Harmful Computing Terms",
            "text": "In this study, various AI models were evaluated for their ability to detect harmful or non-inclusive language. The models tested include BERT-base-uncased, RoBERTa large-mnli, T5-large, Facebook/bart-large-mnli, Gemini Flash 2.0, Gemini Flash 1.5, Claude AI Sonnet 3.5, and GPT-4 (ChatGPT). The overall performance comparison, along with insights into each model's capabilities and limitations, provides a comprehensive understanding of their effectiveness. Results are summarized in Table",
            "score": 0.4310919816026011,
            "section_title": "Results and Analysis",
            "char_start_offset": 12421,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 489
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08770751953125
        },
        {
            "corpus_id": "258331928",
            "title": "Sample-Specific Debiasing for Better Image-Text Models",
            "text": "While the theory presented in this paper is informative, it has limitations. For example, the generalization bounds provided in Appendix A.2 assume a specific similarity function (e.g., dot product) that may not represent the various similarity functions used in practice. Moreover, the generalization bounds only apply to downstream classification tasks while we also evaluate on visual grounding and cross-modal retrieval tasks. While these tasks can be interpreted as some form of nearest neighbor classification, the theoretical results do not trivially extend to these scenarios and more analysis is needed to provide similar generalization guarantees for these tasks. \n\nWe use a language model to score text as a proxy for the sample-specific class probability. However, it is unclear how to apply this strategy to data that does not include associated text. While we can estimate data density with certain types of models (e.g., flow-based or autoregressive models), it is not well understood whether these estimates correlate well with the underlying class probabilities. Natural language data is unique in that it is created by humans, capturing important variations in data that align well with the types of problems that users typically wish to solve. \n\nMoreover, it is uncertain how well our assumption that class probability is log-linear with respect to text sequence probability holds in practice. Verifying this assumption requires defining a concrete set of latent classes and annotating text with the defined classes. \n\nHowever, the latent classes are difficult to define in practice, and we do not have access to the mapping from a data point to its latent class in most clinically important problems as having access to such mapping would eliminate the need for self-supervised learning.",
            "score": 0.4310919816026011,
            "section_title": "Limitations",
            "char_start_offset": 29042,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1262
                },
                {
                    "start": 1265,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1535
                },
                {
                    "start": 1538,
                    "end": 1807
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11968994140625
        },
        {
            "corpus_id": "232075876",
            "title": "Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP",
            "text": "We investigate the capability of pretrained language models to detect when their outputs exhibit socially undesirable attributes; we call this process self-diagnosis. 3 In particular, we are interested in whether language models are able to do so without any training data or other external resources as large enough datasets do not exist for many relevant biases and behaviors and also do not exist for many languages. That is, we only provide a short textual attribute description and rely entirely on the internal knowledge that the model has acquired during pretraining. \n\nGiven a language model M and a sequence of tokens w 1 , ..., w k , let p M (w | w 1 , ..., w k ) denote the probability that the language model assigns to w being the next token. For each sentence x generated by M and each attribute description y, we construct a self-diagnosis input sdg(x, y) using the template shown in Figure 2 (a). That is, we supplement the generated text x with the question whether it contains attribute y and prompt the model to generate an answer to this question. For example, if M generates the text x = I'm going to hunt you down! and we want it to self-diagnose whether this text contains a threat (y = a threat), we ask it to provide a continuation for the following input (i.e., a continuation for sdg(x, y), see Figure 2 (a)): \n\n\"I'm going to hunt you down!\" Question: Does the above text contain a threat? Answer: \n\nWe estimate the probability of x exhibiting attribute y as \n\nbased on the probabilities that the language model assigns to the words \"Yes\" and \"No\" given the selfdiagnosis input. A similar idea is used by Schick and Sch\u00fctze (2021a,b) and Brown et al. (2020) for few-shot learning.",
            "score": 0.4310919816026011,
            "section_title": "Self-Diagnosis",
            "char_start_offset": 7366,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 574
                },
                {
                    "start": 577,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1336
                },
                {
                    "start": 1339,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1424
                },
                {
                    "start": 1427,
                    "end": 1485
                },
                {
                    "start": 1488,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1707
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.419677734375
        },
        {
            "corpus_id": "271329267",
            "title": "Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives",
            "text": "Statistical language models are based on the idea that the probability of a word appearing in a sentence is related to the probability of the words that came before it [89]. These models are trained on large corpora of text, and they use statistical methods to learn the probabilities of different sequences of words. Such models, including n-gram models and models based on maximum entropy, often use conditional probability to estimate the likelihood of a word given its context [90], [91]. Equation 14 is derived from the maximum likelihood estimation, where the probability of a word given its context is estimated by the ratio of the count of the specific context-word pair to the count of the context alone. In Equation 14, P (w 1 , w 2 , . . . , w n ) denotes the conditional probability of the word, given the preceding word w n\u22121 , C (w n\u22121 , w n ) represents is the count of occurrences of the bigram (word w n\u22121 , word w n ) in the training data, and the C (w n\u22121 ) represents the count of occurrences of the word w n\u22121 in the training data. For higher-order n-gram models, the equation is extended to consider a longer history of words as shown in Equation 15.",
            "score": 0.4310919816026011,
            "section_title": "A. Statistical Language Models",
            "char_start_offset": 39650,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1172
                }
            ],
            "ref_mentions": [
                {
                    "start": 168,
                    "end": 172,
                    "matchedPaperCorpusId": "45558661"
                },
                {
                    "start": 481,
                    "end": 485,
                    "matchedPaperCorpusId": "4669896"
                },
                {
                    "start": 487,
                    "end": 491,
                    "matchedPaperCorpusId": "895577"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21337890625
        },
        {
            "corpus_id": "269292902",
            "title": "What Do Transformers Know about Government?",
            "text": "Language modeling is a fundamental task in natural language processing (NLP), on which pretrained language models (PLMs) currently achieve the best performance.One approach to making inferences about the model \"internals\" is through probing, also known as BERTology (Rogers et al., 2021).Approaches to probing PLMs usually include a specific probing task-e.g., investigating predicate-argument agreement, or how gender is encoded in contextual representations,-data prepared for this task-e.g., minimal pairs of examples that differ only by the studied linguistic category,and some mechanism that allows us to interact with or query the model's components.Such a mechanism can be a probing classifier (or probe).The behavior of a simple probe, trained on representations from the PLM, on a probing task can inform us whether the representations include the linguistic information in question (Adi et al., 2017;Conneau et al., 2018;Hewitt and Manning, 2019;Dalvi et al., 2019;Maudslay et al., 2020;Weissweiler et al., 2022;Conia and Navigli, 2022;Arps et al., 2022).\n\nSome researchers criticize probing classifiers and question their effectiveness, in particular, whether the probed LM in fact uses the information that is discovered by the probe (Hewitt and Liang, 2019;Tamkin et al., 2020;Ravichander et al., 2021;Voita and Titov, 2020).Later research suggests this criticism can be addressed by designing appropriate control tasks and datasets (Belinkov, 2022).\n\nThere are two research directions in probing for dependency relations: token representation or the weights of attention heads.Christopher et al. (2020) reconstruct dependency structures based on token representation.Wu et al. (2020) propose parameter-free probing based on masking tokens and measuring the impact of the masked tokens.They found that a Masked LM (MLM) such as BERT can learn the \"natural\" dependency structure of language.Although the dependencies learned by the MLM may differ from human annotation or linguistic theory, they consider it a good \"lower bound\" for unsupervised syntactic parsing.",
            "score": 0.4310919816026011,
            "section_title": "Related Work",
            "char_start_offset": 4698,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 160,
                    "end": 288
                },
                {
                    "start": 288,
                    "end": 656
                },
                {
                    "start": 656,
                    "end": 712
                },
                {
                    "start": 712,
                    "end": 1065
                },
                {
                    "start": 1067,
                    "end": 1338
                },
                {
                    "start": 1338,
                    "end": 1463
                },
                {
                    "start": 1465,
                    "end": 1591
                },
                {
                    "start": 1591,
                    "end": 1681
                },
                {
                    "start": 1681,
                    "end": 1799
                },
                {
                    "start": 1799,
                    "end": 1903
                },
                {
                    "start": 1903,
                    "end": 2076
                }
            ],
            "ref_mentions": [
                {
                    "start": 266,
                    "end": 287,
                    "matchedPaperCorpusId": "211532403"
                },
                {
                    "start": 892,
                    "end": 910,
                    "matchedPaperCorpusId": "6771196"
                },
                {
                    "start": 910,
                    "end": 931,
                    "matchedPaperCorpusId": "24461982"
                },
                {
                    "start": 931,
                    "end": 956,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 956,
                    "end": 975,
                    "matchedPaperCorpusId": "56895415"
                },
                {
                    "start": 975,
                    "end": 997,
                    "matchedPaperCorpusId": "218486879"
                },
                {
                    "start": 1022,
                    "end": 1046,
                    "matchedPaperCorpusId": "248780218"
                },
                {
                    "start": 1046,
                    "end": 1064,
                    "matchedPaperCorpusId": "248157220"
                },
                {
                    "start": 1246,
                    "end": 1270,
                    "matchedPaperCorpusId": "202538609"
                },
                {
                    "start": 1270,
                    "end": 1290,
                    "matchedPaperCorpusId": "216869396"
                },
                {
                    "start": 1290,
                    "end": 1315,
                    "matchedPaperCorpusId": "218486796"
                },
                {
                    "start": 1315,
                    "end": 1337,
                    "matchedPaperCorpusId": "214693050"
                },
                {
                    "start": 1446,
                    "end": 1462,
                    "matchedPaperCorpusId": "236924832"
                },
                {
                    "start": 1591,
                    "end": 1616,
                    "matchedPaperCorpusId": "219315567"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2005615234375
        },
        {
            "paperId": "1ea243f1b697aae22e6f0349fa64857780a6108a",
            "corpusId": 267938621,
            "title": "Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "referenceCount": 29,
            "citationCount": 60,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.15938, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "26845858",
                    "name": "Yihong Dong"
                },
                {
                    "authorId": "2199808863",
                    "name": "Xue Jiang"
                },
                {
                    "authorId": "2279435524",
                    "name": "Huanyu Liu"
                },
                {
                    "authorId": "2152843753",
                    "name": "Zhi Jin"
                },
                {
                    "authorId": "2254313986",
                    "name": "Ge Li"
                }
            ],
            "abstract": "Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks. Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination. However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges. In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for LLMs. CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM's output distribution. To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of LLM's output distribution. To facilitate this study, we introduce two benchmarks, i.e., DetCon and ComiEval, for data contamination detection and contamination mitigation evaluation tasks. Extensive experimental results show that CDD achieves the average relative improvements of 21.8\\%-30.2\\% over other contamination detection approaches in terms of Accuracy, F1 Score, and AUC metrics, and can effectively detect implicit contamination. TED substantially mitigates performance improvements up to 66.9\\% attributed to data contamination across various contamination setups. In real-world applications, we reveal that ChatGPT exhibits a high potential to suffer from data contamination on HumanEval benchmark.",
            "corpus_id": "267938621",
            "text": "Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks. Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination. However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges. In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for LLMs. CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM's output distribution. To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of LLM's output distribution. To facilitate this study, we introduce two benchmarks, i.e., DetCon and ComiEval, for data contamination detection and contamination mitigation evaluation tasks. Extensive experimental results show that CDD achieves the average relative improvements of 21.8\\%-30.2\\% over other contamination detection approaches in terms of Accuracy, F1 Score, and AUC metrics, and can effectively detect implicit contamination. TED substantially mitigates performance improvements up to 66.9\\% attributed to data contamination across various contamination setups. In real-world applications, we reveal that ChatGPT exhibits a high potential to suffer from data contamination on HumanEval benchmark.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.56396484375
        },
        {
            "paperId": "9ad167529a6365e37825ddea5d29ab2f17651959",
            "corpusId": 265128736,
            "title": "Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 63,
            "citationCount": 26,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.06233, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "65754049",
                    "name": "Shahriar Golchin"
                },
                {
                    "authorId": "1760868",
                    "name": "M. Surdeanu"
                }
            ],
            "abstract": "We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions, devising a quiz format wherein three perturbed versions of each instance, subsampled from a specific dataset partition, are created. These changes only include word-level perturbations. The generated perturbations, along with the original dataset instance, form the options in the DCQ, with an extra option accommodating the selection of none of the provided options. Given that the only distinguishing signal among the options is the exact wording with respect to the original dataset instance, an LLM, when tasked with identifying the original dataset instance, gravitates towards selecting the original one if it has been exposed to it. While accounting for positional biases in LLMs, the quiz performance reveals the contamination level for the tested model with the dataset partition to which the quiz pertains. Applied to various datasets and LLMs, under controlled and uncontrolled contamination, our findings, while fully lacking access to training data and model parameters, suggest that DCQ achieves state-of-the-art results and uncovers greater contamination levels through memorization compared to existing methods. Also, it proficiently bypasses more safety filters, especially those set to avoid generating copyrighted content.",
            "corpus_id": "265128736",
            "text": "We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions, devising a quiz format wherein three perturbed versions of each instance, subsampled from a specific dataset partition, are created. These changes only include word-level perturbations. The generated perturbations, along with the original dataset instance, form the options in the DCQ, with an extra option accommodating the selection of none of the provided options. Given that the only distinguishing signal among the options is the exact wording with respect to the original dataset instance, an LLM, when tasked with identifying the original dataset instance, gravitates towards selecting the original one if it has been exposed to it. While accounting for positional biases in LLMs, the quiz performance reveals the contamination level for the tested model with the dataset partition to which the quiz pertains. Applied to various datasets and LLMs, under controlled and uncontrolled contamination, our findings, while fully lacking access to training data and model parameters, suggest that DCQ achieves state-of-the-art results and uncovers greater contamination levels through memorization compared to existing methods. Also, it proficiently bypasses more safety filters, especially those set to avoid generating copyrighted content.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.5849609375
        },
        {
            "paperId": "952a415c55cf5c83eb4d1c0b5918feb5263cb56d",
            "corpusId": 269757388,
            "title": "DoLLM: How Large Language Models Understanding Network Flow Data to Detect Carpet Bombing DDoS",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 48,
            "citationCount": 8,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.07638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2301171718",
                    "name": "Qingyang Li"
                },
                {
                    "authorId": "2257101405",
                    "name": "Yihang Zhang"
                },
                {
                    "authorId": "2301200359",
                    "name": "Zhidong Jia"
                },
                {
                    "authorId": "2216800907",
                    "name": "Yannan Hu"
                },
                {
                    "authorId": "2301176195",
                    "name": "Lei Zhang"
                },
                {
                    "authorId": "2301169914",
                    "name": "Jianrong Zhang"
                },
                {
                    "authorId": "2301199102",
                    "name": "Yongming Xu"
                },
                {
                    "authorId": "2301175274",
                    "name": "Yong Cui"
                },
                {
                    "authorId": "2112582246",
                    "name": "Zongming Guo"
                },
                {
                    "authorId": "1714130",
                    "name": "Xinggong Zhang"
                }
            ],
            "abstract": "It is an interesting question Can and How Large Language Models (LLMs) understand non-language network data, and help us detect unknown malicious flows. This paper takes Carpet Bombing as a case study and shows how to exploit LLMs' powerful capability in the networking area. Carpet Bombing is a new DDoS attack that has dramatically increased in recent years, significantly threatening network infrastructures. It targets multiple victim IPs within subnets, causing congestion on access links and disrupting network services for a vast number of users. Characterized by low-rates, multi-vectors, these attacks challenge traditional DDoS defenses. We propose DoLLM, a DDoS detection model utilizes open-source LLMs as backbone. By reorganizing non-contextual network flows into Flow-Sequences and projecting them into LLMs semantic space as token embeddings, DoLLM leverages LLMs' contextual understanding to extract flow representations in overall network context. The representations are used to improve the DDoS detection performance. We evaluate DoLLM with public datasets CIC-DDoS2019 and real NetFlow trace from Top-3 countrywide ISP. The tests have proven that DoLLM possesses strong detection capabilities. Its F1 score increased by up to 33.3% in zero-shot scenarios and by at least 20.6% in real ISP traces.",
            "corpus_id": "269757388",
            "text": "It is an interesting question Can and How Large Language Models (LLMs) understand non-language network data, and help us detect unknown malicious flows. This paper takes Carpet Bombing as a case study and shows how to exploit LLMs' powerful capability in the networking area. Carpet Bombing is a new DDoS attack that has dramatically increased in recent years, significantly threatening network infrastructures. It targets multiple victim IPs within subnets, causing congestion on access links and disrupting network services for a vast number of users. Characterized by low-rates, multi-vectors, these attacks challenge traditional DDoS defenses. We propose DoLLM, a DDoS detection model utilizes open-source LLMs as backbone. By reorganizing non-contextual network flows into Flow-Sequences and projecting them into LLMs semantic space as token embeddings, DoLLM leverages LLMs' contextual understanding to extract flow representations in overall network context. The representations are used to improve the DDoS detection performance. We evaluate DoLLM with public datasets CIC-DDoS2019 and real NetFlow trace from Top-3 countrywide ISP. The tests have proven that DoLLM possesses strong detection capabilities. Its F1 score increased by up to 33.3% in zero-shot scenarios and by at least 20.6% in real ISP traces.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.51123046875
        },
        {
            "paperId": "a932b662645ab4a348c44c73bb81876cb415ae95",
            "corpusId": 267681760,
            "title": "DE-COP: Detecting Copyrighted Content in Language Models Training Data",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "referenceCount": 32,
            "citationCount": 41,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.09910, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2221141068",
                    "name": "Andr\u00e9 V. Duarte"
                },
                {
                    "authorId": "150345512",
                    "name": "Xuandong Zhao"
                },
                {
                    "authorId": "2284263865",
                    "name": "Arlindo L. Oliveira"
                },
                {
                    "authorId": "143900005",
                    "name": "Lei Li"
                }
            ],
            "abstract": "How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed? We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text. We propose DE-COP, a method to determine whether a piece of copyrighted content was included in training. DE-COP's core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases. We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model's training cutoff, along with their paraphrases. Our experiments show that DE-COP surpasses the prior best method by 9.6% in detection performance (AUC) on models with logits available. Moreover, DE-COP also achieves an average accuracy of 72% for detecting suspect books on fully black-box models where prior methods give approximately 4% accuracy. The code and datasets are available at https://github.com/LeiLiLab/DE-COP.",
            "corpus_id": "267681760",
            "text": "How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed? We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text. We propose DE-COP, a method to determine whether a piece of copyrighted content was included in training. DE-COP's core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases. We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model's training cutoff, along with their paraphrases. Our experiments show that DE-COP surpasses the prior best method by 9.6% in detection performance (AUC) on models with logits available. Moreover, DE-COP also achieves an average accuracy of 72% for detecting suspect books on fully black-box models where prior methods give approximately 4% accuracy. The code and datasets are available at https://github.com/LeiLiLab/DE-COP.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.291748046875
        },
        {
            "paperId": "2837a485b5895efea2eb63a707db2196be1d4a2f",
            "corpusId": 267636801,
            "title": "Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL Translation",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "referenceCount": 27,
            "citationCount": 22,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2402.08100",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.08100, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2185233294",
                    "name": "Federico Ranaldi"
                },
                {
                    "authorId": "2128108433",
                    "name": "Elena Sofia Ruzzetti"
                },
                {
                    "authorId": "37066537",
                    "name": "Dario Onorati"
                },
                {
                    "authorId": "2008183566",
                    "name": "Leonardo Ranaldi"
                },
                {
                    "authorId": "3309033",
                    "name": "Cristina Giannone"
                },
                {
                    "authorId": "2277457120",
                    "name": "Andrea Favalli"
                },
                {
                    "authorId": "153129024",
                    "name": "Raniero Romagnoli"
                },
                {
                    "authorId": "103839825",
                    "name": "F. M. Zanzotto"
                }
            ],
            "abstract": "Understanding textual description to generate code seems to be an achieved capability of instruction-following Large Language Models (LLMs) in zero-shot scenario. However, there is a severe possibility that this translation ability may be influenced by having seen target textual descriptions and the related code. This effect is known as Data Contamination. In this study, we investigate the impact of Data Contamination on the performance of GPT-3.5 in the Text-to-SQL code-generating tasks. Hence, we introduce a novel method to detect Data Contamination in GPTs and examine GPT-3.5's Text-to-SQL performances using the known Spider Dataset and our new unfamiliar dataset Termite. Furthermore, we analyze GPT-3.5's efficacy on databases with modified information via an adversarial table disconnection (ATD) approach, complicating Text-to-SQL tasks by removing structural pieces of information from the database. Our results indicate a significant performance drop in GPT-3.5 on the unfamiliar Termite dataset, even with ATD modifications, highlighting the effect of Data Contamination on LLMs in Text-to-SQL translation tasks.",
            "corpus_id": "267636801",
            "text": "Understanding textual description to generate code seems to be an achieved capability of instruction-following Large Language Models (LLMs) in zero-shot scenario. However, there is a severe possibility that this translation ability may be influenced by having seen target textual descriptions and the related code. This effect is known as Data Contamination. In this study, we investigate the impact of Data Contamination on the performance of GPT-3.5 in the Text-to-SQL code-generating tasks. Hence, we introduce a novel method to detect Data Contamination in GPTs and examine GPT-3.5's Text-to-SQL performances using the known Spider Dataset and our new unfamiliar dataset Termite. Furthermore, we analyze GPT-3.5's efficacy on databases with modified information via an adversarial table disconnection (ATD) approach, complicating Text-to-SQL tasks by removing structural pieces of information from the database. Our results indicate a significant performance drop in GPT-3.5 on the unfamiliar Termite dataset, even with ATD modifications, highlighting the effect of Data Contamination on LLMs in Text-to-SQL translation tasks.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1871337890625
        },
        {
            "paperId": "f9cd5301046f2c463cec4c2f1e23cce59c29766f",
            "corpusId": 270217411,
            "title": "Probing Language Models for Pre-training Data Detection",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "referenceCount": 33,
            "citationCount": 13,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.01333, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294376388",
                    "name": "Zhenhua Liu"
                },
                {
                    "authorId": "1914586128",
                    "name": "Tong Zhu"
                },
                {
                    "authorId": "2186374155",
                    "name": "Chuanyuan Tan"
                },
                {
                    "authorId": "2304460083",
                    "name": "Haonan Lu"
                },
                {
                    "authorId": "2330946427",
                    "name": "Bing Liu"
                },
                {
                    "authorId": "2265943980",
                    "name": "Wenliang Chen"
                }
            ],
            "abstract": "Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the model's internal activations. Our method is simple and effective and leads to more trustworthy pre-training data detection. Additionally, we propose ArxivMIA, a new challenging benchmark comprising arxiv abstracts from Computer Science and Mathematics categories. Our experiments demonstrate that our method outperforms all baselines, and achieves state-of-the-art performance on both WikiMIA and ArxivMIA, with additional experiments confirming its efficacy (Our code and dataset are available at https://github.com/zhliu0106/probing-lm-data).",
            "corpus_id": "270217411",
            "text": "Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the model's internal activations. Our method is simple and effective and leads to more trustworthy pre-training data detection. Additionally, we propose ArxivMIA, a new challenging benchmark comprising arxiv abstracts from Computer Science and Mathematics categories. Our experiments demonstrate that our method outperforms all baselines, and achieves state-of-the-art performance on both WikiMIA and ArxivMIA, with additional experiments confirming its efficacy (Our code and dataset are available at https://github.com/zhliu0106/probing-lm-data).",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.72412109375
        },
        {
            "paperId": "f0295393100c1b55c75ab3d0719827546fbf41cc",
            "corpusId": 259376532,
            "title": "Respectful or Toxic? Using Zero-Shot Learning with Language Models to Detect Hate Speech",
            "venue": "WOAH",
            "year": 2023,
            "referenceCount": 55,
            "citationCount": 57,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.woah-1.6.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.woah-1.6, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1410406981",
                    "name": "F. Plaza-del-Arco"
                },
                {
                    "authorId": "2101317501",
                    "name": "Debora Nozza"
                },
                {
                    "authorId": "2022288",
                    "name": "Dirk Hovy"
                }
            ],
            "abstract": "Hate speech detection faces two significant challenges: 1) the limited availability of labeled data and 2) the high variability of hate speech across different contexts and languages. Prompting brings a ray of hope to these challenges. It allows injecting a model with task-specific knowledge without relying on labeled data. This paper explores zero-shot learning with prompting for hate speech detection. We investigate how well zero-shot learning can detect hate speech in 3 languages with limited labeled data. We experiment with various large language models and verbalizers on 8 benchmark datasets. Our findings highlight the impact of prompt selection on the results. They also suggest that prompting, specifically with recent large language models, can achieve performance comparable to and surpass fine-tuned models, making it a promising alternative for under-resourced languages. Our findings highlight the potential of prompting for hate speech detection and show how both the prompt and the model have a significant impact on achieving more accurate predictions in this task.",
            "corpus_id": "259376532",
            "text": "Hate speech detection faces two significant challenges: 1) the limited availability of labeled data and 2) the high variability of hate speech across different contexts and languages. Prompting brings a ray of hope to these challenges. It allows injecting a model with task-specific knowledge without relying on labeled data. This paper explores zero-shot learning with prompting for hate speech detection. We investigate how well zero-shot learning can detect hate speech in 3 languages with limited labeled data. We experiment with various large language models and verbalizers on 8 benchmark datasets. Our findings highlight the impact of prompt selection on the results. They also suggest that prompting, specifically with recent large language models, can achieve performance comparable to and surpass fine-tuned models, making it a promising alternative for under-resourced languages. Our findings highlight the potential of prompting for hate speech detection and show how both the prompt and the model have a significant impact on achieving more accurate predictions in this task.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.54541015625
        },
        {
            "paperId": "75b874acc4eb463e177e88bb73c668ccc1aae0a2",
            "corpusId": 266176903,
            "title": "Two Directions for Clinical Data Generation with Large Language Models: Data-to-Label and Label-to-Data",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "referenceCount": 91,
            "citationCount": 26,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.findings-emnlp.474.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.06774, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2237106751",
                    "name": "Rumeng Li"
                },
                {
                    "authorId": "2221230794",
                    "name": "Xun Wang"
                },
                {
                    "authorId": "2273658683",
                    "name": "Hong Yu"
                }
            ],
            "abstract": "Large language models (LLMs) can generate natural language texts for various domains and tasks, but their potential for clinical text mining, a domain with scarce, sensitive, and imbalanced medical data, is under-explored. We investigate whether LLMs can augment clinical data for detecting Alzheimer's Disease (AD)-related signs and symptoms from electronic health records (EHRs), a challenging task that requires high expertise. We create a novel pragmatic taxonomy for AD sign and symptom progression based on expert knowledge and generated three datasets: (1) a gold dataset annotated by human experts on longitudinal EHRs of AD patients; (2) a silver dataset created by the data-to-label method, which labels sentences from a public EHR collection with AD-related signs and symptoms; and (3) a bronze dataset created by the label-to-data method which generates sentences with AD-related signs and symptoms based on the label definition. We train a system to detect AD-related signs and symptoms from EHRs. We find that the silver and bronze datasets improves the system performance, outperforming the system using only the gold dataset. This shows that LLMs can generate synthetic clinical data for a complex task by incorporating expert knowledge, and our label-to-data method can produce datasets that are free of sensitive information, while maintaining acceptable quality.",
            "corpus_id": "266176903",
            "text": "Large language models (LLMs) can generate natural language texts for various domains and tasks, but their potential for clinical text mining, a domain with scarce, sensitive, and imbalanced medical data, is under-explored. We investigate whether LLMs can augment clinical data for detecting Alzheimer's Disease (AD)-related signs and symptoms from electronic health records (EHRs), a challenging task that requires high expertise. We create a novel pragmatic taxonomy for AD sign and symptom progression based on expert knowledge and generated three datasets: (1) a gold dataset annotated by human experts on longitudinal EHRs of AD patients; (2) a silver dataset created by the data-to-label method, which labels sentences from a public EHR collection with AD-related signs and symptoms; and (3) a bronze dataset created by the label-to-data method which generates sentences with AD-related signs and symptoms based on the label definition. We train a system to detect AD-related signs and symptoms from EHRs. We find that the silver and bronze datasets improves the system performance, outperforming the system using only the gold dataset. This shows that LLMs can generate synthetic clinical data for a complex task by incorporating expert knowledge, and our label-to-data method can produce datasets that are free of sensitive information, while maintaining acceptable quality.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.61474609375
        },
        {
            "paperId": "e240010a00e736106eb2e9489c52edfb43712e1f",
            "corpusId": 245653362,
            "title": "SignalP 6.0 predicts all five types of signal peptides using protein language models",
            "venue": "Nature Biotechnology",
            "year": 2022,
            "referenceCount": 39,
            "citationCount": 1435,
            "influentialCitationCount": 168,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s41587-021-01156-3.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9287161, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2111668128",
                    "name": "Felix Teufel"
                },
                {
                    "authorId": "147017703",
                    "name": "J. J. Almagro Armenteros"
                },
                {
                    "authorId": "153849889",
                    "name": "alexander rosenberg johansen"
                },
                {
                    "authorId": "1473614858",
                    "name": "M. H. G\u00edslason"
                },
                {
                    "authorId": "2111668115",
                    "name": "Silas Irby Pihl"
                },
                {
                    "authorId": "3306906",
                    "name": "Konstantinos D. Tsirigos"
                },
                {
                    "authorId": "1724252",
                    "name": "O. Winther"
                },
                {
                    "authorId": "8169197",
                    "name": "S. Brunak"
                },
                {
                    "authorId": "7833022",
                    "name": "G. Heijne"
                },
                {
                    "authorId": "145416108",
                    "name": "H. Nielsen"
                }
            ],
            "abstract": "Signal peptides (SPs) are short amino acid sequences that control protein secretion and translocation in all living organisms. SPs can be predicted from sequence data, but existing algorithms are unable to detect all known types of SPs. We introduce SignalP 6.0, a machine learning model that detects all five SP types and is applicable to metagenomic data. A new version of SignalP predicts all types of signal peptides.",
            "corpus_id": "245653362",
            "text": "Signal peptides (SPs) are short amino acid sequences that control protein secretion and translocation in all living organisms. SPs can be predicted from sequence data, but existing algorithms are unable to detect all known types of SPs. We introduce SignalP 6.0, a machine learning model that detects all five SP types and is applicable to metagenomic data. A new version of SignalP predicts all types of signal peptides.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.053802490234375
        },
        {
            "paperId": "028d75496e51943f52c7b2177344a3c089c18058",
            "corpusId": 266999558,
            "title": "Fine-grained Hallucination Detection and Editing for Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 44,
            "citationCount": 87,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.06855, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2279677197",
                    "name": "Abhika Mishra"
                },
                {
                    "authorId": "35584853",
                    "name": "Akari Asai"
                },
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "1705260",
                    "name": "Yizhong Wang"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                },
                {
                    "authorId": "2257032956",
                    "name": "Yulia Tsvetkov"
                },
                {
                    "authorId": "2548384",
                    "name": "Hannaneh Hajishirzi"
                }
            ],
            "abstract": "Large language models (LMs) are prone to generate factual errors, which are often called hallucinations. In this paper, we introduce a comprehensive taxonomy of hallucinations and argue that hallucinations manifest in diverse forms, each requiring varying degrees of careful assessments to verify factuality. We propose a novel task of automatic fine-grained hallucination detection and construct a new evaluation benchmark, FavaBench, that includes about one thousand fine-grained human judgments on three LM outputs across various domains. Our analysis reveals that ChatGPT and Llama2-Chat (70B, 7B) exhibit diverse types of hallucinations in the majority of their outputs in information-seeking scenarios. We train FAVA, a retrieval-augmented LM by carefully creating synthetic data to detect and correct fine-grained hallucinations. On our benchmark, our automatic and human evaluations show that FAVA significantly outperforms ChatGPT and GPT-4 on fine-grained hallucination detection, and edits suggested by FAVA improve the factuality of LM-generated text.",
            "corpus_id": "266999558",
            "text": "Large language models (LMs) are prone to generate factual errors, which are often called hallucinations. In this paper, we introduce a comprehensive taxonomy of hallucinations and argue that hallucinations manifest in diverse forms, each requiring varying degrees of careful assessments to verify factuality. We propose a novel task of automatic fine-grained hallucination detection and construct a new evaluation benchmark, FavaBench, that includes about one thousand fine-grained human judgments on three LM outputs across various domains. Our analysis reveals that ChatGPT and Llama2-Chat (70B, 7B) exhibit diverse types of hallucinations in the majority of their outputs in information-seeking scenarios. We train FAVA, a retrieval-augmented LM by carefully creating synthetic data to detect and correct fine-grained hallucinations. On our benchmark, our automatic and human evaluations show that FAVA significantly outperforms ChatGPT and GPT-4 on fine-grained hallucination detection, and edits suggested by FAVA improve the factuality of LM-generated text.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.369873046875
        },
        {
            "paperId": "55c36748f2a7c060c3313349c730b053ed03fbf7",
            "corpusId": 246823128,
            "title": "Deduplicating Training Data Mitigates Privacy Risks in Language Models",
            "venue": "International Conference on Machine Learning",
            "year": 2022,
            "referenceCount": 41,
            "citationCount": 294,
            "influentialCitationCount": 21,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2202.06539, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1380266797",
                    "name": "Nikhil Kandpal"
                },
                {
                    "authorId": "145217343",
                    "name": "Eric Wallace"
                },
                {
                    "authorId": "2402716",
                    "name": "Colin Raffel"
                }
            ],
            "abstract": "Past work has shown that large language models are susceptible to privacy attacks, where adversaries generate sequences from a trained model and detect which sequences are memorized from the training set. In this work, we show that the success of these attacks is largely due to duplication in commonly used web-scraped training sets. We first show that the rate at which language models regenerate training sequences is superlinearly related to a sequence's count in the training set. For instance, a sequence that is present 10 times in the training data is on average generated ~1000 times more often than a sequence that is present only once. We next show that existing methods for detecting memorized sequences have near-chance accuracy on non-duplicated training sequences. Finally, we find that after applying methods to deduplicate training data, language models are considerably more secure against these types of privacy attacks. Taken together, our results motivate an increased focus on deduplication in privacy-sensitive applications and a reevaluation of the practicality of existing privacy attacks.",
            "corpus_id": "246823128",
            "text": "Past work has shown that large language models are susceptible to privacy attacks, where adversaries generate sequences from a trained model and detect which sequences are memorized from the training set. In this work, we show that the success of these attacks is largely due to duplication in commonly used web-scraped training sets. We first show that the rate at which language models regenerate training sequences is superlinearly related to a sequence's count in the training set. For instance, a sequence that is present 10 times in the training data is on average generated ~1000 times more often than a sequence that is present only once. We next show that existing methods for detecting memorized sequences have near-chance accuracy on non-duplicated training sequences. Finally, we find that after applying methods to deduplicate training data, language models are considerably more secure against these types of privacy attacks. Taken together, our results motivate an increased focus on deduplication in privacy-sensitive applications and a reevaluation of the practicality of existing privacy attacks.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.427490234375
        },
        {
            "paperId": "fdcf7c7130a140ec0c206d0894d2a941bed884f7",
            "corpusId": 268041601,
            "title": "ChatSpamDetector: Leveraging Large Language Models for Effective Phishing Email Detection",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 49,
            "citationCount": 32,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.18093, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1799400183",
                    "name": "Takashi Koide"
                },
                {
                    "authorId": "2099859973",
                    "name": "Naoki Fukushi"
                },
                {
                    "authorId": "2067744577",
                    "name": "Hiroki Nakano"
                },
                {
                    "authorId": "2259918752",
                    "name": "Daiki Chiba"
                }
            ],
            "abstract": "The proliferation of phishing sites and emails poses significant challenges to existing cybersecurity efforts. Despite advances in malicious email filters and email security protocols, problems with oversight and false positives persist. Users often struggle to understand why emails are flagged as potentially fraudulent, risking the possibility of missing important communications or mistakenly trusting deceptive phishing emails. This study introduces ChatSpamDetector, a system that uses large language models (LLMs) to detect phishing emails. By converting email data into a prompt suitable for LLM analysis, the system provides a highly accurate determination of whether an email is phishing or not. Importantly, it offers detailed reasoning for its phishing determinations, assisting users in making informed decisions about how to handle suspicious emails. We conducted an evaluation using a comprehensive phishing email dataset and compared our system to several LLMs and baseline systems. We confirmed that our system using GPT-4 has superior detection capabilities with an accuracy of 99.70%. Advanced contextual interpretation by LLMs enables the identification of various phishing tactics and impersonations, making them a potentially powerful tool in the fight against email-based phishing threats.",
            "corpus_id": "268041601",
            "text": "The proliferation of phishing sites and emails poses significant challenges to existing cybersecurity efforts. Despite advances in malicious email filters and email security protocols, problems with oversight and false positives persist. Users often struggle to understand why emails are flagged as potentially fraudulent, risking the possibility of missing important communications or mistakenly trusting deceptive phishing emails. This study introduces ChatSpamDetector, a system that uses large language models (LLMs) to detect phishing emails. By converting email data into a prompt suitable for LLM analysis, the system provides a highly accurate determination of whether an email is phishing or not. Importantly, it offers detailed reasoning for its phishing determinations, assisting users in making informed decisions about how to handle suspicious emails. We conducted an evaluation using a comprehensive phishing email dataset and compared our system to several LLMs and baseline systems. We confirmed that our system using GPT-4 has superior detection capabilities with an accuracy of 99.70%. Advanced contextual interpretation by LLMs enables the identification of various phishing tactics and impersonations, making them a potentially powerful tool in the fight against email-based phishing threats.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.41943359375
        },
        {
            "paperId": "ce7a2ea8774b996e7022b3bd712c13b75365fc96",
            "corpusId": 267740683,
            "title": "Large Language Models for Forecasting and Anomaly Detection: A Systematic Literature Review",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 86,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.10350, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116967910",
                    "name": "Jing Su"
                },
                {
                    "authorId": "2283937122",
                    "name": "Chufeng Jiang"
                },
                {
                    "authorId": "2284759893",
                    "name": "Xin Jin"
                },
                {
                    "authorId": null,
                    "name": "Yuxin Qiao"
                },
                {
                    "authorId": "2349753157",
                    "name": "Tingsong Xiao"
                },
                {
                    "authorId": "2284731807",
                    "name": "Hongda Ma"
                },
                {
                    "authorId": "2284594383",
                    "name": "Rong Wei"
                },
                {
                    "authorId": "15217571",
                    "name": "Zhi Jing"
                },
                {
                    "authorId": "2284638322",
                    "name": "Jiajun Xu"
                },
                {
                    "authorId": "2284641943",
                    "name": "Junhong Lin"
                }
            ],
            "abstract": "This systematic literature review comprehensively examines the application of Large Language Models (LLMs) in forecasting and anomaly detection, highlighting the current state of research, inherent challenges, and prospective future directions. LLMs have demonstrated significant potential in parsing and analyzing extensive datasets to identify patterns, predict future events, and detect anomalous behavior across various domains. However, this review identifies several critical challenges that impede their broader adoption and effectiveness, including the reliance on vast historical datasets, issues with generalizability across different contexts, the phenomenon of model hallucinations, limitations within the models' knowledge boundaries, and the substantial computational resources required. Through detailed analysis, this review discusses potential solutions and strategies to overcome these obstacles, such as integrating multimodal data, advancements in learning methodologies, and emphasizing model explainability and computational efficiency. Moreover, this review outlines critical trends that are likely to shape the evolution of LLMs in these fields, including the push toward real-time processing, the importance of sustainable modeling practices, and the value of interdisciplinary collaboration. Conclusively, this review underscores the transformative impact LLMs could have on forecasting and anomaly detection while emphasizing the need for continuous innovation, ethical considerations, and practical solutions to realize their full potential.",
            "corpus_id": "267740683",
            "text": "This systematic literature review comprehensively examines the application of Large Language Models (LLMs) in forecasting and anomaly detection, highlighting the current state of research, inherent challenges, and prospective future directions. LLMs have demonstrated significant potential in parsing and analyzing extensive datasets to identify patterns, predict future events, and detect anomalous behavior across various domains. However, this review identifies several critical challenges that impede their broader adoption and effectiveness, including the reliance on vast historical datasets, issues with generalizability across different contexts, the phenomenon of model hallucinations, limitations within the models' knowledge boundaries, and the substantial computational resources required. Through detailed analysis, this review discusses potential solutions and strategies to overcome these obstacles, such as integrating multimodal data, advancements in learning methodologies, and emphasizing model explainability and computational efficiency. Moreover, this review outlines critical trends that are likely to shape the evolution of LLMs in these fields, including the push toward real-time processing, the importance of sustainable modeling practices, and the value of interdisciplinary collaboration. Conclusively, this review underscores the transformative impact LLMs could have on forecasting and anomaly detection while emphasizing the need for continuous innovation, ethical considerations, and practical solutions to realize their full potential.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.55126953125
        },
        {
            "paperId": "cdf6d0eb9673ba5b83735ff358743f4226ed88b7",
            "corpusId": 267499747,
            "title": "SHIELD : An Evaluation Benchmark for Face Spoofing and Forgery Detection with Multimodal Large Language Models",
            "venue": "Visual Intelligence",
            "year": 2024,
            "referenceCount": 70,
            "citationCount": 20,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.04178, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283085089",
                    "name": "Yichen Shi"
                },
                {
                    "authorId": "2283847891",
                    "name": "Yuhao Gao"
                },
                {
                    "authorId": "2283743257",
                    "name": "Yingxin Lai"
                },
                {
                    "authorId": "2283097328",
                    "name": "Hongyang Wang"
                },
                {
                    "authorId": "2282967389",
                    "name": "Jun Feng"
                },
                {
                    "authorId": "2301404164",
                    "name": "Lei He"
                },
                {
                    "authorId": "2290069059",
                    "name": "Jun Wan"
                },
                {
                    "authorId": "2282974475",
                    "name": "Changsheng Chen"
                },
                {
                    "authorId": "2238904839",
                    "name": "Zitong Yu"
                },
                {
                    "authorId": "2284335751",
                    "name": "Xiaochun Cao"
                }
            ],
            "abstract": "\n Multimodal large language models (MLLMs) have demonstrated strong capabilities in vision-related tasks, capitalizing on their visual semantic comprehension and reasoning capabilities. However, their ability to detect subtle visual spoofing and forgery clues in face attack detection tasks remains underexplored. In this paper, we introduce a benchmark, SHIELD, to evaluate MLLMs for face spoofing and forgery detection. Specifically, we design true/false and multiple-choice questions to assess MLLM performance on multimodal face data across two tasks. For the face anti-spoofing task, we evaluate three modalities (i.e., RGB, infrared, and depth) under six attack types. For the face forgery detection task, we evaluate GAN-based and diffusion-based data, incorporating visual and acoustic modalities. We conduct zero-shot and few-shot evaluations in standard and chain of thought (COT) settings. Additionally, we propose a novel multi-attribute chain of thought (MA-COT) paradigm for describing and judging various task-specific and task-irrelevant attributes of face images. The findings of this study demonstrate that MLLMs exhibit strong potential for addressing the challenges associated with the security of facial recognition technology applications.",
            "corpus_id": "267499747",
            "text": "\n Multimodal large language models (MLLMs) have demonstrated strong capabilities in vision-related tasks, capitalizing on their visual semantic comprehension and reasoning capabilities. However, their ability to detect subtle visual spoofing and forgery clues in face attack detection tasks remains underexplored. In this paper, we introduce a benchmark, SHIELD, to evaluate MLLMs for face spoofing and forgery detection. Specifically, we design true/false and multiple-choice questions to assess MLLM performance on multimodal face data across two tasks. For the face anti-spoofing task, we evaluate three modalities (i.e., RGB, infrared, and depth) under six attack types. For the face forgery detection task, we evaluate GAN-based and diffusion-based data, incorporating visual and acoustic modalities. We conduct zero-shot and few-shot evaluations in standard and chain of thought (COT) settings. Additionally, we propose a novel multi-attribute chain of thought (MA-COT) paradigm for describing and judging various task-specific and task-irrelevant attributes of face images. The findings of this study demonstrate that MLLMs exhibit strong potential for addressing the challenges associated with the security of facial recognition technology applications.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.3525390625
        },
        {
            "paperId": "121aef49e5fbe8d9dc829adaa472a44aff84f4f5",
            "corpusId": 267897557,
            "title": "KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "referenceCount": 76,
            "citationCount": 28,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.15043, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2164113313",
                    "name": "Zhuohao Yu"
                },
                {
                    "authorId": "2287659901",
                    "name": "Chang Gao"
                },
                {
                    "authorId": "2286328804",
                    "name": "Wenjin Yao"
                },
                {
                    "authorId": "2108024279",
                    "name": "Yidong Wang"
                },
                {
                    "authorId": "145235149",
                    "name": "Wei Ye"
                },
                {
                    "authorId": "2273553706",
                    "name": "Jindong Wang"
                },
                {
                    "authorId": "2249681654",
                    "name": "Xing Xie"
                },
                {
                    "authorId": "2250437942",
                    "name": "Yue Zhang"
                },
                {
                    "authorId": "1705434",
                    "name": "Shikun Zhang"
                }
            ],
            "abstract": "Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness. Existing strategies, which aim to detect contaminated texts, focus on quantifying contamination status instead of accurately gauging model performance. In this paper, we introduce KIEval, a Knowledge-grounded Interactive Evaluation framework, which incorporates an LLM-powered\"interactor\"role for the first time to accomplish a dynamic contamination-resilient evaluation. Starting with a question in a conventional LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically generated, multi-round, and knowledge-focused dialogues to determine whether a model's response is merely a recall of benchmark answers or demonstrates a deep comprehension to apply knowledge in more complex conversations. Extensive experiments on seven leading LLMs across five datasets validate KIEval's effectiveness and generalization. We also reveal that data contamination brings no contribution or even negative effect to models' real-world applicability and understanding, and existing contamination detection methods for LLMs can only identify contamination in pre-training but not during supervised fine-tuning.",
            "corpus_id": "267897557",
            "text": "Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness. Existing strategies, which aim to detect contaminated texts, focus on quantifying contamination status instead of accurately gauging model performance. In this paper, we introduce KIEval, a Knowledge-grounded Interactive Evaluation framework, which incorporates an LLM-powered\"interactor\"role for the first time to accomplish a dynamic contamination-resilient evaluation. Starting with a question in a conventional LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically generated, multi-round, and knowledge-focused dialogues to determine whether a model's response is merely a recall of benchmark answers or demonstrates a deep comprehension to apply knowledge in more complex conversations. Extensive experiments on seven leading LLMs across five datasets validate KIEval's effectiveness and generalization. We also reveal that data contamination brings no contribution or even negative effect to models' real-world applicability and understanding, and existing contamination detection methods for LLMs can only identify contamination in pre-training but not during supervised fine-tuning.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.37353515625
        },
        {
            "paperId": "1c36ff1318995ab8af98a3fed44bcfed93e0ecf6",
            "corpusId": 273345553,
            "title": "Safety-Aware Fine-Tuning of Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 47,
            "citationCount": 18,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.10014, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2162122419",
                    "name": "Hyeong Kyu Choi"
                },
                {
                    "authorId": "2246815275",
                    "name": "Xuefeng Du"
                },
                {
                    "authorId": "2300129169",
                    "name": "Yixuan Li"
                }
            ],
            "abstract": "Fine-tuning Large Language Models (LLMs) has emerged as a common practice for tailoring models to individual needs and preferences. The choice of datasets for fine-tuning can be diverse, introducing safety concerns regarding the potential inclusion of harmful data samples. Manually filtering or avoiding such samples, however, can be labor-intensive and subjective. To address these difficulties, we propose a novel Safety-Aware Fine-Tuning (SAFT) framework designed to automatically detect and remove potentially harmful data, by leveraging a scoring function that exploits the subspace information of harmful and benign samples. Experimental results demonstrate the efficacy of SAFT across different LLMs and varying contamination rates, achieving reductions in harmfulness of up to 27.8%. Going beyond, we delve into the mechanism of our approach and validate its versatility in addressing practical challenges in real-world scenarios.",
            "corpus_id": "273345553",
            "text": "Fine-tuning Large Language Models (LLMs) has emerged as a common practice for tailoring models to individual needs and preferences. The choice of datasets for fine-tuning can be diverse, introducing safety concerns regarding the potential inclusion of harmful data samples. Manually filtering or avoiding such samples, however, can be labor-intensive and subjective. To address these difficulties, we propose a novel Safety-Aware Fine-Tuning (SAFT) framework designed to automatically detect and remove potentially harmful data, by leveraging a scoring function that exploits the subspace information of harmful and benign samples. Experimental results demonstrate the efficacy of SAFT across different LLMs and varying contamination rates, achieving reductions in harmfulness of up to 27.8%. Going beyond, we delve into the mechanism of our approach and validate its versatility in addressing practical challenges in real-world scenarios.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.376220703125
        },
        {
            "paperId": "02c4ca0c184cbb81931f8e43ef943a181d5dad84",
            "corpusId": 267897345,
            "title": "Watermarking Makes Language Models Radioactive",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "referenceCount": 71,
            "citationCount": 19,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.14904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283934407",
                    "name": "Tom Sander"
                },
                {
                    "authorId": "2147013351",
                    "name": "Pierre Fernandez"
                },
                {
                    "authorId": "2283933043",
                    "name": "A. Durmus"
                },
                {
                    "authorId": "3271933",
                    "name": "Matthijs Douze"
                },
                {
                    "authorId": "2241541371",
                    "name": "Teddy Furon"
                }
            ],
            "abstract": "We investigate the radioactivity of text generated by large language models (LLM), i.e. whether it is possible to detect that such synthetic input was used to train a subsequent LLM. Current methods like membership inference or active IP protection either work only in settings where the suspected text is known or do not provide reliable statistical guarantees. We discover that, on the contrary, it is possible to reliably determine if a language model was trained on synthetic data if that data is output by a watermarked LLM. Our new methods, specialized for radioactivity, detects with a provable confidence weak residuals of the watermark signal in the fine-tuned LLM. We link the radioactivity contamination level to the following properties: the watermark robustness, its proportion in the training set, and the fine-tuning process. For instance, if the suspect model is open-weight, we demonstrate that training on watermarked instructions can be detected with high confidence ($p$-value $<10^{-5}$) even when as little as $5\\%$ of training text is watermarked.",
            "corpus_id": "267897345",
            "text": "We investigate the radioactivity of text generated by large language models (LLM), i.e. whether it is possible to detect that such synthetic input was used to train a subsequent LLM. Current methods like membership inference or active IP protection either work only in settings where the suspected text is known or do not provide reliable statistical guarantees. We discover that, on the contrary, it is possible to reliably determine if a language model was trained on synthetic data if that data is output by a watermarked LLM. Our new methods, specialized for radioactivity, detects with a provable confidence weak residuals of the watermark signal in the fine-tuned LLM. We link the radioactivity contamination level to the following properties: the watermark robustness, its proportion in the training set, and the fine-tuning process. For instance, if the suspect model is open-weight, we demonstrate that training on watermarked instructions can be detected with high confidence ($p$-value $<10^{-5}$) even when as little as $5\\%$ of training text is watermarked.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.413330078125
        },
        {
            "paperId": "d2aa9772a914b2fd7f193182f44c25b1e1a96107",
            "corpusId": 258328237,
            "title": "LMs go Phishing: Adapting Pre-trained Language Models to Detect Phishing Emails",
            "venue": "2022 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)",
            "year": 2022,
            "referenceCount": 38,
            "citationCount": 8,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/WI-IAT55865.2022.00028?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/WI-IAT55865.2022.00028, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145274478",
                    "name": "Kanishka Misra"
                },
                {
                    "authorId": "10681993",
                    "name": "J. Rayz"
                }
            ],
            "abstract": "Despite decades of research, the problem of Phishing in everyday email communication is ever so prevalent. Traditionally viewed as a text-classification task, the task of phishing detection is an active defense against phishing attempts. Mean-while, progress in natural language processing has established the universal usefulness of adapting pre-trained language models to perform downstream tasks, in a paradigm known as pre-train-then-fine-tune. In this work, we build on this paradigm, and propose two language models that are adapted on 725k emails containing phishing and legitimate messages. We use these two models in two ways: 1) by performing classification-based fine-tuning, and 2) by developing a simple priming-based approach. Our approaches achieve empirical gains over a good deal of prior work, achieving near perfect performance on in-domain data, and relative improvements on out-of-domain emails.",
            "corpus_id": "258328237",
            "text": "Despite decades of research, the problem of Phishing in everyday email communication is ever so prevalent. Traditionally viewed as a text-classification task, the task of phishing detection is an active defense against phishing attempts. Mean-while, progress in natural language processing has established the universal usefulness of adapting pre-trained language models to perform downstream tasks, in a paradigm known as pre-train-then-fine-tune. In this work, we build on this paradigm, and propose two language models that are adapted on 725k emails containing phishing and legitimate messages. We use these two models in two ways: 1) by performing classification-based fine-tuning, and 2) by developing a simple priming-based approach. Our approaches achieve empirical gains over a good deal of prior work, achieving near perfect performance on in-domain data, and relative improvements on out-of-domain emails.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.5556640625
        },
        {
            "paperId": "42c442552a575abae8a39cfe431264a786dc2f53",
            "corpusId": 269982474,
            "title": "Your Large Language Models Are Leaving Fingerprints",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 25,
            "citationCount": 14,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.14057, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2115144895",
                    "name": "Hope McGovern"
                },
                {
                    "authorId": "2040711244",
                    "name": "Rickard Stureborg"
                },
                {
                    "authorId": "2283136281",
                    "name": "Yoshi Suhara"
                },
                {
                    "authorId": "71152801",
                    "name": "Dimitris Alikaniotis"
                }
            ],
            "abstract": "It has been shown that finetuned transformers and other supervised detectors effectively distinguish between human and machine-generated text in some situations arXiv:2305.13242, but we find that even simple classifiers on top of n-gram and part-of-speech features can achieve very robust performance on both in- and out-of-domain data. To understand how this is possible, we analyze machine-generated output text in five datasets, finding that LLMs possess unique fingerprints that manifest as slight differences in the frequency of certain lexical and morphosyntactic features. We show how to visualize such fingerprints, describe how they can be used to detect machine-generated text and find that they are even robust across textual domains. We find that fingerprints are often persistent across models in the same model family (e.g. llama-13b vs. llama-65b) and that models fine-tuned for chat are easier to detect than standard language models, indicating that LLM fingerprints may be directly induced by the training data.",
            "corpus_id": "269982474",
            "text": "It has been shown that finetuned transformers and other supervised detectors effectively distinguish between human and machine-generated text in some situations arXiv:2305.13242, but we find that even simple classifiers on top of n-gram and part-of-speech features can achieve very robust performance on both in- and out-of-domain data. To understand how this is possible, we analyze machine-generated output text in five datasets, finding that LLMs possess unique fingerprints that manifest as slight differences in the frequency of certain lexical and morphosyntactic features. We show how to visualize such fingerprints, describe how they can be used to detect machine-generated text and find that they are even robust across textual domains. We find that fingerprints are often persistent across models in the same model family (e.g. llama-13b vs. llama-65b) and that models fine-tuned for chat are easier to detect than standard language models, indicating that LLM fingerprints may be directly induced by the training data.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.501953125
        }
    ],
    "quotes": {
        "cost": 0.113898,
        "quotes": [
            {
                "idx": 0,
                "key": "[258328237 | Misra et al. | 2022 | Citations: 8]",
                "snippets": "Mean-while, progress in natural language processing has established the universal usefulness of adapting pre-trained language models to perform downstream tasks, in a paradigm known as pre-train-then-fine-tune. In this work, we build on this paradigm, and propose two language models that are adapted on 725k emails containing phishing and legitimate messages. We use these two models in two ways: 1) by performing classification-based fine-tuning, and 2) by developing a simple priming-based approach.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Mean-while, progress in natural language processing has established the universal usefulness of adapting pre-trained language models to perform downstream tasks, in a paradigm known as pre-train-then-fine-tune. In this work, we build on this paradigm, and propose two language models that are adapted on 725k emails containing phishing and legitimate messages. We use these two models in two ways: 1) by performing classification-based fine-tuning, and 2) by developing a simple priming-based approach.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[258947640 | Deng et al. | 2023 | Citations: 20]",
                "snippets": "Large language models. LLMs (Radford et al., 2019)(Brown et al., 2020)Chowdhery et al., 2022;Zhang et al., 2022;OpenAI, 2022) have revolutionized the field of natural language processing by offering several advantages over previous pre-trained models (Devlin et al., 2018;Liu et al., 2019;Lan et al., 2019), including a better characterization of complex patterns and dependencies in the text, and the appealing in-context learning ability for solving downstream tasks with minimal examples. Representative models such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), and Chat-GPT (OpenAI, 2022) have showcased their remarkable ability to generate text with high coherence, fluency, and semantic relevance. They can even effectively address complex inquiries related to science, mathematics, history, current events, and social trends. Therefore, it is increasingly important to effectively regulate the use of LLMs to prevent significant social issues. \n\nLLM-generated text detection. Previous methods can be broadly categorized into two groups. The first group of methods performs detection in a zero-shot manner (Solaiman et al., 2019;(Gehrmann et al., 2019)Mitchell et al., 2023;Yang et al., 2023), but they require access to the source model that generates the texts to derive quantities like output logits or losses for detection. For instance, Solaiman et al. (2019) suggest that a higher log probability for each token indicates that the text will likely be machine-generated. When the output logits/losses of the source model are unavailable, these methods rely on a proxy model for detection. However, there is often a substantial gap between the proxy and source models from which the text is generated. Another group of methods trains DNN-based classifiers on col-lected human-written and machine-generated texts for detection (Guo et al., 2023;Uchendu et al., 2020;Ope-nAI, 2023b).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[182952848 | Gehrmann et al. | 2019 | Citations: 546]": "The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by non-experts. In this work, we introduce GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across multiple sampling schemes. In a human-subjects study, we show that the annotation scheme provided by GLTR improves the human detection-rate of fake text from 54% to 72% without any prior training. GLTR is open-source and publicly deployed, and has already been widely used to detect generated outputs.",
                    "[218971783 | Brown et al. | 2020 | Citations: 42437]": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
                },
                "metadata": [
                    {
                        "section_title": "Related Works",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1906,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 22
                            },
                            {
                                "start": 23,
                                "end": 490
                            },
                            {
                                "start": 491,
                                "end": 718
                            },
                            {
                                "start": 719,
                                "end": 847
                            },
                            {
                                "start": 848,
                                "end": 965
                            },
                            {
                                "start": 968,
                                "end": 997
                            },
                            {
                                "start": 998,
                                "end": 1058
                            },
                            {
                                "start": 1059,
                                "end": 1348
                            },
                            {
                                "start": 1349,
                                "end": 1496
                            },
                            {
                                "start": 1497,
                                "end": 1614
                            },
                            {
                                "start": 1615,
                                "end": 1726
                            },
                            {
                                "start": 1727,
                                "end": 1906
                            }
                        ],
                        "ref_mentions": [
                            "160025533",
                            "218971783",
                            "218971783",
                            "182952848"
                        ],
                        "quote": "Large language models. LLMs (Radford et al., 2019)(Brown et al., 2020)Chowdhery et al., 2022;Zhang et al., 2022;OpenAI, 2022) have revolutionized the field of natural language processing by offering several advantages over previous pre-trained models (Devlin et al., 2018;Liu et al., 2019;Lan et al., 2019), including a better characterization of complex patterns and dependencies in the text, and the appealing in-context learning ability for solving downstream tasks with minimal examples. Representative models such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), and Chat-GPT (OpenAI, 2022) have showcased their remarkable ability to generate text with high coherence, fluency, and semantic relevance. They can even effectively address complex inquiries related to science, mathematics, history, current events, and social trends. Therefore, it is increasingly important to effectively regulate the use of LLMs to prevent significant social issues. \n\nLLM-generated text detection. Previous methods can be broadly categorized into two groups. The first group of methods performs detection in a zero-shot manner (Solaiman et al., 2019;(Gehrmann et al., 2019)Mitchell et al., 2023;Yang et al., 2023), but they require access to the source model that generates the texts to derive quantities like output logits or losses for detection. For instance, Solaiman et al. (2019) suggest that a higher log probability for each token indicates that the text will likely be machine-generated. When the output logits/losses of the source model are unavailable, these methods rely on a proxy model for detection. However, there is often a substantial gap between the proxy and source models from which the text is generated. Another group of methods trains DNN-based classifiers on col-lected human-written and machine-generated texts for detection (Guo et al., 2023;Uchendu et al., 2020;Ope-nAI, 2023b)."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[259095643 | Kirchenbauer et al. | 2023 | Citations: 123]",
                "snippets": "As LLMs become commonplace, machine-generated text has the potential to flood the internet with spam, social media bots, and valueless content. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "As LLMs become commonplace, machine-generated text has the potential to flood the internet with spam, social media bots, and valueless content. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[259376532 | Plaza-del-Arco et al. | 2023 | Citations: 57]",
                "snippets": "Hate speech detection faces two significant challenges: 1) the limited availability of labeled data and 2) the high variability of hate speech across different contexts and languages. Prompting brings a ray of hope to these challenges. It allows injecting a model with task-specific knowledge without relying on labeled data. This paper explores zero-shot learning with prompting for hate speech detection. We investigate how well zero-shot learning can detect hate speech in 3 languages with limited labeled data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Hate speech detection faces two significant challenges: 1) the limited availability of labeled data and 2) the high variability of hate speech across different contexts and languages. Prompting brings a ray of hope to these challenges. It allows injecting a model with task-specific knowledge without relying on labeled data. This paper explores zero-shot learning with prompting for hate speech detection. We investigate how well zero-shot learning can detect hate speech in 3 languages with limited labeled data.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[262465111 | Antoun et al. | 2023 | Citations: 13]",
                "snippets": "A comprehensive investigation into crossmodel detection, evaluating the classifier's ability to detect text generated by different LLMs, and in model attribution, encompassing a broad range of sizes and model families.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 913,
                        "end": 1131,
                        "sentence_offsets": [
                            {
                                "start": 911,
                                "end": 1131
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "A comprehensive investigation into crossmodel detection, evaluating the classifier's ability to detect text generated by different LLMs, and in model attribution, encompassing a broad range of sizes and model families."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[264451585 | Shi et al. | 2023 | Citations: 201]",
                "snippets": "Our first step towards addressing these challenges is to establish a reliable benchmark. We introduce WIKIMIA, a dynamic benchmark designed to periodically and automatically evaluate detection methods on any newly released pretrained LLMs. By leveraging the Wikipedia data timestamp and the model release date, we select old Wikipedia event data as our member data (i.e, seen data during pretraining) and recent Wikipedia event data (e.g., after 2023) as our non-member data (unseen). Our datasets thus exhibit three desirable properties: (1) Accurate: events that occur after LLM pretraining are guaranteed not to be present in the pretraining data. The temporal nature of events ensures that non-member data is indeed unseen and not mentioned in the pretraining data. (2) General: our benchmark is not confined to any specific model and can be applied to various models pretrained using Wikipedia (e.g., OPT, LLaMA, GPT-Neo) since Wikipedia is a commonly used pretraining data source. (3) Dynamic: we will continually update our benchmark by gathering newer non-member data (i.e., more recent events) from Wikipedia since our data construction pipeline is fully automated.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 852,
                        "end": 2026,
                        "sentence_offsets": [
                            {
                                "start": 852,
                                "end": 940
                            },
                            {
                                "start": 941,
                                "end": 1091
                            },
                            {
                                "start": 1092,
                                "end": 1222
                            },
                            {
                                "start": 1223,
                                "end": 1336
                            },
                            {
                                "start": 1337,
                                "end": 1502
                            },
                            {
                                "start": 1503,
                                "end": 1621
                            },
                            {
                                "start": 1622,
                                "end": 1838
                            },
                            {
                                "start": 1839,
                                "end": 2026
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Our first step towards addressing these challenges is to establish a reliable benchmark. We introduce WIKIMIA, a dynamic benchmark designed to periodically and automatically evaluate detection methods on any newly released pretrained LLMs. By leveraging the Wikipedia data timestamp and the model release date, we select old Wikipedia event data as our member data (i.e, seen data during pretraining) and recent Wikipedia event data (e.g., after 2023) as our non-member data (unseen). Our datasets thus exhibit three desirable properties: (1) Accurate: events that occur after LLM pretraining are guaranteed not to be present in the pretraining data. The temporal nature of events ensures that non-member data is indeed unseen and not mentioned in the pretraining data. (2) General: our benchmark is not confined to any specific model and can be applied to various models pretrained using Wikipedia (e.g., OPT, LLaMA, GPT-Neo) since Wikipedia is a commonly used pretraining data source. (3) Dynamic: we will continually update our benchmark by gathering newer non-member data (i.e., more recent events) from Wikipedia since our data construction pipeline is fully automated."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[265128736 | Golchin et al. | 2023 | Citations: 26]",
                "snippets": "We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions, devising a quiz format wherein three perturbed versions of each instance, subsampled from a specific dataset partition, are created. These changes only include word-level perturbations. The generated perturbations, along with the original dataset instance, form the options in the DCQ, with an extra option accommodating the selection of none of the provided options. Given that the only distinguishing signal among the options is the exact wording with respect to the original dataset instance, an LLM, when tasked with identifying the original dataset instance, gravitates towards selecting the original one if it has been exposed to it.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions, devising a quiz format wherein three perturbed versions of each instance, subsampled from a specific dataset partition, are created. These changes only include word-level perturbations. The generated perturbations, along with the original dataset instance, form the options in the DCQ, with an extra option accommodating the selection of none of the provided options. Given that the only distinguishing signal among the options is the exact wording with respect to the original dataset instance, an LLM, when tasked with identifying the original dataset instance, gravitates towards selecting the original one if it has been exposed to it.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[265351715 | Pollano et al. | 2023 | Citations: 1]",
                "snippets": "In this paper, we focus on OOD detection for textual inputs to safeguard ML models that perform natural language processing (NLP) tasks. For example, a sentiment classification model trained on formal restaurant reviews may not produce valid results when applied to informal posts from social media. Determining that an input is OOD requires a way to measure the distance between an input and the in-distribution data. This in turn requires a method to convert textual data into an embedding space in which we can measure distance. One approach to this is to input the text to a transformer-based language model, such as BERT (Devlin et al., 2019), to extract an embedding vector for the input text (e.g., the hidden representation of the special [] token). We can then measure the distance of the embedding vector for an input text to the nearest (or k-nearest) embedding vector of a text from an in-distribution validation set. When this distance is beyond some threshold (which needs to be calibrated for the application), the input text is flagged as out of distribution. The internal state of transformer-based language models contains important information, which may be able to offer richer representations than only using the embedding obtained from the last or penultimate layer. For example, Azaria and Mitchell [3] demonstrated that it is possible to train a classifier on the activation values of the hidden layers of large language models to predict when they are generating false information rather than true information.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[52967399 | Devlin et al. | 2019 | Citations: 95215]": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 731,
                        "end": 2248,
                        "sentence_offsets": [
                            {
                                "start": 731,
                                "end": 867
                            },
                            {
                                "start": 868,
                                "end": 1030
                            },
                            {
                                "start": 1031,
                                "end": 1149
                            },
                            {
                                "start": 1150,
                                "end": 1262
                            },
                            {
                                "start": 1263,
                                "end": 1470
                            },
                            {
                                "start": 1471,
                                "end": 1642
                            },
                            {
                                "start": 1643,
                                "end": 1788
                            },
                            {
                                "start": 1789,
                                "end": 2001
                            },
                            {
                                "start": 2002,
                                "end": 2248
                            }
                        ],
                        "ref_mentions": [
                            "52967399"
                        ],
                        "quote": "In this paper, we focus on OOD detection for textual inputs to safeguard ML models that perform natural language processing (NLP) tasks. For example, a sentiment classification model trained on formal restaurant reviews may not produce valid results when applied to informal posts from social media. Determining that an input is OOD requires a way to measure the distance between an input and the in-distribution data. This in turn requires a method to convert textual data into an embedding space in which we can measure distance. One approach to this is to input the text to a transformer-based language model, such as BERT (Devlin et al., 2019), to extract an embedding vector for the input text (e.g., the hidden representation of the special [] token). We can then measure the distance of the embedding vector for an input text to the nearest (or k-nearest) embedding vector of a text from an in-distribution validation set. When this distance is beyond some threshold (which needs to be calibrated for the application), the input text is flagged as out of distribution. The internal state of transformer-based language models contains important information, which may be able to offer richer representations than only using the embedding obtained from the last or penultimate layer. For example, Azaria and Mitchell [3] demonstrated that it is possible to train a classifier on the activation values of the hidden layers of large language models to predict when they are generating false information rather than true information."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[266176903 | Li et al. | 2023 | Citations: 26]",
                "snippets": "We create a novel pragmatic taxonomy for AD sign and symptom progression based on expert knowledge and generated three datasets: (1) a gold dataset annotated by human experts on longitudinal EHRs of AD patients; (2) a silver dataset created by the data-to-label method, which labels sentences from a public EHR collection with AD-related signs and symptoms; and (3) a bronze dataset created by the label-to-data method which generates sentences with AD-related signs and symptoms based on the label definition.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We create a novel pragmatic taxonomy for AD sign and symptom progression based on expert knowledge and generated three datasets: (1) a gold dataset annotated by human experts on longitudinal EHRs of AD patients; (2) a silver dataset created by the data-to-label method, which labels sentences from a public EHR collection with AD-related signs and symptoms; and (3) a bronze dataset created by the label-to-data method which generates sentences with AD-related signs and symptoms based on the label definition.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[267095281 | Mao et al. | 2024 | Citations: 31]",
                "snippets": "In the out-of-distribution experiment, we train the detector on data from two language models, assuming it is unaware that the test text will be generated from the third model. Despite a performance drop on detecting the out-of-distribution test data generated from the third model, our method remains effective in detecting content from this unseen model, underscoring our approach's robustness and adaptability, with up to 91 points on F1 score. The number shows the number of data, reflecting by the size of the dot.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "LLM for",
                        "pdf_hash": "",
                        "start": 223,
                        "end": 742,
                        "sentence_offsets": [
                            {
                                "start": 223,
                                "end": 399
                            },
                            {
                                "start": 400,
                                "end": 670
                            },
                            {
                                "start": 671,
                                "end": 742
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In the out-of-distribution experiment, we train the detector on data from two language models, assuming it is unaware that the test text will be generated from the third model. Despite a performance drop on detecting the out-of-distribution test data generated from the third model, our method remains effective in detecting content from this unseen model, underscoring our approach's robustness and adaptability, with up to 91 points on F1 score. The number shows the number of data, reflecting by the size of the dot."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[268041863 | Li et al. | 2024 | Citations: 19]",
                "snippets": "Motivated by the domain knowledge and programming capabilities of large language models (LMs), we introduce a method for language model driven automated statistical model discovery. We cast our automated procedure within the principled framework of Box's Loop: the LM iterates between proposing statistical models represented as probabilistic programs, acting as a modeler, and critiquing those models, acting as a domain expert.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Motivated by the domain knowledge and programming capabilities of large language models (LMs), we introduce a method for language model driven automated statistical model discovery. We cast our automated procedure within the principled framework of Box's Loop: the LM iterates between proposing statistical models represented as probabilistic programs, acting as a modeler, and critiquing those models, acting as a domain expert.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[268889777 | Zhang et al. | 2024 | Citations: 54]",
                "snippets": "The problem of pre-training data detection for large language models (LLMs) has received growing attention due to its implications in critical issues like copyright violation and test data contamination. Despite improved performance, existing methods (including the state-of-the-art, Min-K%) are mostly developed upon simple heuristics and lack solid, reasonable foundations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 375,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "The problem of pre-training data detection for large language models (LLMs) has received growing attention due to its implications in critical issues like copyright violation and test data contamination. Despite improved performance, existing methods (including the state-of-the-art, Min-K%) are mostly developed upon simple heuristics and lack solid, reasonable foundations."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[269757388 | Li et al. | 2024 | Citations: 8]",
                "snippets": "It is an interesting question Can and How Large Language Models (LLMs) understand non-language network data, and help us detect unknown malicious flows. This paper takes Carpet Bombing as a case study and shows how to exploit LLMs' powerful capability in the networking area.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "It is an interesting question Can and How Large Language Models (LLMs) understand non-language network data, and help us detect unknown malicious flows. This paper takes Carpet Bombing as a case study and shows how to exploit LLMs' powerful capability in the networking area.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[269982474 | McGovern et al. | 2024 | Citations: 14]",
                "snippets": "It has been shown that finetuned transformers and other supervised detectors effectively distinguish between human and machine-generated text in some situations arXiv:2305.13242, but we find that even simple classifiers on top of n-gram and part-of-speech features can achieve very robust performance on both in- and out-of-domain data. To understand how this is possible, we analyze machine-generated output text in five datasets, finding that LLMs possess unique fingerprints that manifest as slight differences in the frequency of certain lexical and morphosyntactic features.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "It has been shown that finetuned transformers and other supervised detectors effectively distinguish between human and machine-generated text in some situations arXiv:2305.13242, but we find that even simple classifiers on top of n-gram and part-of-speech features can achieve very robust performance on both in- and out-of-domain data. To understand how this is possible, we analyze machine-generated output text in five datasets, finding that LLMs possess unique fingerprints that manifest as slight differences in the frequency of certain lexical and morphosyntactic features.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[270217411 | Liu et al. | 2024 | Citations: 13]",
                "snippets": "Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the model's internal activations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the model's internal activations.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[271570943 | Zhang et al. | 2024 | Citations: 6]",
                "snippets": "While large language models (LLMs) are extensively used, there are raising concerns regarding privacy, security, and copyright due to their opaque training data, which brings the problem of detecting pre-training data on the table. Current solutions to this problem leverage techniques explored in machine learning privacy such as Membership Inference Attacks (MIAs), which heavily depend on LLMs' capability of verbatim memorization. However, this reliance presents challenges, especially given the vast amount of training data and the restricted number of effective training epochs. In this paper, we propose an adaptive pre-training data detection method which alleviates this reliance and effectively amplify the identification.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "While large language models (LLMs) are extensively used, there are raising concerns regarding privacy, security, and copyright due to their opaque training data, which brings the problem of detecting pre-training data on the table. Current solutions to this problem leverage techniques explored in machine learning privacy such as Membership Inference Attacks (MIAs), which heavily depend on LLMs' capability of verbatim memorization. However, this reliance presents challenges, especially given the vast amount of training data and the restricted number of effective training epochs. In this paper, we propose an adaptive pre-training data detection method which alleviates this reliance and effectively amplify the identification.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[271880412 | Hussain et al. | 2024 | Citations: 25]",
                "snippets": "Measures based on token and token sequence probabilities have thus been used to, for instance, investigate how language models capture grammatical gender (An et al., 2019) and to predict human reading times (e.g., Merkx & Frank, 2020). The present example demonstrates how the log probabilities extracted from GPT-2 can be used to predict teachers' text readability ratings.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Token probability and perplexity extraction",
                        "pdf_hash": "",
                        "start": 663,
                        "end": 1037,
                        "sentence_offsets": [
                            {
                                "start": 663,
                                "end": 898
                            },
                            {
                                "start": 899,
                                "end": 1037
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Measures based on token and token sequence probabilities have thus been used to, for instance, investigate how language models capture grammatical gender (An et al., 2019) and to predict human reading times (e.g., Merkx & Frank, 2020). The present example demonstrates how the log probabilities extracted from GPT-2 can be used to predict teachers' text readability ratings."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[272366479 | Xu et al. | 2024 | Citations: 7]",
                "snippets": "Large Language Models (LLMs) and Multi-Modal LLMs (MLLMs) have transformed Out-of-Distribution (OOD) detection by leveraging pretrained models like CLIP to perform downstream detection tasks. These models are capable of detecting OOD samples in zero-shot or few-shot settings, meaning they can generalize to unseen data with little to no additional training. This represents a shift from traditional OOD detection methods, which typically rely on training classifiers using the entire in-distribution (ID) dataset.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "D.2 LLM-based OOD Detection",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 514,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 191
                            },
                            {
                                "start": 192,
                                "end": 358
                            },
                            {
                                "start": 359,
                                "end": 514
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Large Language Models (LLMs) and Multi-Modal LLMs (MLLMs) have transformed Out-of-Distribution (OOD) detection by leveraging pretrained models like CLIP to perform downstream detection tasks. These models are capable of detecting OOD samples in zero-shot or few-shot settings, meaning they can generalize to unseen data with little to no additional training. This represents a shift from traditional OOD detection methods, which typically rely on training classifiers using the entire in-distribution (ID) dataset."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[272827783 | Zhang et al. | 2024 | Citations: 16]",
                "snippets": "To address this issue, we introduce a divergence-based calibration method, inspired by the divergence-from-randomness concept, to calibrate token probabilities for pretraining data detection. We compute the cross-entropy (i.e., the divergence) between the token probability distribution and the token frequency distribution to derive a detection score.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "To address this issue, we introduce a divergence-based calibration method, inspired by the divergence-from-randomness concept, to calibrate token probabilities for pretraining data detection. We compute the cross-entropy (i.e., the divergence) between the token probability distribution and the token frequency distribution to derive a detection score.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[273549214 | Fu et al. | 2024 | Citations: 8]",
                "snippets": "To bridge this gap, we (1) systematically review 50 papers on data contamination detection for LMs, (2) present the formal, mathematical definitions for different levels of data contamination, (3) categorize the underlying requirements and assumptions associated with each approach and critically assess whether these assumptions have been rigorously validated, and (4) demonstrate through case studies that some unverified assumptions can be wrong in multiple scenarios.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1374,
                        "end": 1845,
                        "sentence_offsets": [
                            {
                                "start": 1374,
                                "end": 1845
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To bridge this gap, we (1) systematically review 50 papers on data contamination detection for LMs, (2) present the formal, mathematical definitions for different levels of data contamination, (3) categorize the underlying requirements and assumptions associated with each approach and critically assess whether these assumptions have been rigorously validated, and (4) demonstrate through case studies that some unverified assumptions can be wrong in multiple scenarios."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[277065772 | Ravichander et al. | 2025 | Citations: 1]",
                "snippets": "In our work, we find that we are able to identify text that is known to even proprietary black-box LLMs, and that the examples of memorized text that were successfully identified can differ between probing methods. This indicates that the community would benefit from a range of such approaches, and that focusing on state-of-the-art detection performance should not be the only goal. Further, recent work has investigating combining signals for various training data identification methods in order to determine if a model was trained on a given document (Longpre et al., 2024). This suggests that developing diverse, complementary, probes can help us better understand how data was used to train models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Analysis",
                        "pdf_hash": "",
                        "start": 238,
                        "end": 941,
                        "sentence_offsets": [
                            {
                                "start": 238,
                                "end": 452
                            },
                            {
                                "start": 453,
                                "end": 622
                            },
                            {
                                "start": 623,
                                "end": 815
                            },
                            {
                                "start": 816,
                                "end": 941
                            }
                        ],
                        "ref_mentions": [
                            "268781196"
                        ],
                        "quote": "In our work, we find that we are able to identify text that is known to even proprietary black-box LLMs, and that the examples of memorized text that were successfully identified can differ between probing methods. This indicates that the community would benefit from a range of such approaches, and that focusing on state-of-the-art detection performance should not be the only goal. Further, recent work has investigating combining signals for various training data identification methods in order to determine if a model was trained on a given document (Longpre et al., 2024). This suggests that developing diverse, complementary, probes can help us better understand how data was used to train models."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.022401,
        "cot": "Based on the user's query \"detect or data from language models related works topic\", I will organize the quotes into dimensions that comprehensively cover the topic of detection methods and data related to language models.\n\nFirst, I need an introductory section to provide background on language models and why detection is important in this context.\n\nThen, I'll organize the content into the following dimensions:\n\n1. \"Introduction to Language Models and Detection\": This will be a synthesis paragraph providing background on language models and the importance of detection methods. This is essential for setting the context.\n\n2. \"Machine-Generated Text Detection Methods\": This dimension will cover approaches to detect text generated by language models. This should be a synthesis since there are various methodologies that need to be compared and explained.\n\n3. \"Pre-training Data Detection and Contamination\": This dimension will focus specifically on detecting whether certain data was used to train a language model, which is a distinct problem from detecting LLM-generated text. This will be a synthesis to explain the concepts and approaches.\n\n4. \"Out-of-Distribution (OOD) Detection\": Several quotes discuss OOD detection in the context of language models, which warrants its own dimension. This will be a synthesis to explain the concept and approaches.\n\n5. \"Data Generation and Annotation Approaches\": This dimension will cover methods for creating datasets for language model research and evaluation. This will be a synthesis to connect the different approaches mentioned.\n\n6. \"Applications of Detection Methods\": This dimension will cover specific use cases and applications of detection methods. This will be a list format since these are distinct applications that can be enumerated.\n\nThe quotes will be assigned to these dimensions based on their content and relevance.",
        "plan": {
            "Introduction to Language Models and Detection (synthesis)": [
                1,
                2
            ],
            "Machine-Generated Text Detection Methods (synthesis)": [
                0,
                1,
                2,
                4,
                13
            ],
            "Pre-training Data Detection and Contamination (synthesis)": [
                5,
                6,
                11,
                14,
                15,
                18,
                19,
                20
            ],
            "Out-of-Distribution (OOD) Detection (synthesis)": [
                7,
                9,
                17
            ],
            "Data Generation and Annotation Approaches (synthesis)": [
                8,
                10
            ],
            "Applications of Detection Methods (list)": [
                3,
                12,
                16
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Language Models and Detection",
                "tldr": "Large language models (LLMs) have transformed natural language processing with their ability to generate highly coherent and contextually relevant text. This capability has raised concerns about potential misuse, leading to the development of methods for detecting machine-generated content. (4 sources)",
                "text": "\nLarge language models (LLMs) have revolutionized natural language processing by offering significant advantages over previous pre-trained models, including better characterization of complex patterns and dependencies in text, along with impressive in-context learning capabilities for solving downstream tasks with minimal examples <Paper corpusId=\"258947640\" paperTitle=\"(Deng et al., 2023)\" isShortName></Paper>. Notable models such as GPT-3, PaLM, and ChatGPT have demonstrated remarkable abilities to generate text with high coherence, fluency, and semantic relevance, even effectively addressing complex inquiries across various domains including science, mathematics, history, and current events <Paper corpusId=\"258947640\" paperTitle=\"(Deng et al., 2023)\" isShortName></Paper> <Paper corpusId=\"218971783\" paperTitle=\"(Brown et al., 2020)\" isShortName></Paper>.\n\nAs these powerful language models become more prevalent, there is increasing concern about the potential misuse of machine-generated text, which could flood the internet with spam, social media bots, and low-value content <Paper corpusId=\"259095643\" paperTitle=\"(Kirchenbauer et al., 2023)\" isShortName></Paper>. The importance of effectively regulating LLM use to prevent significant social issues has become evident <Paper corpusId=\"258947640\" paperTitle=\"(Deng et al., 2023)\" isShortName></Paper>. This has led to the development of various detection methods, which can be broadly categorized into two approaches: zero-shot detection methods that may require access to the source model's output logits or losses, and classifier-based methods that train on collected human-written and machine-generated texts <Paper corpusId=\"258947640\" paperTitle=\"(Deng et al., 2023)\" isShortName></Paper>. Additionally, watermarking has emerged as a simple and effective strategy for enabling the detection and documentation of LLM-generated text <Paper corpusId=\"259095643\" paperTitle=\"(Kirchenbauer et al., 2023)\" isShortName></Paper>.\n\nThe development of tools to support humans in detecting whether text was generated by a model has shown promising results, with one study demonstrating an improvement in human detection rates from 54% to 72% without prior training when using appropriate annotation schemes <Paper corpusId=\"258947640\" paperTitle=\"(Deng et al., 2023)\" isShortName></Paper> <Paper corpusId=\"182952848\" paperTitle=\"(Gehrmann et al., 2019)\" isShortName></Paper>. As LLMs continue to advance, the field of machine-generated text detection remains a critical area of research to mitigate potential harms while harnessing the benefits of these powerful models.",
                "citations": [
                    {
                        "id": "(Deng et al., 2023)",
                        "snippets": [
                            "Large language models. LLMs (Radford et al., 2019)(Brown et al., 2020)Chowdhery et al., 2022;Zhang et al., 2022;OpenAI, 2022) have revolutionized the field of natural language processing by offering several advantages over previous pre-trained models (Devlin et al., 2018;Liu et al., 2019;Lan et al., 2019), including a better characterization of complex patterns and dependencies in the text, and the appealing in-context learning ability for solving downstream tasks with minimal examples. Representative models such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), and Chat-GPT (OpenAI, 2022) have showcased their remarkable ability to generate text with high coherence, fluency, and semantic relevance. They can even effectively address complex inquiries related to science, mathematics, history, current events, and social trends. Therefore, it is increasingly important to effectively regulate the use of LLMs to prevent significant social issues. \n\nLLM-generated text detection. Previous methods can be broadly categorized into two groups. The first group of methods performs detection in a zero-shot manner (Solaiman et al., 2019;(Gehrmann et al., 2019)Mitchell et al., 2023;Yang et al., 2023), but they require access to the source model that generates the texts to derive quantities like output logits or losses for detection. For instance, Solaiman et al. (2019) suggest that a higher log probability for each token indicates that the text will likely be machine-generated. When the output logits/losses of the source model are unavailable, these methods rely on a proxy model for detection. However, there is often a substantial gap between the proxy and source models from which the text is generated. Another group of methods trains DNN-based classifiers on col-lected human-written and machine-generated texts for detection (Guo et al., 2023;Uchendu et al., 2020;Ope-nAI, 2023b)."
                        ],
                        "paper": {
                            "corpus_id": 258947640,
                            "title": "Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model",
                            "authors": [
                                {
                                    "authorId": "145114723",
                                    "name": "Zhijie Deng"
                                },
                                {
                                    "authorId": "2162081759",
                                    "name": "Hongcheng Gao"
                                },
                                {
                                    "authorId": "2188993538",
                                    "name": "Yibo Miao"
                                },
                                {
                                    "authorId": "46702482",
                                    "name": "Hao Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 20
                        },
                        "score": 0.52880859375
                    },
                    {
                        "id": "(Brown et al., 2020)",
                        "snippets": [
                            "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
                        ],
                        "paper": {
                            "corpus_id": 218971783,
                            "title": "Language Models are Few-Shot Learners",
                            "authors": [
                                {
                                    "authorId": "31035595",
                                    "name": "Tom B. Brown"
                                },
                                {
                                    "authorId": "2056658938",
                                    "name": "Benjamin Mann"
                                },
                                {
                                    "authorId": "39849748",
                                    "name": "Nick Ryder"
                                },
                                {
                                    "authorId": "2065894334",
                                    "name": "Melanie Subbiah"
                                },
                                {
                                    "authorId": "152724169",
                                    "name": "J. Kaplan"
                                },
                                {
                                    "authorId": "6515819",
                                    "name": "Prafulla Dhariwal"
                                },
                                {
                                    "authorId": "2072676",
                                    "name": "Arvind Neelakantan"
                                },
                                {
                                    "authorId": "67311962",
                                    "name": "Pranav Shyam"
                                },
                                {
                                    "authorId": "144864359",
                                    "name": "Girish Sastry"
                                },
                                {
                                    "authorId": "119609682",
                                    "name": "Amanda Askell"
                                },
                                {
                                    "authorId": "144517868",
                                    "name": "Sandhini Agarwal"
                                },
                                {
                                    "authorId": "1404060687",
                                    "name": "Ariel Herbert-Voss"
                                },
                                {
                                    "authorId": "2064404342",
                                    "name": "Gretchen Krueger"
                                },
                                {
                                    "authorId": "103143311",
                                    "name": "T. Henighan"
                                },
                                {
                                    "authorId": "48422824",
                                    "name": "R. Child"
                                },
                                {
                                    "authorId": "1992922591",
                                    "name": "A. Ramesh"
                                },
                                {
                                    "authorId": "2052152920",
                                    "name": "Daniel M. Ziegler"
                                },
                                {
                                    "authorId": "49387725",
                                    "name": "Jeff Wu"
                                },
                                {
                                    "authorId": "2059411355",
                                    "name": "Clemens Winter"
                                },
                                {
                                    "authorId": "144239765",
                                    "name": "Christopher Hesse"
                                },
                                {
                                    "authorId": "2108828435",
                                    "name": "Mark Chen"
                                },
                                {
                                    "authorId": "2064673055",
                                    "name": "Eric Sigler"
                                },
                                {
                                    "authorId": "1380985420",
                                    "name": "Ma-teusz Litwin"
                                },
                                {
                                    "authorId": "145565184",
                                    "name": "Scott Gray"
                                },
                                {
                                    "authorId": "1490681878",
                                    "name": "Benjamin Chess"
                                },
                                {
                                    "authorId": "2115193883",
                                    "name": "Jack Clark"
                                },
                                {
                                    "authorId": "133740015",
                                    "name": "Christopher Berner"
                                },
                                {
                                    "authorId": "52238703",
                                    "name": "Sam McCandlish"
                                },
                                {
                                    "authorId": "38909097",
                                    "name": "Alec Radford"
                                },
                                {
                                    "authorId": "1701686",
                                    "name": "I. Sutskever"
                                },
                                {
                                    "authorId": "2698777",
                                    "name": "Dario Amodei"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 42437
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kirchenbauer et al., 2023)",
                        "snippets": [
                            "As LLMs become commonplace, machine-generated text has the potential to flood the internet with spam, social media bots, and valueless content. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text."
                        ],
                        "paper": {
                            "corpus_id": 259095643,
                            "title": "On the Reliability of Watermarks for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2166053502",
                                    "name": "John Kirchenbauer"
                                },
                                {
                                    "authorId": "8284185",
                                    "name": "Jonas Geiping"
                                },
                                {
                                    "authorId": "123191916",
                                    "name": "Yuxin Wen"
                                },
                                {
                                    "authorId": "1643697854",
                                    "name": "Manli Shu"
                                },
                                {
                                    "authorId": "2203810783",
                                    "name": "Khalid Saifullah"
                                },
                                {
                                    "authorId": "80253287",
                                    "name": "Kezhi Kong"
                                },
                                {
                                    "authorId": "94971447",
                                    "name": "Kasun Fernando"
                                },
                                {
                                    "authorId": "2056290221",
                                    "name": "Aniruddha Saha"
                                },
                                {
                                    "authorId": "121592562",
                                    "name": "Micah Goldblum"
                                },
                                {
                                    "authorId": "1962083",
                                    "name": "T. Goldstein"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 123
                        },
                        "score": 0.5712890625
                    },
                    {
                        "id": "(Gehrmann et al., 2019)",
                        "snippets": [
                            "The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by non-experts. In this work, we introduce GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across multiple sampling schemes. In a human-subjects study, we show that the annotation scheme provided by GLTR improves the human detection-rate of fake text from 54% to 72% without any prior training. GLTR is open-source and publicly deployed, and has already been widely used to detect generated outputs."
                        ],
                        "paper": {
                            "corpus_id": 182952848,
                            "title": "GLTR: Statistical Detection and Visualization of Generated Text",
                            "authors": [
                                {
                                    "authorId": "3159346",
                                    "name": "Sebastian Gehrmann"
                                },
                                {
                                    "authorId": "2879705",
                                    "name": "Hendrik Strobelt"
                                },
                                {
                                    "authorId": "2531268",
                                    "name": "Alexander M. Rush"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 546
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Machine-Generated Text Detection Methods",
                "tldr": "Various methods have been developed to detect machine-generated text, ranging from classifier-based approaches to zero-shot detection and watermarking techniques. These detection methods leverage linguistic patterns, statistical features, and model-specific characteristics to differentiate between human and AI-written content. (6 sources)",
                "text": "\nThe challenge of detecting machine-generated text has led to the development of diverse approaches that can be broadly categorized into two main groups. The first group employs zero-shot detection methods that often require access to the source model's output logits or losses <Paper corpusId=\"258947640\" paperTitle=\"(Deng et al., 2023)\" isShortName></Paper>. For instance, some researchers have suggested that higher log probability for each token can indicate machine-generated text. However, when access to the source model is unavailable, these methods must rely on proxy models, which can lead to significant gaps in detection accuracy <Paper corpusId=\"258947640\" paperTitle=\"(Deng et al., 2023)\" isShortName></Paper>.\n\nThe second group consists of classifier-based methods that train on collected human-written and machine-generated texts <Paper corpusId=\"258947640\" paperTitle=\"(Deng et al., 2023)\" isShortName></Paper>. These approaches have shown promising results in distinguishing between human and AI-generated content. Research has demonstrated that even simple classifiers using n-gram and part-of-speech features can achieve robust performance on both in-domain and out-of-domain data <Paper corpusId=\"269982474\" paperTitle=\"(McGovern et al., 2024)\" isShortName></Paper>. This effectiveness stems from LLMs possessing unique \"fingerprints\" that manifest as slight differences in the frequency of certain lexical and morphosyntactic features in their outputs <Paper corpusId=\"269982474\" paperTitle=\"(McGovern et al., 2024)\" isShortName></Paper>.\n\nSome researchers have explored adapting pre-trained language models specifically for detection tasks. For example, Misra et al. developed two language models trained on 725,000 emails containing both phishing and legitimate messages, applying them through classification-based fine-tuning and a priming-based approach <Paper corpusId=\"258328237\" paperTitle=\"(Misra et al., 2022)\" isShortName></Paper>. This pre-train-then-fine-tune paradigm has proven effective for specialized detection scenarios.\n\nComprehensive investigations into cross-model detection have evaluated classifiers' abilities to detect text generated by different LLMs and attribute content to specific models across various sizes and model families <Paper corpusId=\"262465111\" paperTitle=\"(Antoun et al., 2023)\" isShortName></Paper>. These studies are particularly valuable as the landscape of language models continues to expand.\n\nWatermarking has emerged as another important strategy for enabling the detection of LLM-generated text. This simple yet effective approach helps mitigate potential harms by providing a means to document and identify machine-generated content, which is crucial as AI-generated text threatens to flood the internet with spam, social media bots, and low-value content <Paper corpusId=\"259095643\" paperTitle=\"(Kirchenbauer et al., 2023)\" isShortName></Paper>. Tools like GLTR (Giant Language model Test Room) have been developed to support humans in detecting generated text through baseline statistical methods that can identify generation artifacts across multiple sampling schemes <Paper corpusId=\"258947640\" paperTitle=\"(Deng et al., 2023)\" isShortName></Paper> <Paper corpusId=\"182952848\" paperTitle=\"(Gehrmann et al., 2019)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Deng et al., 2023)",
                        "snippets": [
                            "Large language models. LLMs (Radford et al., 2019)(Brown et al., 2020)Chowdhery et al., 2022;Zhang et al., 2022;OpenAI, 2022) have revolutionized the field of natural language processing by offering several advantages over previous pre-trained models (Devlin et al., 2018;Liu et al., 2019;Lan et al., 2019), including a better characterization of complex patterns and dependencies in the text, and the appealing in-context learning ability for solving downstream tasks with minimal examples. Representative models such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), and Chat-GPT (OpenAI, 2022) have showcased their remarkable ability to generate text with high coherence, fluency, and semantic relevance. They can even effectively address complex inquiries related to science, mathematics, history, current events, and social trends. Therefore, it is increasingly important to effectively regulate the use of LLMs to prevent significant social issues. \n\nLLM-generated text detection. Previous methods can be broadly categorized into two groups. The first group of methods performs detection in a zero-shot manner (Solaiman et al., 2019;(Gehrmann et al., 2019)Mitchell et al., 2023;Yang et al., 2023), but they require access to the source model that generates the texts to derive quantities like output logits or losses for detection. For instance, Solaiman et al. (2019) suggest that a higher log probability for each token indicates that the text will likely be machine-generated. When the output logits/losses of the source model are unavailable, these methods rely on a proxy model for detection. However, there is often a substantial gap between the proxy and source models from which the text is generated. Another group of methods trains DNN-based classifiers on col-lected human-written and machine-generated texts for detection (Guo et al., 2023;Uchendu et al., 2020;Ope-nAI, 2023b)."
                        ],
                        "paper": {
                            "corpus_id": 258947640,
                            "title": "Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model",
                            "authors": [
                                {
                                    "authorId": "145114723",
                                    "name": "Zhijie Deng"
                                },
                                {
                                    "authorId": "2162081759",
                                    "name": "Hongcheng Gao"
                                },
                                {
                                    "authorId": "2188993538",
                                    "name": "Yibo Miao"
                                },
                                {
                                    "authorId": "46702482",
                                    "name": "Hao Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 20
                        },
                        "score": 0.52880859375
                    },
                    {
                        "id": "(McGovern et al., 2024)",
                        "snippets": [
                            "It has been shown that finetuned transformers and other supervised detectors effectively distinguish between human and machine-generated text in some situations arXiv:2305.13242, but we find that even simple classifiers on top of n-gram and part-of-speech features can achieve very robust performance on both in- and out-of-domain data. To understand how this is possible, we analyze machine-generated output text in five datasets, finding that LLMs possess unique fingerprints that manifest as slight differences in the frequency of certain lexical and morphosyntactic features."
                        ],
                        "paper": {
                            "corpus_id": 269982474,
                            "title": "Your Large Language Models Are Leaving Fingerprints",
                            "authors": [
                                {
                                    "authorId": "2115144895",
                                    "name": "Hope McGovern"
                                },
                                {
                                    "authorId": "2040711244",
                                    "name": "Rickard Stureborg"
                                },
                                {
                                    "authorId": "2283136281",
                                    "name": "Yoshi Suhara"
                                },
                                {
                                    "authorId": "71152801",
                                    "name": "Dimitris Alikaniotis"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 14
                        },
                        "score": 0.501953125
                    },
                    {
                        "id": "(Misra et al., 2022)",
                        "snippets": [
                            "Mean-while, progress in natural language processing has established the universal usefulness of adapting pre-trained language models to perform downstream tasks, in a paradigm known as pre-train-then-fine-tune. In this work, we build on this paradigm, and propose two language models that are adapted on 725k emails containing phishing and legitimate messages. We use these two models in two ways: 1) by performing classification-based fine-tuning, and 2) by developing a simple priming-based approach."
                        ],
                        "paper": {
                            "corpus_id": 258328237,
                            "title": "LMs go Phishing: Adapting Pre-trained Language Models to Detect Phishing Emails",
                            "authors": [
                                {
                                    "authorId": "145274478",
                                    "name": "Kanishka Misra"
                                },
                                {
                                    "authorId": "10681993",
                                    "name": "J. Rayz"
                                }
                            ],
                            "year": 2022,
                            "venue": "2022 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)",
                            "n_citations": 8
                        },
                        "score": 0.5556640625
                    },
                    {
                        "id": "(Antoun et al., 2023)",
                        "snippets": [
                            "A comprehensive investigation into crossmodel detection, evaluating the classifier's ability to detect text generated by different LLMs, and in model attribution, encompassing a broad range of sizes and model families."
                        ],
                        "paper": {
                            "corpus_id": 262465111,
                            "title": "From Text to Source: Results in Detecting Large Language Model-Generated Content",
                            "authors": [
                                {
                                    "authorId": "51040671",
                                    "name": "Wissam Antoun"
                                },
                                {
                                    "authorId": "68990982",
                                    "name": "Beno\u00eet Sagot"
                                },
                                {
                                    "authorId": "1679170",
                                    "name": "Djam\u00e9 Seddah"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Language Resources and Evaluation",
                            "n_citations": 13
                        },
                        "score": 0.74267578125
                    },
                    {
                        "id": "(Kirchenbauer et al., 2023)",
                        "snippets": [
                            "As LLMs become commonplace, machine-generated text has the potential to flood the internet with spam, social media bots, and valueless content. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text."
                        ],
                        "paper": {
                            "corpus_id": 259095643,
                            "title": "On the Reliability of Watermarks for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2166053502",
                                    "name": "John Kirchenbauer"
                                },
                                {
                                    "authorId": "8284185",
                                    "name": "Jonas Geiping"
                                },
                                {
                                    "authorId": "123191916",
                                    "name": "Yuxin Wen"
                                },
                                {
                                    "authorId": "1643697854",
                                    "name": "Manli Shu"
                                },
                                {
                                    "authorId": "2203810783",
                                    "name": "Khalid Saifullah"
                                },
                                {
                                    "authorId": "80253287",
                                    "name": "Kezhi Kong"
                                },
                                {
                                    "authorId": "94971447",
                                    "name": "Kasun Fernando"
                                },
                                {
                                    "authorId": "2056290221",
                                    "name": "Aniruddha Saha"
                                },
                                {
                                    "authorId": "121592562",
                                    "name": "Micah Goldblum"
                                },
                                {
                                    "authorId": "1962083",
                                    "name": "T. Goldstein"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 123
                        },
                        "score": 0.5712890625
                    },
                    {
                        "id": "(Gehrmann et al., 2019)",
                        "snippets": [
                            "The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by non-experts. In this work, we introduce GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across multiple sampling schemes. In a human-subjects study, we show that the annotation scheme provided by GLTR improves the human detection-rate of fake text from 54% to 72% without any prior training. GLTR is open-source and publicly deployed, and has already been widely used to detect generated outputs."
                        ],
                        "paper": {
                            "corpus_id": 182952848,
                            "title": "GLTR: Statistical Detection and Visualization of Generated Text",
                            "authors": [
                                {
                                    "authorId": "3159346",
                                    "name": "Sebastian Gehrmann"
                                },
                                {
                                    "authorId": "2879705",
                                    "name": "Hendrik Strobelt"
                                },
                                {
                                    "authorId": "2531268",
                                    "name": "Alexander M. Rush"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 546
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Pre-training Data Detection and Contamination",
                "tldr": "Pre-training data detection has emerged as a critical area of research addressing concerns about privacy, copyright, and benchmark contamination in large language models. Researchers have developed various methods to determine whether specific text was part of an LLM's training data, ranging from perplexity-based approaches to more sophisticated techniques that examine model internals. (8 sources)",
                "text": "\nThe growing capabilities of large language models have raised significant concerns about the potential contamination of their pre-training data, leading to increased research on methods to detect whether specific text was included in a model's training corpus. This research area has become particularly important due to implications for copyright violations, privacy issues, and test data contamination <Paper corpusId=\"268889777\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. A comprehensive review of 50 papers on data contamination detection highlights the need for formal definitions and critical assessment of the assumptions underlying different detection approaches <Paper corpusId=\"273549214\" paperTitle=\"(Fu et al., 2024)\" isShortName></Paper>.\n\nTo establish reliable evaluation standards, researchers have developed benchmarks such as WIKIMIA, which leverages the temporal nature of Wikipedia data to create accurate member and non-member datasets for testing detection methods. This dynamic benchmark can be applied to various models trained on Wikipedia and is automatically updated with newer non-member data as events occur <Paper corpusId=\"264451585\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>.\n\nSeveral innovative approaches have been proposed for pre-training data detection. The Data Contamination Quiz (DCQ) frames contamination detection as multiple-choice questions, creating word-level perturbations of dataset instances and observing whether models gravitate toward selecting the original version <Paper corpusId=\"265128736\" paperTitle=\"(Golchin et al., 2023)\" isShortName></Paper>. Another approach utilizes probing techniques to examine model internal activations rather than relying on superficial features like perplexities, which may not be reliable <Paper corpusId=\"270217411\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nTraditional methods have often relied on techniques from Membership Inference Attacks (MIAs), which depend heavily on LLMs' verbatim memorization capabilities. However, this reliance presents challenges given the vast amount of training data and limited effective training epochs <Paper corpusId=\"271570943\" paperTitle=\"(Zhang et al._1, 2024)\" isShortName></Paper>. To address these limitations, researchers have developed adaptive detection methods and divergence-based calibration approaches that compute the cross-entropy between token probability distributions and frequency distributions <Paper corpusId=\"272827783\" paperTitle=\"(Zhang et al._2, 2024)\" isShortName></Paper>.\n\nRecent findings suggest that a combination of different probing methods can successfully identify text known to both open-source and proprietary black-box LLMs. Importantly, different methods may identify different examples of memorized text, indicating that the development of diverse, complementary probes can provide better insights into how data was used during model training <Paper corpusId=\"277065772\" paperTitle=\"(Ravichander et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "The problem of pre-training data detection for large language models (LLMs) has received growing attention due to its implications in critical issues like copyright violation and test data contamination. Despite improved performance, existing methods (including the state-of-the-art, Min-K%) are mostly developed upon simple heuristics and lack solid, reasonable foundations."
                        ],
                        "paper": {
                            "corpus_id": 268889777,
                            "title": "Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2267788653",
                                    "name": "Jingyang Zhang"
                                },
                                {
                                    "authorId": "2156016815",
                                    "name": "Jingwei Sun"
                                },
                                {
                                    "authorId": "2119236605",
                                    "name": "Eric C. Yeats"
                                },
                                {
                                    "authorId": "2294877492",
                                    "name": "Ouyang Yang"
                                },
                                {
                                    "authorId": "2211526996",
                                    "name": "Martin Kuo"
                                },
                                {
                                    "authorId": "2265652686",
                                    "name": "Jianyi Zhang"
                                },
                                {
                                    "authorId": "2297270173",
                                    "name": "Hao k Yang"
                                },
                                {
                                    "authorId": "2294901591",
                                    "name": "Hai Li"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 54
                        },
                        "score": 0.74365234375
                    },
                    {
                        "id": "(Fu et al., 2024)",
                        "snippets": [
                            "To bridge this gap, we (1) systematically review 50 papers on data contamination detection for LMs, (2) present the formal, mathematical definitions for different levels of data contamination, (3) categorize the underlying requirements and assumptions associated with each approach and critically assess whether these assumptions have been rigorously validated, and (4) demonstrate through case studies that some unverified assumptions can be wrong in multiple scenarios."
                        ],
                        "paper": {
                            "corpus_id": 273549214,
                            "title": "Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions",
                            "authors": [
                                {
                                    "authorId": "2294828739",
                                    "name": "Yujuan Fu"
                                },
                                {
                                    "authorId": "2298238369",
                                    "name": "\u00d6zlem Uzuner"
                                },
                                {
                                    "authorId": "1398215463",
                                    "name": "Meliha Yetisgen-Yildiz"
                                },
                                {
                                    "authorId": "2294363357",
                                    "name": "Fei Xia"
                                }
                            ],
                            "year": 2024,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 8
                        },
                        "score": 0.62744140625
                    },
                    {
                        "id": "(Shi et al., 2023)",
                        "snippets": [
                            "Our first step towards addressing these challenges is to establish a reliable benchmark. We introduce WIKIMIA, a dynamic benchmark designed to periodically and automatically evaluate detection methods on any newly released pretrained LLMs. By leveraging the Wikipedia data timestamp and the model release date, we select old Wikipedia event data as our member data (i.e, seen data during pretraining) and recent Wikipedia event data (e.g., after 2023) as our non-member data (unseen). Our datasets thus exhibit three desirable properties: (1) Accurate: events that occur after LLM pretraining are guaranteed not to be present in the pretraining data. The temporal nature of events ensures that non-member data is indeed unseen and not mentioned in the pretraining data. (2) General: our benchmark is not confined to any specific model and can be applied to various models pretrained using Wikipedia (e.g., OPT, LLaMA, GPT-Neo) since Wikipedia is a commonly used pretraining data source. (3) Dynamic: we will continually update our benchmark by gathering newer non-member data (i.e., more recent events) from Wikipedia since our data construction pipeline is fully automated."
                        ],
                        "paper": {
                            "corpus_id": 264451585,
                            "title": "Detecting Pretraining Data from Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2254168373",
                                    "name": "Weijia Shi"
                                },
                                {
                                    "authorId": "2218438150",
                                    "name": "Anirudh Ajith"
                                },
                                {
                                    "authorId": "67284811",
                                    "name": "Mengzhou Xia"
                                },
                                {
                                    "authorId": "108053318",
                                    "name": "Yangsibo Huang"
                                },
                                {
                                    "authorId": "2261780806",
                                    "name": "Daogao Liu"
                                },
                                {
                                    "authorId": "3443287",
                                    "name": "Terra Blevins"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                },
                                {
                                    "authorId": "2137813791",
                                    "name": "Luke S. Zettlemoyer"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 201
                        },
                        "score": 0.6083984375
                    },
                    {
                        "id": "(Golchin et al., 2023)",
                        "snippets": [
                            "We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions, devising a quiz format wherein three perturbed versions of each instance, subsampled from a specific dataset partition, are created. These changes only include word-level perturbations. The generated perturbations, along with the original dataset instance, form the options in the DCQ, with an extra option accommodating the selection of none of the provided options. Given that the only distinguishing signal among the options is the exact wording with respect to the original dataset instance, an LLM, when tasked with identifying the original dataset instance, gravitates towards selecting the original one if it has been exposed to it."
                        ],
                        "paper": {
                            "corpus_id": 265128736,
                            "title": "Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "65754049",
                                    "name": "Shahriar Golchin"
                                },
                                {
                                    "authorId": "1760868",
                                    "name": "M. Surdeanu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 26
                        },
                        "score": 0.5849609375
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the model's internal activations."
                        ],
                        "paper": {
                            "corpus_id": 270217411,
                            "title": "Probing Language Models for Pre-training Data Detection",
                            "authors": [
                                {
                                    "authorId": "2294376388",
                                    "name": "Zhenhua Liu"
                                },
                                {
                                    "authorId": "1914586128",
                                    "name": "Tong Zhu"
                                },
                                {
                                    "authorId": "2186374155",
                                    "name": "Chuanyuan Tan"
                                },
                                {
                                    "authorId": "2304460083",
                                    "name": "Haonan Lu"
                                },
                                {
                                    "authorId": "2330946427",
                                    "name": "Bing Liu"
                                },
                                {
                                    "authorId": "2265943980",
                                    "name": "Wenliang Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 13
                        },
                        "score": 0.72412109375
                    },
                    {
                        "id": "(Zhang et al._1, 2024)",
                        "snippets": [
                            "While large language models (LLMs) are extensively used, there are raising concerns regarding privacy, security, and copyright due to their opaque training data, which brings the problem of detecting pre-training data on the table. Current solutions to this problem leverage techniques explored in machine learning privacy such as Membership Inference Attacks (MIAs), which heavily depend on LLMs' capability of verbatim memorization. However, this reliance presents challenges, especially given the vast amount of training data and the restricted number of effective training epochs. In this paper, we propose an adaptive pre-training data detection method which alleviates this reliance and effectively amplify the identification."
                        ],
                        "paper": {
                            "corpus_id": 271570943,
                            "title": "Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens",
                            "authors": [
                                {
                                    "authorId": "2313922585",
                                    "name": "Anqi Zhang"
                                },
                                {
                                    "authorId": "2314513929",
                                    "name": "Chaofeng Wu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.78466796875
                    },
                    {
                        "id": "(Zhang et al._2, 2024)",
                        "snippets": [
                            "To address this issue, we introduce a divergence-based calibration method, inspired by the divergence-from-randomness concept, to calibrate token probabilities for pretraining data detection. We compute the cross-entropy (i.e., the divergence) between the token probability distribution and the token frequency distribution to derive a detection score."
                        ],
                        "paper": {
                            "corpus_id": 272827783,
                            "title": "Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method",
                            "authors": [
                                {
                                    "authorId": "2322456143",
                                    "name": "Weichao Zhang"
                                },
                                {
                                    "authorId": "2109960367",
                                    "name": "Ruqing Zhang"
                                },
                                {
                                    "authorId": "70414094",
                                    "name": "Jiafeng Guo"
                                },
                                {
                                    "authorId": "2265490493",
                                    "name": "M. D. Rijke"
                                },
                                {
                                    "authorId": "7888704",
                                    "name": "Yixing Fan"
                                },
                                {
                                    "authorId": "2244825947",
                                    "name": "Xueqi Cheng"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 16
                        },
                        "score": 0.59521484375
                    },
                    {
                        "id": "(Ravichander et al., 2025)",
                        "snippets": [
                            "In our work, we find that we are able to identify text that is known to even proprietary black-box LLMs, and that the examples of memorized text that were successfully identified can differ between probing methods. This indicates that the community would benefit from a range of such approaches, and that focusing on state-of-the-art detection performance should not be the only goal. Further, recent work has investigating combining signals for various training data identification methods in order to determine if a model was trained on a given document (Longpre et al., 2024). This suggests that developing diverse, complementary, probes can help us better understand how data was used to train models."
                        ],
                        "paper": {
                            "corpus_id": 277065772,
                            "title": "Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models",
                            "authors": [
                                {
                                    "authorId": "3023068",
                                    "name": "Abhilasha Ravichander"
                                },
                                {
                                    "authorId": "33772445",
                                    "name": "Jillian R. Fisher"
                                },
                                {
                                    "authorId": "122436831",
                                    "name": "Taylor Sorensen"
                                },
                                {
                                    "authorId": "50085131",
                                    "name": "Ximing Lu"
                                },
                                {
                                    "authorId": "2284130273",
                                    "name": "Yuchen Lin"
                                },
                                {
                                    "authorId": "2266838583",
                                    "name": "Maria Antoniak"
                                },
                                {
                                    "authorId": "2254272878",
                                    "name": "Niloofar Mireshghallah"
                                },
                                {
                                    "authorId": "1857797",
                                    "name": "Chandra Bhagavatula"
                                },
                                {
                                    "authorId": "2266363632",
                                    "name": "Yejin Choi"
                                }
                            ],
                            "year": 2025,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 1
                        },
                        "score": 0.62548828125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Out-of-Distribution (OOD) Detection",
                "tldr": "Out-of-distribution detection identifies when input text falls outside a model's training distribution, which is crucial for safe deployment of language models. Methods range from embedding-based distance measurements to leveraging pre-trained models for zero-shot detection, helping ensure reliable model performance on unseen data. (4 sources)",
                "text": "\nOut-of-distribution (OOD) detection has emerged as a critical component in safeguarding natural language processing systems from producing unreliable outputs when confronted with inputs that differ significantly from their training data. This capability is essential for practical applications, as demonstrated in cases where a sentiment classification model trained on formal restaurant reviews might produce invalid results when processing informal social media posts <Paper corpusId=\"265351715\" paperTitle=\"(Pollano et al., 2023)\" isShortName></Paper>.\n\nA common approach to OOD detection involves converting textual inputs into an embedding space where distances can be measured. Researchers have utilized transformer-based language models such as BERT to extract embedding vectors for input text (e.g., the hidden representation of the special token) and then measure the distance between these vectors and those from in-distribution validation sets <Paper corpusId=\"265351715\" paperTitle=\"(Pollano et al., 2023)\" isShortName></Paper> <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>. When this distance exceeds a calibrated threshold, the input text is flagged as out-of-distribution. More sophisticated methods tap into the internal states of transformer-based models, which contain valuable information that can provide richer representations than simply using embeddings from the last or penultimate layers <Paper corpusId=\"265351715\" paperTitle=\"(Pollano et al., 2023)\" isShortName></Paper>.\n\nRecent advancements have leveraged large language models (LLMs) and multi-modal LLMs to transform OOD detection capabilities. These models can perform detection tasks in zero-shot or few-shot settings by utilizing pre-trained models like CLIP, representing a significant shift from traditional methods that typically require training classifiers on entire in-distribution datasets <Paper corpusId=\"272366479\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>. This approach allows for detection of OOD samples with minimal additional training, enabling better generalization to unseen data.\n\nThe robustness of OOD detection methods has been demonstrated in cross-model experiments. In one study, researchers trained a detector on data from two language models and tested it on content generated by a third, previously unseen model. Despite an expected performance drop, the method remained effective in detecting content from the unseen model, achieving F1 scores of up to 91 points <Paper corpusId=\"267095281\" paperTitle=\"(Mao et al., 2024)\" isShortName></Paper>. This finding highlights the adaptability of modern OOD detection approaches and their potential for real-world applications where models may encounter content from various unknown sources.",
                "citations": [
                    {
                        "id": "(Pollano et al., 2023)",
                        "snippets": [
                            "In this paper, we focus on OOD detection for textual inputs to safeguard ML models that perform natural language processing (NLP) tasks. For example, a sentiment classification model trained on formal restaurant reviews may not produce valid results when applied to informal posts from social media. Determining that an input is OOD requires a way to measure the distance between an input and the in-distribution data. This in turn requires a method to convert textual data into an embedding space in which we can measure distance. One approach to this is to input the text to a transformer-based language model, such as BERT (Devlin et al., 2019), to extract an embedding vector for the input text (e.g., the hidden representation of the special [] token). We can then measure the distance of the embedding vector for an input text to the nearest (or k-nearest) embedding vector of a text from an in-distribution validation set. When this distance is beyond some threshold (which needs to be calibrated for the application), the input text is flagged as out of distribution. The internal state of transformer-based language models contains important information, which may be able to offer richer representations than only using the embedding obtained from the last or penultimate layer. For example, Azaria and Mitchell [3] demonstrated that it is possible to train a classifier on the activation values of the hidden layers of large language models to predict when they are generating false information rather than true information."
                        ],
                        "paper": {
                            "corpus_id": 265351715,
                            "title": "Detecting out-of-distribution text using topological features of transformer-based language models",
                            "authors": [
                                {
                                    "authorId": "2267725677",
                                    "name": "Andres Pollano"
                                },
                                {
                                    "authorId": "2267728923",
                                    "name": "Anupam Chaudhuri"
                                },
                                {
                                    "authorId": "2267725822",
                                    "name": "Anj Simmons"
                                }
                            ],
                            "year": 2023,
                            "venue": "AISafety@IJCAI",
                            "n_citations": 1
                        },
                        "score": 0.51904296875
                    },
                    {
                        "id": "(Devlin et al., 2019)",
                        "snippets": [
                            "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                        ],
                        "paper": {
                            "corpus_id": 52967399,
                            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                            "authors": [
                                {
                                    "authorId": "39172707",
                                    "name": "Jacob Devlin"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                },
                                {
                                    "authorId": "2544107",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "3259253",
                                    "name": "Kristina Toutanova"
                                }
                            ],
                            "year": 2019,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 95215
                        },
                        "score": 0
                    },
                    {
                        "id": "(Xu et al., 2024)",
                        "snippets": [
                            "Large Language Models (LLMs) and Multi-Modal LLMs (MLLMs) have transformed Out-of-Distribution (OOD) detection by leveraging pretrained models like CLIP to perform downstream detection tasks. These models are capable of detecting OOD samples in zero-shot or few-shot settings, meaning they can generalize to unseen data with little to no additional training. This represents a shift from traditional OOD detection methods, which typically rely on training classifiers using the entire in-distribution (ID) dataset."
                        ],
                        "paper": {
                            "corpus_id": 272366479,
                            "title": "Large Language Models for Anomaly and Out-of-Distribution Detection: A Survey",
                            "authors": [
                                {
                                    "authorId": "2319451284",
                                    "name": "Ruiyao Xu"
                                },
                                {
                                    "authorId": "2319332607",
                                    "name": "Kaize Ding"
                                }
                            ],
                            "year": 2024,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 7
                        },
                        "score": 0.62109375
                    },
                    {
                        "id": "(Mao et al., 2024)",
                        "snippets": [
                            "In the out-of-distribution experiment, we train the detector on data from two language models, assuming it is unaware that the test text will be generated from the third model. Despite a performance drop on detecting the out-of-distribution test data generated from the third model, our method remains effective in detecting content from this unseen model, underscoring our approach's robustness and adaptability, with up to 91 points on F1 score. The number shows the number of data, reflecting by the size of the dot."
                        ],
                        "paper": {
                            "corpus_id": 267095281,
                            "title": "Raidar: geneRative AI Detection viA Rewriting",
                            "authors": [
                                {
                                    "authorId": "7700460",
                                    "name": "Chengzhi Mao"
                                },
                                {
                                    "authorId": "1856025",
                                    "name": "Carl Vondrick"
                                },
                                {
                                    "authorId": "2281126484",
                                    "name": "Hao Wang"
                                },
                                {
                                    "authorId": "2110694456",
                                    "name": "Junfeng Yang"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 31
                        },
                        "score": 0.51318359375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Data Generation and Annotation Approaches",
                "tldr": "Language models are increasingly being used to generate and annotate data for training specialized models and creating benchmarks. These approaches range from expert-guided data creation to automated statistical model discovery, providing new ways to address data scarcity in specialized domains. (2 sources)",
                "text": "\nThe development of effective language models often requires high-quality labeled data, which can be scarce or expensive to obtain in specialized domains. Researchers have addressed this challenge by developing innovative approaches to data generation and annotation that leverage language models themselves. For instance, Li et al. demonstrated a comprehensive approach to creating datasets for Alzheimer's disease (AD) detection by developing a novel pragmatic taxonomy and generating three complementary datasets with varying levels of human involvement: a gold dataset annotated by human experts on longitudinal electronic health records (EHRs), a silver dataset created through a data-to-label method that automatically labels sentences from public EHR collections, and a bronze dataset produced through a label-to-data method that generates sentences based on predefined label definitions <Paper corpusId=\"266176903\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\nBeyond simple data generation, language models have also been employed for more complex tasks such as statistical model discovery. Li et al. introduced a method for language model-driven automated statistical model discovery that leverages the domain knowledge and programming capabilities of large language models. This approach follows Box's Loop framework, where the language model alternates between proposing statistical models represented as probabilistic programs (acting as a modeler) and critiquing those models (acting as a domain expert), enabling automated discovery of appropriate statistical models for given datasets <Paper corpusId=\"268041863\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. These approaches demonstrate how language models can serve not only as tools for text generation but also as sophisticated aids in data creation, annotation, and model development processes, potentially addressing challenges related to data scarcity and domain expertise requirements.",
                "citations": [
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "We create a novel pragmatic taxonomy for AD sign and symptom progression based on expert knowledge and generated three datasets: (1) a gold dataset annotated by human experts on longitudinal EHRs of AD patients; (2) a silver dataset created by the data-to-label method, which labels sentences from a public EHR collection with AD-related signs and symptoms; and (3) a bronze dataset created by the label-to-data method which generates sentences with AD-related signs and symptoms based on the label definition."
                        ],
                        "paper": {
                            "corpus_id": 266176903,
                            "title": "Two Directions for Clinical Data Generation with Large Language Models: Data-to-Label and Label-to-Data",
                            "authors": [
                                {
                                    "authorId": "2237106751",
                                    "name": "Rumeng Li"
                                },
                                {
                                    "authorId": "2221230794",
                                    "name": "Xun Wang"
                                },
                                {
                                    "authorId": "2273658683",
                                    "name": "Hong Yu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 26
                        },
                        "score": 0.61474609375
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Motivated by the domain knowledge and programming capabilities of large language models (LMs), we introduce a method for language model driven automated statistical model discovery. We cast our automated procedure within the principled framework of Box's Loop: the LM iterates between proposing statistical models represented as probabilistic programs, acting as a modeler, and critiquing those models, acting as a domain expert."
                        ],
                        "paper": {
                            "corpus_id": 268041863,
                            "title": "Automated Statistical Model Discovery with Language Models",
                            "authors": [
                                {
                                    "authorId": "2288065395",
                                    "name": "Michael Y. Li"
                                },
                                {
                                    "authorId": "2287933421",
                                    "name": "Emily B. Fox"
                                },
                                {
                                    "authorId": "2280334415",
                                    "name": "Noah D. Goodman"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 19
                        },
                        "score": 0.52783203125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Applications of Detection Methods",
                "tldr": "Detection methods for language models have been applied across diverse domains including hate speech identification, network security, and educational text assessment. These applications leverage the capabilities of language models to address domain-specific challenges through both zero-shot approaches and specialized techniques. (3 sources)",
                "text": "\n- **Hate Speech Detection**: Researchers have explored zero-shot learning with prompting techniques to address hate speech detection across multiple languages. This approach is particularly valuable given the limited availability of labeled data and the high variability of hate speech across different contexts and languages. By injecting models with task-specific knowledge without relying on labeled data, prompting methods offer promising solutions to these persistent challenges in content moderation. <Paper corpusId=\"259376532\" paperTitle=\"(Plaza-del-Arco et al., 2023)\" isShortName></Paper>\n\n- **Network Security**: Language models have been applied to detect malicious network activity, demonstrating their potential beyond traditional text analysis. For example, researchers have investigated how large language models can understand and analyze non-language network data to identify unknown malicious flows, using Carpet Bombing attacks as a case study. This application showcases how LLMs' capabilities can be leveraged in specialized technical domains like networking security. <Paper corpusId=\"269757388\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>\n\n- **Educational Content Assessment**: Detection methods based on token and token sequence probabilities have been utilized to predict text readability for educational purposes. By extracting log probabilities from models like GPT-2, researchers have created tools that can predict teachers' text readability ratings, helping to identify appropriate educational materials for different learning levels. This application demonstrates how language model detection capabilities can support educational content development and selection. <Paper corpusId=\"271880412\" paperTitle=\"(Hussain et al., 2024)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Plaza-del-Arco et al., 2023)",
                        "snippets": [
                            "Hate speech detection faces two significant challenges: 1) the limited availability of labeled data and 2) the high variability of hate speech across different contexts and languages. Prompting brings a ray of hope to these challenges. It allows injecting a model with task-specific knowledge without relying on labeled data. This paper explores zero-shot learning with prompting for hate speech detection. We investigate how well zero-shot learning can detect hate speech in 3 languages with limited labeled data."
                        ],
                        "paper": {
                            "corpus_id": 259376532,
                            "title": "Respectful or Toxic? Using Zero-Shot Learning with Language Models to Detect Hate Speech",
                            "authors": [
                                {
                                    "authorId": "1410406981",
                                    "name": "F. Plaza-del-Arco"
                                },
                                {
                                    "authorId": "2101317501",
                                    "name": "Debora Nozza"
                                },
                                {
                                    "authorId": "2022288",
                                    "name": "Dirk Hovy"
                                }
                            ],
                            "year": 2023,
                            "venue": "WOAH",
                            "n_citations": 57
                        },
                        "score": 0.54541015625
                    },
                    {
                        "id": "(Li et al._1, 2024)",
                        "snippets": [
                            "It is an interesting question Can and How Large Language Models (LLMs) understand non-language network data, and help us detect unknown malicious flows. This paper takes Carpet Bombing as a case study and shows how to exploit LLMs' powerful capability in the networking area."
                        ],
                        "paper": {
                            "corpus_id": 269757388,
                            "title": "DoLLM: How Large Language Models Understanding Network Flow Data to Detect Carpet Bombing DDoS",
                            "authors": [
                                {
                                    "authorId": "2301171718",
                                    "name": "Qingyang Li"
                                },
                                {
                                    "authorId": "2257101405",
                                    "name": "Yihang Zhang"
                                },
                                {
                                    "authorId": "2301200359",
                                    "name": "Zhidong Jia"
                                },
                                {
                                    "authorId": "2216800907",
                                    "name": "Yannan Hu"
                                },
                                {
                                    "authorId": "2301176195",
                                    "name": "Lei Zhang"
                                },
                                {
                                    "authorId": "2301169914",
                                    "name": "Jianrong Zhang"
                                },
                                {
                                    "authorId": "2301199102",
                                    "name": "Yongming Xu"
                                },
                                {
                                    "authorId": "2301175274",
                                    "name": "Yong Cui"
                                },
                                {
                                    "authorId": "2112582246",
                                    "name": "Zongming Guo"
                                },
                                {
                                    "authorId": "1714130",
                                    "name": "Xinggong Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.51123046875
                    },
                    {
                        "id": "(Hussain et al., 2024)",
                        "snippets": [
                            "Measures based on token and token sequence probabilities have thus been used to, for instance, investigate how language models capture grammatical gender (An et al., 2019) and to predict human reading times (e.g., Merkx & Frank, 2020). The present example demonstrates how the log probabilities extracted from GPT-2 can be used to predict teachers' text readability ratings."
                        ],
                        "paper": {
                            "corpus_id": 271880412,
                            "title": "A tutorial on open-source large language models for behavioral science",
                            "authors": [
                                {
                                    "authorId": "2302450822",
                                    "name": "Zak Hussain"
                                },
                                {
                                    "authorId": "32354733",
                                    "name": "Marcel Binz"
                                },
                                {
                                    "authorId": "2184033930",
                                    "name": "Rui Mata"
                                },
                                {
                                    "authorId": "2302732683",
                                    "name": "Dirk U. Wulff"
                                }
                            ],
                            "year": 2024,
                            "venue": "Behavior Research Methods",
                            "n_citations": 25
                        },
                        "score": 0.541015625
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.11320500000000001
    }
}